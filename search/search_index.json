{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"TabTune - An Advanced Library for Tabular Model Training and Adaptation \u00b6 Overview \u00b6 TabTune is a powerful and flexible Python library designed to simplify the training and fine-tuning of modern foundation models on tabular data. It provides a high-level, scikit-learn-compatible API that abstracts away the complexities of data preprocessing, model-specific training loops, and benchmarking, letting you focus on delivering results. Whether you are a practitioner aiming for production-grade pipelines or a researcher exploring advanced architectures, TabTune streamlines your workflow for tabular deep learning. \ud83c\udd95 What's New in TabTune \u00b6 Major Features and Updates: PEFT (LoRA) Support: Parameter-efficient fine-tuning using LoRA adapters for most models. Enhanced Tuning Strategies: Easily switch between inference , base-ft , and peft strategies. TabularLeaderboard: Effortless model comparison and benchmarking on your custom splits. New Models Added: TabBiaxial and TabDPT with full PEFT support. PEFT Limitations: Experimental LoRA support for TabPFN and ContextTab. Comprehensive Testing: Robust test suite covering all models and strategies. Rich Documentation: Extensive examples and how-to guides. Performance Optimizations: Improved memory footprint and training throughput. \u2b50 Core Features \u00b6 Unified API: Single interface for model training, inference, and evaluation across multiple tabular model families. Automated Preprocessing: Model-aware data processing for feature scaling, encoding, imputation, and transformation. Flexible Fine-Tuning: Choose between zero-shot inference, full fine-tuning, or memory-efficient PEFT strategies. Model Comparison: Built-in leaderboard for systematic benchmarking and strategy evaluation. Extensible Design: Modular codebase for easy integration of custom data processors and models. \ud83d\udce6 Supported Models \u00b6 Model Family / Paradigm Key Innovation PEFT Support TabPFN-v2 PFN / ICL Bayesian approximation on synthetic \u26a0\ufe0f Experimental TabICL Scalable ICL Two-stage column-row attention \u2705 Full Support TabBiaxial Scalable ICL Custom Biaxial Attention \u2705 Full Support Mitra Scalable ICL 2D attention, synthetic priors \u2705 Full Support ContextTab Semantics-Aware ICL Modality-specific embeddings \u26a0\ufe0f Experimental TabDPT Denoising Transformer Denoising pre-trained transformer \u2705 Full Support \u2705 Full Support: Reliable LoRA integration \u26a0\ufe0f Experimental: Known issues may occur; use base-ft if unstable \u26a1 Quick Start \u00b6 import pandas as pd from sklearn.model_selection import train_test_split import openml from TabularPipeline.pipeline import TabularPipeline # Load dataset dataset = openml . datasets . get_dataset ( 42178 ) X , y , _ , _ = dataset . get_data ( target = dataset . default_target_attribute ) X_train , X_test , y_train , y_test = train_test_split ( X , y , test_size = 0.25 , random_state = 42 ) # Init and fit pipeline pipeline = TabularPipeline ( model_name = \"TabPFN\" , task_type = \"classification\" , tuning_strategy = \"base-ft\" , tuning_params = { \"device\" : \"cpu\" } ) pipeline . fit ( X_train , y_train ) # Save and load pipeline for prediction pipeline . save ( \"churn_pipeline.joblib\" ) loaded_pipeline = TabularPipeline . load ( \"churn_pipeline.joblib\" ) predictions = loaded_pipeline . predict ( X_test ) metrics = pipeline . evaluate ( X_test , y_test ) print ( metrics ) \ud83d\udcdd Why TabTune? \u00b6 No Boilerplate: Avoids repetitive code for model-specific data loading, training, and inference. Consistent Results: Automates best practices for tabular DL research and model selection. Fast Iteration: Easily compare new models with your data, using the same consistent API. Production Ready: Model and config serialization for robust deployment and reproducibility. Community-Driven: Extensible design and open contribution policy. \ud83d\udcd1 Explore the Documentation \u00b6 Getting Started : Installation, setup, and basic usage. User Guide : In-depth tutorials for each component. Supported Models : Model details and design notes. Advanced Topics : PEFT/LoRA, custom preprocessing, and more. API Reference : Complete Python API and class/method details. Testing & Validation : Test guidelines and reliability promises. Examples & Benchmarks : End-to-end code notebooks. \ud83c\udfc6 Example Notebooks \u00b6 Name Task Type Colab Link TabPFN Inference Inference & Fine-Tune Open In Colab Mitra Inference/FineTune Inference & Fine-Tune Open In Colab \ud83d\udcc2 Project Structure \u00b6 TabTune/ \u251c\u2500\u2500 Dataprocess/ \u251c\u2500\u2500 models/ \u251c\u2500\u2500 TabularPipeline/ \u251c\u2500\u2500 TuningManager/ \u251c\u2500\u2500 TabularLeaderboard/ See User Guide for a full file/module breakdown. \ud83d\uddc3\ufe0f License \u00b6 This project is released under the MIT License. Please cite appropriately if used in academic or production projects. \ud83d\udceb Join Community / Contribute \u00b6 Issues and discussions are welcomed on the GitHub issue tracker . Please see the Contributing section for contribution standards, code reviews, and documentation tips. Get started with TabTune and accelerate your tabular deep learning workflows today!","title":"Home"},{"location":"#tabtune-an-advanced-library-for-tabular-model-training-and-adaptation","text":"","title":"TabTune - An Advanced Library for Tabular Model Training and Adaptation"},{"location":"#overview","text":"TabTune is a powerful and flexible Python library designed to simplify the training and fine-tuning of modern foundation models on tabular data. It provides a high-level, scikit-learn-compatible API that abstracts away the complexities of data preprocessing, model-specific training loops, and benchmarking, letting you focus on delivering results. Whether you are a practitioner aiming for production-grade pipelines or a researcher exploring advanced architectures, TabTune streamlines your workflow for tabular deep learning.","title":"Overview"},{"location":"#whats-new-in-tabtune","text":"Major Features and Updates: PEFT (LoRA) Support: Parameter-efficient fine-tuning using LoRA adapters for most models. Enhanced Tuning Strategies: Easily switch between inference , base-ft , and peft strategies. TabularLeaderboard: Effortless model comparison and benchmarking on your custom splits. New Models Added: TabBiaxial and TabDPT with full PEFT support. PEFT Limitations: Experimental LoRA support for TabPFN and ContextTab. Comprehensive Testing: Robust test suite covering all models and strategies. Rich Documentation: Extensive examples and how-to guides. Performance Optimizations: Improved memory footprint and training throughput.","title":"\ud83c\udd95 What's New in TabTune"},{"location":"#core-features","text":"Unified API: Single interface for model training, inference, and evaluation across multiple tabular model families. Automated Preprocessing: Model-aware data processing for feature scaling, encoding, imputation, and transformation. Flexible Fine-Tuning: Choose between zero-shot inference, full fine-tuning, or memory-efficient PEFT strategies. Model Comparison: Built-in leaderboard for systematic benchmarking and strategy evaluation. Extensible Design: Modular codebase for easy integration of custom data processors and models.","title":"\u2b50 Core Features"},{"location":"#supported-models","text":"Model Family / Paradigm Key Innovation PEFT Support TabPFN-v2 PFN / ICL Bayesian approximation on synthetic \u26a0\ufe0f Experimental TabICL Scalable ICL Two-stage column-row attention \u2705 Full Support TabBiaxial Scalable ICL Custom Biaxial Attention \u2705 Full Support Mitra Scalable ICL 2D attention, synthetic priors \u2705 Full Support ContextTab Semantics-Aware ICL Modality-specific embeddings \u26a0\ufe0f Experimental TabDPT Denoising Transformer Denoising pre-trained transformer \u2705 Full Support \u2705 Full Support: Reliable LoRA integration \u26a0\ufe0f Experimental: Known issues may occur; use base-ft if unstable","title":"\ud83d\udce6 Supported Models"},{"location":"#quick-start","text":"import pandas as pd from sklearn.model_selection import train_test_split import openml from TabularPipeline.pipeline import TabularPipeline # Load dataset dataset = openml . datasets . get_dataset ( 42178 ) X , y , _ , _ = dataset . get_data ( target = dataset . default_target_attribute ) X_train , X_test , y_train , y_test = train_test_split ( X , y , test_size = 0.25 , random_state = 42 ) # Init and fit pipeline pipeline = TabularPipeline ( model_name = \"TabPFN\" , task_type = \"classification\" , tuning_strategy = \"base-ft\" , tuning_params = { \"device\" : \"cpu\" } ) pipeline . fit ( X_train , y_train ) # Save and load pipeline for prediction pipeline . save ( \"churn_pipeline.joblib\" ) loaded_pipeline = TabularPipeline . load ( \"churn_pipeline.joblib\" ) predictions = loaded_pipeline . predict ( X_test ) metrics = pipeline . evaluate ( X_test , y_test ) print ( metrics )","title":"\u26a1 Quick Start"},{"location":"#why-tabtune","text":"No Boilerplate: Avoids repetitive code for model-specific data loading, training, and inference. Consistent Results: Automates best practices for tabular DL research and model selection. Fast Iteration: Easily compare new models with your data, using the same consistent API. Production Ready: Model and config serialization for robust deployment and reproducibility. Community-Driven: Extensible design and open contribution policy.","title":"\ud83d\udcdd Why TabTune?"},{"location":"#explore-the-documentation","text":"Getting Started : Installation, setup, and basic usage. User Guide : In-depth tutorials for each component. Supported Models : Model details and design notes. Advanced Topics : PEFT/LoRA, custom preprocessing, and more. API Reference : Complete Python API and class/method details. Testing & Validation : Test guidelines and reliability promises. Examples & Benchmarks : End-to-end code notebooks.","title":"\ud83d\udcd1 Explore the Documentation"},{"location":"#example-notebooks","text":"Name Task Type Colab Link TabPFN Inference Inference & Fine-Tune Open In Colab Mitra Inference/FineTune Inference & Fine-Tune Open In Colab","title":"\ud83c\udfc6 Example Notebooks"},{"location":"#project-structure","text":"TabTune/ \u251c\u2500\u2500 Dataprocess/ \u251c\u2500\u2500 models/ \u251c\u2500\u2500 TabularPipeline/ \u251c\u2500\u2500 TuningManager/ \u251c\u2500\u2500 TabularLeaderboard/ See User Guide for a full file/module breakdown.","title":"\ud83d\udcc2 Project Structure"},{"location":"#license","text":"This project is released under the MIT License. Please cite appropriately if used in academic or production projects.","title":"\ud83d\uddc3\ufe0f License"},{"location":"#join-community-contribute","text":"Issues and discussions are welcomed on the GitHub issue tracker . Please see the Contributing section for contribution standards, code reviews, and documentation tips. Get started with TabTune and accelerate your tabular deep learning workflows today!","title":"\ud83d\udceb Join Community / Contribute"},{"location":"advanced/custom-preprocessing/","text":"Custom Preprocessing: Extending TabTune's Data Pipeline \u00b6 This document explains how to create custom preprocessors, extend the data pipeline, and integrate domain-specific transformations with TabTune. 1. Introduction \u00b6 While TabTune provides comprehensive automatic preprocessing, you may need custom transformations for: Domain-specific feature engineering Specialized encoding for your data Integration with existing pipelines Research and experimentation Non-standard data types This guide shows how to extend TabTune's preprocessing architecture. 2. Preprocessing Architecture \u00b6 2.1 Class Hierarchy \u00b6 BasePreprocessor (Abstract) \u251c\u2500\u2500 StandardPreprocessor (Default) \u251c\u2500\u2500 TabPFNPreprocessor \u251c\u2500\u2500 TabICLPreprocessor \u251c\u2500\u2500 MitraPreprocessor \u251c\u2500\u2500 ContextTabPreprocessor \u251c\u2500\u2500 TabDPTPreprocessor \u251c\u2500\u2500 TabBiaxialPreprocessor \u2514\u2500\u2500 YourCustomPreprocessor 2.2 Base Class Interface \u00b6 from abc import ABC , abstractmethod import pandas as pd class BasePreprocessor ( ABC ): \"\"\"Abstract base for all preprocessors.\"\"\" def __init__ ( self , ** kwargs ): self . config = kwargs @abstractmethod def fit ( self , X : pd . DataFrame , y : pd . Series = None ): \"\"\"Learn preprocessing parameters from data.\"\"\" pass @abstractmethod def transform ( self , X : pd . DataFrame ) -> pd . DataFrame : \"\"\"Apply preprocessing to data.\"\"\" pass def fit_transform ( self , X : pd . DataFrame , y : pd . Series = None ): \"\"\"Fit and transform in one call.\"\"\" self . fit ( X , y ) return self . transform ( X ) def get_config ( self ) -> dict : \"\"\"Return preprocessing configuration.\"\"\" return self . config 3. Creating Custom Preprocessors \u00b6 3.1 Simple Custom Preprocessor \u00b6 Create a basic preprocessor for specialized transformations: import pandas as pd import numpy as np from tabtune.preprocessing.base import BasePreprocessor from sklearn.preprocessing import StandardScaler class CustomFeatureEngineeringPreprocessor ( BasePreprocessor ): \"\"\"Custom preprocessor with feature engineering.\"\"\" def __init__ ( self , scale_numericals = True , create_interactions = True ): super () . __init__ ( scale_numericals = scale_numericals , create_interactions = create_interactions ) self . scaler = StandardScaler () self . numerical_cols = None self . categorical_cols = None def fit ( self , X : pd . DataFrame , y : pd . Series = None ): \"\"\"Learn column types and scaling.\"\"\" # Identify numerical and categorical columns self . numerical_cols = X . select_dtypes ( include = [ np . number ] ) . columns . tolist () self . categorical_cols = X . select_dtypes ( include = [ 'object' , 'category' ] ) . columns . tolist () # Fit scaler on numerical columns if self . config [ 'scale_numericals' ]: self . scaler . fit ( X [ self . numerical_cols ]) def transform ( self , X : pd . DataFrame ) -> pd . DataFrame : \"\"\"Apply transformations.\"\"\" X_transformed = X . copy () # Scale numerical features if self . config [ 'scale_numericals' ]: X_transformed [ self . numerical_cols ] = self . scaler . transform ( X [ self . numerical_cols ] ) # Create interaction features if self . config [ 'create_interactions' ]: X_transformed = self . _create_interactions ( X_transformed ) return X_transformed def _create_interactions ( self , X : pd . DataFrame ) -> pd . DataFrame : \"\"\"Create polynomial interaction features.\"\"\" if len ( self . numerical_cols ) >= 2 : # Create pairwise interactions for i , col1 in enumerate ( self . numerical_cols ): for col2 in self . numerical_cols [ i + 1 :]: X [ f ' { col1 } _x_ { col2 } ' ] = X [ col1 ] * X [ col2 ] return X 3.2 Domain-Specific Preprocessor (Finance Example) \u00b6 import pandas as pd import numpy as np from tabtune.preprocessing.base import BasePreprocessor class FinancialDataPreprocessor ( BasePreprocessor ): \"\"\"Specialized preprocessor for financial data.\"\"\" def __init__ ( self , handle_outliers = True , create_ratios = True , normalize_by_scale = True ): super () . __init__ ( handle_outliers = handle_outliers , create_ratios = create_ratios , normalize_by_scale = normalize_by_scale ) self . outlier_bounds = {} self . scaling_factors = {} def fit ( self , X : pd . DataFrame , y : pd . Series = None ): \"\"\"Learn financial data patterns.\"\"\" # Detect outliers (IQR method) if self . config [ 'handle_outliers' ]: for col in X . select_dtypes ( include = [ np . number ]): Q1 = X [ col ] . quantile ( 0.25 ) Q3 = X [ col ] . quantile ( 0.75 ) IQR = Q3 - Q1 self . outlier_bounds [ col ] = { 'lower' : Q1 - 1.5 * IQR , 'upper' : Q3 + 1.5 * IQR } # Learn scaling factors by sector if self . config [ 'normalize_by_scale' ]: if 'sector' in X . columns : for sector in X [ 'sector' ] . unique (): sector_data = X [ X [ 'sector' ] == sector ] self . scaling_factors [ sector ] = sector_data . mean () def transform ( self , X : pd . DataFrame ) -> pd . DataFrame : \"\"\"Apply financial transformations.\"\"\" X_transformed = X . copy () # Handle outliers with clipping if self . config [ 'handle_outliers' ]: for col , bounds in self . outlier_bounds . items (): X_transformed [ col ] = X_transformed [ col ] . clip ( lower = bounds [ 'lower' ], upper = bounds [ 'upper' ] ) # Create financial ratios if self . config [ 'create_ratios' ]: if 'revenue' in X_transformed . columns and 'costs' in X_transformed . columns : X_transformed [ 'profit_margin' ] = ( X_transformed [ 'revenue' ] - X_transformed [ 'costs' ] ) / X_transformed [ 'revenue' ] # Normalize by sector if self . config [ 'normalize_by_scale' ]: if 'sector' in X_transformed . columns : for sector , factors in self . scaling_factors . items (): sector_mask = X_transformed [ 'sector' ] == sector # Normalize numeric columns by sector mean for col in X_transformed . select_dtypes ( include = [ np . number ]): X_transformed . loc [ sector_mask , col ] /= factors [ col ] return X_transformed 4. Integrating Custom Preprocessors \u00b6 4.1 Register Custom Preprocessor \u00b6 # In your codebase or configuration file from tabtune.data_processor import DataProcessor from your_module import CustomFeatureEngineeringPreprocessor # Register custom preprocessor DataProcessor . register_preprocessor ( 'custom_features' , CustomFeatureEngineeringPreprocessor ) 4.2 Use Custom Preprocessor with Pipeline \u00b6 from tabtune import TabularPipeline # Use custom preprocessor pipeline = TabularPipeline ( model_name = 'TabICL' , tuning_strategy = 'base-ft' , processor_params = { 'preprocessor_type' : 'custom_features' , 'scale_numericals' : True , 'create_interactions' : True } ) pipeline . fit ( X_train , y_train ) 4.3 Chaining Preprocessors \u00b6 Combine multiple preprocessors: class ChainedPreprocessor ( BasePreprocessor ): \"\"\"Sequentially apply multiple preprocessors.\"\"\" def __init__ ( self , preprocessors : list ): super () . __init__ ( preprocessors = preprocessors ) self . preprocessor_chain = preprocessors def fit ( self , X : pd . DataFrame , y : pd . Series = None ): \"\"\"Fit each preprocessor in chain.\"\"\" for preprocessor in self . preprocessor_chain : preprocessor . fit ( X , y ) X = preprocessor . transform ( X ) def transform ( self , X : pd . DataFrame ) -> pd . DataFrame : \"\"\"Apply all preprocessors in sequence.\"\"\" for preprocessor in self . preprocessor_chain : X = preprocessor . transform ( X ) return X # Usage preprocessors = [ CustomFeatureEngineeringPreprocessor ( create_interactions = True ), FinancialDataPreprocessor ( handle_outliers = True ), YourSpecializedPreprocessor () ] chained = ChainedPreprocessor ( preprocessors ) X_transformed = chained . fit_transform ( X_train , y_train ) 5. Feature Engineering Examples \u00b6 5.1 Polynomial Features \u00b6 from sklearn.preprocessing import PolynomialFeatures class PolynomialPreprocessor ( BasePreprocessor ): \"\"\"Add polynomial features.\"\"\" def __init__ ( self , degree = 2 , include_bias = False ): super () . __init__ ( degree = degree , include_bias = include_bias ) self . poly = PolynomialFeatures ( degree = degree , include_bias = include_bias ) self . feature_names = None def fit ( self , X : pd . DataFrame , y : pd . Series = None ): \"\"\"Fit polynomial transformer.\"\"\" numerical = X . select_dtypes ( include = [ np . number ]) self . poly . fit ( numerical ) self . feature_names = self . poly . get_feature_names_out ( numerical . columns ) def transform ( self , X : pd . DataFrame ) -> pd . DataFrame : \"\"\"Transform to polynomial features.\"\"\" numerical = X . select_dtypes ( include = [ np . number ]) poly_features = self . poly . transform ( numerical ) return pd . DataFrame ( poly_features , columns = self . feature_names , index = X . index ) 5.2 Statistical Features \u00b6 class StatisticalFeaturePreprocessor ( BasePreprocessor ): \"\"\"Extract statistical features from groups.\"\"\" def __init__ ( self , groupby_col = None , agg_functions = None ): super () . __init__ ( groupby_col = groupby_col , agg_functions = agg_functions or [ 'mean' , 'std' , 'min' , 'max' ] ) def fit ( self , X : pd . DataFrame , y : pd . Series = None ): \"\"\"No fitting needed for statistical features.\"\"\" pass def transform ( self , X : pd . DataFrame ) -> pd . DataFrame : \"\"\"Create statistical aggregates.\"\"\" X_transformed = X . copy () if self . config [ 'groupby_col' ] and self . config [ 'groupby_col' ] in X : groupby_col = self . config [ 'groupby_col' ] # Aggregate statistics for col in X . select_dtypes ( include = [ np . number ]): for func in self . config [ 'agg_functions' ]: agg_values = X . groupby ( groupby_col )[ col ] . agg ( func ) X_transformed [ f ' { col } _ { func } _by_ { groupby_col } ' ] = ( X [ groupby_col ] . map ( agg_values ) ) return X_transformed 5.3 Text Feature Extraction \u00b6 from sklearn.feature_extraction.text import TfidfVectorizer class TextFeaturePreprocessor ( BasePreprocessor ): \"\"\"Extract features from text columns.\"\"\" def __init__ ( self , text_columns = None , max_features = 100 ): super () . __init__ ( text_columns = text_columns , max_features = max_features ) self . vectorizers = {} def fit ( self , X : pd . DataFrame , y : pd . Series = None ): \"\"\"Fit TF-IDF vectorizers.\"\"\" text_cols = self . config [ 'text_columns' ] or [ col for col in X . columns if X [ col ] . dtype == 'object' ] for col in text_cols : vectorizer = TfidfVectorizer ( max_features = self . config [ 'max_features' ], lowercase = True ) vectorizer . fit ( X [ col ] . astype ( str )) self . vectorizers [ col ] = vectorizer def transform ( self , X : pd . DataFrame ) -> pd . DataFrame : \"\"\"Transform text to features.\"\"\" X_transformed = X . copy () for col , vectorizer in self . vectorizers . items (): tfidf_matrix = vectorizer . transform ( X [ col ] . astype ( str )) feature_names = vectorizer . get_feature_names_out () for i , fname in enumerate ( feature_names ): X_transformed [ f ' { col } _tfidf_ { fname } ' ] = ( tfidf_matrix [:, i ] . toarray () . flatten () ) return X_transformed 6. Integration with scikit-learn \u00b6 6.1 Use scikit-learn Transformers \u00b6 from sklearn.pipeline import Pipeline from sklearn.preprocessing import StandardScaler , RobustScaler from sklearn.decomposition import PCA class SklearnPreprocessor ( BasePreprocessor ): \"\"\"Wrap scikit-learn preprocessing pipeline.\"\"\" def __init__ ( self , steps = None ): super () . __init__ ( steps = steps or []) self . pipeline = Pipeline ( steps or []) def fit ( self , X : pd . DataFrame , y : pd . Series = None ): \"\"\"Fit sklearn pipeline.\"\"\" self . pipeline . fit ( X , y ) def transform ( self , X : pd . DataFrame ) -> pd . DataFrame : \"\"\"Transform using sklearn pipeline.\"\"\" result = self . pipeline . transform ( X ) return pd . DataFrame ( result , index = X . index ) # Usage with sklearn pipeline sklearn_steps = [ ( 'scaler' , RobustScaler ()), ( 'pca' , PCA ( n_components = 50 )) ] preprocessor = SklearnPreprocessor ( steps = sklearn_steps ) pipeline = TabularPipeline ( model_name = 'TabICL' , processor_params = { 'custom_preprocessor' : preprocessor } ) 7. Validation and Monitoring \u00b6 7.1 Validation Framework \u00b6 class ValidatingPreprocessor ( BasePreprocessor ): \"\"\"Preprocessor with validation.\"\"\" def __init__ ( self , validators = None ): super () . __init__ ( validators = validators or []) self . validators = validators def validate ( self , X : pd . DataFrame ) -> dict : \"\"\"Run validators and return results.\"\"\" results = {} for validator in self . validators : results [ validator . name ] = validator ( X ) return results def fit ( self , X : pd . DataFrame , y : pd . Series = None ): \"\"\"Fit with validation.\"\"\" validation_results = self . validate ( X ) for name , passed in validation_results . items (): if not passed : print ( f \"\u26a0\ufe0f Validation failed: { name } \" ) 7.2 Data Quality Checks \u00b6 class DataQualityValidator : \"\"\"Validate data quality before preprocessing.\"\"\" def __init__ ( self , name , check_func ): self . name = name self . check_func = check_func def __call__ ( self , X : pd . DataFrame ) -> bool : \"\"\"Run validation check.\"\"\" return self . check_func ( X ) # Define checks no_all_nulls = DataQualityValidator ( 'no_all_nulls' , lambda X : not X . isnull () . all () . any () ) sufficient_samples = DataQualityValidator ( 'sufficient_samples' , lambda X : len ( X ) >= 100 ) numeric_columns_exist = DataQualityValidator ( 'numeric_columns' , lambda X : X . select_dtypes ( include = [ np . number ]) . shape [ 1 ] > 0 ) 8. Performance Optimization \u00b6 8.1 Caching Transformed Data \u00b6 from functools import lru_cache import hashlib class CachedPreprocessor ( BasePreprocessor ): \"\"\"Cache preprocessor outputs.\"\"\" def __init__ ( self , cache_size = 128 ): super () . __init__ ( cache_size = cache_size ) self . cache = {} self . cache_size = cache_size def _get_cache_key ( self , X : pd . DataFrame ) -> str : \"\"\"Generate cache key from data hash.\"\"\" data_hash = hashlib . md5 ( pd . util . hash_pandas_object ( X ) . values ) . hexdigest () return data_hash def transform ( self , X : pd . DataFrame ) -> pd . DataFrame : \"\"\"Transform with caching.\"\"\" cache_key = self . _get_cache_key ( X ) if cache_key in self . cache : return self . cache [ cache_key ] # Perform transformation result = self . _transform_impl ( X ) # Cache result if space available if len ( self . cache ) < self . cache_size : self . cache [ cache_key ] = result return result def _transform_impl ( self , X : pd . DataFrame ) -> pd . DataFrame : \"\"\"Override this for your transformation.\"\"\" return X 8.2 Parallel Processing \u00b6 from joblib import Parallel , delayed class ParallelPreprocessor ( BasePreprocessor ): \"\"\"Apply preprocessing in parallel.\"\"\" def __init__ ( self , n_jobs =- 1 , batch_size = 1000 ): super () . __init__ ( n_jobs = n_jobs , batch_size = batch_size ) def transform ( self , X : pd . DataFrame ) -> pd . DataFrame : \"\"\"Transform in parallel batches.\"\"\" n_jobs = self . config [ 'n_jobs' ] batch_size = self . config [ 'batch_size' ] # Split into batches batches = [ X . iloc [ i : i + batch_size ] for i in range ( 0 , len ( X ), batch_size ) ] # Process in parallel results = Parallel ( n_jobs = n_jobs )( delayed ( self . _transform_batch )( batch ) for batch in batches ) return pd . concat ( results , ignore_index = True ) def _transform_batch ( self , batch : pd . DataFrame ) -> pd . DataFrame : \"\"\"Override for batch transformation.\"\"\" return batch 9. Testing Custom Preprocessors \u00b6 9.1 Unit Tests \u00b6 import unittest import pandas as pd import numpy as np class TestCustomPreprocessor ( unittest . TestCase ): \"\"\"Test custom preprocessor.\"\"\" def setUp ( self ): \"\"\"Create test data.\"\"\" self . X = pd . DataFrame ({ 'A' : [ 1 , 2 , 3 , 4 , 5 ], 'B' : [ 10 , 20 , 30 , 40 , 50 ], 'C' : [ 'x' , 'y' , 'x' , 'y' , 'z' ] }) self . y = pd . Series ([ 0 , 1 , 0 , 1 , 0 ]) def test_fit_transform ( self ): \"\"\"Test fit_transform.\"\"\" preprocessor = CustomFeatureEngineeringPreprocessor () result = preprocessor . fit_transform ( self . X , self . y ) self . assertEqual ( len ( result ), len ( self . X )) self . assertGreater ( result . shape [ 1 ], self . X . shape [ 1 ]) def test_transform_consistency ( self ): \"\"\"Test consistency across calls.\"\"\" preprocessor = CustomFeatureEngineeringPreprocessor () preprocessor . fit ( self . X , self . y ) result1 = preprocessor . transform ( self . X ) result2 = preprocessor . transform ( self . X ) pd . testing . assert_frame_equal ( result1 , result2 ) def test_no_data_leakage ( self ): \"\"\"Test train/test independence.\"\"\" X_train = self . X . iloc [: 3 ] X_test = self . X . iloc [ 3 :] preprocessor = CustomFeatureEngineeringPreprocessor () preprocessor . fit ( X_train ) result_test = preprocessor . transform ( X_test ) self . assertEqual ( len ( result_test ), len ( X_test )) if __name__ == '__main__' : unittest . main () 10. Best Practices \u00b6 \u2705 Do's \u00b6 \u2705 Inherit from BasePreprocessor \u2705 Implement fit() and transform() \u2705 Prevent data leakage (fit on train only) \u2705 Return DataFrames with proper indices \u2705 Document configuration parameters \u2705 Handle edge cases (empty data, NaNs) \u2705 Test thoroughly \u2705 Cache expensive computations \u274c Don'ts \u00b6 \u274c Don't modify input data in-place \u274c Don't fit on test data \u274c Don't hardcode column names \u274c Don't ignore NaN values silently \u274c Don't create state in transform() \u274c Don't forget to preserve index \u274c Don't skip error handling 11. Real-World Example: Complete Custom Pipeline \u00b6 class ComprehensivePreprocessor ( BasePreprocessor ): \"\"\"Complete preprocessing pipeline.\"\"\" def __init__ ( self ): super () . __init__ () self . numerical_cols = None self . categorical_cols = None self . transformers = {} def fit ( self , X : pd . DataFrame , y : pd . Series = None ): \"\"\"Fit all transformers.\"\"\" self . numerical_cols = X . select_dtypes ( include = [ np . number ] ) . columns . tolist () self . categorical_cols = X . select_dtypes ( include = [ 'object' , 'category' ] ) . columns . tolist () # Fit numerical transformer from sklearn.preprocessing import StandardScaler self . transformers [ 'scaler' ] = StandardScaler () self . transformers [ 'scaler' ] . fit ( X [ self . numerical_cols ]) # Fit categorical transformer from sklearn.preprocessing import LabelEncoder self . transformers [ 'encoders' ] = {} for col in self . categorical_cols : le = LabelEncoder () le . fit ( X [ col ] . astype ( str )) self . transformers [ 'encoders' ][ col ] = le def transform ( self , X : pd . DataFrame ) -> pd . DataFrame : \"\"\"Apply all transformations.\"\"\" X_transformed = pd . DataFrame ( index = X . index ) # Transform numerical scaled = self . transformers [ 'scaler' ] . transform ( X [ self . numerical_cols ] ) X_transformed [ self . numerical_cols ] = scaled # Transform categorical for col in self . categorical_cols : encoded = self . transformers [ 'encoders' ][ col ] . transform ( X [ col ] . astype ( str ) ) X_transformed [ col ] = encoded return X_transformed # Usage preprocessor = ComprehensivePreprocessor () preprocessor . fit ( X_train , y_train ) X_train_processed = preprocessor . transform ( X_train ) X_test_processed = preprocessor . transform ( X_test ) pipeline = TabularPipeline ( model_name = 'TabICL' , processor_params = { 'custom_preprocessor' : preprocessor } ) 12. Next Steps \u00b6 Data Processing - Standard preprocessing API Reference - DataProcessor API Examples - Full examples with custom preprocessing Extend TabTune's preprocessing with custom transformers tailored to your domain!","title":"Custom Preprocessing: Extending TabTune's Data Pipeline"},{"location":"advanced/custom-preprocessing/#custom-preprocessing-extending-tabtunes-data-pipeline","text":"This document explains how to create custom preprocessors, extend the data pipeline, and integrate domain-specific transformations with TabTune.","title":"Custom Preprocessing: Extending TabTune's Data Pipeline"},{"location":"advanced/custom-preprocessing/#1-introduction","text":"While TabTune provides comprehensive automatic preprocessing, you may need custom transformations for: Domain-specific feature engineering Specialized encoding for your data Integration with existing pipelines Research and experimentation Non-standard data types This guide shows how to extend TabTune's preprocessing architecture.","title":"1. Introduction"},{"location":"advanced/custom-preprocessing/#2-preprocessing-architecture","text":"","title":"2. Preprocessing Architecture"},{"location":"advanced/custom-preprocessing/#21-class-hierarchy","text":"BasePreprocessor (Abstract) \u251c\u2500\u2500 StandardPreprocessor (Default) \u251c\u2500\u2500 TabPFNPreprocessor \u251c\u2500\u2500 TabICLPreprocessor \u251c\u2500\u2500 MitraPreprocessor \u251c\u2500\u2500 ContextTabPreprocessor \u251c\u2500\u2500 TabDPTPreprocessor \u251c\u2500\u2500 TabBiaxialPreprocessor \u2514\u2500\u2500 YourCustomPreprocessor","title":"2.1 Class Hierarchy"},{"location":"advanced/custom-preprocessing/#22-base-class-interface","text":"from abc import ABC , abstractmethod import pandas as pd class BasePreprocessor ( ABC ): \"\"\"Abstract base for all preprocessors.\"\"\" def __init__ ( self , ** kwargs ): self . config = kwargs @abstractmethod def fit ( self , X : pd . DataFrame , y : pd . Series = None ): \"\"\"Learn preprocessing parameters from data.\"\"\" pass @abstractmethod def transform ( self , X : pd . DataFrame ) -> pd . DataFrame : \"\"\"Apply preprocessing to data.\"\"\" pass def fit_transform ( self , X : pd . DataFrame , y : pd . Series = None ): \"\"\"Fit and transform in one call.\"\"\" self . fit ( X , y ) return self . transform ( X ) def get_config ( self ) -> dict : \"\"\"Return preprocessing configuration.\"\"\" return self . config","title":"2.2 Base Class Interface"},{"location":"advanced/custom-preprocessing/#3-creating-custom-preprocessors","text":"","title":"3. Creating Custom Preprocessors"},{"location":"advanced/custom-preprocessing/#31-simple-custom-preprocessor","text":"Create a basic preprocessor for specialized transformations: import pandas as pd import numpy as np from tabtune.preprocessing.base import BasePreprocessor from sklearn.preprocessing import StandardScaler class CustomFeatureEngineeringPreprocessor ( BasePreprocessor ): \"\"\"Custom preprocessor with feature engineering.\"\"\" def __init__ ( self , scale_numericals = True , create_interactions = True ): super () . __init__ ( scale_numericals = scale_numericals , create_interactions = create_interactions ) self . scaler = StandardScaler () self . numerical_cols = None self . categorical_cols = None def fit ( self , X : pd . DataFrame , y : pd . Series = None ): \"\"\"Learn column types and scaling.\"\"\" # Identify numerical and categorical columns self . numerical_cols = X . select_dtypes ( include = [ np . number ] ) . columns . tolist () self . categorical_cols = X . select_dtypes ( include = [ 'object' , 'category' ] ) . columns . tolist () # Fit scaler on numerical columns if self . config [ 'scale_numericals' ]: self . scaler . fit ( X [ self . numerical_cols ]) def transform ( self , X : pd . DataFrame ) -> pd . DataFrame : \"\"\"Apply transformations.\"\"\" X_transformed = X . copy () # Scale numerical features if self . config [ 'scale_numericals' ]: X_transformed [ self . numerical_cols ] = self . scaler . transform ( X [ self . numerical_cols ] ) # Create interaction features if self . config [ 'create_interactions' ]: X_transformed = self . _create_interactions ( X_transformed ) return X_transformed def _create_interactions ( self , X : pd . DataFrame ) -> pd . DataFrame : \"\"\"Create polynomial interaction features.\"\"\" if len ( self . numerical_cols ) >= 2 : # Create pairwise interactions for i , col1 in enumerate ( self . numerical_cols ): for col2 in self . numerical_cols [ i + 1 :]: X [ f ' { col1 } _x_ { col2 } ' ] = X [ col1 ] * X [ col2 ] return X","title":"3.1 Simple Custom Preprocessor"},{"location":"advanced/custom-preprocessing/#32-domain-specific-preprocessor-finance-example","text":"import pandas as pd import numpy as np from tabtune.preprocessing.base import BasePreprocessor class FinancialDataPreprocessor ( BasePreprocessor ): \"\"\"Specialized preprocessor for financial data.\"\"\" def __init__ ( self , handle_outliers = True , create_ratios = True , normalize_by_scale = True ): super () . __init__ ( handle_outliers = handle_outliers , create_ratios = create_ratios , normalize_by_scale = normalize_by_scale ) self . outlier_bounds = {} self . scaling_factors = {} def fit ( self , X : pd . DataFrame , y : pd . Series = None ): \"\"\"Learn financial data patterns.\"\"\" # Detect outliers (IQR method) if self . config [ 'handle_outliers' ]: for col in X . select_dtypes ( include = [ np . number ]): Q1 = X [ col ] . quantile ( 0.25 ) Q3 = X [ col ] . quantile ( 0.75 ) IQR = Q3 - Q1 self . outlier_bounds [ col ] = { 'lower' : Q1 - 1.5 * IQR , 'upper' : Q3 + 1.5 * IQR } # Learn scaling factors by sector if self . config [ 'normalize_by_scale' ]: if 'sector' in X . columns : for sector in X [ 'sector' ] . unique (): sector_data = X [ X [ 'sector' ] == sector ] self . scaling_factors [ sector ] = sector_data . mean () def transform ( self , X : pd . DataFrame ) -> pd . DataFrame : \"\"\"Apply financial transformations.\"\"\" X_transformed = X . copy () # Handle outliers with clipping if self . config [ 'handle_outliers' ]: for col , bounds in self . outlier_bounds . items (): X_transformed [ col ] = X_transformed [ col ] . clip ( lower = bounds [ 'lower' ], upper = bounds [ 'upper' ] ) # Create financial ratios if self . config [ 'create_ratios' ]: if 'revenue' in X_transformed . columns and 'costs' in X_transformed . columns : X_transformed [ 'profit_margin' ] = ( X_transformed [ 'revenue' ] - X_transformed [ 'costs' ] ) / X_transformed [ 'revenue' ] # Normalize by sector if self . config [ 'normalize_by_scale' ]: if 'sector' in X_transformed . columns : for sector , factors in self . scaling_factors . items (): sector_mask = X_transformed [ 'sector' ] == sector # Normalize numeric columns by sector mean for col in X_transformed . select_dtypes ( include = [ np . number ]): X_transformed . loc [ sector_mask , col ] /= factors [ col ] return X_transformed","title":"3.2 Domain-Specific Preprocessor (Finance Example)"},{"location":"advanced/custom-preprocessing/#4-integrating-custom-preprocessors","text":"","title":"4. Integrating Custom Preprocessors"},{"location":"advanced/custom-preprocessing/#41-register-custom-preprocessor","text":"# In your codebase or configuration file from tabtune.data_processor import DataProcessor from your_module import CustomFeatureEngineeringPreprocessor # Register custom preprocessor DataProcessor . register_preprocessor ( 'custom_features' , CustomFeatureEngineeringPreprocessor )","title":"4.1 Register Custom Preprocessor"},{"location":"advanced/custom-preprocessing/#42-use-custom-preprocessor-with-pipeline","text":"from tabtune import TabularPipeline # Use custom preprocessor pipeline = TabularPipeline ( model_name = 'TabICL' , tuning_strategy = 'base-ft' , processor_params = { 'preprocessor_type' : 'custom_features' , 'scale_numericals' : True , 'create_interactions' : True } ) pipeline . fit ( X_train , y_train )","title":"4.2 Use Custom Preprocessor with Pipeline"},{"location":"advanced/custom-preprocessing/#43-chaining-preprocessors","text":"Combine multiple preprocessors: class ChainedPreprocessor ( BasePreprocessor ): \"\"\"Sequentially apply multiple preprocessors.\"\"\" def __init__ ( self , preprocessors : list ): super () . __init__ ( preprocessors = preprocessors ) self . preprocessor_chain = preprocessors def fit ( self , X : pd . DataFrame , y : pd . Series = None ): \"\"\"Fit each preprocessor in chain.\"\"\" for preprocessor in self . preprocessor_chain : preprocessor . fit ( X , y ) X = preprocessor . transform ( X ) def transform ( self , X : pd . DataFrame ) -> pd . DataFrame : \"\"\"Apply all preprocessors in sequence.\"\"\" for preprocessor in self . preprocessor_chain : X = preprocessor . transform ( X ) return X # Usage preprocessors = [ CustomFeatureEngineeringPreprocessor ( create_interactions = True ), FinancialDataPreprocessor ( handle_outliers = True ), YourSpecializedPreprocessor () ] chained = ChainedPreprocessor ( preprocessors ) X_transformed = chained . fit_transform ( X_train , y_train )","title":"4.3 Chaining Preprocessors"},{"location":"advanced/custom-preprocessing/#5-feature-engineering-examples","text":"","title":"5. Feature Engineering Examples"},{"location":"advanced/custom-preprocessing/#51-polynomial-features","text":"from sklearn.preprocessing import PolynomialFeatures class PolynomialPreprocessor ( BasePreprocessor ): \"\"\"Add polynomial features.\"\"\" def __init__ ( self , degree = 2 , include_bias = False ): super () . __init__ ( degree = degree , include_bias = include_bias ) self . poly = PolynomialFeatures ( degree = degree , include_bias = include_bias ) self . feature_names = None def fit ( self , X : pd . DataFrame , y : pd . Series = None ): \"\"\"Fit polynomial transformer.\"\"\" numerical = X . select_dtypes ( include = [ np . number ]) self . poly . fit ( numerical ) self . feature_names = self . poly . get_feature_names_out ( numerical . columns ) def transform ( self , X : pd . DataFrame ) -> pd . DataFrame : \"\"\"Transform to polynomial features.\"\"\" numerical = X . select_dtypes ( include = [ np . number ]) poly_features = self . poly . transform ( numerical ) return pd . DataFrame ( poly_features , columns = self . feature_names , index = X . index )","title":"5.1 Polynomial Features"},{"location":"advanced/custom-preprocessing/#52-statistical-features","text":"class StatisticalFeaturePreprocessor ( BasePreprocessor ): \"\"\"Extract statistical features from groups.\"\"\" def __init__ ( self , groupby_col = None , agg_functions = None ): super () . __init__ ( groupby_col = groupby_col , agg_functions = agg_functions or [ 'mean' , 'std' , 'min' , 'max' ] ) def fit ( self , X : pd . DataFrame , y : pd . Series = None ): \"\"\"No fitting needed for statistical features.\"\"\" pass def transform ( self , X : pd . DataFrame ) -> pd . DataFrame : \"\"\"Create statistical aggregates.\"\"\" X_transformed = X . copy () if self . config [ 'groupby_col' ] and self . config [ 'groupby_col' ] in X : groupby_col = self . config [ 'groupby_col' ] # Aggregate statistics for col in X . select_dtypes ( include = [ np . number ]): for func in self . config [ 'agg_functions' ]: agg_values = X . groupby ( groupby_col )[ col ] . agg ( func ) X_transformed [ f ' { col } _ { func } _by_ { groupby_col } ' ] = ( X [ groupby_col ] . map ( agg_values ) ) return X_transformed","title":"5.2 Statistical Features"},{"location":"advanced/custom-preprocessing/#53-text-feature-extraction","text":"from sklearn.feature_extraction.text import TfidfVectorizer class TextFeaturePreprocessor ( BasePreprocessor ): \"\"\"Extract features from text columns.\"\"\" def __init__ ( self , text_columns = None , max_features = 100 ): super () . __init__ ( text_columns = text_columns , max_features = max_features ) self . vectorizers = {} def fit ( self , X : pd . DataFrame , y : pd . Series = None ): \"\"\"Fit TF-IDF vectorizers.\"\"\" text_cols = self . config [ 'text_columns' ] or [ col for col in X . columns if X [ col ] . dtype == 'object' ] for col in text_cols : vectorizer = TfidfVectorizer ( max_features = self . config [ 'max_features' ], lowercase = True ) vectorizer . fit ( X [ col ] . astype ( str )) self . vectorizers [ col ] = vectorizer def transform ( self , X : pd . DataFrame ) -> pd . DataFrame : \"\"\"Transform text to features.\"\"\" X_transformed = X . copy () for col , vectorizer in self . vectorizers . items (): tfidf_matrix = vectorizer . transform ( X [ col ] . astype ( str )) feature_names = vectorizer . get_feature_names_out () for i , fname in enumerate ( feature_names ): X_transformed [ f ' { col } _tfidf_ { fname } ' ] = ( tfidf_matrix [:, i ] . toarray () . flatten () ) return X_transformed","title":"5.3 Text Feature Extraction"},{"location":"advanced/custom-preprocessing/#6-integration-with-scikit-learn","text":"","title":"6. Integration with scikit-learn"},{"location":"advanced/custom-preprocessing/#61-use-scikit-learn-transformers","text":"from sklearn.pipeline import Pipeline from sklearn.preprocessing import StandardScaler , RobustScaler from sklearn.decomposition import PCA class SklearnPreprocessor ( BasePreprocessor ): \"\"\"Wrap scikit-learn preprocessing pipeline.\"\"\" def __init__ ( self , steps = None ): super () . __init__ ( steps = steps or []) self . pipeline = Pipeline ( steps or []) def fit ( self , X : pd . DataFrame , y : pd . Series = None ): \"\"\"Fit sklearn pipeline.\"\"\" self . pipeline . fit ( X , y ) def transform ( self , X : pd . DataFrame ) -> pd . DataFrame : \"\"\"Transform using sklearn pipeline.\"\"\" result = self . pipeline . transform ( X ) return pd . DataFrame ( result , index = X . index ) # Usage with sklearn pipeline sklearn_steps = [ ( 'scaler' , RobustScaler ()), ( 'pca' , PCA ( n_components = 50 )) ] preprocessor = SklearnPreprocessor ( steps = sklearn_steps ) pipeline = TabularPipeline ( model_name = 'TabICL' , processor_params = { 'custom_preprocessor' : preprocessor } )","title":"6.1 Use scikit-learn Transformers"},{"location":"advanced/custom-preprocessing/#7-validation-and-monitoring","text":"","title":"7. Validation and Monitoring"},{"location":"advanced/custom-preprocessing/#71-validation-framework","text":"class ValidatingPreprocessor ( BasePreprocessor ): \"\"\"Preprocessor with validation.\"\"\" def __init__ ( self , validators = None ): super () . __init__ ( validators = validators or []) self . validators = validators def validate ( self , X : pd . DataFrame ) -> dict : \"\"\"Run validators and return results.\"\"\" results = {} for validator in self . validators : results [ validator . name ] = validator ( X ) return results def fit ( self , X : pd . DataFrame , y : pd . Series = None ): \"\"\"Fit with validation.\"\"\" validation_results = self . validate ( X ) for name , passed in validation_results . items (): if not passed : print ( f \"\u26a0\ufe0f Validation failed: { name } \" )","title":"7.1 Validation Framework"},{"location":"advanced/custom-preprocessing/#72-data-quality-checks","text":"class DataQualityValidator : \"\"\"Validate data quality before preprocessing.\"\"\" def __init__ ( self , name , check_func ): self . name = name self . check_func = check_func def __call__ ( self , X : pd . DataFrame ) -> bool : \"\"\"Run validation check.\"\"\" return self . check_func ( X ) # Define checks no_all_nulls = DataQualityValidator ( 'no_all_nulls' , lambda X : not X . isnull () . all () . any () ) sufficient_samples = DataQualityValidator ( 'sufficient_samples' , lambda X : len ( X ) >= 100 ) numeric_columns_exist = DataQualityValidator ( 'numeric_columns' , lambda X : X . select_dtypes ( include = [ np . number ]) . shape [ 1 ] > 0 )","title":"7.2 Data Quality Checks"},{"location":"advanced/custom-preprocessing/#8-performance-optimization","text":"","title":"8. Performance Optimization"},{"location":"advanced/custom-preprocessing/#81-caching-transformed-data","text":"from functools import lru_cache import hashlib class CachedPreprocessor ( BasePreprocessor ): \"\"\"Cache preprocessor outputs.\"\"\" def __init__ ( self , cache_size = 128 ): super () . __init__ ( cache_size = cache_size ) self . cache = {} self . cache_size = cache_size def _get_cache_key ( self , X : pd . DataFrame ) -> str : \"\"\"Generate cache key from data hash.\"\"\" data_hash = hashlib . md5 ( pd . util . hash_pandas_object ( X ) . values ) . hexdigest () return data_hash def transform ( self , X : pd . DataFrame ) -> pd . DataFrame : \"\"\"Transform with caching.\"\"\" cache_key = self . _get_cache_key ( X ) if cache_key in self . cache : return self . cache [ cache_key ] # Perform transformation result = self . _transform_impl ( X ) # Cache result if space available if len ( self . cache ) < self . cache_size : self . cache [ cache_key ] = result return result def _transform_impl ( self , X : pd . DataFrame ) -> pd . DataFrame : \"\"\"Override this for your transformation.\"\"\" return X","title":"8.1 Caching Transformed Data"},{"location":"advanced/custom-preprocessing/#82-parallel-processing","text":"from joblib import Parallel , delayed class ParallelPreprocessor ( BasePreprocessor ): \"\"\"Apply preprocessing in parallel.\"\"\" def __init__ ( self , n_jobs =- 1 , batch_size = 1000 ): super () . __init__ ( n_jobs = n_jobs , batch_size = batch_size ) def transform ( self , X : pd . DataFrame ) -> pd . DataFrame : \"\"\"Transform in parallel batches.\"\"\" n_jobs = self . config [ 'n_jobs' ] batch_size = self . config [ 'batch_size' ] # Split into batches batches = [ X . iloc [ i : i + batch_size ] for i in range ( 0 , len ( X ), batch_size ) ] # Process in parallel results = Parallel ( n_jobs = n_jobs )( delayed ( self . _transform_batch )( batch ) for batch in batches ) return pd . concat ( results , ignore_index = True ) def _transform_batch ( self , batch : pd . DataFrame ) -> pd . DataFrame : \"\"\"Override for batch transformation.\"\"\" return batch","title":"8.2 Parallel Processing"},{"location":"advanced/custom-preprocessing/#9-testing-custom-preprocessors","text":"","title":"9. Testing Custom Preprocessors"},{"location":"advanced/custom-preprocessing/#91-unit-tests","text":"import unittest import pandas as pd import numpy as np class TestCustomPreprocessor ( unittest . TestCase ): \"\"\"Test custom preprocessor.\"\"\" def setUp ( self ): \"\"\"Create test data.\"\"\" self . X = pd . DataFrame ({ 'A' : [ 1 , 2 , 3 , 4 , 5 ], 'B' : [ 10 , 20 , 30 , 40 , 50 ], 'C' : [ 'x' , 'y' , 'x' , 'y' , 'z' ] }) self . y = pd . Series ([ 0 , 1 , 0 , 1 , 0 ]) def test_fit_transform ( self ): \"\"\"Test fit_transform.\"\"\" preprocessor = CustomFeatureEngineeringPreprocessor () result = preprocessor . fit_transform ( self . X , self . y ) self . assertEqual ( len ( result ), len ( self . X )) self . assertGreater ( result . shape [ 1 ], self . X . shape [ 1 ]) def test_transform_consistency ( self ): \"\"\"Test consistency across calls.\"\"\" preprocessor = CustomFeatureEngineeringPreprocessor () preprocessor . fit ( self . X , self . y ) result1 = preprocessor . transform ( self . X ) result2 = preprocessor . transform ( self . X ) pd . testing . assert_frame_equal ( result1 , result2 ) def test_no_data_leakage ( self ): \"\"\"Test train/test independence.\"\"\" X_train = self . X . iloc [: 3 ] X_test = self . X . iloc [ 3 :] preprocessor = CustomFeatureEngineeringPreprocessor () preprocessor . fit ( X_train ) result_test = preprocessor . transform ( X_test ) self . assertEqual ( len ( result_test ), len ( X_test )) if __name__ == '__main__' : unittest . main ()","title":"9.1 Unit Tests"},{"location":"advanced/custom-preprocessing/#10-best-practices","text":"","title":"10. Best Practices"},{"location":"advanced/custom-preprocessing/#dos","text":"\u2705 Inherit from BasePreprocessor \u2705 Implement fit() and transform() \u2705 Prevent data leakage (fit on train only) \u2705 Return DataFrames with proper indices \u2705 Document configuration parameters \u2705 Handle edge cases (empty data, NaNs) \u2705 Test thoroughly \u2705 Cache expensive computations","title":"\u2705 Do's"},{"location":"advanced/custom-preprocessing/#donts","text":"\u274c Don't modify input data in-place \u274c Don't fit on test data \u274c Don't hardcode column names \u274c Don't ignore NaN values silently \u274c Don't create state in transform() \u274c Don't forget to preserve index \u274c Don't skip error handling","title":"\u274c Don'ts"},{"location":"advanced/custom-preprocessing/#11-real-world-example-complete-custom-pipeline","text":"class ComprehensivePreprocessor ( BasePreprocessor ): \"\"\"Complete preprocessing pipeline.\"\"\" def __init__ ( self ): super () . __init__ () self . numerical_cols = None self . categorical_cols = None self . transformers = {} def fit ( self , X : pd . DataFrame , y : pd . Series = None ): \"\"\"Fit all transformers.\"\"\" self . numerical_cols = X . select_dtypes ( include = [ np . number ] ) . columns . tolist () self . categorical_cols = X . select_dtypes ( include = [ 'object' , 'category' ] ) . columns . tolist () # Fit numerical transformer from sklearn.preprocessing import StandardScaler self . transformers [ 'scaler' ] = StandardScaler () self . transformers [ 'scaler' ] . fit ( X [ self . numerical_cols ]) # Fit categorical transformer from sklearn.preprocessing import LabelEncoder self . transformers [ 'encoders' ] = {} for col in self . categorical_cols : le = LabelEncoder () le . fit ( X [ col ] . astype ( str )) self . transformers [ 'encoders' ][ col ] = le def transform ( self , X : pd . DataFrame ) -> pd . DataFrame : \"\"\"Apply all transformations.\"\"\" X_transformed = pd . DataFrame ( index = X . index ) # Transform numerical scaled = self . transformers [ 'scaler' ] . transform ( X [ self . numerical_cols ] ) X_transformed [ self . numerical_cols ] = scaled # Transform categorical for col in self . categorical_cols : encoded = self . transformers [ 'encoders' ][ col ] . transform ( X [ col ] . astype ( str ) ) X_transformed [ col ] = encoded return X_transformed # Usage preprocessor = ComprehensivePreprocessor () preprocessor . fit ( X_train , y_train ) X_train_processed = preprocessor . transform ( X_train ) X_test_processed = preprocessor . transform ( X_test ) pipeline = TabularPipeline ( model_name = 'TabICL' , processor_params = { 'custom_preprocessor' : preprocessor } )","title":"11. Real-World Example: Complete Custom Pipeline"},{"location":"advanced/custom-preprocessing/#12-next-steps","text":"Data Processing - Standard preprocessing API Reference - DataProcessor API Examples - Full examples with custom preprocessing Extend TabTune's preprocessing with custom transformers tailored to your domain!","title":"12. Next Steps"},{"location":"advanced/hyperparameter-tuning/","text":"Hyperparameter Tuning: Optimizing TabTune Model Performance \u00b6 This document provides comprehensive guidance on hyperparameter tuning for TabTune models, including strategies, tools, and best practices for finding optimal configurations. 1. Introduction \u00b6 Hyperparameter tuning is the process of systematically searching for the best model configuration to maximize performance on your specific task. This guide covers: Search Strategies : Grid search, random search, Bayesian optimization Hyperparameter Spaces : Ranges and distributions for each model Tuning Tools : Integration with Optuna, scikit-optimize, and hyperopt Best Practices : Efficient tuning workflows and validation strategies 2. Hyperparameter Landscape \u00b6 2.1 Tunable Hyperparameters by Model \u00b6 Model Critical Important Minor TabPFN epochs, lr temperature batch_size TabICL n_episodes, lr support_size, query_size, n_estimators norm_methods TabBiaxial n_episodes, lr support_size, query_size n_estimators TabDPT support_size, lr k_neighbors, num_layers temperature Mitra support_size, lr batch_size, num_layers d_model ContextTab epochs, lr warmup_steps text_encoder 2.2 Shared Hyperparameters \u00b6 shared_hparams = { 'learning_rate' : [ 1e-5 , 5e-5 , 1e-4 , 5e-4 ], 'epochs' : [ 1 , 3 , 5 , 10 ], 'batch_size' : [ 8 , 16 , 32 , 64 ], 'weight_decay' : [ 0.0 , 0.01 , 0.1 ], 'warmup_steps' : [ 0 , 100 , 500 , 1000 ] } # PEFT-specific peft_hparams = { 'r' : [ 2 , 4 , 8 , 16 ], 'lora_alpha' : [ 4 , 8 , 16 , 32 ], 'lora_dropout' : [ 0.0 , 0.05 , 0.1 , 0.2 ] } 3. Search Strategies \u00b6 3.1 Grid Search \u00b6 Systematic evaluation of all parameter combinations. Advantages : - \u2705 Exhaustive coverage - \u2705 Parallelize easily - \u2705 Reproducible Disadvantages : - \u274c Exponential complexity - \u274c Wasteful on large spaces - \u274c Poor scaling from sklearn.model_selection import ParameterGrid from tabtune import TabularPipeline # Define grid param_grid = { 'learning_rate' : [ 1e-5 , 2e-5 , 5e-5 ], 'epochs' : [ 3 , 5 ], 'batch_size' : [ 16 , 32 ] } # Grid search best_score = 0 best_params = None for params in ParameterGrid ( param_grid ): pipeline = TabularPipeline ( model_name = 'TabICL' , tuning_strategy = 'base-ft' , tuning_params = params ) pipeline . fit ( X_train , y_train ) score = pipeline . evaluate ( X_val , y_val )[ 'accuracy' ] if score > best_score : best_score = score best_params = params print ( f \"New best: { score : .4f } with { params } \" ) print ( f \" \\n Best parameters: { best_params } \" ) print ( f \"Best score: { best_score : .4f } \" ) 3.2 Random Search \u00b6 Random sampling from parameter distributions. Advantages : - \u2705 Covers parameter space more uniformly - \u2705 Scales well to large spaces - \u2705 Simple parallelization Disadvantages : - \u274c May miss optimal region - \u274c Less reproducible import numpy as np from scipy.stats import uniform , randint # Define distributions param_distributions = { 'learning_rate' : uniform ( 1e-5 , 1e-3 ), 'epochs' : randint ( 1 , 20 ), 'batch_size' : randint ( 8 , 128 ), 'weight_decay' : uniform ( 0.0 , 0.1 ) } # Random search n_iter = 20 best_score = 0 best_params = None for i in range ( n_iter ): # Sample random parameters params = { key : dist . rvs () for key , dist in param_distributions . items () } # Convert to integers params [ 'epochs' ] = int ( params [ 'epochs' ]) params [ 'batch_size' ] = int ( params [ 'batch_size' ]) pipeline = TabularPipeline ( model_name = 'TabICL' , tuning_strategy = 'base-ft' , tuning_params = params ) pipeline . fit ( X_train , y_train ) score = pipeline . evaluate ( X_val , y_val )[ 'accuracy' ] if score > best_score : best_score = score best_params = params print ( f \"Iteration { i + 1 } / { n_iter } : { score : .4f } \" ) print ( f \" \\n Best parameters: { best_params } \" ) 3.3 Bayesian Optimization with Optuna \u00b6 Intelligent search using Gaussian processes. Advantages : - \u2705 Intelligent sampling - \u2705 Few evaluations needed - \u2705 Adaptively explores promising regions Disadvantages : - \u274c More complex - \u274c Slower per iteration - \u274c Requires more setup import optuna from optuna.pruners import MedianPruner def objective ( trial ): \"\"\"Optuna objective function.\"\"\" # Suggest hyperparameters learning_rate = trial . suggest_float ( 'learning_rate' , 1e-5 , 1e-3 , log = True ) epochs = trial . suggest_int ( 'epochs' , 1 , 20 ) batch_size = trial . suggest_int ( 'batch_size' , 8 , 128 ) weight_decay = trial . suggest_float ( 'weight_decay' , 0.0 , 0.1 ) tuning_params = { 'device' : 'cuda' , 'learning_rate' : learning_rate , 'epochs' : epochs , 'batch_size' : batch_size , 'weight_decay' : weight_decay } # Train and evaluate pipeline = TabularPipeline ( model_name = 'TabICL' , tuning_strategy = 'base-ft' , tuning_params = tuning_params ) pipeline . fit ( X_train , y_train ) score = pipeline . evaluate ( X_val , y_val )[ 'accuracy' ] return score # Create study study = optuna . create_study ( direction = 'maximize' , pruner = MedianPruner () ) # Optimize study . optimize ( objective , n_trials = 50 , n_jobs = 1 ) # Get results print ( f \"Best score: { study . best_value : .4f } \" ) print ( f \"Best parameters: { study . best_params } \" ) 4. Model-Specific Tuning \u00b6 4.1 TabPFN Tuning \u00b6 Focus on inference parameters since base-ft is not primary use case: # Key hyperparameters tabpfn_hparams = { 'n_estimators' : [ 8 , 16 , 32 ], # Ensemble size 'softmax_temperature' : [ 0.5 , 0.9 , 1.5 ], # Confidence 'epochs' : [ 1 , 3 , 5 ], # If fine-tuning 'learning_rate' : [ 1e-5 , 2e-5 , 5e-5 ] # If fine-tuning } # Fine-tune only if needed pipeline = TabularPipeline ( model_name = 'TabPFN' , tuning_strategy = 'base-ft' , tuning_params = { 'device' : 'cuda' , 'epochs' : 3 , 'learning_rate' : 2e-5 } ) 4.2 TabICL/TabBiaxial Tuning \u00b6 Optimize episodic training parameters: # Key hyperparameters tabicl_hparams = { 'support_size' : [ 24 , 48 , 96 ], # Context size 'query_size' : [ 16 , 32 , 64 ], # Query size 'n_episodes' : [ 500 , 1000 , 2000 ], # Training episodes 'learning_rate' : [ 1e-5 , 2e-5 , 5e-5 ], 'n_estimators' : [ 16 , 32 , 64 ] # Ensemble } # Recommended defaults best_config = { 'support_size' : 48 , 'query_size' : 32 , 'n_episodes' : 1000 , 'learning_rate' : 2e-5 , 'epochs' : 5 } 4.3 TabDPT Tuning \u00b6 Leverage pre-training and large context: # Key hyperparameters tabdpt_hparams = { 'support_size' : [ 512 , 1024 , 2048 ], # Large context 'query_size' : [ 128 , 256 , 512 ], 'k_neighbors' : [ 3 , 5 , 10 ], # k-NN context 'num_layers' : [ 2 , 4 , 8 ], # Architecture 'learning_rate' : [ 1e-5 , 2e-5 , 5e-5 ] } # Recommended for large datasets best_config = { 'support_size' : 1024 , 'query_size' : 256 , 'k_neighbors' : 5 , 'learning_rate' : 2e-5 , 'epochs' : 3 # Few due to pre-training } 4.4 Mitra Tuning \u00b6 Optimize 2D attention parameters: # Key hyperparameters mitra_hparams = { 'support_size' : [ 64 , 128 , 256 ], 'query_size' : [ 64 , 128 , 256 ], 'd_model' : [ 32 , 64 , 128 ], # Embedding dim 'num_layers' : [ 1 , 2 , 4 ], 'batch_size' : [ 2 , 4 , 8 ], # Must be small 'learning_rate' : [ 1e-5 , 2e-5 , 5e-5 ] } # Recommended best_config = { 'support_size' : 128 , 'query_size' : 128 , 'd_model' : 64 , 'num_layers' : 2 , 'batch_size' : 4 , # Critical: keep small 'learning_rate' : 1e-5 } 4.5 PEFT Tuning \u00b6 Optimize LoRA parameters: # LoRA hyperparameters peft_hparams = { 'r' : [ 2 , 4 , 8 , 16 ], 'lora_alpha' : [ 4 , 8 , 16 , 32 ], # Usually 2x rank 'lora_dropout' : [ 0.05 , 0.1 , 0.2 ], 'learning_rate' : [ 1e-4 , 2e-4 , 5e-4 ] # Higher than base-ft } # Recommended best_peft_config = { 'r' : 8 , 'lora_alpha' : 16 , 'lora_dropout' : 0.05 , 'learning_rate' : 2e-4 # 10x base-ft } 5. Cross-Validation Strategy \u00b6 5.1 k-Fold Cross-Validation \u00b6 from sklearn.model_selection import KFold import numpy as np def cross_validate_hyperparams ( X , y , model_name , params , k = 5 ): \"\"\"Evaluate hyperparameters via k-fold CV.\"\"\" kf = KFold ( n_splits = k , shuffle = True , random_state = 42 ) scores = [] for fold_idx , ( train_idx , val_idx ) in enumerate ( kf . split ( X )): X_train_fold = X . iloc [ train_idx ] X_val_fold = X . iloc [ val_idx ] y_train_fold = y . iloc [ train_idx ] y_val_fold = y . iloc [ val_idx ] # Train and evaluate pipeline = TabularPipeline ( model_name = model_name , tuning_strategy = 'base-ft' , tuning_params = params ) pipeline . fit ( X_train_fold , y_train_fold ) score = pipeline . evaluate ( X_val_fold , y_val_fold )[ 'accuracy' ] scores . append ( score ) print ( f \"Fold { fold_idx + 1 } / { k } : { score : .4f } \" ) mean_score = np . mean ( scores ) std_score = np . std ( scores ) print ( f \" \\n Mean: { mean_score : .4f } \u00b1 { std_score : .4f } \" ) return mean_score , std_score # Usage mean , std = cross_validate_hyperparams ( X_train , y_train , model_name = 'TabICL' , params = { 'epochs' : 5 , 'learning_rate' : 2e-5 }, k = 5 ) 5.2 Stratified k-Fold \u00b6 For imbalanced classification: from sklearn.model_selection import StratifiedKFold # Ensures class distribution preserved skf = StratifiedKFold ( n_splits = 5 , shuffle = True , random_state = 42 ) for train_idx , val_idx in skf . split ( X , y ): X_train_fold = X . iloc [ train_idx ] X_val_fold = X . iloc [ val_idx ] # ... training code 6. Efficient Tuning Workflows \u00b6 6.1 Cascading Search \u00b6 Start coarse, then fine-grained: # Stage 1: Coarse grid search stage1_params = { 'learning_rate' : [ 1e-5 , 5e-5 , 1e-4 ], 'epochs' : [ 3 , 5 , 10 ] } # Find best in stage 1 best_params_stage1 = grid_search ( stage1_params ) # Stage 2: Fine-grained around best stage2_params = { 'learning_rate' : [ best_params_stage1 [ 'learning_rate' ] / 2 , best_params_stage1 [ 'learning_rate' ], best_params_stage1 [ 'learning_rate' ] * 2 ], 'epochs' : [ best_params_stage1 [ 'epochs' ] - 1 , best_params_stage1 [ 'epochs' ], best_params_stage1 [ 'epochs' ] + 1 ] } # Find best in stage 2 best_params_stage2 = grid_search ( stage2_params ) 6.2 Early Stopping During Tuning \u00b6 class EarlyStoppingTuner : \"\"\"Tuner with early stopping.\"\"\" def __init__ ( self , patience = 3 ): self . patience = patience self . best_score = 0 self . no_improve_count = 0 def should_stop ( self , score ): \"\"\"Check if tuning should stop.\"\"\" if score > self . best_score : self . best_score = score self . no_improve_count = 0 return False else : self . no_improve_count += 1 return self . no_improve_count >= self . patience # Usage tuner = EarlyStoppingTuner ( patience = 5 ) for params in param_grid : # ... train and evaluate ... if tuner . should_stop ( score ): print ( f \"Early stopping after { tuner . no_improve_count } iterations\" ) break 7. Complete Tuning Examples \u00b6 7.1 Simple Grid Search with Leaderboard \u00b6 from tabtune import TabularLeaderboard # Define parameter grid param_grid = { 'learning_rate' : [ 1e-5 , 2e-5 , 5e-5 ], 'epochs' : [ 3 , 5 ], 'batch_size' : [ 16 , 32 ] } # Generate all combinations from itertools import product param_combinations = [ dict ( zip ( param_grid . keys (), values )) for values in product ( * param_grid . values ()) ] # Use leaderboard for comparison leaderboard = TabularLeaderboard ( X_train , X_val , y_train , y_val ) for i , params in enumerate ( param_combinations ): config_name = f \"Config_ { i + 1 } _ { params [ 'learning_rate' ] } _ { params [ 'epochs' ] } \" leaderboard . add_model ( 'TabICL' , 'base-ft' , name = config_name , tuning_params = params ) results = leaderboard . run ( rank_by = 'accuracy' ) print ( leaderboard . get_ranking ()) 7.2 Bayesian Optimization with Pruning \u00b6 import optuna def objective_with_pruning ( trial ): \"\"\"Objective with early pruning.\"\"\" learning_rate = trial . suggest_float ( 'lr' , 1e-5 , 1e-3 , log = True ) epochs = trial . suggest_int ( 'epochs' , 1 , 20 ) pipeline = TabularPipeline ( model_name = 'TabICL' , tuning_strategy = 'base-ft' , tuning_params = { 'learning_rate' : learning_rate , 'epochs' : epochs } ) # Train with intermediate reporting for epoch in range ( epochs ): # ... train for one epoch ... # Evaluate score = pipeline . evaluate ( X_val , y_val )[ 'accuracy' ] # Report intermediate value trial . report ( score , epoch ) # Prune if not promising if trial . should_prune (): raise optuna . TrialPruned () return score # Create study with pruning study = optuna . create_study ( direction = 'maximize' , pruner = optuna . pruners . MedianPruner () ) study . optimize ( objective_with_pruning , n_trials = 30 ) 7.3 Parallel Tuning with Ray \u00b6 import ray from ray import tune # Initialize Ray ray . init () def train_model ( config ): \"\"\"Trainable function for Ray.\"\"\" pipeline = TabularPipeline ( model_name = 'TabICL' , tuning_strategy = 'base-ft' , tuning_params = config ) pipeline . fit ( X_train , y_train ) metrics = pipeline . evaluate ( X_val , y_val ) return metrics # Parallel tuning results = tune . run ( train_model , config = { 'learning_rate' : tune . loguniform ( 1e-5 , 1e-3 ), 'epochs' : tune . randint ( 1 , 20 ), 'batch_size' : tune . choice ([ 16 , 32 , 64 ]) }, num_samples = 30 , verbose = 1 ) ray . shutdown () 8. Tuning Summary & Defaults \u00b6 8.1 Quick Reference Table \u00b6 Model Learning Rate Epochs Support Size Key Parameter TabPFN 2e-5 3-5 N/A n_estimators TabICL 2e-5 5 48 n_episodes TabBiaxial 2e-5 5 48 n_episodes TabDPT 2e-5 3 1024 k_neighbors Mitra 1e-5 3 128 batch_size ContextTab 1e-4 10 N/A warmup_steps PEFT 2e-4 5 48 rank (r) 8.2 Tuning Priority \u00b6 Learning Rate : 80% of impact Epochs : 10% of impact Batch/Support Size : 5% of impact Other : 5% of impact 9. Best Practices \u00b6 \u2705 Do's \u00b6 \u2705 Start with default hyperparameters \u2705 Use cross-validation for robustness \u2705 Tune on validation set, evaluate on test set \u2705 Focus on high-impact hyperparameters first \u2705 Use multiple seeds for stability \u2705 Log all experiments \u2705 Parallelize when possible \u274c Don'ts \u00b6 \u274c Don't tune on test set \u274c Don't use learning rate as only tunable parameter \u274c Don't ignore data size when choosing ranges \u274c Don't forget to freeze random seed \u274c Don't tune without validation set \u274c Don't skip early stopping 10. Common Pitfalls & Solutions \u00b6 Issue: \"Tuning is too slow\" \u00b6 Solution : - Use Bayesian optimization instead of grid search - Parallelize across cores - Use early stopping Issue: \"Best tuned model still overfits\" \u00b6 Solution : - Increase regularization (weight decay) - Use PEFT instead of base-ft - Reduce learning rate - Add dropout Issue: \"Tuning results don't transfer to test set\" \u00b6 Solution : - Use larger validation set - Use cross-validation - Don't overfit to validation set - Use proper hyperparameter ranges 11. Next Steps \u00b6 Tuning Strategies - Strategy details Model Selection - Choosing models TabularLeaderboard - Systematic comparison PEFT & LoRA - PEFT-specific tuning Systematic hyperparameter tuning unlocks the full potential of TabTune models!","title":"Hyperparameter Tuning"},{"location":"advanced/hyperparameter-tuning/#hyperparameter-tuning-optimizing-tabtune-model-performance","text":"This document provides comprehensive guidance on hyperparameter tuning for TabTune models, including strategies, tools, and best practices for finding optimal configurations.","title":"Hyperparameter Tuning: Optimizing TabTune Model Performance"},{"location":"advanced/hyperparameter-tuning/#1-introduction","text":"Hyperparameter tuning is the process of systematically searching for the best model configuration to maximize performance on your specific task. This guide covers: Search Strategies : Grid search, random search, Bayesian optimization Hyperparameter Spaces : Ranges and distributions for each model Tuning Tools : Integration with Optuna, scikit-optimize, and hyperopt Best Practices : Efficient tuning workflows and validation strategies","title":"1. Introduction"},{"location":"advanced/hyperparameter-tuning/#2-hyperparameter-landscape","text":"","title":"2. Hyperparameter Landscape"},{"location":"advanced/hyperparameter-tuning/#21-tunable-hyperparameters-by-model","text":"Model Critical Important Minor TabPFN epochs, lr temperature batch_size TabICL n_episodes, lr support_size, query_size, n_estimators norm_methods TabBiaxial n_episodes, lr support_size, query_size n_estimators TabDPT support_size, lr k_neighbors, num_layers temperature Mitra support_size, lr batch_size, num_layers d_model ContextTab epochs, lr warmup_steps text_encoder","title":"2.1 Tunable Hyperparameters by Model"},{"location":"advanced/hyperparameter-tuning/#22-shared-hyperparameters","text":"shared_hparams = { 'learning_rate' : [ 1e-5 , 5e-5 , 1e-4 , 5e-4 ], 'epochs' : [ 1 , 3 , 5 , 10 ], 'batch_size' : [ 8 , 16 , 32 , 64 ], 'weight_decay' : [ 0.0 , 0.01 , 0.1 ], 'warmup_steps' : [ 0 , 100 , 500 , 1000 ] } # PEFT-specific peft_hparams = { 'r' : [ 2 , 4 , 8 , 16 ], 'lora_alpha' : [ 4 , 8 , 16 , 32 ], 'lora_dropout' : [ 0.0 , 0.05 , 0.1 , 0.2 ] }","title":"2.2 Shared Hyperparameters"},{"location":"advanced/hyperparameter-tuning/#3-search-strategies","text":"","title":"3. Search Strategies"},{"location":"advanced/hyperparameter-tuning/#31-grid-search","text":"Systematic evaluation of all parameter combinations. Advantages : - \u2705 Exhaustive coverage - \u2705 Parallelize easily - \u2705 Reproducible Disadvantages : - \u274c Exponential complexity - \u274c Wasteful on large spaces - \u274c Poor scaling from sklearn.model_selection import ParameterGrid from tabtune import TabularPipeline # Define grid param_grid = { 'learning_rate' : [ 1e-5 , 2e-5 , 5e-5 ], 'epochs' : [ 3 , 5 ], 'batch_size' : [ 16 , 32 ] } # Grid search best_score = 0 best_params = None for params in ParameterGrid ( param_grid ): pipeline = TabularPipeline ( model_name = 'TabICL' , tuning_strategy = 'base-ft' , tuning_params = params ) pipeline . fit ( X_train , y_train ) score = pipeline . evaluate ( X_val , y_val )[ 'accuracy' ] if score > best_score : best_score = score best_params = params print ( f \"New best: { score : .4f } with { params } \" ) print ( f \" \\n Best parameters: { best_params } \" ) print ( f \"Best score: { best_score : .4f } \" )","title":"3.1 Grid Search"},{"location":"advanced/hyperparameter-tuning/#32-random-search","text":"Random sampling from parameter distributions. Advantages : - \u2705 Covers parameter space more uniformly - \u2705 Scales well to large spaces - \u2705 Simple parallelization Disadvantages : - \u274c May miss optimal region - \u274c Less reproducible import numpy as np from scipy.stats import uniform , randint # Define distributions param_distributions = { 'learning_rate' : uniform ( 1e-5 , 1e-3 ), 'epochs' : randint ( 1 , 20 ), 'batch_size' : randint ( 8 , 128 ), 'weight_decay' : uniform ( 0.0 , 0.1 ) } # Random search n_iter = 20 best_score = 0 best_params = None for i in range ( n_iter ): # Sample random parameters params = { key : dist . rvs () for key , dist in param_distributions . items () } # Convert to integers params [ 'epochs' ] = int ( params [ 'epochs' ]) params [ 'batch_size' ] = int ( params [ 'batch_size' ]) pipeline = TabularPipeline ( model_name = 'TabICL' , tuning_strategy = 'base-ft' , tuning_params = params ) pipeline . fit ( X_train , y_train ) score = pipeline . evaluate ( X_val , y_val )[ 'accuracy' ] if score > best_score : best_score = score best_params = params print ( f \"Iteration { i + 1 } / { n_iter } : { score : .4f } \" ) print ( f \" \\n Best parameters: { best_params } \" )","title":"3.2 Random Search"},{"location":"advanced/hyperparameter-tuning/#33-bayesian-optimization-with-optuna","text":"Intelligent search using Gaussian processes. Advantages : - \u2705 Intelligent sampling - \u2705 Few evaluations needed - \u2705 Adaptively explores promising regions Disadvantages : - \u274c More complex - \u274c Slower per iteration - \u274c Requires more setup import optuna from optuna.pruners import MedianPruner def objective ( trial ): \"\"\"Optuna objective function.\"\"\" # Suggest hyperparameters learning_rate = trial . suggest_float ( 'learning_rate' , 1e-5 , 1e-3 , log = True ) epochs = trial . suggest_int ( 'epochs' , 1 , 20 ) batch_size = trial . suggest_int ( 'batch_size' , 8 , 128 ) weight_decay = trial . suggest_float ( 'weight_decay' , 0.0 , 0.1 ) tuning_params = { 'device' : 'cuda' , 'learning_rate' : learning_rate , 'epochs' : epochs , 'batch_size' : batch_size , 'weight_decay' : weight_decay } # Train and evaluate pipeline = TabularPipeline ( model_name = 'TabICL' , tuning_strategy = 'base-ft' , tuning_params = tuning_params ) pipeline . fit ( X_train , y_train ) score = pipeline . evaluate ( X_val , y_val )[ 'accuracy' ] return score # Create study study = optuna . create_study ( direction = 'maximize' , pruner = MedianPruner () ) # Optimize study . optimize ( objective , n_trials = 50 , n_jobs = 1 ) # Get results print ( f \"Best score: { study . best_value : .4f } \" ) print ( f \"Best parameters: { study . best_params } \" )","title":"3.3 Bayesian Optimization with Optuna"},{"location":"advanced/hyperparameter-tuning/#4-model-specific-tuning","text":"","title":"4. Model-Specific Tuning"},{"location":"advanced/hyperparameter-tuning/#41-tabpfn-tuning","text":"Focus on inference parameters since base-ft is not primary use case: # Key hyperparameters tabpfn_hparams = { 'n_estimators' : [ 8 , 16 , 32 ], # Ensemble size 'softmax_temperature' : [ 0.5 , 0.9 , 1.5 ], # Confidence 'epochs' : [ 1 , 3 , 5 ], # If fine-tuning 'learning_rate' : [ 1e-5 , 2e-5 , 5e-5 ] # If fine-tuning } # Fine-tune only if needed pipeline = TabularPipeline ( model_name = 'TabPFN' , tuning_strategy = 'base-ft' , tuning_params = { 'device' : 'cuda' , 'epochs' : 3 , 'learning_rate' : 2e-5 } )","title":"4.1 TabPFN Tuning"},{"location":"advanced/hyperparameter-tuning/#42-tabicltabbiaxial-tuning","text":"Optimize episodic training parameters: # Key hyperparameters tabicl_hparams = { 'support_size' : [ 24 , 48 , 96 ], # Context size 'query_size' : [ 16 , 32 , 64 ], # Query size 'n_episodes' : [ 500 , 1000 , 2000 ], # Training episodes 'learning_rate' : [ 1e-5 , 2e-5 , 5e-5 ], 'n_estimators' : [ 16 , 32 , 64 ] # Ensemble } # Recommended defaults best_config = { 'support_size' : 48 , 'query_size' : 32 , 'n_episodes' : 1000 , 'learning_rate' : 2e-5 , 'epochs' : 5 }","title":"4.2 TabICL/TabBiaxial Tuning"},{"location":"advanced/hyperparameter-tuning/#43-tabdpt-tuning","text":"Leverage pre-training and large context: # Key hyperparameters tabdpt_hparams = { 'support_size' : [ 512 , 1024 , 2048 ], # Large context 'query_size' : [ 128 , 256 , 512 ], 'k_neighbors' : [ 3 , 5 , 10 ], # k-NN context 'num_layers' : [ 2 , 4 , 8 ], # Architecture 'learning_rate' : [ 1e-5 , 2e-5 , 5e-5 ] } # Recommended for large datasets best_config = { 'support_size' : 1024 , 'query_size' : 256 , 'k_neighbors' : 5 , 'learning_rate' : 2e-5 , 'epochs' : 3 # Few due to pre-training }","title":"4.3 TabDPT Tuning"},{"location":"advanced/hyperparameter-tuning/#44-mitra-tuning","text":"Optimize 2D attention parameters: # Key hyperparameters mitra_hparams = { 'support_size' : [ 64 , 128 , 256 ], 'query_size' : [ 64 , 128 , 256 ], 'd_model' : [ 32 , 64 , 128 ], # Embedding dim 'num_layers' : [ 1 , 2 , 4 ], 'batch_size' : [ 2 , 4 , 8 ], # Must be small 'learning_rate' : [ 1e-5 , 2e-5 , 5e-5 ] } # Recommended best_config = { 'support_size' : 128 , 'query_size' : 128 , 'd_model' : 64 , 'num_layers' : 2 , 'batch_size' : 4 , # Critical: keep small 'learning_rate' : 1e-5 }","title":"4.4 Mitra Tuning"},{"location":"advanced/hyperparameter-tuning/#45-peft-tuning","text":"Optimize LoRA parameters: # LoRA hyperparameters peft_hparams = { 'r' : [ 2 , 4 , 8 , 16 ], 'lora_alpha' : [ 4 , 8 , 16 , 32 ], # Usually 2x rank 'lora_dropout' : [ 0.05 , 0.1 , 0.2 ], 'learning_rate' : [ 1e-4 , 2e-4 , 5e-4 ] # Higher than base-ft } # Recommended best_peft_config = { 'r' : 8 , 'lora_alpha' : 16 , 'lora_dropout' : 0.05 , 'learning_rate' : 2e-4 # 10x base-ft }","title":"4.5 PEFT Tuning"},{"location":"advanced/hyperparameter-tuning/#5-cross-validation-strategy","text":"","title":"5. Cross-Validation Strategy"},{"location":"advanced/hyperparameter-tuning/#51-k-fold-cross-validation","text":"from sklearn.model_selection import KFold import numpy as np def cross_validate_hyperparams ( X , y , model_name , params , k = 5 ): \"\"\"Evaluate hyperparameters via k-fold CV.\"\"\" kf = KFold ( n_splits = k , shuffle = True , random_state = 42 ) scores = [] for fold_idx , ( train_idx , val_idx ) in enumerate ( kf . split ( X )): X_train_fold = X . iloc [ train_idx ] X_val_fold = X . iloc [ val_idx ] y_train_fold = y . iloc [ train_idx ] y_val_fold = y . iloc [ val_idx ] # Train and evaluate pipeline = TabularPipeline ( model_name = model_name , tuning_strategy = 'base-ft' , tuning_params = params ) pipeline . fit ( X_train_fold , y_train_fold ) score = pipeline . evaluate ( X_val_fold , y_val_fold )[ 'accuracy' ] scores . append ( score ) print ( f \"Fold { fold_idx + 1 } / { k } : { score : .4f } \" ) mean_score = np . mean ( scores ) std_score = np . std ( scores ) print ( f \" \\n Mean: { mean_score : .4f } \u00b1 { std_score : .4f } \" ) return mean_score , std_score # Usage mean , std = cross_validate_hyperparams ( X_train , y_train , model_name = 'TabICL' , params = { 'epochs' : 5 , 'learning_rate' : 2e-5 }, k = 5 )","title":"5.1 k-Fold Cross-Validation"},{"location":"advanced/hyperparameter-tuning/#52-stratified-k-fold","text":"For imbalanced classification: from sklearn.model_selection import StratifiedKFold # Ensures class distribution preserved skf = StratifiedKFold ( n_splits = 5 , shuffle = True , random_state = 42 ) for train_idx , val_idx in skf . split ( X , y ): X_train_fold = X . iloc [ train_idx ] X_val_fold = X . iloc [ val_idx ] # ... training code","title":"5.2 Stratified k-Fold"},{"location":"advanced/hyperparameter-tuning/#6-efficient-tuning-workflows","text":"","title":"6. Efficient Tuning Workflows"},{"location":"advanced/hyperparameter-tuning/#61-cascading-search","text":"Start coarse, then fine-grained: # Stage 1: Coarse grid search stage1_params = { 'learning_rate' : [ 1e-5 , 5e-5 , 1e-4 ], 'epochs' : [ 3 , 5 , 10 ] } # Find best in stage 1 best_params_stage1 = grid_search ( stage1_params ) # Stage 2: Fine-grained around best stage2_params = { 'learning_rate' : [ best_params_stage1 [ 'learning_rate' ] / 2 , best_params_stage1 [ 'learning_rate' ], best_params_stage1 [ 'learning_rate' ] * 2 ], 'epochs' : [ best_params_stage1 [ 'epochs' ] - 1 , best_params_stage1 [ 'epochs' ], best_params_stage1 [ 'epochs' ] + 1 ] } # Find best in stage 2 best_params_stage2 = grid_search ( stage2_params )","title":"6.1 Cascading Search"},{"location":"advanced/hyperparameter-tuning/#62-early-stopping-during-tuning","text":"class EarlyStoppingTuner : \"\"\"Tuner with early stopping.\"\"\" def __init__ ( self , patience = 3 ): self . patience = patience self . best_score = 0 self . no_improve_count = 0 def should_stop ( self , score ): \"\"\"Check if tuning should stop.\"\"\" if score > self . best_score : self . best_score = score self . no_improve_count = 0 return False else : self . no_improve_count += 1 return self . no_improve_count >= self . patience # Usage tuner = EarlyStoppingTuner ( patience = 5 ) for params in param_grid : # ... train and evaluate ... if tuner . should_stop ( score ): print ( f \"Early stopping after { tuner . no_improve_count } iterations\" ) break","title":"6.2 Early Stopping During Tuning"},{"location":"advanced/hyperparameter-tuning/#7-complete-tuning-examples","text":"","title":"7. Complete Tuning Examples"},{"location":"advanced/hyperparameter-tuning/#71-simple-grid-search-with-leaderboard","text":"from tabtune import TabularLeaderboard # Define parameter grid param_grid = { 'learning_rate' : [ 1e-5 , 2e-5 , 5e-5 ], 'epochs' : [ 3 , 5 ], 'batch_size' : [ 16 , 32 ] } # Generate all combinations from itertools import product param_combinations = [ dict ( zip ( param_grid . keys (), values )) for values in product ( * param_grid . values ()) ] # Use leaderboard for comparison leaderboard = TabularLeaderboard ( X_train , X_val , y_train , y_val ) for i , params in enumerate ( param_combinations ): config_name = f \"Config_ { i + 1 } _ { params [ 'learning_rate' ] } _ { params [ 'epochs' ] } \" leaderboard . add_model ( 'TabICL' , 'base-ft' , name = config_name , tuning_params = params ) results = leaderboard . run ( rank_by = 'accuracy' ) print ( leaderboard . get_ranking ())","title":"7.1 Simple Grid Search with Leaderboard"},{"location":"advanced/hyperparameter-tuning/#72-bayesian-optimization-with-pruning","text":"import optuna def objective_with_pruning ( trial ): \"\"\"Objective with early pruning.\"\"\" learning_rate = trial . suggest_float ( 'lr' , 1e-5 , 1e-3 , log = True ) epochs = trial . suggest_int ( 'epochs' , 1 , 20 ) pipeline = TabularPipeline ( model_name = 'TabICL' , tuning_strategy = 'base-ft' , tuning_params = { 'learning_rate' : learning_rate , 'epochs' : epochs } ) # Train with intermediate reporting for epoch in range ( epochs ): # ... train for one epoch ... # Evaluate score = pipeline . evaluate ( X_val , y_val )[ 'accuracy' ] # Report intermediate value trial . report ( score , epoch ) # Prune if not promising if trial . should_prune (): raise optuna . TrialPruned () return score # Create study with pruning study = optuna . create_study ( direction = 'maximize' , pruner = optuna . pruners . MedianPruner () ) study . optimize ( objective_with_pruning , n_trials = 30 )","title":"7.2 Bayesian Optimization with Pruning"},{"location":"advanced/hyperparameter-tuning/#73-parallel-tuning-with-ray","text":"import ray from ray import tune # Initialize Ray ray . init () def train_model ( config ): \"\"\"Trainable function for Ray.\"\"\" pipeline = TabularPipeline ( model_name = 'TabICL' , tuning_strategy = 'base-ft' , tuning_params = config ) pipeline . fit ( X_train , y_train ) metrics = pipeline . evaluate ( X_val , y_val ) return metrics # Parallel tuning results = tune . run ( train_model , config = { 'learning_rate' : tune . loguniform ( 1e-5 , 1e-3 ), 'epochs' : tune . randint ( 1 , 20 ), 'batch_size' : tune . choice ([ 16 , 32 , 64 ]) }, num_samples = 30 , verbose = 1 ) ray . shutdown ()","title":"7.3 Parallel Tuning with Ray"},{"location":"advanced/hyperparameter-tuning/#8-tuning-summary-defaults","text":"","title":"8. Tuning Summary &amp; Defaults"},{"location":"advanced/hyperparameter-tuning/#81-quick-reference-table","text":"Model Learning Rate Epochs Support Size Key Parameter TabPFN 2e-5 3-5 N/A n_estimators TabICL 2e-5 5 48 n_episodes TabBiaxial 2e-5 5 48 n_episodes TabDPT 2e-5 3 1024 k_neighbors Mitra 1e-5 3 128 batch_size ContextTab 1e-4 10 N/A warmup_steps PEFT 2e-4 5 48 rank (r)","title":"8.1 Quick Reference Table"},{"location":"advanced/hyperparameter-tuning/#82-tuning-priority","text":"Learning Rate : 80% of impact Epochs : 10% of impact Batch/Support Size : 5% of impact Other : 5% of impact","title":"8.2 Tuning Priority"},{"location":"advanced/hyperparameter-tuning/#9-best-practices","text":"","title":"9. Best Practices"},{"location":"advanced/hyperparameter-tuning/#dos","text":"\u2705 Start with default hyperparameters \u2705 Use cross-validation for robustness \u2705 Tune on validation set, evaluate on test set \u2705 Focus on high-impact hyperparameters first \u2705 Use multiple seeds for stability \u2705 Log all experiments \u2705 Parallelize when possible","title":"\u2705 Do's"},{"location":"advanced/hyperparameter-tuning/#donts","text":"\u274c Don't tune on test set \u274c Don't use learning rate as only tunable parameter \u274c Don't ignore data size when choosing ranges \u274c Don't forget to freeze random seed \u274c Don't tune without validation set \u274c Don't skip early stopping","title":"\u274c Don'ts"},{"location":"advanced/hyperparameter-tuning/#10-common-pitfalls-solutions","text":"","title":"10. Common Pitfalls &amp; Solutions"},{"location":"advanced/hyperparameter-tuning/#issue-tuning-is-too-slow","text":"Solution : - Use Bayesian optimization instead of grid search - Parallelize across cores - Use early stopping","title":"Issue: \"Tuning is too slow\""},{"location":"advanced/hyperparameter-tuning/#issue-best-tuned-model-still-overfits","text":"Solution : - Increase regularization (weight decay) - Use PEFT instead of base-ft - Reduce learning rate - Add dropout","title":"Issue: \"Best tuned model still overfits\""},{"location":"advanced/hyperparameter-tuning/#issue-tuning-results-dont-transfer-to-test-set","text":"Solution : - Use larger validation set - Use cross-validation - Don't overfit to validation set - Use proper hyperparameter ranges","title":"Issue: \"Tuning results don't transfer to test set\""},{"location":"advanced/hyperparameter-tuning/#11-next-steps","text":"Tuning Strategies - Strategy details Model Selection - Choosing models TabularLeaderboard - Systematic comparison PEFT & LoRA - PEFT-specific tuning Systematic hyperparameter tuning unlocks the full potential of TabTune models!","title":"11. Next Steps"},{"location":"advanced/memory-optimization/","text":"Memory Optimization: Techniques for Training Large Models \u00b6 This document provides comprehensive strategies for optimizing memory usage when training TabTune models, enabling efficient use of limited GPU/CPU resources. 1. Introduction \u00b6 Memory optimization is critical for: Training on GPUs with limited VRAM (< 8GB) Processing large datasets (100K+ rows) Using large model architectures Running multiple experiments simultaneously Deploying in resource-constrained environments This guide covers techniques, tools, and trade-offs for memory efficiency. 2. Memory Profiling \u00b6 2.1 Understanding Memory Usage \u00b6 Memory Breakdown (typical forward/backward pass): Model weights: 40-50% (frozen in PEFT) Optimizer states: 20-30% (momentum, variance) Gradients: 10-15% Activations: 10-20% (intermediate values) DataLoader buffers: 5-10% \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Total: 100% 2.2 Profiling Tools \u00b6 PyTorch Memory Profiler \u00b6 import torch from torch.profiler import profile , record_function , ProfilerActivity # Basic memory measurement torch . cuda . empty_cache () torch . cuda . reset_peak_memory_stats () # Your training code pipeline . fit ( X_train , y_train ) # Print memory stats print ( f \"Peak memory: { torch . cuda . max_memory_allocated () / 1e9 : .2f } GB\" ) print ( f \"Current memory: { torch . cuda . memory_allocated () / 1e9 : .2f } GB\" ) # Detailed profiling with profile ( activities = [ ProfilerActivity . CPU , ProfilerActivity . CUDA ], record_shapes = True ) as prof : pipeline . fit ( X_train , y_train ) print ( prof . key_averages () . table ( sort_by = \"cuda_memory_usage\" )) Memory Monitoring Script \u00b6 import psutil import nvidia_smi class MemoryMonitor : \"\"\"Monitor memory usage during training.\"\"\" def __init__ ( self ): self . peak_gpu = 0 self . peak_cpu = 0 def record ( self ): \"\"\"Record current memory usage.\"\"\" try : nvidia_smi . nvmlInit () handle = nvidia_smi . nvmlDeviceGetHandleByIndex ( 0 ) info = nvidia_smi . nvmlDeviceGetMemoryInfo ( handle ) gpu_mem = info . used / 1e9 self . peak_gpu = max ( self . peak_gpu , gpu_mem ) except : gpu_mem = 0 # CPU memory process = psutil . Process () cpu_mem = process . memory_info () . rss / 1e9 self . peak_cpu = max ( self . peak_cpu , cpu_mem ) return gpu_mem , cpu_mem def report ( self ): \"\"\"Print memory report.\"\"\" print ( f \"Peak GPU: { self . peak_gpu : .2f } GB\" ) print ( f \"Peak CPU: { self . peak_cpu : .2f } GB\" ) # Usage monitor = MemoryMonitor () # Periodically call during training for epoch in range ( num_epochs ): gpu_mem , cpu_mem = monitor . record () print ( f \"Epoch { epoch } : GPU { gpu_mem : .2f } GB, CPU { cpu_mem : .2f } GB\" ) monitor . report () 3. Optimization Techniques \u00b6 3.1 PEFT (LoRA) - Primary Technique \u00b6 Impact : 60-90% memory reduction # Base fine-tuning: high memory pipeline_base = TabularPipeline ( model_name = 'TabICL' , tuning_strategy = 'base-ft' , tuning_params = { 'device' : 'cuda' , 'epochs' : 5 } ) # Memory: ~12 GB for 100K samples # PEFT fine-tuning: low memory pipeline_peft = TabularPipeline ( model_name = 'TabICL' , tuning_strategy = 'peft' , tuning_params = { 'device' : 'cuda' , 'epochs' : 5 , 'peft_config' : { 'r' : 8 } } ) # Memory: ~4 GB for same task Choosing PEFT config for memory : # Ultra-constrained (2GB GPU) peft_config = { 'r' : 2 , 'lora_alpha' : 4 , 'lora_dropout' : 0.2 } # Memory-constrained (4GB GPU) peft_config = { 'r' : 4 , 'lora_alpha' : 8 , 'lora_dropout' : 0.1 } # Moderate constraint (6GB GPU) peft_config = { 'r' : 8 , 'lora_alpha' : 16 , 'lora_dropout' : 0.05 } 3.2 Batch Size Reduction \u00b6 Impact : 20-40% memory reduction per halving # Large batch (high memory) tuning_params = { 'batch_size' : 64 , 'support_size' : 512 } # Memory: ~12 GB # Reduced batch (medium memory) tuning_params = { 'batch_size' : 32 , 'support_size' : 256 } # Memory: ~6 GB # Small batch (low memory) tuning_params = { 'batch_size' : 8 , 'support_size' : 64 } # Memory: ~2 GB Trade-offs : - Smaller batch: More gradient noise, longer convergence - Larger batch: Faster convergence, higher memory 3.3 Gradient Accumulation \u00b6 Simulate larger batch without increased memory: tuning_params = { 'batch_size' : 8 , # Actual batch 'gradient_accumulation_steps' : 4 , # Accumulate 4x # Effective batch = 32 'device' : 'cuda' } # Memory cost: Similar to batch_size=8 # Effective batch size benefit: batch_size=32 3.4 Mixed Precision Training \u00b6 Impact : 20-30% memory reduction # Standard (float32): high memory tuning_params = { 'device' : 'cuda' , 'mixed_precision' : None # Full precision } # Half precision (float16): lower memory tuning_params = { 'device' : 'cuda' , 'mixed_precision' : 'fp16' # Use 16-bit floats } # BFloat16 (better stability) tuning_params = { 'device' : 'cuda' , 'mixed_precision' : 'bf16' } Implementation : import torch from torch.cuda.amp import autocast , GradScaler # Setup for mixed precision scaler = GradScaler () # In training loop with autocast ( dtype = torch . float16 ): loss = compute_loss ( ... ) scaler . scale ( loss ) . backward () scaler . step ( optimizer ) scaler . update () 3.5 Gradient Checkpointing \u00b6 Trade computation for memory: tuning_params = { 'device' : 'cuda' , 'gradient_checkpoint' : True # Save memory at cost of computation } # Memory reduction: ~30-50% # Computation increase: ~20-30% (recompute activations) 3.6 Data Loading Optimization \u00b6 # Efficient data loading loader_config = { 'batch_size' : 32 , 'num_workers' : 4 , # Parallel loading 'pin_memory' : True , # Faster CPU\u2192GPU transfer 'persistent_workers' : True , # Keep workers alive 'prefetch_factor' : 2 # Prefetch next batches } # Or with TabTune tuning_params = { 'num_workers' : 4 , 'pin_memory' : True , 'device' : 'cuda' } 3.7 Model Architecture Reduction \u00b6 Reduce model complexity: # Large model: high memory model_params = { 'd_model' : 256 , # Embedding dimension 'num_layers' : 8 , # Transformer layers 'num_heads' : 16 # Attention heads } # Medium model: medium memory model_params = { 'd_model' : 128 , 'num_layers' : 4 , 'num_heads' : 8 } # Small model: low memory model_params = { 'd_model' : 64 , 'num_layers' : 2 , 'num_heads' : 4 } 4. Optimization Strategies by Constraint \u00b6 4.1 Severe Constraint (2GB GPU) \u00b6 pipeline = TabularPipeline ( model_name = 'TabICL' , tuning_strategy = 'peft' , model_params = { 'n_estimators' : 8 # Reduce ensemble }, tuning_params = { 'device' : 'cuda' , 'epochs' : 3 , 'learning_rate' : 2e-4 , 'batch_size' : 4 , # Very small 'support_size' : 32 , # Small context 'query_size' : 16 , 'mixed_precision' : 'fp16' , # Use half precision 'gradient_checkpoint' : True , 'num_workers' : 0 , # No parallel loading 'peft_config' : { 'r' : 2 , # Very low rank 'lora_alpha' : 4 , 'lora_dropout' : 0.2 } } ) 4.2 Moderate Constraint (4GB GPU) \u00b6 pipeline = TabularPipeline ( model_name = 'TabICL' , tuning_strategy = 'peft' , tuning_params = { 'device' : 'cuda' , 'epochs' : 5 , 'learning_rate' : 2e-4 , 'batch_size' : 8 , 'support_size' : 64 , 'query_size' : 32 , 'mixed_precision' : 'fp16' , 'num_workers' : 2 , 'peft_config' : { 'r' : 4 , 'lora_alpha' : 8 , 'lora_dropout' : 0.1 } } ) 4.3 Comfortable (8GB+ GPU) \u00b6 pipeline = TabularPipeline ( model_name = 'TabICL' , tuning_strategy = 'base-ft' , # Full fine-tuning possible tuning_params = { 'device' : 'cuda' , 'epochs' : 5 , 'learning_rate' : 2e-5 , 'batch_size' : 32 , 'support_size' : 128 , 'query_size' : 64 , 'num_workers' : 4 , 'gradient_accumulation_steps' : 2 } ) 5. Advanced Techniques \u00b6 5.1 Activation Checkpointing \u00b6 import torch.utils.checkpoint as checkpoint class CheckpointedModel ( torch . nn . Module ): \"\"\"Wrap model with checkpointing.\"\"\" def __init__ ( self , model ): super () . __init__ () self . model = model def forward ( self , x ): # Checkpoint during forward pass return checkpoint . checkpoint ( self . model , x , use_reentrant = False ) 5.2 Quantization \u00b6 Reduce model precision: import torch.quantization as quantization def quantize_model ( model , backend = 'fbgemm' ): \"\"\"Quantize model to int8.\"\"\" model . qconfig = quantization . get_default_qconfig ( backend ) quantization . prepare ( model , inplace = True ) # Calibrate on data... quantization . convert ( model , inplace = True ) return model # Usage quantized_pipeline = quantize_model ( pipeline . model ) 5.3 Parameter Sharing \u00b6 Share weights across layers: class ParameterSharingModel ( torch . nn . Module ): \"\"\"Model with shared parameters.\"\"\" def __init__ ( self , shared_layer , num_repeats ): super () . __init__ () self . shared_layer = shared_layer self . num_repeats = num_repeats def forward ( self , x ): for _ in range ( self . num_repeats ): x = self . shared_layer ( x ) return x 5.4 Knowledge Distillation \u00b6 Train smaller student model from larger teacher: # Large teacher model (high accuracy) teacher = TabularPipeline ( model_name = 'TabDPT' , tuning_strategy = 'base-ft' ) teacher . fit ( X_train , y_train ) # Small student model (low memory) student = TabularPipeline ( model_name = 'TabPFN' , tuning_strategy = 'peft' ) # Distillation: train student to match teacher for batch in dataloader : teacher_logits = teacher . model ( batch ) student_logits = student . model ( batch ) # KL divergence loss loss = nn . KLDivLoss ()( nn . log_softmax ( student_logits ), nn . softmax ( teacher_logits ) ) # ... optimize ... 6. Memory-Time Trade-offs \u00b6 6.1 Time-Memory Pareto Frontier \u00b6 configurations = [ { 'name' : 'Max Speed' , 'batch_size' : 64 , 'precision' : 'fp32' , 'time' : 30 , # minutes 'memory' : 16 # GB }, { 'name' : 'Balanced' , 'batch_size' : 32 , 'precision' : 'fp32' , 'time' : 45 , 'memory' : 12 }, { 'name' : 'Efficient' , 'batch_size' : 16 , 'precision' : 'fp16' , 'time' : 60 , 'memory' : 6 }, { 'name' : 'Ultra-Efficient' , 'batch_size' : 8 , 'precision' : 'fp16' , 'peft' : True , 'time' : 90 , 'memory' : 3 } ] 6.2 Choosing Configuration \u00b6 def choose_config ( gpu_memory_gb , time_budget_hours ): \"\"\"Choose config based on constraints.\"\"\" if gpu_memory_gb < 4 : return 'peft' # Must use PEFT elif gpu_memory_gb < 8 : return 'fp16_batch16' elif gpu_memory_gb < 16 : return 'fp16_batch32' else : return 'base_ft' # Usage config = choose_config ( gpu_memory_gb = 6 , time_budget = 4 ) 7. Monitoring During Training \u00b6 7.1 Real-time Memory Tracking \u00b6 import logging from datetime import datetime class MemoryTracker : \"\"\"Track memory throughout training.\"\"\" def __init__ ( self , log_interval = 100 ): self . log_interval = log_interval self . iteration = 0 self . history = [] def step ( self ): \"\"\"Call after each training step.\"\"\" self . iteration += 1 if self . iteration % self . log_interval == 0 : peak = torch . cuda . max_memory_allocated () / 1e9 current = torch . cuda . memory_allocated () / 1e9 self . history . append ({ 'iteration' : self . iteration , 'peak' : peak , 'current' : current , 'time' : datetime . now () }) print ( f \"[ { self . iteration } ] Peak: { peak : .2f } GB, Current: { current : .2f } GB\" ) def plot ( self ): \"\"\"Plot memory over time.\"\"\" import matplotlib.pyplot as plt iterations = [ h [ 'iteration' ] for h in self . history ] peaks = [ h [ 'peak' ] for h in self . history ] plt . plot ( iterations , peaks ) plt . xlabel ( 'Iteration' ) plt . ylabel ( 'Peak Memory (GB)' ) plt . title ( 'GPU Memory Usage Over Time' ) plt . show () # Usage tracker = MemoryTracker () for batch in dataloader : # ... training step ... tracker . step () tracker . plot () 8. Debugging Memory Issues \u00b6 8.1 OOM (Out of Memory) Error Handling \u00b6 def safe_train ( model , dataloader , max_retries = 3 ): \"\"\"Train with automatic memory adaptation.\"\"\" batch_size = 32 for attempt in range ( max_retries ): try : # Try training with current batch size for batch in dataloader : # ... training code ... pass return # Success except RuntimeError as e : if 'out of memory' in str ( e ) . lower (): # Reduce batch size and retry batch_size //= 2 print ( f \"OOM detected. Retrying with batch_size= { batch_size } \" ) # Clear cache torch . cuda . empty_cache () # Recreate dataloader with smaller batch dataloader = DataLoader ( dataset , batch_size = batch_size ) else : raise raise RuntimeError ( f \"Failed after { max_retries } attempts\" ) 8.2 Memory Leak Detection \u00b6 import tracemalloc tracemalloc . start () # Your training code pipeline . fit ( X_train , y_train ) current , peak = tracemalloc . get_traced_memory () print ( f \"Current: { current / 1e9 : .2f } GB; Peak: { peak / 1e9 : .2f } GB\" ) # Find top memory allocations snapshot = tracemalloc . take_snapshot () top_stats = snapshot . statistics ( 'lineno' ) print ( \"[ Top 10 ]\" ) for stat in top_stats [: 10 ]: print ( stat ) 9. Complete Example: Memory-Optimized Training \u00b6 from tabtune import TabularPipeline import torch def memory_optimized_training ( X_train , y_train , X_val , y_val , gpu_memory_gb = 4 ): \"\"\"Train with automatic memory optimization.\"\"\" # Determine configuration if gpu_memory_gb < 4 : use_peft = True batch_size = 4 support_size = 32 mixed_precision = 'fp16' rank = 2 elif gpu_memory_gb < 8 : use_peft = True batch_size = 8 support_size = 64 mixed_precision = 'fp16' rank = 4 else : use_peft = False batch_size = 32 support_size = 128 mixed_precision = None rank = 8 # Create pipeline strategy = 'peft' if use_peft else 'base-ft' tuning_params = { 'device' : 'cuda' , 'epochs' : 5 , 'learning_rate' : 2e-4 if use_peft else 2e-5 , 'batch_size' : batch_size , 'support_size' : support_size , 'num_workers' : 0 , 'pin_memory' : False if gpu_memory_gb < 4 else True } if mixed_precision : tuning_params [ 'mixed_precision' ] = mixed_precision if use_peft : tuning_params [ 'peft_config' ] = { 'r' : rank , 'lora_alpha' : 2 * rank , 'lora_dropout' : 0.05 } pipeline = TabularPipeline ( model_name = 'TabICL' , tuning_strategy = strategy , tuning_params = tuning_params ) # Train with monitoring print ( f \"Training with strategy= { strategy } , batch_size= { batch_size } \" ) pipeline . fit ( X_train , y_train ) # Evaluate metrics = pipeline . evaluate ( X_val , y_val ) print ( f \"Validation accuracy: { metrics [ 'accuracy' ] : .4f } \" ) return pipeline # Usage pipeline = memory_optimized_training ( X_train , y_train , X_val , y_val , gpu_memory_gb = 4 ) 10. Quick Reference \u00b6 Memory Reduction Techniques (by impact) \u00b6 Technique Memory Saving Time Overhead Effort PEFT 60-90% None Low Batch size \u00f72 50% None Low Mixed precision 20-30% 20-30% Medium Gradient accumulation 0% 0% Low Gradient checkpoint 30-50% 20-30% Medium Quantization 75% 5-10% High 11. Best Practices \u00b6 \u2705 Do's \u00b6 \u2705 Profile memory before optimization \u2705 Use PEFT for memory-constrained environments \u2705 Start with small batch sizes \u2705 Use mixed precision when possible \u2705 Monitor memory during training \u2705 Empty cache between experiments \u2705 Use gradient accumulation for large effective batches \u274c Don'ts \u00b6 \u274c Don't forget to clear cache between runs \u274c Don't use full precision when half works \u274c Don't load entire dataset to memory \u274c Don't tune without monitoring memory \u274c Don't ignore OOM errors 12. Next Steps \u00b6 Tuning Strategies - PEFT details PEFT & LoRA - Memory-efficient fine-tuning Advanced Topics - Advanced optimizations Multi-GPU - Distributed training Optimize memory strategically to train powerful models on limited resources!","title":"Memory Optimization: Techniques for Training Large Models"},{"location":"advanced/memory-optimization/#memory-optimization-techniques-for-training-large-models","text":"This document provides comprehensive strategies for optimizing memory usage when training TabTune models, enabling efficient use of limited GPU/CPU resources.","title":"Memory Optimization: Techniques for Training Large Models"},{"location":"advanced/memory-optimization/#1-introduction","text":"Memory optimization is critical for: Training on GPUs with limited VRAM (< 8GB) Processing large datasets (100K+ rows) Using large model architectures Running multiple experiments simultaneously Deploying in resource-constrained environments This guide covers techniques, tools, and trade-offs for memory efficiency.","title":"1. Introduction"},{"location":"advanced/memory-optimization/#2-memory-profiling","text":"","title":"2. Memory Profiling"},{"location":"advanced/memory-optimization/#21-understanding-memory-usage","text":"Memory Breakdown (typical forward/backward pass): Model weights: 40-50% (frozen in PEFT) Optimizer states: 20-30% (momentum, variance) Gradients: 10-15% Activations: 10-20% (intermediate values) DataLoader buffers: 5-10% \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Total: 100%","title":"2.1 Understanding Memory Usage"},{"location":"advanced/memory-optimization/#22-profiling-tools","text":"","title":"2.2 Profiling Tools"},{"location":"advanced/memory-optimization/#pytorch-memory-profiler","text":"import torch from torch.profiler import profile , record_function , ProfilerActivity # Basic memory measurement torch . cuda . empty_cache () torch . cuda . reset_peak_memory_stats () # Your training code pipeline . fit ( X_train , y_train ) # Print memory stats print ( f \"Peak memory: { torch . cuda . max_memory_allocated () / 1e9 : .2f } GB\" ) print ( f \"Current memory: { torch . cuda . memory_allocated () / 1e9 : .2f } GB\" ) # Detailed profiling with profile ( activities = [ ProfilerActivity . CPU , ProfilerActivity . CUDA ], record_shapes = True ) as prof : pipeline . fit ( X_train , y_train ) print ( prof . key_averages () . table ( sort_by = \"cuda_memory_usage\" ))","title":"PyTorch Memory Profiler"},{"location":"advanced/memory-optimization/#memory-monitoring-script","text":"import psutil import nvidia_smi class MemoryMonitor : \"\"\"Monitor memory usage during training.\"\"\" def __init__ ( self ): self . peak_gpu = 0 self . peak_cpu = 0 def record ( self ): \"\"\"Record current memory usage.\"\"\" try : nvidia_smi . nvmlInit () handle = nvidia_smi . nvmlDeviceGetHandleByIndex ( 0 ) info = nvidia_smi . nvmlDeviceGetMemoryInfo ( handle ) gpu_mem = info . used / 1e9 self . peak_gpu = max ( self . peak_gpu , gpu_mem ) except : gpu_mem = 0 # CPU memory process = psutil . Process () cpu_mem = process . memory_info () . rss / 1e9 self . peak_cpu = max ( self . peak_cpu , cpu_mem ) return gpu_mem , cpu_mem def report ( self ): \"\"\"Print memory report.\"\"\" print ( f \"Peak GPU: { self . peak_gpu : .2f } GB\" ) print ( f \"Peak CPU: { self . peak_cpu : .2f } GB\" ) # Usage monitor = MemoryMonitor () # Periodically call during training for epoch in range ( num_epochs ): gpu_mem , cpu_mem = monitor . record () print ( f \"Epoch { epoch } : GPU { gpu_mem : .2f } GB, CPU { cpu_mem : .2f } GB\" ) monitor . report ()","title":"Memory Monitoring Script"},{"location":"advanced/memory-optimization/#3-optimization-techniques","text":"","title":"3. Optimization Techniques"},{"location":"advanced/memory-optimization/#31-peft-lora-primary-technique","text":"Impact : 60-90% memory reduction # Base fine-tuning: high memory pipeline_base = TabularPipeline ( model_name = 'TabICL' , tuning_strategy = 'base-ft' , tuning_params = { 'device' : 'cuda' , 'epochs' : 5 } ) # Memory: ~12 GB for 100K samples # PEFT fine-tuning: low memory pipeline_peft = TabularPipeline ( model_name = 'TabICL' , tuning_strategy = 'peft' , tuning_params = { 'device' : 'cuda' , 'epochs' : 5 , 'peft_config' : { 'r' : 8 } } ) # Memory: ~4 GB for same task Choosing PEFT config for memory : # Ultra-constrained (2GB GPU) peft_config = { 'r' : 2 , 'lora_alpha' : 4 , 'lora_dropout' : 0.2 } # Memory-constrained (4GB GPU) peft_config = { 'r' : 4 , 'lora_alpha' : 8 , 'lora_dropout' : 0.1 } # Moderate constraint (6GB GPU) peft_config = { 'r' : 8 , 'lora_alpha' : 16 , 'lora_dropout' : 0.05 }","title":"3.1 PEFT (LoRA) - Primary Technique"},{"location":"advanced/memory-optimization/#32-batch-size-reduction","text":"Impact : 20-40% memory reduction per halving # Large batch (high memory) tuning_params = { 'batch_size' : 64 , 'support_size' : 512 } # Memory: ~12 GB # Reduced batch (medium memory) tuning_params = { 'batch_size' : 32 , 'support_size' : 256 } # Memory: ~6 GB # Small batch (low memory) tuning_params = { 'batch_size' : 8 , 'support_size' : 64 } # Memory: ~2 GB Trade-offs : - Smaller batch: More gradient noise, longer convergence - Larger batch: Faster convergence, higher memory","title":"3.2 Batch Size Reduction"},{"location":"advanced/memory-optimization/#33-gradient-accumulation","text":"Simulate larger batch without increased memory: tuning_params = { 'batch_size' : 8 , # Actual batch 'gradient_accumulation_steps' : 4 , # Accumulate 4x # Effective batch = 32 'device' : 'cuda' } # Memory cost: Similar to batch_size=8 # Effective batch size benefit: batch_size=32","title":"3.3 Gradient Accumulation"},{"location":"advanced/memory-optimization/#34-mixed-precision-training","text":"Impact : 20-30% memory reduction # Standard (float32): high memory tuning_params = { 'device' : 'cuda' , 'mixed_precision' : None # Full precision } # Half precision (float16): lower memory tuning_params = { 'device' : 'cuda' , 'mixed_precision' : 'fp16' # Use 16-bit floats } # BFloat16 (better stability) tuning_params = { 'device' : 'cuda' , 'mixed_precision' : 'bf16' } Implementation : import torch from torch.cuda.amp import autocast , GradScaler # Setup for mixed precision scaler = GradScaler () # In training loop with autocast ( dtype = torch . float16 ): loss = compute_loss ( ... ) scaler . scale ( loss ) . backward () scaler . step ( optimizer ) scaler . update ()","title":"3.4 Mixed Precision Training"},{"location":"advanced/memory-optimization/#35-gradient-checkpointing","text":"Trade computation for memory: tuning_params = { 'device' : 'cuda' , 'gradient_checkpoint' : True # Save memory at cost of computation } # Memory reduction: ~30-50% # Computation increase: ~20-30% (recompute activations)","title":"3.5 Gradient Checkpointing"},{"location":"advanced/memory-optimization/#36-data-loading-optimization","text":"# Efficient data loading loader_config = { 'batch_size' : 32 , 'num_workers' : 4 , # Parallel loading 'pin_memory' : True , # Faster CPU\u2192GPU transfer 'persistent_workers' : True , # Keep workers alive 'prefetch_factor' : 2 # Prefetch next batches } # Or with TabTune tuning_params = { 'num_workers' : 4 , 'pin_memory' : True , 'device' : 'cuda' }","title":"3.6 Data Loading Optimization"},{"location":"advanced/memory-optimization/#37-model-architecture-reduction","text":"Reduce model complexity: # Large model: high memory model_params = { 'd_model' : 256 , # Embedding dimension 'num_layers' : 8 , # Transformer layers 'num_heads' : 16 # Attention heads } # Medium model: medium memory model_params = { 'd_model' : 128 , 'num_layers' : 4 , 'num_heads' : 8 } # Small model: low memory model_params = { 'd_model' : 64 , 'num_layers' : 2 , 'num_heads' : 4 }","title":"3.7 Model Architecture Reduction"},{"location":"advanced/memory-optimization/#4-optimization-strategies-by-constraint","text":"","title":"4. Optimization Strategies by Constraint"},{"location":"advanced/memory-optimization/#41-severe-constraint-2gb-gpu","text":"pipeline = TabularPipeline ( model_name = 'TabICL' , tuning_strategy = 'peft' , model_params = { 'n_estimators' : 8 # Reduce ensemble }, tuning_params = { 'device' : 'cuda' , 'epochs' : 3 , 'learning_rate' : 2e-4 , 'batch_size' : 4 , # Very small 'support_size' : 32 , # Small context 'query_size' : 16 , 'mixed_precision' : 'fp16' , # Use half precision 'gradient_checkpoint' : True , 'num_workers' : 0 , # No parallel loading 'peft_config' : { 'r' : 2 , # Very low rank 'lora_alpha' : 4 , 'lora_dropout' : 0.2 } } )","title":"4.1 Severe Constraint (2GB GPU)"},{"location":"advanced/memory-optimization/#42-moderate-constraint-4gb-gpu","text":"pipeline = TabularPipeline ( model_name = 'TabICL' , tuning_strategy = 'peft' , tuning_params = { 'device' : 'cuda' , 'epochs' : 5 , 'learning_rate' : 2e-4 , 'batch_size' : 8 , 'support_size' : 64 , 'query_size' : 32 , 'mixed_precision' : 'fp16' , 'num_workers' : 2 , 'peft_config' : { 'r' : 4 , 'lora_alpha' : 8 , 'lora_dropout' : 0.1 } } )","title":"4.2 Moderate Constraint (4GB GPU)"},{"location":"advanced/memory-optimization/#43-comfortable-8gb-gpu","text":"pipeline = TabularPipeline ( model_name = 'TabICL' , tuning_strategy = 'base-ft' , # Full fine-tuning possible tuning_params = { 'device' : 'cuda' , 'epochs' : 5 , 'learning_rate' : 2e-5 , 'batch_size' : 32 , 'support_size' : 128 , 'query_size' : 64 , 'num_workers' : 4 , 'gradient_accumulation_steps' : 2 } )","title":"4.3 Comfortable (8GB+ GPU)"},{"location":"advanced/memory-optimization/#5-advanced-techniques","text":"","title":"5. Advanced Techniques"},{"location":"advanced/memory-optimization/#51-activation-checkpointing","text":"import torch.utils.checkpoint as checkpoint class CheckpointedModel ( torch . nn . Module ): \"\"\"Wrap model with checkpointing.\"\"\" def __init__ ( self , model ): super () . __init__ () self . model = model def forward ( self , x ): # Checkpoint during forward pass return checkpoint . checkpoint ( self . model , x , use_reentrant = False )","title":"5.1 Activation Checkpointing"},{"location":"advanced/memory-optimization/#52-quantization","text":"Reduce model precision: import torch.quantization as quantization def quantize_model ( model , backend = 'fbgemm' ): \"\"\"Quantize model to int8.\"\"\" model . qconfig = quantization . get_default_qconfig ( backend ) quantization . prepare ( model , inplace = True ) # Calibrate on data... quantization . convert ( model , inplace = True ) return model # Usage quantized_pipeline = quantize_model ( pipeline . model )","title":"5.2 Quantization"},{"location":"advanced/memory-optimization/#53-parameter-sharing","text":"Share weights across layers: class ParameterSharingModel ( torch . nn . Module ): \"\"\"Model with shared parameters.\"\"\" def __init__ ( self , shared_layer , num_repeats ): super () . __init__ () self . shared_layer = shared_layer self . num_repeats = num_repeats def forward ( self , x ): for _ in range ( self . num_repeats ): x = self . shared_layer ( x ) return x","title":"5.3 Parameter Sharing"},{"location":"advanced/memory-optimization/#54-knowledge-distillation","text":"Train smaller student model from larger teacher: # Large teacher model (high accuracy) teacher = TabularPipeline ( model_name = 'TabDPT' , tuning_strategy = 'base-ft' ) teacher . fit ( X_train , y_train ) # Small student model (low memory) student = TabularPipeline ( model_name = 'TabPFN' , tuning_strategy = 'peft' ) # Distillation: train student to match teacher for batch in dataloader : teacher_logits = teacher . model ( batch ) student_logits = student . model ( batch ) # KL divergence loss loss = nn . KLDivLoss ()( nn . log_softmax ( student_logits ), nn . softmax ( teacher_logits ) ) # ... optimize ...","title":"5.4 Knowledge Distillation"},{"location":"advanced/memory-optimization/#6-memory-time-trade-offs","text":"","title":"6. Memory-Time Trade-offs"},{"location":"advanced/memory-optimization/#61-time-memory-pareto-frontier","text":"configurations = [ { 'name' : 'Max Speed' , 'batch_size' : 64 , 'precision' : 'fp32' , 'time' : 30 , # minutes 'memory' : 16 # GB }, { 'name' : 'Balanced' , 'batch_size' : 32 , 'precision' : 'fp32' , 'time' : 45 , 'memory' : 12 }, { 'name' : 'Efficient' , 'batch_size' : 16 , 'precision' : 'fp16' , 'time' : 60 , 'memory' : 6 }, { 'name' : 'Ultra-Efficient' , 'batch_size' : 8 , 'precision' : 'fp16' , 'peft' : True , 'time' : 90 , 'memory' : 3 } ]","title":"6.1 Time-Memory Pareto Frontier"},{"location":"advanced/memory-optimization/#62-choosing-configuration","text":"def choose_config ( gpu_memory_gb , time_budget_hours ): \"\"\"Choose config based on constraints.\"\"\" if gpu_memory_gb < 4 : return 'peft' # Must use PEFT elif gpu_memory_gb < 8 : return 'fp16_batch16' elif gpu_memory_gb < 16 : return 'fp16_batch32' else : return 'base_ft' # Usage config = choose_config ( gpu_memory_gb = 6 , time_budget = 4 )","title":"6.2 Choosing Configuration"},{"location":"advanced/memory-optimization/#7-monitoring-during-training","text":"","title":"7. Monitoring During Training"},{"location":"advanced/memory-optimization/#71-real-time-memory-tracking","text":"import logging from datetime import datetime class MemoryTracker : \"\"\"Track memory throughout training.\"\"\" def __init__ ( self , log_interval = 100 ): self . log_interval = log_interval self . iteration = 0 self . history = [] def step ( self ): \"\"\"Call after each training step.\"\"\" self . iteration += 1 if self . iteration % self . log_interval == 0 : peak = torch . cuda . max_memory_allocated () / 1e9 current = torch . cuda . memory_allocated () / 1e9 self . history . append ({ 'iteration' : self . iteration , 'peak' : peak , 'current' : current , 'time' : datetime . now () }) print ( f \"[ { self . iteration } ] Peak: { peak : .2f } GB, Current: { current : .2f } GB\" ) def plot ( self ): \"\"\"Plot memory over time.\"\"\" import matplotlib.pyplot as plt iterations = [ h [ 'iteration' ] for h in self . history ] peaks = [ h [ 'peak' ] for h in self . history ] plt . plot ( iterations , peaks ) plt . xlabel ( 'Iteration' ) plt . ylabel ( 'Peak Memory (GB)' ) plt . title ( 'GPU Memory Usage Over Time' ) plt . show () # Usage tracker = MemoryTracker () for batch in dataloader : # ... training step ... tracker . step () tracker . plot ()","title":"7.1 Real-time Memory Tracking"},{"location":"advanced/memory-optimization/#8-debugging-memory-issues","text":"","title":"8. Debugging Memory Issues"},{"location":"advanced/memory-optimization/#81-oom-out-of-memory-error-handling","text":"def safe_train ( model , dataloader , max_retries = 3 ): \"\"\"Train with automatic memory adaptation.\"\"\" batch_size = 32 for attempt in range ( max_retries ): try : # Try training with current batch size for batch in dataloader : # ... training code ... pass return # Success except RuntimeError as e : if 'out of memory' in str ( e ) . lower (): # Reduce batch size and retry batch_size //= 2 print ( f \"OOM detected. Retrying with batch_size= { batch_size } \" ) # Clear cache torch . cuda . empty_cache () # Recreate dataloader with smaller batch dataloader = DataLoader ( dataset , batch_size = batch_size ) else : raise raise RuntimeError ( f \"Failed after { max_retries } attempts\" )","title":"8.1 OOM (Out of Memory) Error Handling"},{"location":"advanced/memory-optimization/#82-memory-leak-detection","text":"import tracemalloc tracemalloc . start () # Your training code pipeline . fit ( X_train , y_train ) current , peak = tracemalloc . get_traced_memory () print ( f \"Current: { current / 1e9 : .2f } GB; Peak: { peak / 1e9 : .2f } GB\" ) # Find top memory allocations snapshot = tracemalloc . take_snapshot () top_stats = snapshot . statistics ( 'lineno' ) print ( \"[ Top 10 ]\" ) for stat in top_stats [: 10 ]: print ( stat )","title":"8.2 Memory Leak Detection"},{"location":"advanced/memory-optimization/#9-complete-example-memory-optimized-training","text":"from tabtune import TabularPipeline import torch def memory_optimized_training ( X_train , y_train , X_val , y_val , gpu_memory_gb = 4 ): \"\"\"Train with automatic memory optimization.\"\"\" # Determine configuration if gpu_memory_gb < 4 : use_peft = True batch_size = 4 support_size = 32 mixed_precision = 'fp16' rank = 2 elif gpu_memory_gb < 8 : use_peft = True batch_size = 8 support_size = 64 mixed_precision = 'fp16' rank = 4 else : use_peft = False batch_size = 32 support_size = 128 mixed_precision = None rank = 8 # Create pipeline strategy = 'peft' if use_peft else 'base-ft' tuning_params = { 'device' : 'cuda' , 'epochs' : 5 , 'learning_rate' : 2e-4 if use_peft else 2e-5 , 'batch_size' : batch_size , 'support_size' : support_size , 'num_workers' : 0 , 'pin_memory' : False if gpu_memory_gb < 4 else True } if mixed_precision : tuning_params [ 'mixed_precision' ] = mixed_precision if use_peft : tuning_params [ 'peft_config' ] = { 'r' : rank , 'lora_alpha' : 2 * rank , 'lora_dropout' : 0.05 } pipeline = TabularPipeline ( model_name = 'TabICL' , tuning_strategy = strategy , tuning_params = tuning_params ) # Train with monitoring print ( f \"Training with strategy= { strategy } , batch_size= { batch_size } \" ) pipeline . fit ( X_train , y_train ) # Evaluate metrics = pipeline . evaluate ( X_val , y_val ) print ( f \"Validation accuracy: { metrics [ 'accuracy' ] : .4f } \" ) return pipeline # Usage pipeline = memory_optimized_training ( X_train , y_train , X_val , y_val , gpu_memory_gb = 4 )","title":"9. Complete Example: Memory-Optimized Training"},{"location":"advanced/memory-optimization/#10-quick-reference","text":"","title":"10. Quick Reference"},{"location":"advanced/memory-optimization/#memory-reduction-techniques-by-impact","text":"Technique Memory Saving Time Overhead Effort PEFT 60-90% None Low Batch size \u00f72 50% None Low Mixed precision 20-30% 20-30% Medium Gradient accumulation 0% 0% Low Gradient checkpoint 30-50% 20-30% Medium Quantization 75% 5-10% High","title":"Memory Reduction Techniques (by impact)"},{"location":"advanced/memory-optimization/#11-best-practices","text":"","title":"11. Best Practices"},{"location":"advanced/memory-optimization/#dos","text":"\u2705 Profile memory before optimization \u2705 Use PEFT for memory-constrained environments \u2705 Start with small batch sizes \u2705 Use mixed precision when possible \u2705 Monitor memory during training \u2705 Empty cache between experiments \u2705 Use gradient accumulation for large effective batches","title":"\u2705 Do's"},{"location":"advanced/memory-optimization/#donts","text":"\u274c Don't forget to clear cache between runs \u274c Don't use full precision when half works \u274c Don't load entire dataset to memory \u274c Don't tune without monitoring memory \u274c Don't ignore OOM errors","title":"\u274c Don'ts"},{"location":"advanced/memory-optimization/#12-next-steps","text":"Tuning Strategies - PEFT details PEFT & LoRA - Memory-efficient fine-tuning Advanced Topics - Advanced optimizations Multi-GPU - Distributed training Optimize memory strategically to train powerful models on limited resources!","title":"12. Next Steps"},{"location":"advanced/multi-gpu/","text":"Multi-GPU Training: Scaling TabTune Across Multiple GPUs \u00b6 This document provides comprehensive guidance on leveraging multiple GPUs for distributed training with TabTune, enabling faster training and handling of larger models and datasets. 1. Introduction \u00b6 Multi-GPU training accelerates TabTune workflows through: Data Parallelism : Distribute data across GPUs Model Parallelism : Distribute model across GPUs Distributed Optimization : Synchronized gradient updates Scaling : Near-linear speedup with multiple GPUs This guide covers setup, strategies, and best practices. 2. Multi-GPU Fundamentals \u00b6 2.1 Parallelism Strategies \u00b6 flowchart TD A[Multi-GPU Training] --> B[Data Parallelism] A --> C[Model Parallelism] A --> D[Pipeline Parallelism] B --> B1[\"Each GPU gets data batch\"] B --> B2[\"Model replicated on each GPU\"] B --> B3[\"Gradients averaged across GPUs\"] C --> C1[\"Model layers split across GPUs\"] C --> C2[\"Each GPU processes different layers\"] C --> C3[\"Communication between GPUs\"] D --> D1[\"Stages of model pipeline\"] D --> D2[\"Different stages on different GPUs\"] D --> D3[\"Micro-batching for efficiency\"] 2.2 Data Parallelism (Most Common) \u00b6 Recommended for TabTune - simplest and most effective: # Single GPU training model = TabularPipeline ( model_name = 'TabICL' ) # Loss: 100% on GPU 0 # Data parallel (2 GPUs) # Batch split into 2 sub-batches # GPU 0: sub-batch 1 # GPU 1: sub-batch 2 # Gradients averaged 3. Setup Requirements \u00b6 3.1 Hardware Prerequisites \u00b6 import torch # Check GPU availability print ( f \"GPUs available: { torch . cuda . device_count () } \" ) print ( f \"Current GPU: { torch . cuda . current_device () } \" ) for i in range ( torch . cuda . device_count ()): gpu = torch . cuda . get_device_properties ( i ) print ( f \"GPU { i } : { gpu . name } ( { gpu . total_memory / 1e9 : .1f } GB)\" ) # Recommended print ( f \" \\n Availability check:\" ) print ( f \"CUDA available: { torch . cuda . is_available () } \" ) print ( f \"NCCL available: { torch . distributed . is_nccl_available () } \" ) 3.2 Software Stack \u00b6 # Install required packages pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121 pip install torch-distributed-rpc pip install horovod # Optional: for advanced distributed training pip install pytorch-lightning # Optional: simplified multi-GPU setup 4. Data Parallel Training \u00b6 4.1 DataParallel (Simpler, Single-Machine) \u00b6 import torch import torch.nn as nn class DataParallelWrapper : \"\"\"Wrap TabTune pipeline with DataParallel.\"\"\" def __init__ ( self , model , device_ids = None ): self . model = nn . DataParallel ( model , device_ids = device_ids ) self . device_ids = device_ids or list ( range ( torch . cuda . device_count ())) def fit ( self , X_train , y_train , ** kwargs ): \"\"\"Training with DataParallel.\"\"\" # Batch automatically split across GPUs return self . model . module . fit ( X_train , y_train , ** kwargs ) def predict ( self , X_test ): \"\"\"Inference on primary GPU.\"\"\" return self . model . module . predict ( X_test ) # Usage if torch . cuda . device_count () > 1 : model = DataParallelWrapper ( TabularPipeline ( model_name = 'TabICL' ), device_ids = [ 0 , 1 , 2 , 3 ] # Use GPUs 0-3 ) else : model = TabularPipeline ( model_name = 'TabICL' ) model . fit ( X_train , y_train ) 4.2 DistributedDataParallel (Recommended) \u00b6 More efficient than DataParallel: import torch import torch.distributed as dist from torch.nn.parallel import DistributedDataParallel def setup_distributed (): \"\"\"Setup distributed training environment.\"\"\" dist . init_process_group ( backend = 'nccl' , # NVIDIA Collective Communications Library init_method = 'env://' ) def cleanup (): \"\"\"Cleanup distributed environment.\"\"\" dist . destroy_process_group () def train_distributed (): \"\"\"Distributed training.\"\"\" setup_distributed () # Get process rank and world size rank = dist . get_rank () world_size = dist . get_world_size () # Create model on current device device = torch . device ( f 'cuda: { rank } ' ) pipeline = TabularPipeline ( model_name = 'TabICL' , tuning_strategy = 'base-ft' , tuning_params = { 'device' : device } ) # Wrap with DistributedDataParallel model = DistributedDataParallel ( pipeline . model , device_ids = [ rank ]) # Training code model . fit ( X_train , y_train ) cleanup () # Launch with: python -m torch.distributed.launch --nproc_per_node=4 script.py 5. Launch Methods \u00b6 5.1 torchrun (PyTorch 1.10+) \u00b6 Recommended method : # Single-machine, 4 GPUs torchrun --nproc_per_node = 4 train_script.py # Multi-machine (8 GPUs total) torchrun \\ --nproc_per_node = 4 \\ --nnodes = 2 \\ --node_rank = 0 \\ --master_addr = 192 .168.1.100 \\ --master_port = 29500 \\ train_script.py 5.2 torch.distributed.launch (Legacy) \u00b6 # Single-machine, 4 GPUs python -m torch.distributed.launch \\ --nproc_per_node = 4 \\ train_script.py # With additional args python -m torch.distributed.launch \\ --nproc_per_node = 4 \\ train_script.py \\ --learning_rate 2e-5 \\ --epochs 5 5.3 Manual Launch \u00b6 # Terminal 1: GPU 0 CUDA_VISIBLE_DEVICES = 0 python train_script.py # Terminal 2: GPU 1 CUDA_VISIBLE_DEVICES = 1 python train_script.py # Terminal 3: GPU 2 CUDA_VISIBLE_DEVICES = 2 python train_script.py # Terminal 4: GPU 3 CUDA_VISIBLE_DEVICES = 3 python train_script.py 6. Training Scripts \u00b6 6.1 Complete Distributed Training Script \u00b6 import os import torch import torch.distributed as dist from torch.nn.parallel import DistributedDataParallel from torch.utils.data import DataLoader , DistributedSampler from tabtune import TabularPipeline def main (): \"\"\"Main training function.\"\"\" # Initialize distributed training dist . init_process_group ( backend = 'nccl' ) rank = dist . get_rank () world_size = dist . get_world_size () # Set device device = torch . device ( f 'cuda: { rank } ' ) torch . cuda . set_device ( device ) # Print rank info if rank == 0 : print ( f \"Training on { world_size } GPUs\" ) # Load data X_train , y_train = load_data () # Create distributed sampler train_sampler = DistributedSampler ( dataset = range ( len ( X_train )), num_replicas = world_size , rank = rank , shuffle = True ) # Create dataloader train_loader = DataLoader ( train_sampler , batch_size = 32 , num_workers = 2 ) # Create model pipeline = TabularPipeline ( model_name = 'TabICL' , tuning_strategy = 'base-ft' , tuning_params = { 'device' : device , 'epochs' : 5 , 'learning_rate' : 2e-5 } ) # Wrap with DistributedDataParallel pipeline . model = DistributedDataParallel ( pipeline . model , device_ids = [ rank ] ) # Training if rank == 0 : print ( \"Starting training...\" ) pipeline . fit ( X_train [ train_sampler . indices ], y_train [ train_sampler . indices ]) # Evaluation on rank 0 only if rank == 0 : metrics = pipeline . evaluate ( X_test , y_test ) print ( f \"Validation Accuracy: { metrics [ 'accuracy' ] : .4f } \" ) # Cleanup dist . destroy_process_group () if __name__ == '__main__' : main () 6.2 Synchronization Across Ranks \u00b6 def train_with_sync (): \"\"\"Training with synchronization points.\"\"\" dist . init_process_group ( backend = 'nccl' ) rank = dist . get_rank () # Training for epoch in range ( num_epochs ): # ... training code ... # Synchronize all ranks dist . barrier () # Evaluation (on rank 0 only) if rank == 0 : metrics = evaluate () print ( f \"Epoch { epoch } : { metrics } \" ) # Broadcast best checkpoint from rank 0 if rank == 0 : best_state = pipeline . model . state_dict () else : best_state = None # Distribute to all ranks dist . broadcast_object_list ([ best_state ], src = 0 ) if rank != 0 : pipeline . model . load_state_dict ( best_state [ 0 ]) dist . destroy_process_group () 7. Performance Optimization \u00b6 7.1 Gradient Accumulation \u00b6 Increase effective batch size without memory increase: def train_with_accumulation ( num_accumulation_steps = 4 ): \"\"\"Training with gradient accumulation.\"\"\" optimizer = torch . optim . AdamW ( model . parameters ()) for epoch in range ( num_epochs ): for i , batch in enumerate ( dataloader ): # Forward pass loss = model ( batch ) # Backward (accumulate gradients) ( loss / num_accumulation_steps ) . backward () # Update weights if ( i + 1 ) % num_accumulation_steps == 0 : optimizer . step () optimizer . zero_grad () 7.2 Overlapping Computation & Communication \u00b6 def train_with_overlap (): \"\"\"Training with async communication.\"\"\" for epoch in range ( num_epochs ): for batch in dataloader : # Forward & backward (computation) loss = model ( batch ) loss . backward () # Start async gradient reduction reduction_future = dist . all_reduce ( model . grad , async_op = True # Non-blocking ) # Do other work while reducing # ... # Wait for reduction to complete reduction_future . wait () # Update weights optimizer . step () 7.3 Mixed Precision with Distributed Training \u00b6 from torch.cuda.amp import autocast , GradScaler def train_mixed_precision (): \"\"\"Multi-GPU training with mixed precision.\"\"\" scaler = GradScaler () for epoch in range ( num_epochs ): for batch in dataloader : # Forward with autocast with autocast ( dtype = torch . float16 ): loss = model ( batch ) # Backward with scaling scaler . scale ( loss ) . backward () # All-reduce scaled gradients for param in model . parameters (): if param . grad is not None : dist . all_reduce ( param . grad ) # Update weights scaler . step ( optimizer ) scaler . update () 8. Scaling Efficiency \u00b6 8.1 Linear Scaling Rule \u00b6 # Optimal learning rate for N GPUs base_learning_rate = 2e-5 num_gpus = 4 optimal_learning_rate = base_learning_rate * num_gpus # Why: Batch size \u00d7 num_gpus, so learning rate should scale # Or more conservatively: optimal_learning_rate = base_learning_rate * ( num_gpus ** 0.5 ) 8.2 Speedup Analysis \u00b6 def analyze_scaling ( times_single_gpu , times_multi_gpu ): \"\"\"Analyze multi-GPU speedup.\"\"\" num_gpus = len ( times_multi_gpu ) speedup = times_single_gpu / times_multi_gpu efficiency = speedup / num_gpus # Ideally 1.0 (100%) print ( f \"GPU Count: { num_gpus } \" ) print ( f \"Time (1 GPU): { times_single_gpu : .2f } s\" ) print ( f \"Time ( { num_gpus } GPUs): { times_multi_gpu : .2f } s\" ) print ( f \"Speedup: { speedup : .2f } x ( { efficiency * 100 : .1f } % efficiency)\" ) return speedup , efficiency # Typical results # 2 GPUs: 1.8x speedup (90% efficiency) # 4 GPUs: 3.5x speedup (87.5% efficiency) # 8 GPUs: 6.5x speedup (81% efficiency) 9. Distributed Challenges & Solutions \u00b6 9.1 Communication Overhead \u00b6 # Problem: Communication becomes bottleneck # Solution 1: Larger batch size tuning_params = { 'batch_size' : 128 , # Instead of 32 } # Solution 2: Gradient accumulation tuning_params = { 'batch_size' : 32 , 'gradient_accumulation_steps' : 4 # Effective: 128 } # Solution 3: Reduce communication frequency # Communicate every N steps instead of every step 9.2 Load Imbalance \u00b6 # Problem: Some GPUs finish before others # Solution: Dynamic load balancing def balanced_sampler ( dataset , num_replicas , rank ): \"\"\"Create balanced sampler across ranks.\"\"\" # Ensure each rank gets similar amount of work samples_per_rank = len ( dataset ) // num_replicas remainder = len ( dataset ) % num_replicas start = rank * samples_per_rank + min ( rank , remainder ) end = start + samples_per_rank + ( 1 if rank < remainder else 0 ) indices = list ( range ( start , end )) return DistributedSampler ( indices , num_replicas = num_replicas , rank = rank ) 9.3 Gradient Divergence \u00b6 # Problem: Different GPUs compute different gradients # Solution: Proper synchronization def synchronized_training (): \"\"\"Ensure all ranks have synchronized gradients.\"\"\" for batch in dataloader : # Compute gradients loss = model ( batch ) loss . backward () # Synchronize gradients across all ranks for param in model . parameters (): dist . all_reduce ( param . grad ) param . grad /= world_size # Average # Update optimizer . step () 10. Complete Multi-GPU Example \u00b6 #!/usr/bin/env python \"\"\"Multi-GPU training example.\"\"\" import os import torch import torch.distributed as dist from torch.nn.parallel import DistributedDataParallel from tabtune import TabularPipeline import argparse def main (): parser = argparse . ArgumentParser () parser . add_argument ( '--epochs' , type = int , default = 5 ) parser . add_argument ( '--batch_size' , type = int , default = 32 ) parser . add_argument ( '--learning_rate' , type = float , default = 2e-5 ) args = parser . parse_args () # Initialize distributed training dist . init_process_group ( backend = 'nccl' ) rank = dist . get_rank () world_size = dist . get_world_size () device = torch . device ( f 'cuda: { rank } ' ) if rank == 0 : print ( f \"Starting training on { world_size } GPUs\" ) # Load data X_train , y_train , X_test , y_test = load_dataset () # Scale learning rate with batch size scaled_lr = args . learning_rate * world_size # Create pipeline pipeline = TabularPipeline ( model_name = 'TabICL' , tuning_strategy = 'base-ft' , tuning_params = { 'device' : device , 'epochs' : args . epochs , 'learning_rate' : scaled_lr , 'batch_size' : args . batch_size } ) # Wrap with DDP pipeline . model = DistributedDataParallel ( pipeline . model , device_ids = [ rank ] ) # Training if rank == 0 : print ( f \"Training with LR= { scaled_lr : .2e } , batch_size= { args . batch_size } \" ) pipeline . fit ( X_train , y_train ) # Synchronize before evaluation dist . barrier () # Evaluation on primary GPU only if rank == 0 : metrics = pipeline . evaluate ( X_test , y_test ) print ( f \"Final accuracy: { metrics [ 'accuracy' ] : .4f } \" ) dist . destroy_process_group () if __name__ == '__main__' : main () Launch: torchrun --nproc_per_node = 4 train_distributed.py --epochs 5 --batch_size 32 11. Debugging Multi-GPU Issues \u00b6 11.1 Common Problems \u00b6 # Problem: Hanging/deadlock # Solution: Use timeout and debug flags os . environ [ 'NCCL_DEBUG' ] = 'INFO' os . environ [ 'TORCH_DISTRIBUTED_DEBUG' ] = 'DETAIL' dist . init_process_group ( backend = 'nccl' , timeout = timedelta ( minutes = 30 ) ) # Problem: GPU memory imbalance # Solution: Check and balance if rank == 0 : for i in range ( world_size ): print ( f \"GPU { i } : { torch . cuda . get_device_properties ( i ) . total_memory / 1e9 : .1f } GB\" ) # Problem: Rank synchronization issues # Solution: Add explicit barriers dist . barrier () # Wait for all ranks 12. Best Practices \u00b6 \u2705 Do's \u00b6 \u2705 Use DistributedDataParallel over DataParallel \u2705 Scale learning rate with batch size \u2705 Use proper samplers (DistributedSampler) \u2705 Synchronize at checkpoints \u2705 Evaluate on rank 0 only \u2705 Monitor all GPU memory usage \u2705 Test on fewer GPUs first \u274c Don'ts \u00b6 \u274c Don't use DataParallel for multi-machine \u274c Don't forget to set environment variables \u274c Don't run inference on all ranks \u274c Don't ignore communication overhead \u274c Don't over-subscribe GPU memory \u274c Don't change data on different ranks 13. Performance Benchmarks \u00b6 Dataset: 500K samples, TabICL model 1 GPU: Time: 120 minutes Memory: 12 GB 2 GPUs (DDP): Time: 65 minutes (1.85x speedup) Memory: 6 GB per GPU Efficiency: 92.5% 4 GPUs (DDP): Time: 35 minutes (3.43x speedup) Memory: 3 GB per GPU Efficiency: 85.7% 8 GPUs (DDP): Time: 20 minutes (6.0x speedup) Memory: 1.5 GB per GPU Efficiency: 75% 14. Quick Reference \u00b6 Aspect Single GPU Multi-GPU DDP Setup Simple Moderate Communication None NCCL Speedup 1x ~(GPUs-0.3) Memory/GPU Full Full/GPUs Best Use Development Production 15. Next Steps \u00b6 Memory Optimization - Memory management with DDP Hyperparameter Tuning - Scaling learning rates Tuning Strategies - PEFT with DDP Examples - Multi-GPU benchmarks Scale TabTune efficiently across multiple GPUs for production-grade training!","title":"Multi-GPU Training: Scaling TabTune Across Multiple GPUs"},{"location":"advanced/multi-gpu/#multi-gpu-training-scaling-tabtune-across-multiple-gpus","text":"This document provides comprehensive guidance on leveraging multiple GPUs for distributed training with TabTune, enabling faster training and handling of larger models and datasets.","title":"Multi-GPU Training: Scaling TabTune Across Multiple GPUs"},{"location":"advanced/multi-gpu/#1-introduction","text":"Multi-GPU training accelerates TabTune workflows through: Data Parallelism : Distribute data across GPUs Model Parallelism : Distribute model across GPUs Distributed Optimization : Synchronized gradient updates Scaling : Near-linear speedup with multiple GPUs This guide covers setup, strategies, and best practices.","title":"1. Introduction"},{"location":"advanced/multi-gpu/#2-multi-gpu-fundamentals","text":"","title":"2. Multi-GPU Fundamentals"},{"location":"advanced/multi-gpu/#21-parallelism-strategies","text":"flowchart TD A[Multi-GPU Training] --> B[Data Parallelism] A --> C[Model Parallelism] A --> D[Pipeline Parallelism] B --> B1[\"Each GPU gets data batch\"] B --> B2[\"Model replicated on each GPU\"] B --> B3[\"Gradients averaged across GPUs\"] C --> C1[\"Model layers split across GPUs\"] C --> C2[\"Each GPU processes different layers\"] C --> C3[\"Communication between GPUs\"] D --> D1[\"Stages of model pipeline\"] D --> D2[\"Different stages on different GPUs\"] D --> D3[\"Micro-batching for efficiency\"]","title":"2.1 Parallelism Strategies"},{"location":"advanced/multi-gpu/#22-data-parallelism-most-common","text":"Recommended for TabTune - simplest and most effective: # Single GPU training model = TabularPipeline ( model_name = 'TabICL' ) # Loss: 100% on GPU 0 # Data parallel (2 GPUs) # Batch split into 2 sub-batches # GPU 0: sub-batch 1 # GPU 1: sub-batch 2 # Gradients averaged","title":"2.2 Data Parallelism (Most Common)"},{"location":"advanced/multi-gpu/#3-setup-requirements","text":"","title":"3. Setup Requirements"},{"location":"advanced/multi-gpu/#31-hardware-prerequisites","text":"import torch # Check GPU availability print ( f \"GPUs available: { torch . cuda . device_count () } \" ) print ( f \"Current GPU: { torch . cuda . current_device () } \" ) for i in range ( torch . cuda . device_count ()): gpu = torch . cuda . get_device_properties ( i ) print ( f \"GPU { i } : { gpu . name } ( { gpu . total_memory / 1e9 : .1f } GB)\" ) # Recommended print ( f \" \\n Availability check:\" ) print ( f \"CUDA available: { torch . cuda . is_available () } \" ) print ( f \"NCCL available: { torch . distributed . is_nccl_available () } \" )","title":"3.1 Hardware Prerequisites"},{"location":"advanced/multi-gpu/#32-software-stack","text":"# Install required packages pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121 pip install torch-distributed-rpc pip install horovod # Optional: for advanced distributed training pip install pytorch-lightning # Optional: simplified multi-GPU setup","title":"3.2 Software Stack"},{"location":"advanced/multi-gpu/#4-data-parallel-training","text":"","title":"4. Data Parallel Training"},{"location":"advanced/multi-gpu/#41-dataparallel-simpler-single-machine","text":"import torch import torch.nn as nn class DataParallelWrapper : \"\"\"Wrap TabTune pipeline with DataParallel.\"\"\" def __init__ ( self , model , device_ids = None ): self . model = nn . DataParallel ( model , device_ids = device_ids ) self . device_ids = device_ids or list ( range ( torch . cuda . device_count ())) def fit ( self , X_train , y_train , ** kwargs ): \"\"\"Training with DataParallel.\"\"\" # Batch automatically split across GPUs return self . model . module . fit ( X_train , y_train , ** kwargs ) def predict ( self , X_test ): \"\"\"Inference on primary GPU.\"\"\" return self . model . module . predict ( X_test ) # Usage if torch . cuda . device_count () > 1 : model = DataParallelWrapper ( TabularPipeline ( model_name = 'TabICL' ), device_ids = [ 0 , 1 , 2 , 3 ] # Use GPUs 0-3 ) else : model = TabularPipeline ( model_name = 'TabICL' ) model . fit ( X_train , y_train )","title":"4.1 DataParallel (Simpler, Single-Machine)"},{"location":"advanced/multi-gpu/#42-distributeddataparallel-recommended","text":"More efficient than DataParallel: import torch import torch.distributed as dist from torch.nn.parallel import DistributedDataParallel def setup_distributed (): \"\"\"Setup distributed training environment.\"\"\" dist . init_process_group ( backend = 'nccl' , # NVIDIA Collective Communications Library init_method = 'env://' ) def cleanup (): \"\"\"Cleanup distributed environment.\"\"\" dist . destroy_process_group () def train_distributed (): \"\"\"Distributed training.\"\"\" setup_distributed () # Get process rank and world size rank = dist . get_rank () world_size = dist . get_world_size () # Create model on current device device = torch . device ( f 'cuda: { rank } ' ) pipeline = TabularPipeline ( model_name = 'TabICL' , tuning_strategy = 'base-ft' , tuning_params = { 'device' : device } ) # Wrap with DistributedDataParallel model = DistributedDataParallel ( pipeline . model , device_ids = [ rank ]) # Training code model . fit ( X_train , y_train ) cleanup () # Launch with: python -m torch.distributed.launch --nproc_per_node=4 script.py","title":"4.2 DistributedDataParallel (Recommended)"},{"location":"advanced/multi-gpu/#5-launch-methods","text":"","title":"5. Launch Methods"},{"location":"advanced/multi-gpu/#51-torchrun-pytorch-110","text":"Recommended method : # Single-machine, 4 GPUs torchrun --nproc_per_node = 4 train_script.py # Multi-machine (8 GPUs total) torchrun \\ --nproc_per_node = 4 \\ --nnodes = 2 \\ --node_rank = 0 \\ --master_addr = 192 .168.1.100 \\ --master_port = 29500 \\ train_script.py","title":"5.1 torchrun (PyTorch 1.10+)"},{"location":"advanced/multi-gpu/#52-torchdistributedlaunch-legacy","text":"# Single-machine, 4 GPUs python -m torch.distributed.launch \\ --nproc_per_node = 4 \\ train_script.py # With additional args python -m torch.distributed.launch \\ --nproc_per_node = 4 \\ train_script.py \\ --learning_rate 2e-5 \\ --epochs 5","title":"5.2 torch.distributed.launch (Legacy)"},{"location":"advanced/multi-gpu/#53-manual-launch","text":"# Terminal 1: GPU 0 CUDA_VISIBLE_DEVICES = 0 python train_script.py # Terminal 2: GPU 1 CUDA_VISIBLE_DEVICES = 1 python train_script.py # Terminal 3: GPU 2 CUDA_VISIBLE_DEVICES = 2 python train_script.py # Terminal 4: GPU 3 CUDA_VISIBLE_DEVICES = 3 python train_script.py","title":"5.3 Manual Launch"},{"location":"advanced/multi-gpu/#6-training-scripts","text":"","title":"6. Training Scripts"},{"location":"advanced/multi-gpu/#61-complete-distributed-training-script","text":"import os import torch import torch.distributed as dist from torch.nn.parallel import DistributedDataParallel from torch.utils.data import DataLoader , DistributedSampler from tabtune import TabularPipeline def main (): \"\"\"Main training function.\"\"\" # Initialize distributed training dist . init_process_group ( backend = 'nccl' ) rank = dist . get_rank () world_size = dist . get_world_size () # Set device device = torch . device ( f 'cuda: { rank } ' ) torch . cuda . set_device ( device ) # Print rank info if rank == 0 : print ( f \"Training on { world_size } GPUs\" ) # Load data X_train , y_train = load_data () # Create distributed sampler train_sampler = DistributedSampler ( dataset = range ( len ( X_train )), num_replicas = world_size , rank = rank , shuffle = True ) # Create dataloader train_loader = DataLoader ( train_sampler , batch_size = 32 , num_workers = 2 ) # Create model pipeline = TabularPipeline ( model_name = 'TabICL' , tuning_strategy = 'base-ft' , tuning_params = { 'device' : device , 'epochs' : 5 , 'learning_rate' : 2e-5 } ) # Wrap with DistributedDataParallel pipeline . model = DistributedDataParallel ( pipeline . model , device_ids = [ rank ] ) # Training if rank == 0 : print ( \"Starting training...\" ) pipeline . fit ( X_train [ train_sampler . indices ], y_train [ train_sampler . indices ]) # Evaluation on rank 0 only if rank == 0 : metrics = pipeline . evaluate ( X_test , y_test ) print ( f \"Validation Accuracy: { metrics [ 'accuracy' ] : .4f } \" ) # Cleanup dist . destroy_process_group () if __name__ == '__main__' : main ()","title":"6.1 Complete Distributed Training Script"},{"location":"advanced/multi-gpu/#62-synchronization-across-ranks","text":"def train_with_sync (): \"\"\"Training with synchronization points.\"\"\" dist . init_process_group ( backend = 'nccl' ) rank = dist . get_rank () # Training for epoch in range ( num_epochs ): # ... training code ... # Synchronize all ranks dist . barrier () # Evaluation (on rank 0 only) if rank == 0 : metrics = evaluate () print ( f \"Epoch { epoch } : { metrics } \" ) # Broadcast best checkpoint from rank 0 if rank == 0 : best_state = pipeline . model . state_dict () else : best_state = None # Distribute to all ranks dist . broadcast_object_list ([ best_state ], src = 0 ) if rank != 0 : pipeline . model . load_state_dict ( best_state [ 0 ]) dist . destroy_process_group ()","title":"6.2 Synchronization Across Ranks"},{"location":"advanced/multi-gpu/#7-performance-optimization","text":"","title":"7. Performance Optimization"},{"location":"advanced/multi-gpu/#71-gradient-accumulation","text":"Increase effective batch size without memory increase: def train_with_accumulation ( num_accumulation_steps = 4 ): \"\"\"Training with gradient accumulation.\"\"\" optimizer = torch . optim . AdamW ( model . parameters ()) for epoch in range ( num_epochs ): for i , batch in enumerate ( dataloader ): # Forward pass loss = model ( batch ) # Backward (accumulate gradients) ( loss / num_accumulation_steps ) . backward () # Update weights if ( i + 1 ) % num_accumulation_steps == 0 : optimizer . step () optimizer . zero_grad ()","title":"7.1 Gradient Accumulation"},{"location":"advanced/multi-gpu/#72-overlapping-computation-communication","text":"def train_with_overlap (): \"\"\"Training with async communication.\"\"\" for epoch in range ( num_epochs ): for batch in dataloader : # Forward & backward (computation) loss = model ( batch ) loss . backward () # Start async gradient reduction reduction_future = dist . all_reduce ( model . grad , async_op = True # Non-blocking ) # Do other work while reducing # ... # Wait for reduction to complete reduction_future . wait () # Update weights optimizer . step ()","title":"7.2 Overlapping Computation &amp; Communication"},{"location":"advanced/multi-gpu/#73-mixed-precision-with-distributed-training","text":"from torch.cuda.amp import autocast , GradScaler def train_mixed_precision (): \"\"\"Multi-GPU training with mixed precision.\"\"\" scaler = GradScaler () for epoch in range ( num_epochs ): for batch in dataloader : # Forward with autocast with autocast ( dtype = torch . float16 ): loss = model ( batch ) # Backward with scaling scaler . scale ( loss ) . backward () # All-reduce scaled gradients for param in model . parameters (): if param . grad is not None : dist . all_reduce ( param . grad ) # Update weights scaler . step ( optimizer ) scaler . update ()","title":"7.3 Mixed Precision with Distributed Training"},{"location":"advanced/multi-gpu/#8-scaling-efficiency","text":"","title":"8. Scaling Efficiency"},{"location":"advanced/multi-gpu/#81-linear-scaling-rule","text":"# Optimal learning rate for N GPUs base_learning_rate = 2e-5 num_gpus = 4 optimal_learning_rate = base_learning_rate * num_gpus # Why: Batch size \u00d7 num_gpus, so learning rate should scale # Or more conservatively: optimal_learning_rate = base_learning_rate * ( num_gpus ** 0.5 )","title":"8.1 Linear Scaling Rule"},{"location":"advanced/multi-gpu/#82-speedup-analysis","text":"def analyze_scaling ( times_single_gpu , times_multi_gpu ): \"\"\"Analyze multi-GPU speedup.\"\"\" num_gpus = len ( times_multi_gpu ) speedup = times_single_gpu / times_multi_gpu efficiency = speedup / num_gpus # Ideally 1.0 (100%) print ( f \"GPU Count: { num_gpus } \" ) print ( f \"Time (1 GPU): { times_single_gpu : .2f } s\" ) print ( f \"Time ( { num_gpus } GPUs): { times_multi_gpu : .2f } s\" ) print ( f \"Speedup: { speedup : .2f } x ( { efficiency * 100 : .1f } % efficiency)\" ) return speedup , efficiency # Typical results # 2 GPUs: 1.8x speedup (90% efficiency) # 4 GPUs: 3.5x speedup (87.5% efficiency) # 8 GPUs: 6.5x speedup (81% efficiency)","title":"8.2 Speedup Analysis"},{"location":"advanced/multi-gpu/#9-distributed-challenges-solutions","text":"","title":"9. Distributed Challenges &amp; Solutions"},{"location":"advanced/multi-gpu/#91-communication-overhead","text":"# Problem: Communication becomes bottleneck # Solution 1: Larger batch size tuning_params = { 'batch_size' : 128 , # Instead of 32 } # Solution 2: Gradient accumulation tuning_params = { 'batch_size' : 32 , 'gradient_accumulation_steps' : 4 # Effective: 128 } # Solution 3: Reduce communication frequency # Communicate every N steps instead of every step","title":"9.1 Communication Overhead"},{"location":"advanced/multi-gpu/#92-load-imbalance","text":"# Problem: Some GPUs finish before others # Solution: Dynamic load balancing def balanced_sampler ( dataset , num_replicas , rank ): \"\"\"Create balanced sampler across ranks.\"\"\" # Ensure each rank gets similar amount of work samples_per_rank = len ( dataset ) // num_replicas remainder = len ( dataset ) % num_replicas start = rank * samples_per_rank + min ( rank , remainder ) end = start + samples_per_rank + ( 1 if rank < remainder else 0 ) indices = list ( range ( start , end )) return DistributedSampler ( indices , num_replicas = num_replicas , rank = rank )","title":"9.2 Load Imbalance"},{"location":"advanced/multi-gpu/#93-gradient-divergence","text":"# Problem: Different GPUs compute different gradients # Solution: Proper synchronization def synchronized_training (): \"\"\"Ensure all ranks have synchronized gradients.\"\"\" for batch in dataloader : # Compute gradients loss = model ( batch ) loss . backward () # Synchronize gradients across all ranks for param in model . parameters (): dist . all_reduce ( param . grad ) param . grad /= world_size # Average # Update optimizer . step ()","title":"9.3 Gradient Divergence"},{"location":"advanced/multi-gpu/#10-complete-multi-gpu-example","text":"#!/usr/bin/env python \"\"\"Multi-GPU training example.\"\"\" import os import torch import torch.distributed as dist from torch.nn.parallel import DistributedDataParallel from tabtune import TabularPipeline import argparse def main (): parser = argparse . ArgumentParser () parser . add_argument ( '--epochs' , type = int , default = 5 ) parser . add_argument ( '--batch_size' , type = int , default = 32 ) parser . add_argument ( '--learning_rate' , type = float , default = 2e-5 ) args = parser . parse_args () # Initialize distributed training dist . init_process_group ( backend = 'nccl' ) rank = dist . get_rank () world_size = dist . get_world_size () device = torch . device ( f 'cuda: { rank } ' ) if rank == 0 : print ( f \"Starting training on { world_size } GPUs\" ) # Load data X_train , y_train , X_test , y_test = load_dataset () # Scale learning rate with batch size scaled_lr = args . learning_rate * world_size # Create pipeline pipeline = TabularPipeline ( model_name = 'TabICL' , tuning_strategy = 'base-ft' , tuning_params = { 'device' : device , 'epochs' : args . epochs , 'learning_rate' : scaled_lr , 'batch_size' : args . batch_size } ) # Wrap with DDP pipeline . model = DistributedDataParallel ( pipeline . model , device_ids = [ rank ] ) # Training if rank == 0 : print ( f \"Training with LR= { scaled_lr : .2e } , batch_size= { args . batch_size } \" ) pipeline . fit ( X_train , y_train ) # Synchronize before evaluation dist . barrier () # Evaluation on primary GPU only if rank == 0 : metrics = pipeline . evaluate ( X_test , y_test ) print ( f \"Final accuracy: { metrics [ 'accuracy' ] : .4f } \" ) dist . destroy_process_group () if __name__ == '__main__' : main () Launch: torchrun --nproc_per_node = 4 train_distributed.py --epochs 5 --batch_size 32","title":"10. Complete Multi-GPU Example"},{"location":"advanced/multi-gpu/#11-debugging-multi-gpu-issues","text":"","title":"11. Debugging Multi-GPU Issues"},{"location":"advanced/multi-gpu/#111-common-problems","text":"# Problem: Hanging/deadlock # Solution: Use timeout and debug flags os . environ [ 'NCCL_DEBUG' ] = 'INFO' os . environ [ 'TORCH_DISTRIBUTED_DEBUG' ] = 'DETAIL' dist . init_process_group ( backend = 'nccl' , timeout = timedelta ( minutes = 30 ) ) # Problem: GPU memory imbalance # Solution: Check and balance if rank == 0 : for i in range ( world_size ): print ( f \"GPU { i } : { torch . cuda . get_device_properties ( i ) . total_memory / 1e9 : .1f } GB\" ) # Problem: Rank synchronization issues # Solution: Add explicit barriers dist . barrier () # Wait for all ranks","title":"11.1 Common Problems"},{"location":"advanced/multi-gpu/#12-best-practices","text":"","title":"12. Best Practices"},{"location":"advanced/multi-gpu/#dos","text":"\u2705 Use DistributedDataParallel over DataParallel \u2705 Scale learning rate with batch size \u2705 Use proper samplers (DistributedSampler) \u2705 Synchronize at checkpoints \u2705 Evaluate on rank 0 only \u2705 Monitor all GPU memory usage \u2705 Test on fewer GPUs first","title":"\u2705 Do's"},{"location":"advanced/multi-gpu/#donts","text":"\u274c Don't use DataParallel for multi-machine \u274c Don't forget to set environment variables \u274c Don't run inference on all ranks \u274c Don't ignore communication overhead \u274c Don't over-subscribe GPU memory \u274c Don't change data on different ranks","title":"\u274c Don'ts"},{"location":"advanced/multi-gpu/#13-performance-benchmarks","text":"Dataset: 500K samples, TabICL model 1 GPU: Time: 120 minutes Memory: 12 GB 2 GPUs (DDP): Time: 65 minutes (1.85x speedup) Memory: 6 GB per GPU Efficiency: 92.5% 4 GPUs (DDP): Time: 35 minutes (3.43x speedup) Memory: 3 GB per GPU Efficiency: 85.7% 8 GPUs (DDP): Time: 20 minutes (6.0x speedup) Memory: 1.5 GB per GPU Efficiency: 75%","title":"13. Performance Benchmarks"},{"location":"advanced/multi-gpu/#14-quick-reference","text":"Aspect Single GPU Multi-GPU DDP Setup Simple Moderate Communication None NCCL Speedup 1x ~(GPUs-0.3) Memory/GPU Full Full/GPUs Best Use Development Production","title":"14. Quick Reference"},{"location":"advanced/multi-gpu/#15-next-steps","text":"Memory Optimization - Memory management with DDP Hyperparameter Tuning - Scaling learning rates Tuning Strategies - PEFT with DDP Examples - Multi-GPU benchmarks Scale TabTune efficiently across multiple GPUs for production-grade training!","title":"15. Next Steps"},{"location":"advanced/peft-lora/","text":"PEFT & LoRA: Parameter-Efficient Fine-Tuning for Tabular Models \u00b6 This document provides an in-depth guide to Parameter-Efficient Fine-Tuning (PEFT) using Low-Rank Adaptation (LoRA) for TabTune models. Learn the theory, implementation, and best practices for memory-efficient model adaptation. 1. Introduction to PEFT and LoRA \u00b6 1.1 What is PEFT? \u00b6 Parameter-Efficient Fine-Tuning (PEFT) is a set of techniques to adapt large pre-trained models using only a small fraction of the total parameters, dramatically reducing: Memory consumption (90% reduction) Training time (2-3x speedup) Storage requirements (only store small adapters) 1.2 What is LoRA? \u00b6 Low-Rank Adaptation (LoRA) is a specific PEFT technique that: Freezes pre-trained model weights Adds small trainable \"adapter\" layers Uses low-rank decomposition for efficiency Trains only 1-10% of parameters 1.3 Key Innovation \u00b6 Instead of updating all weights, LoRA learns a low-rank approximation of weight updates: [ W' = W_0 + \\Delta W = W_0 + BA ] Where: - (W_0): Original frozen weights (large) - (\\Delta W = BA): Low-rank decomposition - (A): Input projection (small) - (B): Output projection (small) - (r): Rank (typically 4-16, much smaller than weight dimensions) 2. Mathematical Foundation \u00b6 2.1 Low-Rank Decomposition \u00b6 For a weight matrix (W \\in \\mathbb{R}^{d_{out} \\times d_{in}}), LoRA represents updates as: [ \\Delta W = BA, \\quad B \\in \\mathbb{R}^{d_{out} \\times r}, A \\in \\mathbb{R}^{r \\times d_{in}} ] Complexity Reduction : - Full weights: (d_{out} \\times d_{in}) parameters - LoRA: (r(d_{out} + d_{in})) parameters - Compression ratio: (\\frac{r(d_{out} + d_{in})}{d_{out} \\times d_{in}}) Example : For a 768\u00d7768 weight matrix: - Full: 589,824 parameters - LoRA (r=8): 12,288 parameters - Compression: 98% reduction 2.2 Scaling \u00b6 To balance adaptation magnitude, LoRA scales the output: [ h = W_0 x + \\alpha \\frac{1}{r} B(Ax) ] Where (\\alpha) (lora_alpha) controls the scaling factor (\\frac{\\alpha}{r}). Effect of Alpha : - Higher alpha: Larger adaptation magnitude - Default: (\\alpha = 2r) (empirically optimal) 2.3 Dropout \u00b6 Dropout is applied to the input before LoRA projection for regularization: [ h = W_0 x + \\alpha \\frac{1}{r} B(\\text{dropout}(Ax)) ] 3. LoRA in TabTune \u00b6 3.1 LoRA Configuration \u00b6 peft_config = { 'r' : 8 , # Rank (main hyperparameter) 'lora_alpha' : 16 , # Scaling factor 'lora_dropout' : 0.05 , # Dropout probability 'target_modules' : None , # Modules to adapt (model default) 'bias' : 'none' # Bias handling } 3.2 LoRA Linear Layer \u00b6 TabTune implements LoRALinear that wraps standard PyTorch linear layers: class LoRALinear ( nn . Module ): def __init__ ( self , base_linear , r = 8 , alpha = 16 , dropout = 0.05 ): super () . __init__ () self . base = base_linear # Frozen base layer self . lora_A = nn . Linear ( ... , r ) # Adapter A self . lora_B = nn . Linear ( r , ... ) # Adapter B self . dropout = nn . Dropout ( dropout ) self . scaling = alpha / r def forward ( self , x ): # Base forward (no gradients) base_out = self . base ( x ) # LoRA forward lora_out = self . lora_B ( self . lora_A ( self . dropout ( x ))) * self . scaling return base_out + lora_out 3.3 Weight Freezing \u00b6 Base model weights: requires_grad=False LoRA adapters: requires_grad=True Enables gradient computation only on adapters 4. LoRA Hyperparameter Tuning \u00b6 4.1 Rank Selection \u00b6 The rank r is the most critical hyperparameter: Rank Parameters Memory Accuracy Speed r=2 Minimal Very Low Lower \u2b50\u2b50\u2b50\u2b50\u2b50 r=4 Low Low Good \u2b50\u2b50\u2b50\u2b50\u2b50 r=8 Moderate Moderate Better \u2b50\u2b50\u2b50\u2b50 r=16 High High Best \u2b50\u2b50\u2b50 r=32 Very High Very High Optimal \u2b50\u2b50 Guidelines : - Small data (10K) : r=4 (enough for adaptation) - Medium data (100K) : r=8 (balanced) - Large data (1M) : r=16 (more expressive) - Very constrained : r=2 (minimum viable) Rule of Thumb : [ r = \\max(4, \\frac{\\text{dataset_size}}{50000}) ] 4.2 Alpha Selection \u00b6 Alpha controls the magnitude of LoRA contribution: # Recommended: alpha = 2 * rank peft_config = { 'r' : 8 , 'lora_alpha' : 16 , # = 2 * 8 'lora_dropout' : 0.05 } Effects : - Alpha too low : Adaptation weak, training slow - Alpha = 2r : Empirically optimal - Alpha too high : Training unstable, may diverge 4.3 Dropout Probability \u00b6 LoRA dropout acts as regularization: lora_dropout_values = { 0.0 : 'No regularization (may overfit)' , 0.05 : 'Light regularization (default)' , 0.1 : 'Moderate regularization' , 0.2 : 'Strong regularization (for small data)' } Selection : - Large data : 0.05 (default) - Small data : 0.1-0.2 (prevent overfitting) - Already regularized model : 0.0-0.05 5. Target Module Selection \u00b6 5.1 Module Hierarchy \u00b6 TabTune identifies target modules via pattern matching: # LoRA targets linear transformation layers target_modules = { 'column_embeddings' : 'Column feature processing' , 'row_attention' : 'Row-wise interactions' , 'prediction_head' : 'Final prediction' , 'decoder' : 'Feature reconstruction' } 5.2 Model-Specific Defaults \u00b6 Each model has pre-configured target modules optimized for LoRA: TabICL : target_modules = [ 'col_embedder.tf_col' , 'row_interactor' , 'icl_predictor.tf_icl' , 'icl_predictor.decoder' ] TabDPT : target_modules = [ 'transformer_encoder' , 'encoder' , 'y_encoder' , 'head' ] Mitra : target_modules = [ 'x_embedding' , 'layers' , 'final_layer' ] 5.3 Custom Target Selection \u00b6 Override defaults for specific needs: pipeline = TabularPipeline ( model_name = 'TabICL' , tuning_strategy = 'peft' , tuning_params = { 'peft_config' : { 'r' : 8 , 'lora_alpha' : 16 , 'target_modules' : [ 'col_embedder.tf_col' , # Only column embedder 'icl_predictor.decoder' # Plus decoder ] } } ) 6. LoRA Training \u00b6 6.1 Typical Training Loop \u00b6 from tabtune import TabularPipeline # Create pipeline with PEFT pipeline = TabularPipeline ( model_name = 'TabICL' , tuning_strategy = 'peft' , tuning_params = { 'device' : 'cuda' , 'epochs' : 5 , 'learning_rate' : 2e-4 , # Typically higher than base-ft 'peft_config' : { 'r' : 8 , 'lora_alpha' : 16 } } ) # Training (only adapters are updated) pipeline . fit ( X_train , y_train ) # Inference predictions = pipeline . predict ( X_test ) 6.2 Learning Rate Strategy \u00b6 LoRA uses different learning rates than base fine-tuning: # Base fine-tuning (all parameters) learning_rate_base_ft = 2e-5 # LoRA fine-tuning (small parameters) learning_rate_peft = 2e-4 # 10x higher typical # Rationale: Smaller parameter updates need larger learning rates 6.3 Optimizer Configuration \u00b6 # LoRA-specific optimizer settings optimizer_config = { 'optimizer' : 'adamw' , 'learning_rate' : 2e-4 , 'weight_decay' : 0.01 , 'eps' : 1e-8 , 'betas' : ( 0.9 , 0.999 ) } 7. Memory Analysis \u00b6 7.1 Memory Breakdown \u00b6 Base Fine-Tuning (full model): Model weights: 500 MB Optimizer states: 1 GB (Adam: 2x weights) Gradients: 500 MB Activations: 200 MB \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Total: ~2.2 GB per forward/backward LoRA Fine-Tuning (adapters only): Model weights: 500 MB (frozen, no gradients) LoRA adapters: 5 MB Optimizer states: 10 MB (only adapters) Gradients: 5 MB Activations: 200 MB \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Total: ~700 MB per forward/backward ~70% reduction 7.2 Practical Memory Savings \u00b6 Model Base-FT LoRA Savings TabICL 12 GB 4 GB 66% TabDPT 24 GB 8 GB 66% Mitra 20 GB 6 GB 70% 9. Complete Example \u00b6 9.1 Memory-Constrained Scenario \u00b6 from tabtune import TabularPipeline import torch # Check available GPU memory print ( f \"Available GPU memory: { torch . cuda . get_device_properties ( 0 ) . total_memory / 1e9 : .1f } GB\" ) # LoRA for 4GB GPU pipeline = TabularPipeline ( model_name = 'TabICL' , tuning_strategy = 'peft' , tuning_params = { 'device' : 'cuda' , 'epochs' : 5 , 'learning_rate' : 2e-4 , 'batch_size' : 16 , 'peft_config' : { 'r' : 4 , # Lower rank for less memory 'lora_alpha' : 8 , 'lora_dropout' : 0.1 } } ) pipeline . fit ( X_train , y_train ) metrics = pipeline . evaluate ( X_test , y_test ) print ( f \"LoRA Accuracy: { metrics [ 'accuracy' ] : .4f } \" ) 9.2 Rank Exploration \u00b6 from tabtune import TabularLeaderboard # Compare different LoRA ranks lb = TabularLeaderboard ( X_train , X_test , y_train , y_test ) for r in [ 2 , 4 , 8 , 16 ]: lb . add_model ( 'TabICL' , 'peft' , name = f 'LoRA-r { r } ' , tuning_params = { 'epochs' : 5 , 'peft_config' : { 'r' : r , 'lora_alpha' : 2 * r } } ) results = lb . run ( rank_by = 'accuracy' ) print ( lb . get_ranking ()) 9.3 Memory-Speed Trade-off \u00b6 # Explore memory-speed-accuracy trade-off configs = [ { 'r' : 2 , 'name' : 'Ultra-Light' }, { 'r' : 4 , 'name' : 'Light' }, { 'r' : 8 , 'name' : 'Medium' }, { 'r' : 16 , 'name' : 'Heavy' } ] for config in configs : pipeline = TabularPipeline ( model_name = 'TabICL' , tuning_strategy = 'peft' , tuning_params = { 'device' : 'cuda' , 'epochs' : 3 , 'peft_config' : { 'r' : config [ 'r' ]} } ) # Time training import time start = time . time () pipeline . fit ( X_train , y_train ) elapsed = time . time () - start # Get memory usage mem = torch . cuda . max_memory_allocated () / 1e9 # Evaluate metrics = pipeline . evaluate ( X_test , y_test ) print ( f \" { config [ 'name' ] : 12 } | Rank: { config [ 'r' ] : 2 } | \" f \"Time: { elapsed : 6.1f } s | Memory: { mem : 5.1f } GB | \" f \"Accuracy: { metrics [ 'accuracy' ] : .4f } \" ) 10. Saving and Loading LoRA Models \u00b6 10.1 Save LoRA Adapters Only \u00b6 import torch # Save only LoRA adapter weights (minimal storage) lora_state = { 'rank' : 8 , 'alpha' : 16 , 'lora_a' : pipeline . model . lora_A . state_dict (), 'lora_b' : pipeline . model . lora_B . state_dict () } torch . save ( lora_state , 'lora_adapters.pt' ) # ~1-5 MB 10.2 Load and Merge \u00b6 # Load adapters and merge with base model lora_state = torch . load ( 'lora_adapters.pt' ) # Merge LoRA into base weights (optional, for inference optimization) merged_weights = base_weights + ( lora_B @ lora_A ) * alpha / r 10.3 Full Pipeline Serialization \u00b6 # Save complete pipeline with LoRA adapters pipeline . save ( 'pipeline_with_lora.joblib' ) # Load and use loaded = TabularPipeline . load ( 'pipeline_with_lora.joblib' ) predictions = loaded . predict ( X_test ) 11. Troubleshooting \u00b6 Issue: \"LoRA accuracy much lower than base-FT\" \u00b6 Solution : Increase rank peft_config = { 'r' : 16 , # Instead of 8 'lora_alpha' : 32 } Issue: \"Training diverging with LoRA\" \u00b6 Solution : Reduce learning rate tuning_params = { 'learning_rate' : 1e-4 , # Instead of 2e-4 'warmup_steps' : 500 } Issue: \"Still out of memory with LoRA\" \u00b6 Solution : Further reduce parameters peft_config = { 'r' : 2 , # Minimum viable 'lora_dropout' : 0.2 # Stronger regularization } tuning_params = { 'batch_size' : 4 # Smaller batches } Issue: \"LoRA inference slow\" \u00b6 Solution : Use merged weights # Merge LoRA weights into base after training merged_model = merge_lora_weights ( pipeline . model ) 13. Best Practices \u00b6 \u2705 Do's \u00b6 \u2705 Start with r=8 (good default) \u2705 Use 2x learning rate for LoRA vs base-FT \u2705 Include warmup phase (prevent instability) \u2705 Monitor gradient norms \u2705 Use gradient clipping \u2705 Save adapter weights separately \u2705 Test rank selection with leaderboard \u274c Don'ts \u00b6 \u274c Don't use same learning rate as base-FT \u274c Don't train very low ranks (r<2) without good reason \u274c Don't skip regularization on small data \u274c Don't forget to freeze base model \u274c Don't use LoRA on tiny models (overhead not worth it) 14. Comparison: Base-FT vs LoRA \u00b6 Aspect Base-FT LoRA Winner Accuracy High Medium-High Base-FT (~1% better) Memory Very High Low LoRA (70% savings) Speed Slow Fast LoRA (2-3x faster) Storage Huge Tiny LoRA (100x smaller) Scalability Limited Excellent LoRA Production Complex Simple LoRA Learning Curve Medium Low LoRA 15. Quick Reference \u00b6 Task r Alpha Dropout LR Small data (10K) 4 8 0.1 2e-4 Medium data (100K) 8 16 0.05 2e-4 Large data (1M) 16 32 0.02 1e-4 Memory limited 2 4 0.2 1e-4 Max accuracy 16 32 0.05 5e-5 16. Next Steps \u00b6 Tuning Strategies - Compare strategies Hyperparameter Tuning - Full optimization guide Models Overview - PEFT support per model TabularLeaderboard - Compare configurations LoRA enables efficient fine-tuning of large tabular models. Use it for memory-constrained environments while maintaining strong performance!","title":"PEFT & LoRA"},{"location":"advanced/peft-lora/#peft-lora-parameter-efficient-fine-tuning-for-tabular-models","text":"This document provides an in-depth guide to Parameter-Efficient Fine-Tuning (PEFT) using Low-Rank Adaptation (LoRA) for TabTune models. Learn the theory, implementation, and best practices for memory-efficient model adaptation.","title":"PEFT &amp; LoRA: Parameter-Efficient Fine-Tuning for Tabular Models"},{"location":"advanced/peft-lora/#1-introduction-to-peft-and-lora","text":"","title":"1. Introduction to PEFT and LoRA"},{"location":"advanced/peft-lora/#11-what-is-peft","text":"Parameter-Efficient Fine-Tuning (PEFT) is a set of techniques to adapt large pre-trained models using only a small fraction of the total parameters, dramatically reducing: Memory consumption (90% reduction) Training time (2-3x speedup) Storage requirements (only store small adapters)","title":"1.1 What is PEFT?"},{"location":"advanced/peft-lora/#12-what-is-lora","text":"Low-Rank Adaptation (LoRA) is a specific PEFT technique that: Freezes pre-trained model weights Adds small trainable \"adapter\" layers Uses low-rank decomposition for efficiency Trains only 1-10% of parameters","title":"1.2 What is LoRA?"},{"location":"advanced/peft-lora/#13-key-innovation","text":"Instead of updating all weights, LoRA learns a low-rank approximation of weight updates: [ W' = W_0 + \\Delta W = W_0 + BA ] Where: - (W_0): Original frozen weights (large) - (\\Delta W = BA): Low-rank decomposition - (A): Input projection (small) - (B): Output projection (small) - (r): Rank (typically 4-16, much smaller than weight dimensions)","title":"1.3 Key Innovation"},{"location":"advanced/peft-lora/#2-mathematical-foundation","text":"","title":"2. Mathematical Foundation"},{"location":"advanced/peft-lora/#21-low-rank-decomposition","text":"For a weight matrix (W \\in \\mathbb{R}^{d_{out} \\times d_{in}}), LoRA represents updates as: [ \\Delta W = BA, \\quad B \\in \\mathbb{R}^{d_{out} \\times r}, A \\in \\mathbb{R}^{r \\times d_{in}} ] Complexity Reduction : - Full weights: (d_{out} \\times d_{in}) parameters - LoRA: (r(d_{out} + d_{in})) parameters - Compression ratio: (\\frac{r(d_{out} + d_{in})}{d_{out} \\times d_{in}}) Example : For a 768\u00d7768 weight matrix: - Full: 589,824 parameters - LoRA (r=8): 12,288 parameters - Compression: 98% reduction","title":"2.1 Low-Rank Decomposition"},{"location":"advanced/peft-lora/#22-scaling","text":"To balance adaptation magnitude, LoRA scales the output: [ h = W_0 x + \\alpha \\frac{1}{r} B(Ax) ] Where (\\alpha) (lora_alpha) controls the scaling factor (\\frac{\\alpha}{r}). Effect of Alpha : - Higher alpha: Larger adaptation magnitude - Default: (\\alpha = 2r) (empirically optimal)","title":"2.2 Scaling"},{"location":"advanced/peft-lora/#23-dropout","text":"Dropout is applied to the input before LoRA projection for regularization: [ h = W_0 x + \\alpha \\frac{1}{r} B(\\text{dropout}(Ax)) ]","title":"2.3 Dropout"},{"location":"advanced/peft-lora/#3-lora-in-tabtune","text":"","title":"3. LoRA in TabTune"},{"location":"advanced/peft-lora/#31-lora-configuration","text":"peft_config = { 'r' : 8 , # Rank (main hyperparameter) 'lora_alpha' : 16 , # Scaling factor 'lora_dropout' : 0.05 , # Dropout probability 'target_modules' : None , # Modules to adapt (model default) 'bias' : 'none' # Bias handling }","title":"3.1 LoRA Configuration"},{"location":"advanced/peft-lora/#32-lora-linear-layer","text":"TabTune implements LoRALinear that wraps standard PyTorch linear layers: class LoRALinear ( nn . Module ): def __init__ ( self , base_linear , r = 8 , alpha = 16 , dropout = 0.05 ): super () . __init__ () self . base = base_linear # Frozen base layer self . lora_A = nn . Linear ( ... , r ) # Adapter A self . lora_B = nn . Linear ( r , ... ) # Adapter B self . dropout = nn . Dropout ( dropout ) self . scaling = alpha / r def forward ( self , x ): # Base forward (no gradients) base_out = self . base ( x ) # LoRA forward lora_out = self . lora_B ( self . lora_A ( self . dropout ( x ))) * self . scaling return base_out + lora_out","title":"3.2 LoRA Linear Layer"},{"location":"advanced/peft-lora/#33-weight-freezing","text":"Base model weights: requires_grad=False LoRA adapters: requires_grad=True Enables gradient computation only on adapters","title":"3.3 Weight Freezing"},{"location":"advanced/peft-lora/#4-lora-hyperparameter-tuning","text":"","title":"4. LoRA Hyperparameter Tuning"},{"location":"advanced/peft-lora/#41-rank-selection","text":"The rank r is the most critical hyperparameter: Rank Parameters Memory Accuracy Speed r=2 Minimal Very Low Lower \u2b50\u2b50\u2b50\u2b50\u2b50 r=4 Low Low Good \u2b50\u2b50\u2b50\u2b50\u2b50 r=8 Moderate Moderate Better \u2b50\u2b50\u2b50\u2b50 r=16 High High Best \u2b50\u2b50\u2b50 r=32 Very High Very High Optimal \u2b50\u2b50 Guidelines : - Small data (10K) : r=4 (enough for adaptation) - Medium data (100K) : r=8 (balanced) - Large data (1M) : r=16 (more expressive) - Very constrained : r=2 (minimum viable) Rule of Thumb : [ r = \\max(4, \\frac{\\text{dataset_size}}{50000}) ]","title":"4.1 Rank Selection"},{"location":"advanced/peft-lora/#42-alpha-selection","text":"Alpha controls the magnitude of LoRA contribution: # Recommended: alpha = 2 * rank peft_config = { 'r' : 8 , 'lora_alpha' : 16 , # = 2 * 8 'lora_dropout' : 0.05 } Effects : - Alpha too low : Adaptation weak, training slow - Alpha = 2r : Empirically optimal - Alpha too high : Training unstable, may diverge","title":"4.2 Alpha Selection"},{"location":"advanced/peft-lora/#43-dropout-probability","text":"LoRA dropout acts as regularization: lora_dropout_values = { 0.0 : 'No regularization (may overfit)' , 0.05 : 'Light regularization (default)' , 0.1 : 'Moderate regularization' , 0.2 : 'Strong regularization (for small data)' } Selection : - Large data : 0.05 (default) - Small data : 0.1-0.2 (prevent overfitting) - Already regularized model : 0.0-0.05","title":"4.3 Dropout Probability"},{"location":"advanced/peft-lora/#5-target-module-selection","text":"","title":"5. Target Module Selection"},{"location":"advanced/peft-lora/#51-module-hierarchy","text":"TabTune identifies target modules via pattern matching: # LoRA targets linear transformation layers target_modules = { 'column_embeddings' : 'Column feature processing' , 'row_attention' : 'Row-wise interactions' , 'prediction_head' : 'Final prediction' , 'decoder' : 'Feature reconstruction' }","title":"5.1 Module Hierarchy"},{"location":"advanced/peft-lora/#52-model-specific-defaults","text":"Each model has pre-configured target modules optimized for LoRA: TabICL : target_modules = [ 'col_embedder.tf_col' , 'row_interactor' , 'icl_predictor.tf_icl' , 'icl_predictor.decoder' ] TabDPT : target_modules = [ 'transformer_encoder' , 'encoder' , 'y_encoder' , 'head' ] Mitra : target_modules = [ 'x_embedding' , 'layers' , 'final_layer' ]","title":"5.2 Model-Specific Defaults"},{"location":"advanced/peft-lora/#53-custom-target-selection","text":"Override defaults for specific needs: pipeline = TabularPipeline ( model_name = 'TabICL' , tuning_strategy = 'peft' , tuning_params = { 'peft_config' : { 'r' : 8 , 'lora_alpha' : 16 , 'target_modules' : [ 'col_embedder.tf_col' , # Only column embedder 'icl_predictor.decoder' # Plus decoder ] } } )","title":"5.3 Custom Target Selection"},{"location":"advanced/peft-lora/#6-lora-training","text":"","title":"6. LoRA Training"},{"location":"advanced/peft-lora/#61-typical-training-loop","text":"from tabtune import TabularPipeline # Create pipeline with PEFT pipeline = TabularPipeline ( model_name = 'TabICL' , tuning_strategy = 'peft' , tuning_params = { 'device' : 'cuda' , 'epochs' : 5 , 'learning_rate' : 2e-4 , # Typically higher than base-ft 'peft_config' : { 'r' : 8 , 'lora_alpha' : 16 } } ) # Training (only adapters are updated) pipeline . fit ( X_train , y_train ) # Inference predictions = pipeline . predict ( X_test )","title":"6.1 Typical Training Loop"},{"location":"advanced/peft-lora/#62-learning-rate-strategy","text":"LoRA uses different learning rates than base fine-tuning: # Base fine-tuning (all parameters) learning_rate_base_ft = 2e-5 # LoRA fine-tuning (small parameters) learning_rate_peft = 2e-4 # 10x higher typical # Rationale: Smaller parameter updates need larger learning rates","title":"6.2 Learning Rate Strategy"},{"location":"advanced/peft-lora/#63-optimizer-configuration","text":"# LoRA-specific optimizer settings optimizer_config = { 'optimizer' : 'adamw' , 'learning_rate' : 2e-4 , 'weight_decay' : 0.01 , 'eps' : 1e-8 , 'betas' : ( 0.9 , 0.999 ) }","title":"6.3 Optimizer Configuration"},{"location":"advanced/peft-lora/#7-memory-analysis","text":"","title":"7. Memory Analysis"},{"location":"advanced/peft-lora/#71-memory-breakdown","text":"Base Fine-Tuning (full model): Model weights: 500 MB Optimizer states: 1 GB (Adam: 2x weights) Gradients: 500 MB Activations: 200 MB \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Total: ~2.2 GB per forward/backward LoRA Fine-Tuning (adapters only): Model weights: 500 MB (frozen, no gradients) LoRA adapters: 5 MB Optimizer states: 10 MB (only adapters) Gradients: 5 MB Activations: 200 MB \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Total: ~700 MB per forward/backward ~70% reduction","title":"7.1 Memory Breakdown"},{"location":"advanced/peft-lora/#72-practical-memory-savings","text":"Model Base-FT LoRA Savings TabICL 12 GB 4 GB 66% TabDPT 24 GB 8 GB 66% Mitra 20 GB 6 GB 70%","title":"7.2 Practical Memory Savings"},{"location":"advanced/peft-lora/#9-complete-example","text":"","title":"9. Complete Example"},{"location":"advanced/peft-lora/#91-memory-constrained-scenario","text":"from tabtune import TabularPipeline import torch # Check available GPU memory print ( f \"Available GPU memory: { torch . cuda . get_device_properties ( 0 ) . total_memory / 1e9 : .1f } GB\" ) # LoRA for 4GB GPU pipeline = TabularPipeline ( model_name = 'TabICL' , tuning_strategy = 'peft' , tuning_params = { 'device' : 'cuda' , 'epochs' : 5 , 'learning_rate' : 2e-4 , 'batch_size' : 16 , 'peft_config' : { 'r' : 4 , # Lower rank for less memory 'lora_alpha' : 8 , 'lora_dropout' : 0.1 } } ) pipeline . fit ( X_train , y_train ) metrics = pipeline . evaluate ( X_test , y_test ) print ( f \"LoRA Accuracy: { metrics [ 'accuracy' ] : .4f } \" )","title":"9.1 Memory-Constrained Scenario"},{"location":"advanced/peft-lora/#92-rank-exploration","text":"from tabtune import TabularLeaderboard # Compare different LoRA ranks lb = TabularLeaderboard ( X_train , X_test , y_train , y_test ) for r in [ 2 , 4 , 8 , 16 ]: lb . add_model ( 'TabICL' , 'peft' , name = f 'LoRA-r { r } ' , tuning_params = { 'epochs' : 5 , 'peft_config' : { 'r' : r , 'lora_alpha' : 2 * r } } ) results = lb . run ( rank_by = 'accuracy' ) print ( lb . get_ranking ())","title":"9.2 Rank Exploration"},{"location":"advanced/peft-lora/#93-memory-speed-trade-off","text":"# Explore memory-speed-accuracy trade-off configs = [ { 'r' : 2 , 'name' : 'Ultra-Light' }, { 'r' : 4 , 'name' : 'Light' }, { 'r' : 8 , 'name' : 'Medium' }, { 'r' : 16 , 'name' : 'Heavy' } ] for config in configs : pipeline = TabularPipeline ( model_name = 'TabICL' , tuning_strategy = 'peft' , tuning_params = { 'device' : 'cuda' , 'epochs' : 3 , 'peft_config' : { 'r' : config [ 'r' ]} } ) # Time training import time start = time . time () pipeline . fit ( X_train , y_train ) elapsed = time . time () - start # Get memory usage mem = torch . cuda . max_memory_allocated () / 1e9 # Evaluate metrics = pipeline . evaluate ( X_test , y_test ) print ( f \" { config [ 'name' ] : 12 } | Rank: { config [ 'r' ] : 2 } | \" f \"Time: { elapsed : 6.1f } s | Memory: { mem : 5.1f } GB | \" f \"Accuracy: { metrics [ 'accuracy' ] : .4f } \" )","title":"9.3 Memory-Speed Trade-off"},{"location":"advanced/peft-lora/#10-saving-and-loading-lora-models","text":"","title":"10. Saving and Loading LoRA Models"},{"location":"advanced/peft-lora/#101-save-lora-adapters-only","text":"import torch # Save only LoRA adapter weights (minimal storage) lora_state = { 'rank' : 8 , 'alpha' : 16 , 'lora_a' : pipeline . model . lora_A . state_dict (), 'lora_b' : pipeline . model . lora_B . state_dict () } torch . save ( lora_state , 'lora_adapters.pt' ) # ~1-5 MB","title":"10.1 Save LoRA Adapters Only"},{"location":"advanced/peft-lora/#102-load-and-merge","text":"# Load adapters and merge with base model lora_state = torch . load ( 'lora_adapters.pt' ) # Merge LoRA into base weights (optional, for inference optimization) merged_weights = base_weights + ( lora_B @ lora_A ) * alpha / r","title":"10.2 Load and Merge"},{"location":"advanced/peft-lora/#103-full-pipeline-serialization","text":"# Save complete pipeline with LoRA adapters pipeline . save ( 'pipeline_with_lora.joblib' ) # Load and use loaded = TabularPipeline . load ( 'pipeline_with_lora.joblib' ) predictions = loaded . predict ( X_test )","title":"10.3 Full Pipeline Serialization"},{"location":"advanced/peft-lora/#11-troubleshooting","text":"","title":"11. Troubleshooting"},{"location":"advanced/peft-lora/#issue-lora-accuracy-much-lower-than-base-ft","text":"Solution : Increase rank peft_config = { 'r' : 16 , # Instead of 8 'lora_alpha' : 32 }","title":"Issue: \"LoRA accuracy much lower than base-FT\""},{"location":"advanced/peft-lora/#issue-training-diverging-with-lora","text":"Solution : Reduce learning rate tuning_params = { 'learning_rate' : 1e-4 , # Instead of 2e-4 'warmup_steps' : 500 }","title":"Issue: \"Training diverging with LoRA\""},{"location":"advanced/peft-lora/#issue-still-out-of-memory-with-lora","text":"Solution : Further reduce parameters peft_config = { 'r' : 2 , # Minimum viable 'lora_dropout' : 0.2 # Stronger regularization } tuning_params = { 'batch_size' : 4 # Smaller batches }","title":"Issue: \"Still out of memory with LoRA\""},{"location":"advanced/peft-lora/#issue-lora-inference-slow","text":"Solution : Use merged weights # Merge LoRA weights into base after training merged_model = merge_lora_weights ( pipeline . model )","title":"Issue: \"LoRA inference slow\""},{"location":"advanced/peft-lora/#13-best-practices","text":"","title":"13. Best Practices"},{"location":"advanced/peft-lora/#dos","text":"\u2705 Start with r=8 (good default) \u2705 Use 2x learning rate for LoRA vs base-FT \u2705 Include warmup phase (prevent instability) \u2705 Monitor gradient norms \u2705 Use gradient clipping \u2705 Save adapter weights separately \u2705 Test rank selection with leaderboard","title":"\u2705 Do's"},{"location":"advanced/peft-lora/#donts","text":"\u274c Don't use same learning rate as base-FT \u274c Don't train very low ranks (r<2) without good reason \u274c Don't skip regularization on small data \u274c Don't forget to freeze base model \u274c Don't use LoRA on tiny models (overhead not worth it)","title":"\u274c Don'ts"},{"location":"advanced/peft-lora/#14-comparison-base-ft-vs-lora","text":"Aspect Base-FT LoRA Winner Accuracy High Medium-High Base-FT (~1% better) Memory Very High Low LoRA (70% savings) Speed Slow Fast LoRA (2-3x faster) Storage Huge Tiny LoRA (100x smaller) Scalability Limited Excellent LoRA Production Complex Simple LoRA Learning Curve Medium Low LoRA","title":"14. Comparison: Base-FT vs LoRA"},{"location":"advanced/peft-lora/#15-quick-reference","text":"Task r Alpha Dropout LR Small data (10K) 4 8 0.1 2e-4 Medium data (100K) 8 16 0.05 2e-4 Large data (1M) 16 32 0.02 1e-4 Memory limited 2 4 0.2 1e-4 Max accuracy 16 32 0.05 5e-5","title":"15. Quick Reference"},{"location":"advanced/peft-lora/#16-next-steps","text":"Tuning Strategies - Compare strategies Hyperparameter Tuning - Full optimization guide Models Overview - PEFT support per model TabularLeaderboard - Compare configurations LoRA enables efficient fine-tuning of large tabular models. Use it for memory-constrained environments while maintaining strong performance!","title":"16. Next Steps"},{"location":"examples/classification/","text":"Classification Examples: End-to-End Workflows with TabTune \u00b6 This document provides practical, complete examples for classification tasks using TabTune across various scenarios and complexity levels. 1. Quick Start Classification \u00b6 1.1 5-Minute Example \u00b6 Minimal code to get predictions: from tabtune import TabularPipeline from sklearn.datasets import load_breast_cancer from sklearn.model_selection import train_test_split # Load dataset X , y = load_breast_cancer ( return_X_y = True ) X_train , X_test , y_train , y_test = train_test_split ( X , y , test_size = 0.2 ) # Create and train pipeline = TabularPipeline ( model_name = 'TabPFN' , tuning_strategy = 'inference' ) pipeline . fit ( X_train , y_train ) # Predict predictions = pipeline . predict ( X_test ) metrics = pipeline . evaluate ( X_test , y_test ) print ( f \"Accuracy: { metrics [ 'accuracy' ] : .4f } \" ) 1.2 10-Minute Example with Fine-Tuning \u00b6 from tabtune import TabularPipeline import pandas as pd from sklearn.model_selection import train_test_split # Load your data df = pd . read_csv ( 'your_data.csv' ) X = df . drop ( 'target' , axis = 1 ) y = df [ 'target' ] X_train , X_test , y_train , y_test = train_test_split ( X , y , test_size = 0.2 ) # Create pipeline with fine-tuning pipeline = TabularPipeline ( model_name = 'TabICL' , tuning_strategy = 'base-ft' , tuning_params = { 'device' : 'cuda' , 'epochs' : 5 , 'learning_rate' : 2e-5 } ) # Train pipeline . fit ( X_train , y_train ) # Evaluate metrics = pipeline . evaluate ( X_test , y_test ) print ( f \"Accuracy: { metrics [ 'accuracy' ] : .4f } \" ) print ( f \"F1 Score: { metrics [ 'f1_score' ] : .4f } \" ) print ( f \"ROC AUC: { metrics [ 'roc_auc_score' ] : .4f } \" ) 2. Binary Classification \u00b6 2.1 Credit Card Fraud Detection \u00b6 Real-world binary classification example: import pandas as pd import numpy as np from sklearn.model_selection import train_test_split from tabtune import TabularPipeline # Load fraud detection dataset df = pd . read_csv ( 'creditcard.csv' ) # Separate features and target X = df . drop ( 'Class' , axis = 1 ) # Class: 0=normal, 1=fraud y = df [ 'Class' ] print ( f \"Class distribution: { y . value_counts () . to_dict () } \" ) # Split data X_train , X_test , y_train , y_test = train_test_split ( X , y , test_size = 0.2 , random_state = 42 , stratify = y ) # Train pipeline pipeline = TabularPipeline ( model_name = 'TabICL' , tuning_strategy = 'peft' , tuning_params = { 'device' : 'cuda' , 'epochs' : 5 , 'learning_rate' : 2e-4 , 'peft_config' : { 'r' : 8 , 'lora_alpha' : 16 } } ) print ( \"Training...\" ) pipeline . fit ( X_train , y_train ) # Evaluate metrics = pipeline . evaluate ( X_test , y_test ) print ( \" \\n === Results ===\" ) print ( f \"Accuracy: { metrics [ 'accuracy' ] : .4f } \" ) print ( f \"F1 Score: { metrics [ 'f1_score' ] : .4f } \" ) print ( f \"ROC AUC: { metrics [ 'roc_auc_score' ] : .4f } \" ) # Save for deployment pipeline . save ( 'fraud_detection_model.joblib' ) # In production loaded = TabularPipeline . load ( 'fraud_detection_model.joblib' ) new_transactions = pd . read_csv ( 'new_transactions.csv' ) fraud_predictions = loaded . predict ( new_transactions ) 2.2 Customer Churn Prediction \u00b6 import pandas as pd from tabtune import TabularPipeline from sklearn.model_selection import train_test_split # Load customer data df = pd . read_csv ( 'customer_churn.csv' ) # Preprocessing X = df . drop ([ 'CustomerID' , 'Churn' ], axis = 1 ) y = ( df [ 'Churn' ] == 'Yes' ) . astype ( int ) X_train , X_test , y_train , y_test = train_test_split ( X , y , test_size = 0.2 ) # Pipeline pipeline = TabularPipeline ( model_name = 'TabBiaxial' , tuning_strategy = 'base-ft' , tuning_params = { 'device' : 'cuda' , 'epochs' : 5 , 'support_size' : 48 , 'query_size' : 32 , 'learning_rate' : 2e-5 } ) pipeline . fit ( X_train , y_train ) metrics = pipeline . evaluate ( X_test , y_test ) print ( f \"Churn Prediction Accuracy: { metrics [ 'accuracy' ] : .4f } \" ) # Identify high-risk customers churn_probs = pipeline . predict_proba ( X_test )[:, 1 ] high_risk = np . where ( churn_probs > 0.7 )[ 0 ] print ( f \"High-risk customers: { len ( high_risk ) } \" ) 3. Multi-Class Classification \u00b6 3.1 Iris Dataset (3-Class) \u00b6 Classic machine learning example: from sklearn.datasets import load_iris from sklearn.model_selection import train_test_split from tabtune import TabularPipeline # Load iris dataset X , y = load_iris ( return_X_y = True ) X_train , X_test , y_train , y_test = train_test_split ( X , y , test_size = 0.2 , random_state = 42 ) # Simple inference baseline print ( \"=== Inference Baseline ===\" ) pipeline_inf = TabularPipeline ( model_name = 'TabPFN' , tuning_strategy = 'inference' ) pipeline_inf . fit ( X_train , y_train ) inf_metrics = pipeline_inf . evaluate ( X_test , y_test ) print ( f \"Inference Accuracy: { inf_metrics [ 'accuracy' ] : .4f } \" ) # Fine-tuned model print ( \" \\n === Fine-Tuned Model ===\" ) pipeline_ft = TabularPipeline ( model_name = 'TabICL' , tuning_strategy = 'base-ft' , tuning_params = { 'device' : 'cuda' , 'epochs' : 5 , 'support_size' : 48 , 'query_size' : 32 } ) pipeline_ft . fit ( X_train , y_train ) ft_metrics = pipeline_ft . evaluate ( X_test , y_test ) print ( f \"Fine-Tuned Accuracy: { ft_metrics [ 'accuracy' ] : .4f } \" ) # Compare improvement = ( ft_metrics [ 'accuracy' ] - inf_metrics [ 'accuracy' ]) * 100 print ( f \" \\n Improvement: + { improvement : .2f } %\" ) 3.2 Multi-Class Document Classification \u00b6 import pandas as pd from tabtune import TabularPipeline from sklearn.feature_extraction.text import TfidfVectorizer from sklearn.model_selection import train_test_split # Load documents df = pd . read_csv ( 'documents.csv' ) # Columns: text, category # Feature extraction from text vectorizer = TfidfVectorizer ( max_features = 500 ) X = vectorizer . fit_transform ( df [ 'text' ]) . toarray () y = pd . factorize ( df [ 'category' ])[ 0 ] X_train , X_test , y_train , y_test = train_test_split ( X , y , test_size = 0.2 ) # Train classifier pipeline = TabularPipeline ( model_name = 'TabICL' , tuning_strategy = 'base-ft' , tuning_params = { 'epochs' : 5 } ) pipeline . fit ( X_train , y_train ) metrics = pipeline . evaluate ( X_test , y_test ) print ( f \"Classification Accuracy: { metrics [ 'accuracy' ] : .4f } \" ) print ( f \"Weighted F1: { metrics [ 'f1_score' ] : .4f } \" ) 5. Cross-Validation \u00b6 5.1 k-Fold Cross-Validation \u00b6 from sklearn.model_selection import StratifiedKFold import numpy as np from tabtune import TabularPipeline def cross_validate ( X , y , model_name , params , k = 5 ): \"\"\"Perform k-fold cross-validation.\"\"\" skf = StratifiedKFold ( n_splits = k , shuffle = True , random_state = 42 ) scores = [] f1_scores = [] for fold_idx , ( train_idx , val_idx ) in enumerate ( skf . split ( X , y )): X_train_fold = X . iloc [ train_idx ] X_val_fold = X . iloc [ val_idx ] y_train_fold = y . iloc [ train_idx ] y_val_fold = y . iloc [ val_idx ] # Train pipeline = TabularPipeline ( model_name = model_name , tuning_strategy = 'base-ft' , tuning_params = params ) pipeline . fit ( X_train_fold , y_train_fold ) metrics = pipeline . evaluate ( X_val_fold , y_val_fold ) scores . append ( metrics [ 'accuracy' ]) f1_scores . append ( metrics [ 'f1_score' ]) print ( f \"Fold { fold_idx + 1 } / { k } : Acc= { metrics [ 'accuracy' ] : .4f } , \" f \"F1= { metrics [ 'f1_score' ] : .4f } \" ) print ( f \" \\n Mean Accuracy: { np . mean ( scores ) : .4f } \u00b1 { np . std ( scores ) : .4f } \" ) print ( f \"Mean F1 Score: { np . mean ( f1_scores ) : .4f } \u00b1 { np . std ( f1_scores ) : .4f } \" ) return scores , f1_scores # Usage import pandas as pd df = pd . read_csv ( 'data.csv' ) X = df . drop ( 'target' , axis = 1 ) y = df [ 'target' ] scores , f1s = cross_validate ( X , y , model_name = 'TabICL' , params = { 'epochs' : 5 }, k = 5 ) 9. Real-World Datasets \u00b6 9.1 Adult Income Dataset \u00b6 Predicting income level (binary classification): import pandas as pd from tabtune import TabularPipeline from sklearn.model_selection import train_test_split import openml # Download from OpenML dataset = openml . datasets . get_dataset ( 1590 ) # Adult dataset X , y , _ , _ = dataset . get_data ( target = dataset . default_target_attribute ) X_train , X_test , y_train , y_test = train_test_split ( X , y , test_size = 0.2 ) # Train pipeline = TabularPipeline ( model_name = 'TabDPT' , tuning_strategy = 'base-ft' , tuning_params = { 'device' : 'cuda' , 'epochs' : 3 , 'support_size' : 1024 } ) pipeline . fit ( X_train , y_train ) metrics = pipeline . evaluate ( X_test , y_test ) print ( f \"Adult Dataset Results:\" ) print ( f \"Accuracy: { metrics [ 'accuracy' ] : .4f } \" ) print ( f \"F1 Score: { metrics [ 'f1_score' ] : .4f } \" ) 9.2 MNIST-Like Tabular Classification \u00b6 from sklearn.datasets import load_digits from tabtune import TabularPipeline from sklearn.model_selection import train_test_split # Load digits (multi-class: 0-9) X , y = load_digits ( return_X_y = True ) X_train , X_test , y_train , y_test = train_test_split ( X , y , test_size = 0.2 ) # Classify pipeline = TabularPipeline ( model_name = 'TabICL' , tuning_strategy = 'peft' , tuning_params = { 'device' : 'cuda' , 'epochs' : 3 , 'peft_config' : { 'r' : 8 } } ) pipeline . fit ( X_train , y_train ) metrics = pipeline . evaluate ( X_test , y_test ) print ( f \"Digit Recognition (0-9):\" ) print ( f \"Accuracy: { metrics [ 'accuracy' ] : .4f } \" ) print ( f \"Classes: { len ( set ( y )) } \" ) 10. Troubleshooting \u00b6 10.1 Common Issues \u00b6 # Issue: Low accuracy # Solution: Try different model and strategy from tabtune import TabularLeaderboard lb = TabularLeaderboard ( X_train , X_val , y_train , y_val ) for model in [ 'TabPFN' , 'TabICL' , 'TabDPT' ]: lb . add_model ( model , 'inference' ) results = lb . run () # Compare # Issue: Out of memory # Solution: Use PEFT pipeline = TabularPipeline ( model_name = 'TabICL' , tuning_strategy = 'peft' , tuning_params = { 'peft_config' : { 'r' : 4 }} ) # Issue: Slow training # Solution: Reduce batch size or use fewer epochs pipeline = TabularPipeline ( tuning_params = { 'batch_size' : 8 , 'epochs' : 3 } ) 11. Next Steps \u00b6 Model Selection Guide - Choose right model Hyperparameter Tuning - Optimize performance TabularLeaderboard - Compare models systematically Saving & Loading - Deploy models These examples cover the full spectrum of classification tasks with TabTune!","title":"Classification Tasks"},{"location":"examples/classification/#classification-examples-end-to-end-workflows-with-tabtune","text":"This document provides practical, complete examples for classification tasks using TabTune across various scenarios and complexity levels.","title":"Classification Examples: End-to-End Workflows with TabTune"},{"location":"examples/classification/#1-quick-start-classification","text":"","title":"1. Quick Start Classification"},{"location":"examples/classification/#11-5-minute-example","text":"Minimal code to get predictions: from tabtune import TabularPipeline from sklearn.datasets import load_breast_cancer from sklearn.model_selection import train_test_split # Load dataset X , y = load_breast_cancer ( return_X_y = True ) X_train , X_test , y_train , y_test = train_test_split ( X , y , test_size = 0.2 ) # Create and train pipeline = TabularPipeline ( model_name = 'TabPFN' , tuning_strategy = 'inference' ) pipeline . fit ( X_train , y_train ) # Predict predictions = pipeline . predict ( X_test ) metrics = pipeline . evaluate ( X_test , y_test ) print ( f \"Accuracy: { metrics [ 'accuracy' ] : .4f } \" )","title":"1.1 5-Minute Example"},{"location":"examples/classification/#12-10-minute-example-with-fine-tuning","text":"from tabtune import TabularPipeline import pandas as pd from sklearn.model_selection import train_test_split # Load your data df = pd . read_csv ( 'your_data.csv' ) X = df . drop ( 'target' , axis = 1 ) y = df [ 'target' ] X_train , X_test , y_train , y_test = train_test_split ( X , y , test_size = 0.2 ) # Create pipeline with fine-tuning pipeline = TabularPipeline ( model_name = 'TabICL' , tuning_strategy = 'base-ft' , tuning_params = { 'device' : 'cuda' , 'epochs' : 5 , 'learning_rate' : 2e-5 } ) # Train pipeline . fit ( X_train , y_train ) # Evaluate metrics = pipeline . evaluate ( X_test , y_test ) print ( f \"Accuracy: { metrics [ 'accuracy' ] : .4f } \" ) print ( f \"F1 Score: { metrics [ 'f1_score' ] : .4f } \" ) print ( f \"ROC AUC: { metrics [ 'roc_auc_score' ] : .4f } \" )","title":"1.2 10-Minute Example with Fine-Tuning"},{"location":"examples/classification/#2-binary-classification","text":"","title":"2. Binary Classification"},{"location":"examples/classification/#21-credit-card-fraud-detection","text":"Real-world binary classification example: import pandas as pd import numpy as np from sklearn.model_selection import train_test_split from tabtune import TabularPipeline # Load fraud detection dataset df = pd . read_csv ( 'creditcard.csv' ) # Separate features and target X = df . drop ( 'Class' , axis = 1 ) # Class: 0=normal, 1=fraud y = df [ 'Class' ] print ( f \"Class distribution: { y . value_counts () . to_dict () } \" ) # Split data X_train , X_test , y_train , y_test = train_test_split ( X , y , test_size = 0.2 , random_state = 42 , stratify = y ) # Train pipeline pipeline = TabularPipeline ( model_name = 'TabICL' , tuning_strategy = 'peft' , tuning_params = { 'device' : 'cuda' , 'epochs' : 5 , 'learning_rate' : 2e-4 , 'peft_config' : { 'r' : 8 , 'lora_alpha' : 16 } } ) print ( \"Training...\" ) pipeline . fit ( X_train , y_train ) # Evaluate metrics = pipeline . evaluate ( X_test , y_test ) print ( \" \\n === Results ===\" ) print ( f \"Accuracy: { metrics [ 'accuracy' ] : .4f } \" ) print ( f \"F1 Score: { metrics [ 'f1_score' ] : .4f } \" ) print ( f \"ROC AUC: { metrics [ 'roc_auc_score' ] : .4f } \" ) # Save for deployment pipeline . save ( 'fraud_detection_model.joblib' ) # In production loaded = TabularPipeline . load ( 'fraud_detection_model.joblib' ) new_transactions = pd . read_csv ( 'new_transactions.csv' ) fraud_predictions = loaded . predict ( new_transactions )","title":"2.1 Credit Card Fraud Detection"},{"location":"examples/classification/#22-customer-churn-prediction","text":"import pandas as pd from tabtune import TabularPipeline from sklearn.model_selection import train_test_split # Load customer data df = pd . read_csv ( 'customer_churn.csv' ) # Preprocessing X = df . drop ([ 'CustomerID' , 'Churn' ], axis = 1 ) y = ( df [ 'Churn' ] == 'Yes' ) . astype ( int ) X_train , X_test , y_train , y_test = train_test_split ( X , y , test_size = 0.2 ) # Pipeline pipeline = TabularPipeline ( model_name = 'TabBiaxial' , tuning_strategy = 'base-ft' , tuning_params = { 'device' : 'cuda' , 'epochs' : 5 , 'support_size' : 48 , 'query_size' : 32 , 'learning_rate' : 2e-5 } ) pipeline . fit ( X_train , y_train ) metrics = pipeline . evaluate ( X_test , y_test ) print ( f \"Churn Prediction Accuracy: { metrics [ 'accuracy' ] : .4f } \" ) # Identify high-risk customers churn_probs = pipeline . predict_proba ( X_test )[:, 1 ] high_risk = np . where ( churn_probs > 0.7 )[ 0 ] print ( f \"High-risk customers: { len ( high_risk ) } \" )","title":"2.2 Customer Churn Prediction"},{"location":"examples/classification/#3-multi-class-classification","text":"","title":"3. Multi-Class Classification"},{"location":"examples/classification/#31-iris-dataset-3-class","text":"Classic machine learning example: from sklearn.datasets import load_iris from sklearn.model_selection import train_test_split from tabtune import TabularPipeline # Load iris dataset X , y = load_iris ( return_X_y = True ) X_train , X_test , y_train , y_test = train_test_split ( X , y , test_size = 0.2 , random_state = 42 ) # Simple inference baseline print ( \"=== Inference Baseline ===\" ) pipeline_inf = TabularPipeline ( model_name = 'TabPFN' , tuning_strategy = 'inference' ) pipeline_inf . fit ( X_train , y_train ) inf_metrics = pipeline_inf . evaluate ( X_test , y_test ) print ( f \"Inference Accuracy: { inf_metrics [ 'accuracy' ] : .4f } \" ) # Fine-tuned model print ( \" \\n === Fine-Tuned Model ===\" ) pipeline_ft = TabularPipeline ( model_name = 'TabICL' , tuning_strategy = 'base-ft' , tuning_params = { 'device' : 'cuda' , 'epochs' : 5 , 'support_size' : 48 , 'query_size' : 32 } ) pipeline_ft . fit ( X_train , y_train ) ft_metrics = pipeline_ft . evaluate ( X_test , y_test ) print ( f \"Fine-Tuned Accuracy: { ft_metrics [ 'accuracy' ] : .4f } \" ) # Compare improvement = ( ft_metrics [ 'accuracy' ] - inf_metrics [ 'accuracy' ]) * 100 print ( f \" \\n Improvement: + { improvement : .2f } %\" )","title":"3.1 Iris Dataset (3-Class)"},{"location":"examples/classification/#32-multi-class-document-classification","text":"import pandas as pd from tabtune import TabularPipeline from sklearn.feature_extraction.text import TfidfVectorizer from sklearn.model_selection import train_test_split # Load documents df = pd . read_csv ( 'documents.csv' ) # Columns: text, category # Feature extraction from text vectorizer = TfidfVectorizer ( max_features = 500 ) X = vectorizer . fit_transform ( df [ 'text' ]) . toarray () y = pd . factorize ( df [ 'category' ])[ 0 ] X_train , X_test , y_train , y_test = train_test_split ( X , y , test_size = 0.2 ) # Train classifier pipeline = TabularPipeline ( model_name = 'TabICL' , tuning_strategy = 'base-ft' , tuning_params = { 'epochs' : 5 } ) pipeline . fit ( X_train , y_train ) metrics = pipeline . evaluate ( X_test , y_test ) print ( f \"Classification Accuracy: { metrics [ 'accuracy' ] : .4f } \" ) print ( f \"Weighted F1: { metrics [ 'f1_score' ] : .4f } \" )","title":"3.2 Multi-Class Document Classification"},{"location":"examples/classification/#5-cross-validation","text":"","title":"5. Cross-Validation"},{"location":"examples/classification/#51-k-fold-cross-validation","text":"from sklearn.model_selection import StratifiedKFold import numpy as np from tabtune import TabularPipeline def cross_validate ( X , y , model_name , params , k = 5 ): \"\"\"Perform k-fold cross-validation.\"\"\" skf = StratifiedKFold ( n_splits = k , shuffle = True , random_state = 42 ) scores = [] f1_scores = [] for fold_idx , ( train_idx , val_idx ) in enumerate ( skf . split ( X , y )): X_train_fold = X . iloc [ train_idx ] X_val_fold = X . iloc [ val_idx ] y_train_fold = y . iloc [ train_idx ] y_val_fold = y . iloc [ val_idx ] # Train pipeline = TabularPipeline ( model_name = model_name , tuning_strategy = 'base-ft' , tuning_params = params ) pipeline . fit ( X_train_fold , y_train_fold ) metrics = pipeline . evaluate ( X_val_fold , y_val_fold ) scores . append ( metrics [ 'accuracy' ]) f1_scores . append ( metrics [ 'f1_score' ]) print ( f \"Fold { fold_idx + 1 } / { k } : Acc= { metrics [ 'accuracy' ] : .4f } , \" f \"F1= { metrics [ 'f1_score' ] : .4f } \" ) print ( f \" \\n Mean Accuracy: { np . mean ( scores ) : .4f } \u00b1 { np . std ( scores ) : .4f } \" ) print ( f \"Mean F1 Score: { np . mean ( f1_scores ) : .4f } \u00b1 { np . std ( f1_scores ) : .4f } \" ) return scores , f1_scores # Usage import pandas as pd df = pd . read_csv ( 'data.csv' ) X = df . drop ( 'target' , axis = 1 ) y = df [ 'target' ] scores , f1s = cross_validate ( X , y , model_name = 'TabICL' , params = { 'epochs' : 5 }, k = 5 )","title":"5.1 k-Fold Cross-Validation"},{"location":"examples/classification/#9-real-world-datasets","text":"","title":"9. Real-World Datasets"},{"location":"examples/classification/#91-adult-income-dataset","text":"Predicting income level (binary classification): import pandas as pd from tabtune import TabularPipeline from sklearn.model_selection import train_test_split import openml # Download from OpenML dataset = openml . datasets . get_dataset ( 1590 ) # Adult dataset X , y , _ , _ = dataset . get_data ( target = dataset . default_target_attribute ) X_train , X_test , y_train , y_test = train_test_split ( X , y , test_size = 0.2 ) # Train pipeline = TabularPipeline ( model_name = 'TabDPT' , tuning_strategy = 'base-ft' , tuning_params = { 'device' : 'cuda' , 'epochs' : 3 , 'support_size' : 1024 } ) pipeline . fit ( X_train , y_train ) metrics = pipeline . evaluate ( X_test , y_test ) print ( f \"Adult Dataset Results:\" ) print ( f \"Accuracy: { metrics [ 'accuracy' ] : .4f } \" ) print ( f \"F1 Score: { metrics [ 'f1_score' ] : .4f } \" )","title":"9.1 Adult Income Dataset"},{"location":"examples/classification/#92-mnist-like-tabular-classification","text":"from sklearn.datasets import load_digits from tabtune import TabularPipeline from sklearn.model_selection import train_test_split # Load digits (multi-class: 0-9) X , y = load_digits ( return_X_y = True ) X_train , X_test , y_train , y_test = train_test_split ( X , y , test_size = 0.2 ) # Classify pipeline = TabularPipeline ( model_name = 'TabICL' , tuning_strategy = 'peft' , tuning_params = { 'device' : 'cuda' , 'epochs' : 3 , 'peft_config' : { 'r' : 8 } } ) pipeline . fit ( X_train , y_train ) metrics = pipeline . evaluate ( X_test , y_test ) print ( f \"Digit Recognition (0-9):\" ) print ( f \"Accuracy: { metrics [ 'accuracy' ] : .4f } \" ) print ( f \"Classes: { len ( set ( y )) } \" )","title":"9.2 MNIST-Like Tabular Classification"},{"location":"examples/classification/#10-troubleshooting","text":"","title":"10. Troubleshooting"},{"location":"examples/classification/#101-common-issues","text":"# Issue: Low accuracy # Solution: Try different model and strategy from tabtune import TabularLeaderboard lb = TabularLeaderboard ( X_train , X_val , y_train , y_val ) for model in [ 'TabPFN' , 'TabICL' , 'TabDPT' ]: lb . add_model ( model , 'inference' ) results = lb . run () # Compare # Issue: Out of memory # Solution: Use PEFT pipeline = TabularPipeline ( model_name = 'TabICL' , tuning_strategy = 'peft' , tuning_params = { 'peft_config' : { 'r' : 4 }} ) # Issue: Slow training # Solution: Reduce batch size or use fewer epochs pipeline = TabularPipeline ( tuning_params = { 'batch_size' : 8 , 'epochs' : 3 } )","title":"10.1 Common Issues"},{"location":"examples/classification/#11-next-steps","text":"Model Selection Guide - Choose right model Hyperparameter Tuning - Optimize performance TabularLeaderboard - Compare models systematically Saving & Loading - Deploy models These examples cover the full spectrum of classification tasks with TabTune!","title":"11. Next Steps"},{"location":"examples/large-datasets/","text":"","title":"Large Datasets"},{"location":"examples/peft-examples/","text":"PEFT Examples: Practical Parameter-Efficient Fine-Tuning Workflows \u00b6 This document provides practical, production-ready examples for using LoRA and PEFT techniques with TabTune across various scenarios and constraints. 1. Quick Start PEFT \u00b6 1.1 5-Minute PEFT Example \u00b6 Minimal code to use LoRA: from tabtune import TabularPipeline from sklearn.datasets import load_breast_cancer from sklearn.model_selection import train_test_split # Load dataset X , y = load_breast_cancer ( return_X_y = True ) X_train , X_test , y_train , y_test = train_test_split ( X , y , test_size = 0.2 ) # Create PEFT pipeline pipeline = TabularPipeline ( model_name = 'TabICL' , tuning_strategy = 'peft' , tuning_params = { 'device' : 'cuda' , 'epochs' : 5 , 'learning_rate' : 2e-4 , 'peft_config' : { 'r' : 8 , 'lora_alpha' : 16 , 'lora_dropout' : 0.05 } } ) # Train (much faster and lighter than base-ft) pipeline . fit ( X_train , y_train ) # Evaluate metrics = pipeline . evaluate ( X_test , y_test ) print ( f \"Accuracy: { metrics [ 'accuracy' ] : .4f } \" ) print ( f \"Model size: 1-2% of full model\" ) 1.2 Comparing Base-FT vs PEFT \u00b6 import time import torch from tabtune import TabularPipeline X_train , X_test , y_train , y_test = load_data () # Method 1: Base Fine-Tuning print ( \"=== Base Fine-Tuning ===\" ) torch . cuda . reset_peak_memory_stats () start = time . time () pipeline_base = TabularPipeline ( model_name = 'TabICL' , tuning_strategy = 'base-ft' , tuning_params = { 'epochs' : 5 } ) pipeline_base . fit ( X_train , y_train ) base_time = time . time () - start base_memory = torch . cuda . max_memory_allocated () / 1e9 metrics_base = pipeline_base . evaluate ( X_test , y_test ) print ( f \"Time: { base_time : .1f } s\" ) print ( f \"Memory: { base_memory : .1f } GB\" ) print ( f \"Accuracy: { metrics_base [ 'accuracy' ] : .4f } \" ) # Method 2: PEFT print ( \" \\n === PEFT (LoRA) ===\" ) torch . cuda . reset_peak_memory_stats () start = time . time () pipeline_peft = TabularPipeline ( model_name = 'TabICL' , tuning_strategy = 'peft' , tuning_params = { 'epochs' : 5 , 'peft_config' : { 'r' : 8 } } ) pipeline_peft . fit ( X_train , y_train ) peft_time = time . time () - start peft_memory = torch . cuda . max_memory_allocated () / 1e9 metrics_peft = pipeline_peft . evaluate ( X_test , y_test ) print ( f \"Time: { peft_time : .1f } s\" ) print ( f \"Memory: { peft_memory : .1f } GB\" ) print ( f \"Accuracy: { metrics_peft [ 'accuracy' ] : .4f } \" ) # Comparison print ( \" \\n === Comparison ===\" ) print ( f \"Speedup: { base_time / peft_time : .1f } x\" ) print ( f \"Memory savings: { ( 1 - peft_memory / base_memory ) * 100 : .0f } %\" ) print ( f \"Accuracy loss: { ( metrics_base [ 'accuracy' ] - metrics_peft [ 'accuracy' ]) * 100 : .2f } %\" ) 2. Memory-Constrained Training \u00b6 2.1 Training on Limited GPU (4GB) \u00b6 import torch from tabtune import TabularPipeline # Check available GPU memory available_memory = torch . cuda . get_device_properties ( 0 ) . total_memory / 1e9 print ( f \"Available GPU memory: { available_memory : .1f } GB\" ) if available_memory < 4 : print ( \"Using ultra-efficient PEFT configuration...\" ) pipeline = TabularPipeline ( model_name = 'TabICL' , tuning_strategy = 'peft' , tuning_params = { 'device' : 'cuda' , 'epochs' : 3 , 'learning_rate' : 2e-4 , 'batch_size' : 4 , # Small batch 'support_size' : 32 , # Small context 'query_size' : 16 , 'num_workers' : 0 , # No parallel loading 'peft_config' : { 'r' : 4 , # Very low rank 'lora_alpha' : 8 , 'lora_dropout' : 0.1 } } ) elif available_memory < 8 : print ( \"Using efficient PEFT configuration...\" ) pipeline = TabularPipeline ( model_name = 'TabICL' , tuning_strategy = 'peft' , tuning_params = { 'device' : 'cuda' , 'epochs' : 5 , 'learning_rate' : 2e-4 , 'batch_size' : 8 , 'support_size' : 64 , 'query_size' : 32 , 'peft_config' : { 'r' : 8 , 'lora_alpha' : 16 , 'lora_dropout' : 0.05 } } ) else : print ( \"Sufficient memory. Using standard PEFT...\" ) pipeline = TabularPipeline ( model_name = 'TabICL' , tuning_strategy = 'peft' , tuning_params = { 'device' : 'cuda' , 'epochs' : 5 , 'learning_rate' : 2e-4 , 'peft_config' : { 'r' : 8 } } ) # Train pipeline . fit ( X_train , y_train ) 4. LoRA Rank Selection \u00b6 4.1 Choosing Optimal Rank \u00b6 import numpy as np from tabtune import TabularPipeline def evaluate_rank ( X_train , X_test , y_train , y_test , rank ): \"\"\"Evaluate specific LoRA rank.\"\"\" pipeline = TabularPipeline ( model_name = 'TabICL' , tuning_strategy = 'peft' , tuning_params = { 'device' : 'cuda' , 'epochs' : 3 , 'peft_config' : { 'r' : rank , 'lora_alpha' : 2 * rank , 'lora_dropout' : 0.05 } } ) pipeline . fit ( X_train , y_train ) metrics = pipeline . evaluate ( X_test , y_test ) return metrics # Evaluate multiple ranks ranks = [ 2 , 4 , 8 , 16 , 32 ] results = {} print ( \"Evaluating LoRA ranks...\" ) for r in ranks : print ( f \" Testing rank { r } ...\" , end = '' , flush = True ) metrics = evaluate_rank ( X_train , X_test , y_train , y_test , r ) results [ r ] = metrics [ 'accuracy' ] print ( f \" Accuracy: { metrics [ 'accuracy' ] : .4f } \" ) # Find optimal rank optimal_rank = max ( results , key = results . get ) optimal_acc = results [ optimal_rank ] print ( f \" \\n Optimal rank: { optimal_rank } (Accuracy: { optimal_acc : .4f } )\" ) # Plot results import matplotlib.pyplot as plt plt . figure ( figsize = ( 10 , 6 )) plt . plot ( ranks , [ results [ r ] for r in ranks ], 'o-' , linewidth = 2 , markersize = 8 ) plt . xlabel ( 'LoRA Rank' ) plt . ylabel ( 'Accuracy' ) plt . title ( 'LoRA Rank vs Model Accuracy' ) plt . grid ( True , alpha = 0.3 ) plt . savefig ( 'rank_analysis.png' ) 6. Advanced PEFT Techniques \u00b6 6.1 Custom Target Modules \u00b6 from tabtune import TabularPipeline # Train only specific layers pipeline = TabularPipeline ( model_name = 'TabICL' , tuning_strategy = 'peft' , tuning_params = { 'device' : 'cuda' , 'epochs' : 5 , 'learning_rate' : 2e-4 , 'peft_config' : { 'r' : 8 , 'lora_alpha' : 16 , 'target_modules' : [ 'col_embedder.tf_col' , # Column embedder only 'icl_predictor.decoder' # Plus decoder ] } } ) pipeline . fit ( X_train , y_train ) 6.2 LoRA with Different Learning Rates \u00b6 from tabtune import TabularPipeline # Higher learning rate for smaller LoRA modules pipeline = TabularPipeline ( model_name = 'TabICL' , tuning_strategy = 'peft' , tuning_params = { 'device' : 'cuda' , 'epochs' : 5 , 'learning_rate' : 1e-3 , # 10x higher for PEFT 'peft_config' : { 'r' : 8 , 'lora_alpha' : 16 } } ) pipeline . fit ( X_train , y_train ) 9. Troubleshooting PEFT \u00b6 9.1 Common PEFT Issues \u00b6 # Issue 1: PEFT accuracy much lower than base-ft # Solution: Increase rank peft_config = { 'r' : 16 , # Instead of 4 'lora_alpha' : 32 } # Issue 2: PEFT training diverging # Solution: Reduce learning rate tuning_params = { 'learning_rate' : 1e-4 # Instead of 2e-4 } # Issue 3: PEFT still using too much memory # Solution: Combine PEFT + mixed precision + gradient accumulation tuning_params = { 'learning_rate' : 2e-4 , 'batch_size' : 4 , 'gradient_accumulation_steps' : 8 , 'mixed_precision' : 'fp16' , 'peft_config' : { 'r' : 4 } } # Issue 4: PEFT slower than expected # Solution: Verify LoRA is applied print ( pipeline . model ) # Check for LoRA modules 10. PEFT Best Practices \u00b6 \u2705 Do's \u00b6 \u2705 Start with r=8 (good default) \u2705 Use 2x learning rate for PEFT \u2705 Include warmup steps \u2705 Monitor gradient norms \u2705 Use gradient clipping \u2705 Test rank selection \u2705 Save adapter weights separately \u274c Don'ts \u00b6 \u274c Don't use same learning rate as base-ft \u274c Don't use rank < 2 \u274c Don't skip regularization \u274c Don't forget to scale learning rate \u274c Don't train very long (overfit risk) 11. PEFT Performance Summary \u00b6 Typical Results on 100K Sample Classification Task: Base Fine-Tuning: Training Time: 30 minutes Memory: 12 GB Accuracy: 90.5% Model Size: 500 MB PEFT (r=8): Training Time: 10 minutes (3x faster) Memory: 3 GB (75% reduction) Accuracy: 89.8% (0.7% loss) Model Size: 5 MB (100x smaller) Trade-off Analysis: Speed: 3x faster Memory: 75% reduction Storage: 100x smaller Accuracy: Only 0.7% lower RECOMMENDATION: Use PEFT for most scenarios 12. Quick Reference \u00b6 Scenario r alpha dropout LR Notes Memory constrained 4 8 0.1 1e-4 Ultra-low resource Standard 8 16 0.05 2e-4 Default, balanced High accuracy 16 32 0.02 1e-4 Best results Large data (1M) 8 16 0.05 2e-4 TabDPT recommended 13. Next Steps \u00b6 PEFT & LoRA - Theory and mathematics Memory Optimization - Memory techniques Hyperparameter Tuning - Optimization Classification Examples - Complete workflows Master PEFT for efficient, production-ready fine-tuning!","title":"PEFT Fine-Tuning"},{"location":"examples/peft-examples/#peft-examples-practical-parameter-efficient-fine-tuning-workflows","text":"This document provides practical, production-ready examples for using LoRA and PEFT techniques with TabTune across various scenarios and constraints.","title":"PEFT Examples: Practical Parameter-Efficient Fine-Tuning Workflows"},{"location":"examples/peft-examples/#1-quick-start-peft","text":"","title":"1. Quick Start PEFT"},{"location":"examples/peft-examples/#11-5-minute-peft-example","text":"Minimal code to use LoRA: from tabtune import TabularPipeline from sklearn.datasets import load_breast_cancer from sklearn.model_selection import train_test_split # Load dataset X , y = load_breast_cancer ( return_X_y = True ) X_train , X_test , y_train , y_test = train_test_split ( X , y , test_size = 0.2 ) # Create PEFT pipeline pipeline = TabularPipeline ( model_name = 'TabICL' , tuning_strategy = 'peft' , tuning_params = { 'device' : 'cuda' , 'epochs' : 5 , 'learning_rate' : 2e-4 , 'peft_config' : { 'r' : 8 , 'lora_alpha' : 16 , 'lora_dropout' : 0.05 } } ) # Train (much faster and lighter than base-ft) pipeline . fit ( X_train , y_train ) # Evaluate metrics = pipeline . evaluate ( X_test , y_test ) print ( f \"Accuracy: { metrics [ 'accuracy' ] : .4f } \" ) print ( f \"Model size: 1-2% of full model\" )","title":"1.1 5-Minute PEFT Example"},{"location":"examples/peft-examples/#12-comparing-base-ft-vs-peft","text":"import time import torch from tabtune import TabularPipeline X_train , X_test , y_train , y_test = load_data () # Method 1: Base Fine-Tuning print ( \"=== Base Fine-Tuning ===\" ) torch . cuda . reset_peak_memory_stats () start = time . time () pipeline_base = TabularPipeline ( model_name = 'TabICL' , tuning_strategy = 'base-ft' , tuning_params = { 'epochs' : 5 } ) pipeline_base . fit ( X_train , y_train ) base_time = time . time () - start base_memory = torch . cuda . max_memory_allocated () / 1e9 metrics_base = pipeline_base . evaluate ( X_test , y_test ) print ( f \"Time: { base_time : .1f } s\" ) print ( f \"Memory: { base_memory : .1f } GB\" ) print ( f \"Accuracy: { metrics_base [ 'accuracy' ] : .4f } \" ) # Method 2: PEFT print ( \" \\n === PEFT (LoRA) ===\" ) torch . cuda . reset_peak_memory_stats () start = time . time () pipeline_peft = TabularPipeline ( model_name = 'TabICL' , tuning_strategy = 'peft' , tuning_params = { 'epochs' : 5 , 'peft_config' : { 'r' : 8 } } ) pipeline_peft . fit ( X_train , y_train ) peft_time = time . time () - start peft_memory = torch . cuda . max_memory_allocated () / 1e9 metrics_peft = pipeline_peft . evaluate ( X_test , y_test ) print ( f \"Time: { peft_time : .1f } s\" ) print ( f \"Memory: { peft_memory : .1f } GB\" ) print ( f \"Accuracy: { metrics_peft [ 'accuracy' ] : .4f } \" ) # Comparison print ( \" \\n === Comparison ===\" ) print ( f \"Speedup: { base_time / peft_time : .1f } x\" ) print ( f \"Memory savings: { ( 1 - peft_memory / base_memory ) * 100 : .0f } %\" ) print ( f \"Accuracy loss: { ( metrics_base [ 'accuracy' ] - metrics_peft [ 'accuracy' ]) * 100 : .2f } %\" )","title":"1.2 Comparing Base-FT vs PEFT"},{"location":"examples/peft-examples/#2-memory-constrained-training","text":"","title":"2. Memory-Constrained Training"},{"location":"examples/peft-examples/#21-training-on-limited-gpu-4gb","text":"import torch from tabtune import TabularPipeline # Check available GPU memory available_memory = torch . cuda . get_device_properties ( 0 ) . total_memory / 1e9 print ( f \"Available GPU memory: { available_memory : .1f } GB\" ) if available_memory < 4 : print ( \"Using ultra-efficient PEFT configuration...\" ) pipeline = TabularPipeline ( model_name = 'TabICL' , tuning_strategy = 'peft' , tuning_params = { 'device' : 'cuda' , 'epochs' : 3 , 'learning_rate' : 2e-4 , 'batch_size' : 4 , # Small batch 'support_size' : 32 , # Small context 'query_size' : 16 , 'num_workers' : 0 , # No parallel loading 'peft_config' : { 'r' : 4 , # Very low rank 'lora_alpha' : 8 , 'lora_dropout' : 0.1 } } ) elif available_memory < 8 : print ( \"Using efficient PEFT configuration...\" ) pipeline = TabularPipeline ( model_name = 'TabICL' , tuning_strategy = 'peft' , tuning_params = { 'device' : 'cuda' , 'epochs' : 5 , 'learning_rate' : 2e-4 , 'batch_size' : 8 , 'support_size' : 64 , 'query_size' : 32 , 'peft_config' : { 'r' : 8 , 'lora_alpha' : 16 , 'lora_dropout' : 0.05 } } ) else : print ( \"Sufficient memory. Using standard PEFT...\" ) pipeline = TabularPipeline ( model_name = 'TabICL' , tuning_strategy = 'peft' , tuning_params = { 'device' : 'cuda' , 'epochs' : 5 , 'learning_rate' : 2e-4 , 'peft_config' : { 'r' : 8 } } ) # Train pipeline . fit ( X_train , y_train )","title":"2.1 Training on Limited GPU (4GB)"},{"location":"examples/peft-examples/#4-lora-rank-selection","text":"","title":"4. LoRA Rank Selection"},{"location":"examples/peft-examples/#41-choosing-optimal-rank","text":"import numpy as np from tabtune import TabularPipeline def evaluate_rank ( X_train , X_test , y_train , y_test , rank ): \"\"\"Evaluate specific LoRA rank.\"\"\" pipeline = TabularPipeline ( model_name = 'TabICL' , tuning_strategy = 'peft' , tuning_params = { 'device' : 'cuda' , 'epochs' : 3 , 'peft_config' : { 'r' : rank , 'lora_alpha' : 2 * rank , 'lora_dropout' : 0.05 } } ) pipeline . fit ( X_train , y_train ) metrics = pipeline . evaluate ( X_test , y_test ) return metrics # Evaluate multiple ranks ranks = [ 2 , 4 , 8 , 16 , 32 ] results = {} print ( \"Evaluating LoRA ranks...\" ) for r in ranks : print ( f \" Testing rank { r } ...\" , end = '' , flush = True ) metrics = evaluate_rank ( X_train , X_test , y_train , y_test , r ) results [ r ] = metrics [ 'accuracy' ] print ( f \" Accuracy: { metrics [ 'accuracy' ] : .4f } \" ) # Find optimal rank optimal_rank = max ( results , key = results . get ) optimal_acc = results [ optimal_rank ] print ( f \" \\n Optimal rank: { optimal_rank } (Accuracy: { optimal_acc : .4f } )\" ) # Plot results import matplotlib.pyplot as plt plt . figure ( figsize = ( 10 , 6 )) plt . plot ( ranks , [ results [ r ] for r in ranks ], 'o-' , linewidth = 2 , markersize = 8 ) plt . xlabel ( 'LoRA Rank' ) plt . ylabel ( 'Accuracy' ) plt . title ( 'LoRA Rank vs Model Accuracy' ) plt . grid ( True , alpha = 0.3 ) plt . savefig ( 'rank_analysis.png' )","title":"4.1 Choosing Optimal Rank"},{"location":"examples/peft-examples/#6-advanced-peft-techniques","text":"","title":"6. Advanced PEFT Techniques"},{"location":"examples/peft-examples/#61-custom-target-modules","text":"from tabtune import TabularPipeline # Train only specific layers pipeline = TabularPipeline ( model_name = 'TabICL' , tuning_strategy = 'peft' , tuning_params = { 'device' : 'cuda' , 'epochs' : 5 , 'learning_rate' : 2e-4 , 'peft_config' : { 'r' : 8 , 'lora_alpha' : 16 , 'target_modules' : [ 'col_embedder.tf_col' , # Column embedder only 'icl_predictor.decoder' # Plus decoder ] } } ) pipeline . fit ( X_train , y_train )","title":"6.1 Custom Target Modules"},{"location":"examples/peft-examples/#62-lora-with-different-learning-rates","text":"from tabtune import TabularPipeline # Higher learning rate for smaller LoRA modules pipeline = TabularPipeline ( model_name = 'TabICL' , tuning_strategy = 'peft' , tuning_params = { 'device' : 'cuda' , 'epochs' : 5 , 'learning_rate' : 1e-3 , # 10x higher for PEFT 'peft_config' : { 'r' : 8 , 'lora_alpha' : 16 } } ) pipeline . fit ( X_train , y_train )","title":"6.2 LoRA with Different Learning Rates"},{"location":"examples/peft-examples/#9-troubleshooting-peft","text":"","title":"9. Troubleshooting PEFT"},{"location":"examples/peft-examples/#91-common-peft-issues","text":"# Issue 1: PEFT accuracy much lower than base-ft # Solution: Increase rank peft_config = { 'r' : 16 , # Instead of 4 'lora_alpha' : 32 } # Issue 2: PEFT training diverging # Solution: Reduce learning rate tuning_params = { 'learning_rate' : 1e-4 # Instead of 2e-4 } # Issue 3: PEFT still using too much memory # Solution: Combine PEFT + mixed precision + gradient accumulation tuning_params = { 'learning_rate' : 2e-4 , 'batch_size' : 4 , 'gradient_accumulation_steps' : 8 , 'mixed_precision' : 'fp16' , 'peft_config' : { 'r' : 4 } } # Issue 4: PEFT slower than expected # Solution: Verify LoRA is applied print ( pipeline . model ) # Check for LoRA modules","title":"9.1 Common PEFT Issues"},{"location":"examples/peft-examples/#10-peft-best-practices","text":"","title":"10. PEFT Best Practices"},{"location":"examples/peft-examples/#dos","text":"\u2705 Start with r=8 (good default) \u2705 Use 2x learning rate for PEFT \u2705 Include warmup steps \u2705 Monitor gradient norms \u2705 Use gradient clipping \u2705 Test rank selection \u2705 Save adapter weights separately","title":"\u2705 Do's"},{"location":"examples/peft-examples/#donts","text":"\u274c Don't use same learning rate as base-ft \u274c Don't use rank < 2 \u274c Don't skip regularization \u274c Don't forget to scale learning rate \u274c Don't train very long (overfit risk)","title":"\u274c Don'ts"},{"location":"examples/peft-examples/#11-peft-performance-summary","text":"Typical Results on 100K Sample Classification Task: Base Fine-Tuning: Training Time: 30 minutes Memory: 12 GB Accuracy: 90.5% Model Size: 500 MB PEFT (r=8): Training Time: 10 minutes (3x faster) Memory: 3 GB (75% reduction) Accuracy: 89.8% (0.7% loss) Model Size: 5 MB (100x smaller) Trade-off Analysis: Speed: 3x faster Memory: 75% reduction Storage: 100x smaller Accuracy: Only 0.7% lower RECOMMENDATION: Use PEFT for most scenarios","title":"11. PEFT Performance Summary"},{"location":"examples/peft-examples/#12-quick-reference","text":"Scenario r alpha dropout LR Notes Memory constrained 4 8 0.1 1e-4 Ultra-low resource Standard 8 16 0.05 2e-4 Default, balanced High accuracy 16 32 0.02 1e-4 Best results Large data (1M) 8 16 0.05 2e-4 TabDPT recommended","title":"12. Quick Reference"},{"location":"examples/peft-examples/#13-next-steps","text":"PEFT & LoRA - Theory and mathematics Memory Optimization - Memory techniques Hyperparameter Tuning - Optimization Classification Examples - Complete workflows Master PEFT for efficient, production-ready fine-tuning!","title":"13. Next Steps"},{"location":"getting-started/installation/","text":"Installation \u00b6 This guide will walk you through installing TabTune and its dependencies for optimal performance across different environments. System Requirements \u00b6 Python Version \u00b6 Python 3.11+ (required) Python 3.12+ (recommended for best performance) Hardware \u00b6 Minimum : 8GB RAM, 2GB free disk space Recommended : 16GB+ RAM, NVIDIA GPU with 8GB+ VRAM For Large Datasets : 32GB+ RAM, multiple GPUs Installation Methods \u00b6 Method 1: Install from Source (Recommended) \u00b6 Clone the repository git clone https://github.com/AryaXAI/TabTune.git pip install -r requirements.txt cd TabTune pip install -e . Create virtual environment # Using venv python -m venv tabtune-env source tabtune-env/bin/activate # Linux/macOS # tabtune-env\\Scripts\\activate # Windows # Or using conda conda create -n tabtune python = 3 .11 conda activate tabtune GPU Support For optimal performance with large models, install CUDA-enabled PyTorch. Check your CUDA version with nvidia-smi . Core Dependencies \u00b6 The following packages are automatically installed with TabTune: Essential Packages \u00b6 # Core ML libraries torch> = 2 .0.0 numpy> = 1 .21.0 pandas> = 1 .3.0 scikit-learn> = 1 .0.0 # Data handling openml> = 0 .12.0 datasets> = 2 .0.0 # PEFT support peft> = 0 .4.0 accelerate> = 0 .20.0 transformers> = 4 .30.0 # Utilities joblib> = 1 .0.0 tqdm> = 4 .60.0 Model-Specific Dependencies \u00b6 # For ContextTab (requires HuggingFace Hub access) huggingface-hub> = 0 .15.0 sentence-transformers> = 2 .2.0 # For advanced preprocessing category-encoders> = 2 .5.0 Verify Installation \u00b6 Quick Verification \u00b6 import torch print ( f \"PyTorch version: { torch . __version__ } \" ) print ( f \"CUDA available: { torch . cuda . is_available () } \" ) # Test TabTune import from TabularPipeline.pipeline import TabularPipeline print ( \"\u2705 TabTune successfully installed!\" ) GPU Verification \u00b6 import torch if torch . cuda . is_available (): print ( f \"\u2705 CUDA available: { torch . cuda . get_device_name ( 0 ) } \" ) print ( f \"GPU Memory: { torch . cuda . get_device_properties ( 0 ) . total_memory / 1e9 : .1f } GB\" ) else : print ( \"\u2139\ufe0f CUDA not available, using CPU\" ) Model Loading Test \u00b6 from TabularPipeline.pipeline import TabularPipeline import pandas as pd # Quick smoke test df = pd . DataFrame ({ 'a' : [ 1 , 2 , 3 ], 'b' : [ 4 , 5 , 6 ]}) y = pd . Series ([ 0 , 1 , 0 ]) pipeline = TabularPipeline ( model_name = \"TabPFN\" , tuning_strategy = \"inference\" ) print ( \"\u2705 Pipeline creation successful!\" ) Troubleshooting \u00b6 Common Installation Issues \u00b6 Issue: ModuleNotFoundError for torch \u00b6 # Solution: Reinstall PyTorch with correct CUDA version pip uninstall torch torchvision torchaudio pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/check_version_in_requirements Issue: CUDA out of memory during model loading \u00b6 # Solution: Use smaller batch sizes or CPU fallback pipeline = TabularPipeline ( model_name = \"TabPFN\" , tuning_strategy = \"inference\" , tuning_params = { \"device\" : \"cpu\" , \"batch_size\" : 16 } ) Issue: ContextTab model access denied \u00b6 # Solution: Set up HuggingFace token export HF_TOKEN = \"your_huggingface_token\" # Or login interactively huggingface-cli login Issue: Permission denied on Windows \u00b6 # Solution: Run as administrator or use --user flag pip install --user -e . Memory Issues \u00b6 Large Dataset Handling \u00b6 # Use chunked processing for large datasets tuning_params = { \"batch_size\" : 8 , # Reduce batch size \"gradient_accumulation_steps\" : 4 , # Maintain effective batch size \"device\" : \"cuda\" } PEFT Memory Optimization \u00b6 # Use PEFT for memory-efficient fine-tuning pipeline = TabularPipeline ( model_name = \"TabICL\" , tuning_strategy = \"peft\" , tuning_params = { \"peft_config\" : { \"r\" : 4 } # Lower rank for less memory } ) Environment Variables \u00b6 Set these environment variables for optimal performance: # HuggingFace token for gated models export HF_TOKEN = \"your_token_here\" # Disable tokenizers parallelism warnings export TOKENIZERS_PARALLELISM = false # CUDA memory management export PYTORCH_CUDA_ALLOC_CONF = max_split_size_mb:512 # For debugging export CUDA_LAUNCH_BLOCKING = 1 Next Steps \u00b6 After successful installation: Quick Start Guide - Run your first tabtune example Basic Concepts - Understand the core architecture Model Selection - Choose the right model for your task Installation Complete You're now ready to start using TabTune! If you encounter any issues, please check our FAQ or open an issue on GitHub .","title":"Installation"},{"location":"getting-started/installation/#installation","text":"This guide will walk you through installing TabTune and its dependencies for optimal performance across different environments.","title":"Installation"},{"location":"getting-started/installation/#system-requirements","text":"","title":"System Requirements"},{"location":"getting-started/installation/#python-version","text":"Python 3.11+ (required) Python 3.12+ (recommended for best performance)","title":"Python Version"},{"location":"getting-started/installation/#hardware","text":"Minimum : 8GB RAM, 2GB free disk space Recommended : 16GB+ RAM, NVIDIA GPU with 8GB+ VRAM For Large Datasets : 32GB+ RAM, multiple GPUs","title":"Hardware"},{"location":"getting-started/installation/#installation-methods","text":"","title":"Installation Methods"},{"location":"getting-started/installation/#method-1-install-from-source-recommended","text":"Clone the repository git clone https://github.com/AryaXAI/TabTune.git pip install -r requirements.txt cd TabTune pip install -e . Create virtual environment # Using venv python -m venv tabtune-env source tabtune-env/bin/activate # Linux/macOS # tabtune-env\\Scripts\\activate # Windows # Or using conda conda create -n tabtune python = 3 .11 conda activate tabtune GPU Support For optimal performance with large models, install CUDA-enabled PyTorch. Check your CUDA version with nvidia-smi .","title":"Method 1: Install from Source (Recommended)"},{"location":"getting-started/installation/#core-dependencies","text":"The following packages are automatically installed with TabTune:","title":"Core Dependencies"},{"location":"getting-started/installation/#essential-packages","text":"# Core ML libraries torch> = 2 .0.0 numpy> = 1 .21.0 pandas> = 1 .3.0 scikit-learn> = 1 .0.0 # Data handling openml> = 0 .12.0 datasets> = 2 .0.0 # PEFT support peft> = 0 .4.0 accelerate> = 0 .20.0 transformers> = 4 .30.0 # Utilities joblib> = 1 .0.0 tqdm> = 4 .60.0","title":"Essential Packages"},{"location":"getting-started/installation/#model-specific-dependencies","text":"# For ContextTab (requires HuggingFace Hub access) huggingface-hub> = 0 .15.0 sentence-transformers> = 2 .2.0 # For advanced preprocessing category-encoders> = 2 .5.0","title":"Model-Specific Dependencies"},{"location":"getting-started/installation/#verify-installation","text":"","title":"Verify Installation"},{"location":"getting-started/installation/#quick-verification","text":"import torch print ( f \"PyTorch version: { torch . __version__ } \" ) print ( f \"CUDA available: { torch . cuda . is_available () } \" ) # Test TabTune import from TabularPipeline.pipeline import TabularPipeline print ( \"\u2705 TabTune successfully installed!\" )","title":"Quick Verification"},{"location":"getting-started/installation/#gpu-verification","text":"import torch if torch . cuda . is_available (): print ( f \"\u2705 CUDA available: { torch . cuda . get_device_name ( 0 ) } \" ) print ( f \"GPU Memory: { torch . cuda . get_device_properties ( 0 ) . total_memory / 1e9 : .1f } GB\" ) else : print ( \"\u2139\ufe0f CUDA not available, using CPU\" )","title":"GPU Verification"},{"location":"getting-started/installation/#model-loading-test","text":"from TabularPipeline.pipeline import TabularPipeline import pandas as pd # Quick smoke test df = pd . DataFrame ({ 'a' : [ 1 , 2 , 3 ], 'b' : [ 4 , 5 , 6 ]}) y = pd . Series ([ 0 , 1 , 0 ]) pipeline = TabularPipeline ( model_name = \"TabPFN\" , tuning_strategy = \"inference\" ) print ( \"\u2705 Pipeline creation successful!\" )","title":"Model Loading Test"},{"location":"getting-started/installation/#troubleshooting","text":"","title":"Troubleshooting"},{"location":"getting-started/installation/#common-installation-issues","text":"","title":"Common Installation Issues"},{"location":"getting-started/installation/#issue-modulenotfounderror-for-torch","text":"# Solution: Reinstall PyTorch with correct CUDA version pip uninstall torch torchvision torchaudio pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/check_version_in_requirements","title":"Issue: ModuleNotFoundError for torch"},{"location":"getting-started/installation/#issue-cuda-out-of-memory-during-model-loading","text":"# Solution: Use smaller batch sizes or CPU fallback pipeline = TabularPipeline ( model_name = \"TabPFN\" , tuning_strategy = \"inference\" , tuning_params = { \"device\" : \"cpu\" , \"batch_size\" : 16 } )","title":"Issue: CUDA out of memory during model loading"},{"location":"getting-started/installation/#issue-contexttab-model-access-denied","text":"# Solution: Set up HuggingFace token export HF_TOKEN = \"your_huggingface_token\" # Or login interactively huggingface-cli login","title":"Issue: ContextTab model access denied"},{"location":"getting-started/installation/#issue-permission-denied-on-windows","text":"# Solution: Run as administrator or use --user flag pip install --user -e .","title":"Issue: Permission denied on Windows"},{"location":"getting-started/installation/#memory-issues","text":"","title":"Memory Issues"},{"location":"getting-started/installation/#large-dataset-handling","text":"# Use chunked processing for large datasets tuning_params = { \"batch_size\" : 8 , # Reduce batch size \"gradient_accumulation_steps\" : 4 , # Maintain effective batch size \"device\" : \"cuda\" }","title":"Large Dataset Handling"},{"location":"getting-started/installation/#peft-memory-optimization","text":"# Use PEFT for memory-efficient fine-tuning pipeline = TabularPipeline ( model_name = \"TabICL\" , tuning_strategy = \"peft\" , tuning_params = { \"peft_config\" : { \"r\" : 4 } # Lower rank for less memory } )","title":"PEFT Memory Optimization"},{"location":"getting-started/installation/#environment-variables","text":"Set these environment variables for optimal performance: # HuggingFace token for gated models export HF_TOKEN = \"your_token_here\" # Disable tokenizers parallelism warnings export TOKENIZERS_PARALLELISM = false # CUDA memory management export PYTORCH_CUDA_ALLOC_CONF = max_split_size_mb:512 # For debugging export CUDA_LAUNCH_BLOCKING = 1","title":"Environment Variables"},{"location":"getting-started/installation/#next-steps","text":"After successful installation: Quick Start Guide - Run your first tabtune example Basic Concepts - Understand the core architecture Model Selection - Choose the right model for your task Installation Complete You're now ready to start using TabTune! If you encounter any issues, please check our FAQ or open an issue on GitHub .","title":"Next Steps"},{"location":"getting-started/quick-start/","text":"Quick Start \u00b6 This quick start guide demonstrates how to run a complete end-to-end workflow with TabTune in just a few steps. 1. Prepare Your Environment \u00b6 Ensure you have installed TabTune and its dependencies as per the Installation Guide . Activate your virtual environment: # If using venv source tabtune-env/bin/activate # If using conda autoconda activate tabtune 2. Load a Dataset \u00b6 We use the Telco Customer Churn dataset from OpenML for this example. import openml from sklearn.model_selection import train_test_split # Load dataset dataset = openml . datasets . get_dataset ( 42178 ) X , y , _ , _ = dataset . get_data ( target = dataset . default_target_attribute ) # Split into train and test sets X_train , X_test , y_train , y_test = train_test_split ( X , y , test_size = 0.25 , random_state = 42 ) print ( \"Train shape:\" , X_train . shape ) print ( \"Test shape:\" , X_test . shape ) 3. Initialize and Configure the Pipeline \u00b6 Use the TabularPipeline class to define your model, task type, and tuning strategy. from tabtune import TabularPipeline # Base fine-tuning example pipeline = TabularPipeline ( model_name = \"TabPFN\" , task_type = \"classification\" , tuning_strategy = \"base-ft\" , tuning_params = { \"device\" : \"cuda\" , # or \"cpu\" if GPU unavailable \"epochs\" : 3 , \"learning_rate\" : 2e-5 , \"show_progress\" : True } ) 4. Fit the Model \u00b6 Train your pipeline on the training data. pipeline . fit ( X_train , y_train ) During training, TabTune will automatically handle data preprocessing and apply the chosen tuning strategy. 5. Evaluate and Predict \u00b6 After training, evaluate performance and generate predictions: # Evaluate on test set metrics = pipeline . evaluate ( X_test , y_test ) print ( \"Evaluation metrics:\" , metrics ) # Make predictions predictions = pipeline . predict ( X_test ) Supported metrics include: Accuracy , Weighted F1 Score , and ROC AUC Score . 6. Save and Load the Pipeline \u00b6 Persist your trained pipeline for later use: # Save to disk pipeline . save ( \"tabtune_pipeline.joblib\" ) # Load from disk loaded_pipeline = TabularPipeline . load ( \"tabtune_pipeline.joblib\" ) results = loaded_pipeline . predict ( X_test ) 7. Try PEFT (LoRA) Strategy \u00b6 Switch to parameter-efficient fine-tuning with minimal code changes: # PEFT fine-tuning peft_pipeline = TabularPipeline ( model_name = \"TabPFN\" , task_type = \"classification\" , tuning_strategy = \"peft\" , tuning_params = { \"device\" : \"cuda\" , \"epochs\" : 3 , \"learning_rate\" : 2e-4 , \"peft_config\" : { \"r\" : 8 , \"lora_alpha\" : 16 , \"lora_dropout\" : 0.05 } } ) peft_pipeline . fit ( X_train , y_train ) metrics_peft = peft_pipeline . evaluate ( X_test , y_test ) print ( \"PEFT metrics:\" , metrics_peft ) Next Steps \u00b6 Explore advanced configurations in the User Guide Compare multiple models with the TabularLeaderboard Dive into PEFT internals in PEFT & LoRA","title":"Quick Start"},{"location":"getting-started/quick-start/#quick-start","text":"This quick start guide demonstrates how to run a complete end-to-end workflow with TabTune in just a few steps.","title":"Quick Start"},{"location":"getting-started/quick-start/#1-prepare-your-environment","text":"Ensure you have installed TabTune and its dependencies as per the Installation Guide . Activate your virtual environment: # If using venv source tabtune-env/bin/activate # If using conda autoconda activate tabtune","title":"1. Prepare Your Environment"},{"location":"getting-started/quick-start/#2-load-a-dataset","text":"We use the Telco Customer Churn dataset from OpenML for this example. import openml from sklearn.model_selection import train_test_split # Load dataset dataset = openml . datasets . get_dataset ( 42178 ) X , y , _ , _ = dataset . get_data ( target = dataset . default_target_attribute ) # Split into train and test sets X_train , X_test , y_train , y_test = train_test_split ( X , y , test_size = 0.25 , random_state = 42 ) print ( \"Train shape:\" , X_train . shape ) print ( \"Test shape:\" , X_test . shape )","title":"2. Load a Dataset"},{"location":"getting-started/quick-start/#3-initialize-and-configure-the-pipeline","text":"Use the TabularPipeline class to define your model, task type, and tuning strategy. from tabtune import TabularPipeline # Base fine-tuning example pipeline = TabularPipeline ( model_name = \"TabPFN\" , task_type = \"classification\" , tuning_strategy = \"base-ft\" , tuning_params = { \"device\" : \"cuda\" , # or \"cpu\" if GPU unavailable \"epochs\" : 3 , \"learning_rate\" : 2e-5 , \"show_progress\" : True } )","title":"3. Initialize and Configure the Pipeline"},{"location":"getting-started/quick-start/#4-fit-the-model","text":"Train your pipeline on the training data. pipeline . fit ( X_train , y_train ) During training, TabTune will automatically handle data preprocessing and apply the chosen tuning strategy.","title":"4. Fit the Model"},{"location":"getting-started/quick-start/#5-evaluate-and-predict","text":"After training, evaluate performance and generate predictions: # Evaluate on test set metrics = pipeline . evaluate ( X_test , y_test ) print ( \"Evaluation metrics:\" , metrics ) # Make predictions predictions = pipeline . predict ( X_test ) Supported metrics include: Accuracy , Weighted F1 Score , and ROC AUC Score .","title":"5. Evaluate and Predict"},{"location":"getting-started/quick-start/#6-save-and-load-the-pipeline","text":"Persist your trained pipeline for later use: # Save to disk pipeline . save ( \"tabtune_pipeline.joblib\" ) # Load from disk loaded_pipeline = TabularPipeline . load ( \"tabtune_pipeline.joblib\" ) results = loaded_pipeline . predict ( X_test )","title":"6. Save and Load the Pipeline"},{"location":"getting-started/quick-start/#7-try-peft-lora-strategy","text":"Switch to parameter-efficient fine-tuning with minimal code changes: # PEFT fine-tuning peft_pipeline = TabularPipeline ( model_name = \"TabPFN\" , task_type = \"classification\" , tuning_strategy = \"peft\" , tuning_params = { \"device\" : \"cuda\" , \"epochs\" : 3 , \"learning_rate\" : 2e-4 , \"peft_config\" : { \"r\" : 8 , \"lora_alpha\" : 16 , \"lora_dropout\" : 0.05 } } ) peft_pipeline . fit ( X_train , y_train ) metrics_peft = peft_pipeline . evaluate ( X_test , y_test ) print ( \"PEFT metrics:\" , metrics_peft )","title":"7. Try PEFT (LoRA) Strategy"},{"location":"getting-started/quick-start/#next-steps","text":"Explore advanced configurations in the User Guide Compare multiple models with the TabularLeaderboard Dive into PEFT internals in PEFT & LoRA","title":"Next Steps"},{"location":"models/contexttab/","text":"ContextTab: Semantics-Aware In-Context Learning \u00b6 ContextTab is a semantically-aware tabular model that integrates modality-specific embeddings to leverage semantic information from feature names, descriptions, and mixed data types. This document provides comprehensive guidance for using ContextTab with TabTune. 1. Introduction \u00b6 What is ContextTab? ContextTab (ConTextTabClassifier) is an advanced in-context learning model uniquely designed to: Leverage Feature Semantics : Understands column names and semantic meaning Text Integration : Handles free-text features naturally Modality-Aware Processing : Different encoders for different data modalities Semantic Embeddings : Uses pre-trained text embeddings for features Heterogeneous Data : Mixed numerical, categorical, and text features Key Innovation : Combines tabular features with semantic embeddings of feature names, enabling the model to understand what features represent semantically. 2. Architecture \u00b6 2.1 High-Level Design \u00b6 flowchart LR A[Raw Data] --> B[Feature Names] A --> C[Feature Values] B --> D[Text Encoder] C --> E[Value Encoder] D --> F[Semantic Embeddings] E --> G[Value Embeddings] F --> H[Fusion Layer] G --> H H --> I[ICL Module] I --> J[Context Processing] J --> K[Predictions] 2.3 Semantic Processing Pipeline \u00b6 Feature Names Feature Values \u2193 \u2193 Text Encoder Value Encoder \u2193 \u2193 Semantic Vectors Value Vectors \u2193 \u2193 \u251c\u2500\u2500\u2500\u2500\u2500Semantic Fusion\u2500\u2500\u2500\u2500\u2500\u2524 \u2193 Joint Representation \u2193 ICL Predictor \u2193 Predictions 3. Inference Parameters \u00b6 3.1 Complete Parameter Reference \u00b6 model_params = { # Text encoding 'text_encoder' : 'sentence-transformers/all-MiniLM-L6-v2' , # BERT model 'text_dim' : 384 , # Text embedding dimension # Value encoding 'value_dim' : 64 , # Value embedding dimension 'categorical_encoding' : 'embedding' , # 'embedding' or 'onehot' # Fusion and processing 'fusion_dim' : 128 , # Fused embedding dimension 'dropout' : 0.1 , # Dropout rate # Training behavior 'use_cache' : True , # Cache embeddings 'seed' : 42 # Reproducibility } 3.2 Parameter Descriptions \u00b6 Parameter Type Default Description text_encoder str 'all-MiniLM-L6-v2' Hugging Face model ID text_dim int 384 Output dimension of text encoder value_dim int 64 Dimension for value embeddings categorical_encoding str 'embedding' How to encode categoricals fusion_dim int 128 Fused representation dimension dropout float 0.1 Dropout probability use_cache bool True Cache computed embeddings seed int 42 Random seed 3.3 Text Encoder Options \u00b6 # Different pre-trained models (trade-off: speed vs quality) text_encoders = { 'all-MiniLM-L6-v2' : { 'dim' : 384 , 'speed' : 'Fast' , 'quality' : 'Good' }, 'all-MiniLM-L12-v2' : { 'dim' : 384 , 'speed' : 'Medium' , 'quality' : 'Better' }, 'all-mpnet-base-v2' : { 'dim' : 768 , 'speed' : 'Slow' , 'quality' : 'Best' } } 4. Fine-Tuning with ContextTab \u00b6 ContextTab supports base fine-tuning via episodic training. 4.1 Fine-Tuning Parameters \u00b6 tuning_params = { 'device' : 'cuda' , 'epochs' : 10 , # More epochs typically needed 'learning_rate' : 1e-4 , # Higher than other models 'optimizer' : 'adamw' , # Optimizer type 'batch_size' : 8 , # Standard batch size 'show_progress' : True # Progress bar } 4.2 Fine-Tuning Best Practices \u00b6 Epochs : 5-15 (longer than most models) Learning Rate : 1e-4 to 5e-4 (higher than TabICL) Warmup : Include warmup for stability Scheduler : Use cosine decay for better convergence Early Stopping : Important due to text embedding complexity 4.3 Fine-Tuning Stability \u00b6 ContextTab training can be unstable due to: - Complex text-value fusion - High-dimensional embeddings - Cross-modality interactions Recommendations : tuning_params = { 'device' : 'cuda' , 'epochs' : 10 , 'learning_rate' : 1e-4 , 'show_progress' : True } 5. Setup Requirements \u00b6 5.1 HuggingFace Hub Access \u00b6 ContextTab requires access to gated models on Hugging Face: # Install HuggingFace CLI pip install huggingface-hub # Login with your token huggingface-cli login # Or set environment variable export HF_TOKEN = 'your_token_here' 5.2 Verify Setup \u00b6 from huggingface_hub import login import os # Check token hf_token = os . getenv ( 'HF_TOKEN' ) if hf_token : login ( hf_token ) print ( \"\u2705 Logged into Hugging Face Hub\" ) else : print ( \"\u26a0\ufe0f HF_TOKEN not set - may fail for gated models\" ) 5.3 Model Download \u00b6 First-time usage downloads model (~2GB): # First use will download model pipeline = TabularPipeline ( model_name = 'ContextTab' , tuning_strategy = 'inference' ) # ... downloads and caches model 6. Usage Patterns \u00b6 6.1 Inference Only \u00b6 from tabtune import TabularPipeline import os # Set HF token os . environ [ 'HF_TOKEN' ] = 'your_token' pipeline = TabularPipeline ( model_name = 'ContextTab' , tuning_strategy = 'inference' , model_params = { 'text_encoder' : 'sentence-transformers/all-MiniLM-L6-v2' , 'fusion_dim' : 128 } ) pipeline . fit ( X_train , y_train ) predictions = pipeline . predict ( X_test ) 6.2 Base Fine-Tuning \u00b6 pipeline = TabularPipeline ( model_name = 'ContextTab' , tuning_strategy = 'base-ft' , tuning_params = { 'device' : 'cuda' , 'epochs' : 10 , 'learning_rate' : 1e-4 , 'warmup_steps' : 200 , 'batch_size' : 8 , 'show_progress' : True } ) pipeline . fit ( X_train , y_train ) metrics = pipeline . evaluate ( X_test , y_test ) 6.3 PEFT Fine-Tuning (Experimental) \u00b6 # \u26a0\ufe0f PEFT support is experimental for ContextTab pipeline = TabularPipeline ( model_name = 'ContextTab' , tuning_strategy = 'peft' , tuning_params = { 'device' : 'cuda' , 'epochs' : 10 , 'learning_rate' : 2e-4 , 'peft_config' : { 'r' : 8 , 'lora_alpha' : 16 , 'lora_dropout' : 0.05 } } ) # May have issues - base-ft recommended pipeline . fit ( X_train , y_train ) 7. LoRA Target Modules (Experimental) \u00b6 When using PEFT, ContextTab targets: target_modules = [ 'in_context_encoder' , # Text encoder 'dense' , # Value encoder 'output_head' , # Prediction head 'embeddings' # Embedding layers ] 7.1 PEFT Status \u00b6 \u26a0\ufe0f Experimental : PEFT support for ContextTab is experimental because: - Complex embedding pipeline - Cross-modality fusion issues - Potential prediction inconsistencies Recommendation : Use base-ft strategy instead of peft 8. Complete Examples \u00b6 8.1 Text-Heavy Dataset \u00b6 from tabtune import TabularPipeline import os # Example: Customer survey data with text responses os . environ [ 'HF_TOKEN' ] = 'your_token' # X contains columns like: # - age (numerical) # - category (categorical) # - feedback_text (text) # - rating (numerical) pipeline = TabularPipeline ( model_name = 'ContextTab' , tuning_strategy = 'base-ft' , tuning_params = { 'device' : 'cuda' , 'epochs' : 10 , 'learning_rate' : 1e-4 , 'warmup_steps' : 200 , 'show_progress' : True } ) pipeline . fit ( X_train , y_train ) metrics = pipeline . evaluate ( X_test , y_test ) print ( f \"Accuracy: { metrics [ 'accuracy' ] : .4f } \" ) 8.2 Semantic Feature Names \u00b6 ContextTab leverages meaningful feature names: # Good: Descriptive feature names (ContextTab works well) X = pd . DataFrame ({ 'customer_age' : [ 25 , 30 , 45 ], 'total_purchases_amount' : [ 100 , 250 , 5000 ], 'years_as_customer' : [ 1 , 5 , 10 ], 'product_category_preference' : [ 'electronics' , 'books' , 'home' ] }) # Less Good: Generic feature names (ContextTab has less semantic info) X = pd . DataFrame ({ 'f1' : [ 25 , 30 , 45 ], 'f2' : [ 100 , 250 , 5000 ], 'f3' : [ 1 , 5 , 10 ], 'f4' : [ 'electronics' , 'books' , 'home' ] }) 8.3 Production Deployment - Saving using joblib \u00b6 import joblib import os os . environ [ 'HF_TOKEN' ] = 'your_token' # Train pipeline = TabularPipeline ( model_name = 'ContextTab' , tuning_strategy = 'base-ft' , tuning_params = { 'epochs' : 10 , 'learning_rate' : 1e-4 } ) pipeline . fit ( X_train , y_train ) metrics = pipeline . evaluate ( X_test , y_test ) # Save pipeline . save ( 'contexttab_production.joblib' ) # In production (ensure HF_TOKEN is set) loaded = TabularPipeline . load ( 'contexttab_production.joblib' ) predictions = loaded . predict ( X_new ) 9. Performance Characteristics \u00b6 9.1 Speed Benchmarks \u00b6 Operation Time Notes Inference (batch=1000) 2-4s Text encoding overhead Fine-tuning (10 epochs, 100K) 30-45m Longer training Prediction latency 20-100ms Per sample Text embedding cache 1-2s One-time at startup 9.2 Memory Usage \u00b6 Scenario Memory GPU VRAM Inference 6-8 GB 4GB minimum Fine-tuning 10-14 GB 8GB recommended Large text dim Up to 16 GB 10GB+ needed With caching Add 1-2 GB For embeddings 11. Troubleshooting \u00b6 Issue: \"HuggingFace login required\" \u00b6 Solution : export HF_TOKEN = 'hf_xxxxxxxxxxxx' # or huggingface-cli login Issue: \"Model download fails\" \u00b6 Solution : Check internet and token from huggingface_hub import model_info try : info = model_info ( 'sentence-transformers/all-MiniLM-L6-v2' ) print ( \"\u2705 Model accessible\" ) except Exception as e : print ( f \"\u274c Model access failed: { e } \" ) Issue: \"Slow inference due to text encoding\" \u00b6 Solution : Use faster text encoder model_params = { 'text_encoder' : 'sentence-transformers/all-MiniLM-L6-v2' , # Fastest # instead of 'all-mpnet-base-v2' # Slowest } Issue: \"Training unstable or diverging\" \u00b6 Solution : Increase regularization tuning_params = { 'learning_rate' : 5e-5 , # Reduce 'warmup_steps' : 500 , # Increase 'weight_decay' : 0.1 , # Increase 'gradient_clip_value' : 0.5 # Tighter } Issue: \"Out of memory during training\" \u00b6 Solution : Reduce batch size tuning_params = { 'batch_size' : 4 # Instead of 8 } 12. Best Practices \u00b6 \u2705 Do's \u00b6 \u2705 Use descriptive feature names \u2705 Include text columns when available \u2705 Set HF_TOKEN before use \u2705 Use base-ft strategy (not peft) \u2705 Include longer warmup phases \u2705 Cache embeddings for repeated use \u274c Don'ts \u00b6 \u274c Don't use PEFT (experimental) \u274c Don't use on pure numerical data (use TabICL) \u274c Don't forget to set HF_TOKEN \u274c Don't use very large batch sizes \u274c Don't skip gradient clipping \u274c Don't use without semantic feature names 13. When to Use ContextTab \u00b6 Use ContextTab when : - \u2705 Dataset has text columns/features - \u2705 Feature names are semantic/meaningful - \u2705 Mixed data types (numerical + categorical + text) - \u2705 You have HuggingFace Hub access - \u2705 Accuracy is priority over speed Don't use ContextTab for : - \u274c Pure numerical data (use TabDPT) - \u274c Generic feature names (limited benefit) - \u274c Memory-constrained environments - \u274c When you need fast training - \u274c Without HuggingFace access 14. Comparison with Other Models \u00b6 Aspect ContextTab TabICL TabDPT Mitra Text Support \u2705 Excellent \u274c No \u274c No \u274c No Semantic Names \u2705 Uses \u274c Ignores \u274c Ignores \u274c Ignores Speed Medium Fast Slow Slow Memory Moderate Moderate High Very High Mixed Data \u2705 Excellent Good Good Good Accuracy High Good Excellent Excellent PEFT \u26a0\ufe0f Exp \u2705 Full \u2705 Full \u2705 Full 15. Quick Reference \u00b6 Task Strategy Config Epochs Quick baseline inference default 0 Mixed data base-ft learning_rate=1e-4 10 Text-heavy base-ft warmup=200 10 Memory limited base-ft batch_size=4 10 Max accuracy base-ft full tune 15 16. Next Steps \u00b6 Model Selection - Compare with other models Tuning Strategies - Fine-tuning details TabularLeaderboard - Benchmark ContextTab HuggingFace Hub - Access gated models ContextTab excels with text-enriched tabular data and semantic feature understanding. Use it when your data includes text or has meaningful feature names!","title":"ConTextTab"},{"location":"models/contexttab/#contexttab-semantics-aware-in-context-learning","text":"ContextTab is a semantically-aware tabular model that integrates modality-specific embeddings to leverage semantic information from feature names, descriptions, and mixed data types. This document provides comprehensive guidance for using ContextTab with TabTune.","title":"ContextTab: Semantics-Aware In-Context Learning"},{"location":"models/contexttab/#1-introduction","text":"What is ContextTab? ContextTab (ConTextTabClassifier) is an advanced in-context learning model uniquely designed to: Leverage Feature Semantics : Understands column names and semantic meaning Text Integration : Handles free-text features naturally Modality-Aware Processing : Different encoders for different data modalities Semantic Embeddings : Uses pre-trained text embeddings for features Heterogeneous Data : Mixed numerical, categorical, and text features Key Innovation : Combines tabular features with semantic embeddings of feature names, enabling the model to understand what features represent semantically.","title":"1. Introduction"},{"location":"models/contexttab/#2-architecture","text":"","title":"2. Architecture"},{"location":"models/contexttab/#21-high-level-design","text":"flowchart LR A[Raw Data] --> B[Feature Names] A --> C[Feature Values] B --> D[Text Encoder] C --> E[Value Encoder] D --> F[Semantic Embeddings] E --> G[Value Embeddings] F --> H[Fusion Layer] G --> H H --> I[ICL Module] I --> J[Context Processing] J --> K[Predictions]","title":"2.1 High-Level Design"},{"location":"models/contexttab/#23-semantic-processing-pipeline","text":"Feature Names Feature Values \u2193 \u2193 Text Encoder Value Encoder \u2193 \u2193 Semantic Vectors Value Vectors \u2193 \u2193 \u251c\u2500\u2500\u2500\u2500\u2500Semantic Fusion\u2500\u2500\u2500\u2500\u2500\u2524 \u2193 Joint Representation \u2193 ICL Predictor \u2193 Predictions","title":"2.3 Semantic Processing Pipeline"},{"location":"models/contexttab/#3-inference-parameters","text":"","title":"3. Inference Parameters"},{"location":"models/contexttab/#31-complete-parameter-reference","text":"model_params = { # Text encoding 'text_encoder' : 'sentence-transformers/all-MiniLM-L6-v2' , # BERT model 'text_dim' : 384 , # Text embedding dimension # Value encoding 'value_dim' : 64 , # Value embedding dimension 'categorical_encoding' : 'embedding' , # 'embedding' or 'onehot' # Fusion and processing 'fusion_dim' : 128 , # Fused embedding dimension 'dropout' : 0.1 , # Dropout rate # Training behavior 'use_cache' : True , # Cache embeddings 'seed' : 42 # Reproducibility }","title":"3.1 Complete Parameter Reference"},{"location":"models/contexttab/#32-parameter-descriptions","text":"Parameter Type Default Description text_encoder str 'all-MiniLM-L6-v2' Hugging Face model ID text_dim int 384 Output dimension of text encoder value_dim int 64 Dimension for value embeddings categorical_encoding str 'embedding' How to encode categoricals fusion_dim int 128 Fused representation dimension dropout float 0.1 Dropout probability use_cache bool True Cache computed embeddings seed int 42 Random seed","title":"3.2 Parameter Descriptions"},{"location":"models/contexttab/#33-text-encoder-options","text":"# Different pre-trained models (trade-off: speed vs quality) text_encoders = { 'all-MiniLM-L6-v2' : { 'dim' : 384 , 'speed' : 'Fast' , 'quality' : 'Good' }, 'all-MiniLM-L12-v2' : { 'dim' : 384 , 'speed' : 'Medium' , 'quality' : 'Better' }, 'all-mpnet-base-v2' : { 'dim' : 768 , 'speed' : 'Slow' , 'quality' : 'Best' } }","title":"3.3 Text Encoder Options"},{"location":"models/contexttab/#4-fine-tuning-with-contexttab","text":"ContextTab supports base fine-tuning via episodic training.","title":"4. Fine-Tuning with ContextTab"},{"location":"models/contexttab/#41-fine-tuning-parameters","text":"tuning_params = { 'device' : 'cuda' , 'epochs' : 10 , # More epochs typically needed 'learning_rate' : 1e-4 , # Higher than other models 'optimizer' : 'adamw' , # Optimizer type 'batch_size' : 8 , # Standard batch size 'show_progress' : True # Progress bar }","title":"4.1 Fine-Tuning Parameters"},{"location":"models/contexttab/#42-fine-tuning-best-practices","text":"Epochs : 5-15 (longer than most models) Learning Rate : 1e-4 to 5e-4 (higher than TabICL) Warmup : Include warmup for stability Scheduler : Use cosine decay for better convergence Early Stopping : Important due to text embedding complexity","title":"4.2 Fine-Tuning Best Practices"},{"location":"models/contexttab/#43-fine-tuning-stability","text":"ContextTab training can be unstable due to: - Complex text-value fusion - High-dimensional embeddings - Cross-modality interactions Recommendations : tuning_params = { 'device' : 'cuda' , 'epochs' : 10 , 'learning_rate' : 1e-4 , 'show_progress' : True }","title":"4.3 Fine-Tuning Stability"},{"location":"models/contexttab/#5-setup-requirements","text":"","title":"5. Setup Requirements"},{"location":"models/contexttab/#51-huggingface-hub-access","text":"ContextTab requires access to gated models on Hugging Face: # Install HuggingFace CLI pip install huggingface-hub # Login with your token huggingface-cli login # Or set environment variable export HF_TOKEN = 'your_token_here'","title":"5.1 HuggingFace Hub Access"},{"location":"models/contexttab/#52-verify-setup","text":"from huggingface_hub import login import os # Check token hf_token = os . getenv ( 'HF_TOKEN' ) if hf_token : login ( hf_token ) print ( \"\u2705 Logged into Hugging Face Hub\" ) else : print ( \"\u26a0\ufe0f HF_TOKEN not set - may fail for gated models\" )","title":"5.2 Verify Setup"},{"location":"models/contexttab/#53-model-download","text":"First-time usage downloads model (~2GB): # First use will download model pipeline = TabularPipeline ( model_name = 'ContextTab' , tuning_strategy = 'inference' ) # ... downloads and caches model","title":"5.3 Model Download"},{"location":"models/contexttab/#6-usage-patterns","text":"","title":"6. Usage Patterns"},{"location":"models/contexttab/#61-inference-only","text":"from tabtune import TabularPipeline import os # Set HF token os . environ [ 'HF_TOKEN' ] = 'your_token' pipeline = TabularPipeline ( model_name = 'ContextTab' , tuning_strategy = 'inference' , model_params = { 'text_encoder' : 'sentence-transformers/all-MiniLM-L6-v2' , 'fusion_dim' : 128 } ) pipeline . fit ( X_train , y_train ) predictions = pipeline . predict ( X_test )","title":"6.1 Inference Only"},{"location":"models/contexttab/#62-base-fine-tuning","text":"pipeline = TabularPipeline ( model_name = 'ContextTab' , tuning_strategy = 'base-ft' , tuning_params = { 'device' : 'cuda' , 'epochs' : 10 , 'learning_rate' : 1e-4 , 'warmup_steps' : 200 , 'batch_size' : 8 , 'show_progress' : True } ) pipeline . fit ( X_train , y_train ) metrics = pipeline . evaluate ( X_test , y_test )","title":"6.2 Base Fine-Tuning"},{"location":"models/contexttab/#63-peft-fine-tuning-experimental","text":"# \u26a0\ufe0f PEFT support is experimental for ContextTab pipeline = TabularPipeline ( model_name = 'ContextTab' , tuning_strategy = 'peft' , tuning_params = { 'device' : 'cuda' , 'epochs' : 10 , 'learning_rate' : 2e-4 , 'peft_config' : { 'r' : 8 , 'lora_alpha' : 16 , 'lora_dropout' : 0.05 } } ) # May have issues - base-ft recommended pipeline . fit ( X_train , y_train )","title":"6.3 PEFT Fine-Tuning (Experimental)"},{"location":"models/contexttab/#7-lora-target-modules-experimental","text":"When using PEFT, ContextTab targets: target_modules = [ 'in_context_encoder' , # Text encoder 'dense' , # Value encoder 'output_head' , # Prediction head 'embeddings' # Embedding layers ]","title":"7. LoRA Target Modules (Experimental)"},{"location":"models/contexttab/#71-peft-status","text":"\u26a0\ufe0f Experimental : PEFT support for ContextTab is experimental because: - Complex embedding pipeline - Cross-modality fusion issues - Potential prediction inconsistencies Recommendation : Use base-ft strategy instead of peft","title":"7.1 PEFT Status"},{"location":"models/contexttab/#8-complete-examples","text":"","title":"8. Complete Examples"},{"location":"models/contexttab/#81-text-heavy-dataset","text":"from tabtune import TabularPipeline import os # Example: Customer survey data with text responses os . environ [ 'HF_TOKEN' ] = 'your_token' # X contains columns like: # - age (numerical) # - category (categorical) # - feedback_text (text) # - rating (numerical) pipeline = TabularPipeline ( model_name = 'ContextTab' , tuning_strategy = 'base-ft' , tuning_params = { 'device' : 'cuda' , 'epochs' : 10 , 'learning_rate' : 1e-4 , 'warmup_steps' : 200 , 'show_progress' : True } ) pipeline . fit ( X_train , y_train ) metrics = pipeline . evaluate ( X_test , y_test ) print ( f \"Accuracy: { metrics [ 'accuracy' ] : .4f } \" )","title":"8.1 Text-Heavy Dataset"},{"location":"models/contexttab/#82-semantic-feature-names","text":"ContextTab leverages meaningful feature names: # Good: Descriptive feature names (ContextTab works well) X = pd . DataFrame ({ 'customer_age' : [ 25 , 30 , 45 ], 'total_purchases_amount' : [ 100 , 250 , 5000 ], 'years_as_customer' : [ 1 , 5 , 10 ], 'product_category_preference' : [ 'electronics' , 'books' , 'home' ] }) # Less Good: Generic feature names (ContextTab has less semantic info) X = pd . DataFrame ({ 'f1' : [ 25 , 30 , 45 ], 'f2' : [ 100 , 250 , 5000 ], 'f3' : [ 1 , 5 , 10 ], 'f4' : [ 'electronics' , 'books' , 'home' ] })","title":"8.2 Semantic Feature Names"},{"location":"models/contexttab/#83-production-deployment-saving-using-joblib","text":"import joblib import os os . environ [ 'HF_TOKEN' ] = 'your_token' # Train pipeline = TabularPipeline ( model_name = 'ContextTab' , tuning_strategy = 'base-ft' , tuning_params = { 'epochs' : 10 , 'learning_rate' : 1e-4 } ) pipeline . fit ( X_train , y_train ) metrics = pipeline . evaluate ( X_test , y_test ) # Save pipeline . save ( 'contexttab_production.joblib' ) # In production (ensure HF_TOKEN is set) loaded = TabularPipeline . load ( 'contexttab_production.joblib' ) predictions = loaded . predict ( X_new )","title":"8.3 Production Deployment - Saving using joblib"},{"location":"models/contexttab/#9-performance-characteristics","text":"","title":"9. Performance Characteristics"},{"location":"models/contexttab/#91-speed-benchmarks","text":"Operation Time Notes Inference (batch=1000) 2-4s Text encoding overhead Fine-tuning (10 epochs, 100K) 30-45m Longer training Prediction latency 20-100ms Per sample Text embedding cache 1-2s One-time at startup","title":"9.1 Speed Benchmarks"},{"location":"models/contexttab/#92-memory-usage","text":"Scenario Memory GPU VRAM Inference 6-8 GB 4GB minimum Fine-tuning 10-14 GB 8GB recommended Large text dim Up to 16 GB 10GB+ needed With caching Add 1-2 GB For embeddings","title":"9.2 Memory Usage"},{"location":"models/contexttab/#11-troubleshooting","text":"","title":"11. Troubleshooting"},{"location":"models/contexttab/#issue-huggingface-login-required","text":"Solution : export HF_TOKEN = 'hf_xxxxxxxxxxxx' # or huggingface-cli login","title":"Issue: \"HuggingFace login required\""},{"location":"models/contexttab/#issue-model-download-fails","text":"Solution : Check internet and token from huggingface_hub import model_info try : info = model_info ( 'sentence-transformers/all-MiniLM-L6-v2' ) print ( \"\u2705 Model accessible\" ) except Exception as e : print ( f \"\u274c Model access failed: { e } \" )","title":"Issue: \"Model download fails\""},{"location":"models/contexttab/#issue-slow-inference-due-to-text-encoding","text":"Solution : Use faster text encoder model_params = { 'text_encoder' : 'sentence-transformers/all-MiniLM-L6-v2' , # Fastest # instead of 'all-mpnet-base-v2' # Slowest }","title":"Issue: \"Slow inference due to text encoding\""},{"location":"models/contexttab/#issue-training-unstable-or-diverging","text":"Solution : Increase regularization tuning_params = { 'learning_rate' : 5e-5 , # Reduce 'warmup_steps' : 500 , # Increase 'weight_decay' : 0.1 , # Increase 'gradient_clip_value' : 0.5 # Tighter }","title":"Issue: \"Training unstable or diverging\""},{"location":"models/contexttab/#issue-out-of-memory-during-training","text":"Solution : Reduce batch size tuning_params = { 'batch_size' : 4 # Instead of 8 }","title":"Issue: \"Out of memory during training\""},{"location":"models/contexttab/#12-best-practices","text":"","title":"12. Best Practices"},{"location":"models/contexttab/#dos","text":"\u2705 Use descriptive feature names \u2705 Include text columns when available \u2705 Set HF_TOKEN before use \u2705 Use base-ft strategy (not peft) \u2705 Include longer warmup phases \u2705 Cache embeddings for repeated use","title":"\u2705 Do's"},{"location":"models/contexttab/#donts","text":"\u274c Don't use PEFT (experimental) \u274c Don't use on pure numerical data (use TabICL) \u274c Don't forget to set HF_TOKEN \u274c Don't use very large batch sizes \u274c Don't skip gradient clipping \u274c Don't use without semantic feature names","title":"\u274c Don'ts"},{"location":"models/contexttab/#13-when-to-use-contexttab","text":"Use ContextTab when : - \u2705 Dataset has text columns/features - \u2705 Feature names are semantic/meaningful - \u2705 Mixed data types (numerical + categorical + text) - \u2705 You have HuggingFace Hub access - \u2705 Accuracy is priority over speed Don't use ContextTab for : - \u274c Pure numerical data (use TabDPT) - \u274c Generic feature names (limited benefit) - \u274c Memory-constrained environments - \u274c When you need fast training - \u274c Without HuggingFace access","title":"13. When to Use ContextTab"},{"location":"models/contexttab/#14-comparison-with-other-models","text":"Aspect ContextTab TabICL TabDPT Mitra Text Support \u2705 Excellent \u274c No \u274c No \u274c No Semantic Names \u2705 Uses \u274c Ignores \u274c Ignores \u274c Ignores Speed Medium Fast Slow Slow Memory Moderate Moderate High Very High Mixed Data \u2705 Excellent Good Good Good Accuracy High Good Excellent Excellent PEFT \u26a0\ufe0f Exp \u2705 Full \u2705 Full \u2705 Full","title":"14. Comparison with Other Models"},{"location":"models/contexttab/#15-quick-reference","text":"Task Strategy Config Epochs Quick baseline inference default 0 Mixed data base-ft learning_rate=1e-4 10 Text-heavy base-ft warmup=200 10 Memory limited base-ft batch_size=4 10 Max accuracy base-ft full tune 15","title":"15. Quick Reference"},{"location":"models/contexttab/#16-next-steps","text":"Model Selection - Compare with other models Tuning Strategies - Fine-tuning details TabularLeaderboard - Benchmark ContextTab HuggingFace Hub - Access gated models ContextTab excels with text-enriched tabular data and semantic feature understanding. Use it when your data includes text or has meaningful feature names!","title":"16. Next Steps"},{"location":"models/mitra/","text":"Mitra: 2D Cross-Attention for Tabular Data \u00b6 Mitra (also known as Tab2D) is a sophisticated tabular model featuring 2D cross-attention mechanisms for modeling both row-wise and column-wise dependencies. This document provides comprehensive guidance for using Mitra with TabTune. 1. Introduction \u00b6 What is Mitra? Mitra is an advanced in-context learning model that captures complex interactions through: 2D Cross-Attention : Simultaneous row (sample) and column (feature) modeling Synthetic Priors : Pre-trained representations for tabular data Mixed-Type Feature Handling : Natural support for numerical and categorical data Episodic Training : Task-specific adaptation via meta-learning Key Innovation : 2D attention mechanism simultaneously models relationships both across rows (samples) and columns (features), enabling superior pattern discovery. 2. Architecture \u00b6 2.1 High-Level Design \u00b6 flowchart LR A[Input Features] --> B[Feature Embeddings] B --> C[Row Embeddings] C --> D[2D Cross-Attention] D --> E[Feature-Sample Interactions] E --> F[Prediction Head] F --> G[Logits] G --> H[Final Predictions] 2.3 2D Attention Mechanism \u00b6 Row Embeddings Feature Embeddings \u2193 \u2193 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25002D Cross-Attention\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2193 Joint Row-Feature Representation \u2193 Prediction Head \u2193 Output 2.4 Synthetic Priors \u00b6 Mitra incorporates synthetic prior knowledge: - Pre-trained on diverse tabular data - Captures common patterns - Accelerates learning on new tasks - Improves generalization 3. Inference Parameters \u00b6 3.1 Complete Parameter Reference \u00b6 model_params = { # Architecture parameters 'd_model' : 64 , # Feature embedding dimension 'd_ff' : 128 , # Feedforward hidden dimension 'num_heads' : 4 , # Attention heads 'num_layers' : 2 , # Stacked layers # Training behavior 'dropout' : 0.1 , # Dropout rate 'use_synthetic_prior' : True , # Use pre-trained prior 'seed' : 42 # Reproducibility } 3.2 Parameter Descriptions \u00b6 Parameter Type Default Range Description d_model int 64 32-256 Feature embedding dimension d_ff int 128 64-512 Feedforward network hidden size num_heads int 4 2-8 Number of attention heads num_layers int 2 1-4 Number of transformer layers dropout float 0.1 0.0-0.3 Dropout probability seed int 42 0+ Random seed 3.3 Architecture Tuning \u00b6 Config Speed Accuracy Memory Small: d_model=32, num_layers=1 \u2b50\u2b50\u2b50\u2b50\u2b50 \u2b50\u2b50 \u2b50 Medium: d_model=64, num_layers=2 \u2b50\u2b50\u2b50\u2b50 \u2b50\u2b50\u2b50 \u2b50\u2b50 Large: d_model=128, num_layers=4 \u2b50\u2b50\u2b50 \u2b50\u2b50\u2b50\u2b50\u2b50 \u2b50\u2b50\u2b50 4. Fine-Tuning with Mitra \u00b6 Mitra uses episodic fine-tuning for task-specific adaptation. 4.1 Fine-Tuning Parameters \u00b6 tuning_params = { 'device' : 'cuda' , 'epochs' : 3 , # Training epochs 'learning_rate' : 1e-5 , # Optimizer learning rate 'optimizer' : 'adamw' , # Optimizer type # Episodic parameters 'support_size' : 128 , # Support set size 'query_size' : 128 , # Query set size 'n_episodes' : 500 , # Episodes per epoch 'steps_per_epoch' : 50 , # Gradient steps per epoch 'batch_size' : 4 , # Episodes per batch (small!) 'show_progress' : True # Progress bar } 4.2 Key Parameters \u00b6 Parameter Type Default Description support_size int 128 Context samples per episode query_size int 128 Query samples per episode n_episodes int 500 Total episodes for training steps_per_epoch int 50 Gradient updates per epoch batch_size int 4 Episodes per batch (keep small) 4.3 Why Small Batch Size? \u00b6 Mitra's 2D attention is computationally expensive: - Attention complexity: (O(n^2)) for both rows and columns - Memory grows rapidly with batch size - Empirically: batch_size=4-8 is optimal - Larger: Use gradient accumulation instead 4.4 Fine-Tuning Guidelines \u00b6 Support/Query Balance : support_size = 128 # Larger context for pattern discovery query_size = 128 # Balance for gradient signal Learning Rate Strategy : - Start: 1e-5 - If converging slowly: increase to 2e-5 - If diverging: decrease to 5e-6 Episode Count : - Small dataset (10K): 500 episodes - Medium dataset (100K): 1000 episodes - Large dataset (500K): 2000 episodes 5. LoRA Target Modules \u00b6 When using PEFT, Mitra targets these modules: target_modules = [ 'x_embedding' , # Feature embedder 'layers' , # Attention layers 'final_layer' # Prediction head ] 5.1 Default PEFT Configuration \u00b6 peft_config = { 'r' : 8 , 'lora_alpha' : 16 , 'lora_dropout' : 0.05 , 'target_modules' : None # Uses defaults above } 5.2 PEFT Rank Guidelines \u00b6 Rank Memory Speed Accuracy r=4 Minimal Fast Good r=8 Low Moderate Better r=16 Moderate Slower Best 6. Usage Patterns \u00b6 6.1 Inference Only \u00b6 from tabtune import TabularPipeline pipeline = TabularPipeline ( model_name = 'Mitra' , tuning_strategy = 'inference' , model_params = { 'd_model' : 64 , 'num_layers' : 2 , 'use_synthetic_prior' : True } ) pipeline . fit ( X_train , y_train ) # Preprocessing only predictions = pipeline . predict ( X_test ) 6.2 Base Fine-Tuning \u00b6 pipeline = TabularPipeline ( model_name = 'Mitra' , tuning_strategy = 'base-ft' , tuning_params = { 'device' : 'cuda' , 'epochs' : 3 , 'learning_rate' : 1e-5 , 'support_size' : 128 , 'query_size' : 128 , 'batch_size' : 4 , # Small! 'show_progress' : True } ) pipeline . fit ( X_train , y_train ) metrics = pipeline . evaluate ( X_test , y_test ) 6.3 PEFT Fine-Tuning \u00b6 pipeline = TabularPipeline ( model_name = 'Mitra' , tuning_strategy = 'peft' , tuning_params = { 'device' : 'cuda' , 'epochs' : 3 , 'learning_rate' : 2e-5 , # Higher for PEFT 'support_size' : 64 , # Reduced for memory 'query_size' : 64 , 'batch_size' : 2 , # Very small 'peft_config' : { 'r' : 8 , 'lora_alpha' : 16 , 'lora_dropout' : 0.05 } } ) pipeline . fit ( X_train , y_train ) 6.4 Architecture Customization \u00b6 pipeline = TabularPipeline ( model_name = 'Mitra' , tuning_strategy = 'base-ft' , model_params = { 'd_model' : 128 , # Larger embeddings 'num_layers' : 4 , # More layers 'num_heads' : 8 , # More heads 'd_ff' : 256 }, tuning_params = { 'epochs' : 5 , 'learning_rate' : 1e-5 } ) 7. Complete Examples \u00b6 7.1 Basic Workflow \u00b6 from tabtune import TabularPipeline from sklearn.model_selection import train_test_split import pandas as pd # Load data df = pd . read_csv ( 'structured_data.csv' ) X = df . drop ( 'target' , axis = 1 ) y = df [ 'target' ] # Split X_train , X_test , y_train , y_test = train_test_split ( X , y , test_size = 0.2 , random_state = 42 ) # Train with Mitra pipeline = TabularPipeline ( model_name = 'Mitra' , tuning_strategy = 'base-ft' , tuning_params = { 'device' : 'cuda' , 'epochs' : 3 , 'support_size' : 128 , 'query_size' : 128 , 'batch_size' : 4 , 'learning_rate' : 1e-5 } ) pipeline . fit ( X_train , y_train ) metrics = pipeline . evaluate ( X_test , y_test ) print ( f \"Accuracy: { metrics [ 'accuracy' ] : .4f } \" ) print ( f \"F1 Score: { metrics [ 'f1_score' ] : .4f } \" ) 7.2 PEFT for Memory Constraints \u00b6 # Use PEFT when memory is limited pipeline = TabularPipeline ( model_name = 'Mitra' , tuning_strategy = 'peft' , tuning_params = { 'device' : 'cuda' , 'epochs' : 3 , 'learning_rate' : 2e-5 , 'support_size' : 64 , # Reduced 'query_size' : 64 , # Reduced 'batch_size' : 2 , # Very small 'steps_per_epoch' : 30 , # Fewer steps 'peft_config' : { 'r' : 4 , # Lower rank 'lora_alpha' : 8 , 'lora_dropout' : 0.1 } } ) pipeline . fit ( X_train , y_train ) 7.3 Architecture Search \u00b6 from tabtune import TabularLeaderboard # Compare architectures lb = TabularLeaderboard ( X_train , X_test , y_train , y_test ) # Small model lb . add_model ( 'Mitra' , 'base-ft' , name = 'Mitra-Small' , model_params = { 'd_model' : 32 , 'num_layers' : 1 }, tuning_params = { 'epochs' : 3 } ) # Medium model lb . add_model ( 'Mitra' , 'base-ft' , name = 'Mitra-Medium' , model_params = { 'd_model' : 64 , 'num_layers' : 2 }, tuning_params = { 'epochs' : 3 } ) # Large model lb . add_model ( 'Mitra' , 'base-ft' , name = 'Mitra-Large' , model_params = { 'd_model' : 128 , 'num_layers' : 4 }, tuning_params = { 'epochs' : 3 } ) lb . run ( rank_by = 'accuracy' ) 7.4 Production Deployment - Saving using joblib \u00b6 import joblib # Train optimal model pipeline = TabularPipeline ( model_name = 'Mitra' , tuning_strategy = 'base-ft' , tuning_params = { 'device' : 'cuda' , 'epochs' : 5 , 'support_size' : 128 , 'batch_size' : 4 , 'learning_rate' : 1e-5 } ) pipeline . fit ( X_train , y_train ) metrics = pipeline . evaluate ( X_test , y_test ) # Save for deployment pipeline . save ( 'mitra_production.joblib' ) # In production loaded = TabularPipeline . load ( 'mitra_production.joblib' ) predictions = loaded . predict ( X_new ) 8. Performance Characteristics \u00b6 8.1 Speed Benchmarks \u00b6 Operation Time Notes Inference (batch=1000) 3-5s 2D attention overhead Base FT (3 epochs, 100K) 45-60m Slow but powerful PEFT (3 epochs, 100K) 20-30m Better speed Prediction latency 20-100ms Per sample 8.2 Memory Usage \u00b6 Scenario Memory GPU VRAM Inference 8-10 GB 6GB minimum Base FT 16-20 GB 12GB recommended PEFT 10-12 GB 6-8GB sufficient Large model Up to 24 GB 16GB+ needed 9. Best Practices \u00b6 \u2705 Do's \u00b6 \u2705 Use small batch sizes (2-4) \u2705 Start with medium architecture (d_model=64) \u2705 Monitor memory usage actively \u2705 Use PEFT on constrained systems \u2705 Increase support/query sizes for pattern discovery \u2705 Use synthetic priors (faster convergence) \u274c Don'ts \u00b6 \u274c Don't use large batch sizes (causes OOM) \u274c Don't use very large models on small GPUs \u274c Don't skip warmup steps \u274c Don't disable gradient clipping \u274c Don't train for too many epochs (overfit risk) 10. Troubleshooting \u00b6 Issue: \"CUDA out of memory\" \u00b6 Solution 1 : Reduce batch size tuning_params = { 'batch_size' : 2 , # Instead of 4 'support_size' : 64 , # Instead of 128 'query_size' : 64 } Solution 2 : Use PEFT tuning_strategy = 'peft' Issue: \"Training very slow\" \u00b6 Solution : Reduce model size model_params = { 'd_model' : 32 , # Instead of 64 'num_layers' : 1 # Instead of 2 } Issue: \"Low accuracy\" \u00b6 Solution : Increase support set size tuning_params = { 'support_size' : 256 , # More context 'query_size' : 256 , 'n_episodes' : 1000 # More training } Issue: \"Overfitting on small datasets\" \u00b6 Solution : Use regularization tuning_params = { 'weight_decay' : 0.1 , # Increase regularization 'dropout' : 0.2 # In model_params } 11. Comparison with Other Models \u00b6 Aspect Mitra TabICL TabDPT TabBiaxial Speed Slow Fast Slow Medium Memory Very High Moderate High High Accuracy Excellent Good Excellent Very Good Complexity Complex Simple Medium Medium Small Data Good Good Okay Good Large Data Good Good Excellent Good PEFT \u2705 Full \u2705 Full \u2705 Full \u2705 Full 12. Quick Reference \u00b6 Use Case Config Batch Size Support Small data (10K) d_model=64, layers=2 4 128 Medium data (100K) d_model=64, layers=2 4 256 Large data (500K) d_model=128, layers=4 2 512 Memory limited PEFT, r=4 2 64 Max accuracy d_model=128, layers=4 4 512 13. Next Steps \u00b6 Model Selection - Compare with other models Tuning Strategies - Fine-tuning details Advanced PEFT - LoRA optimization TabularLeaderboard - Benchmark Mitra Mitra excels at capturing complex 2D patterns in structured tabular data. Use it when maximum accuracy and pattern discovery are priorities!","title":"Mitra"},{"location":"models/mitra/#mitra-2d-cross-attention-for-tabular-data","text":"Mitra (also known as Tab2D) is a sophisticated tabular model featuring 2D cross-attention mechanisms for modeling both row-wise and column-wise dependencies. This document provides comprehensive guidance for using Mitra with TabTune.","title":"Mitra: 2D Cross-Attention for Tabular Data"},{"location":"models/mitra/#1-introduction","text":"What is Mitra? Mitra is an advanced in-context learning model that captures complex interactions through: 2D Cross-Attention : Simultaneous row (sample) and column (feature) modeling Synthetic Priors : Pre-trained representations for tabular data Mixed-Type Feature Handling : Natural support for numerical and categorical data Episodic Training : Task-specific adaptation via meta-learning Key Innovation : 2D attention mechanism simultaneously models relationships both across rows (samples) and columns (features), enabling superior pattern discovery.","title":"1. Introduction"},{"location":"models/mitra/#2-architecture","text":"","title":"2. Architecture"},{"location":"models/mitra/#21-high-level-design","text":"flowchart LR A[Input Features] --> B[Feature Embeddings] B --> C[Row Embeddings] C --> D[2D Cross-Attention] D --> E[Feature-Sample Interactions] E --> F[Prediction Head] F --> G[Logits] G --> H[Final Predictions]","title":"2.1 High-Level Design"},{"location":"models/mitra/#23-2d-attention-mechanism","text":"Row Embeddings Feature Embeddings \u2193 \u2193 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25002D Cross-Attention\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2193 Joint Row-Feature Representation \u2193 Prediction Head \u2193 Output","title":"2.3 2D Attention Mechanism"},{"location":"models/mitra/#24-synthetic-priors","text":"Mitra incorporates synthetic prior knowledge: - Pre-trained on diverse tabular data - Captures common patterns - Accelerates learning on new tasks - Improves generalization","title":"2.4 Synthetic Priors"},{"location":"models/mitra/#3-inference-parameters","text":"","title":"3. Inference Parameters"},{"location":"models/mitra/#31-complete-parameter-reference","text":"model_params = { # Architecture parameters 'd_model' : 64 , # Feature embedding dimension 'd_ff' : 128 , # Feedforward hidden dimension 'num_heads' : 4 , # Attention heads 'num_layers' : 2 , # Stacked layers # Training behavior 'dropout' : 0.1 , # Dropout rate 'use_synthetic_prior' : True , # Use pre-trained prior 'seed' : 42 # Reproducibility }","title":"3.1 Complete Parameter Reference"},{"location":"models/mitra/#32-parameter-descriptions","text":"Parameter Type Default Range Description d_model int 64 32-256 Feature embedding dimension d_ff int 128 64-512 Feedforward network hidden size num_heads int 4 2-8 Number of attention heads num_layers int 2 1-4 Number of transformer layers dropout float 0.1 0.0-0.3 Dropout probability seed int 42 0+ Random seed","title":"3.2 Parameter Descriptions"},{"location":"models/mitra/#33-architecture-tuning","text":"Config Speed Accuracy Memory Small: d_model=32, num_layers=1 \u2b50\u2b50\u2b50\u2b50\u2b50 \u2b50\u2b50 \u2b50 Medium: d_model=64, num_layers=2 \u2b50\u2b50\u2b50\u2b50 \u2b50\u2b50\u2b50 \u2b50\u2b50 Large: d_model=128, num_layers=4 \u2b50\u2b50\u2b50 \u2b50\u2b50\u2b50\u2b50\u2b50 \u2b50\u2b50\u2b50","title":"3.3 Architecture Tuning"},{"location":"models/mitra/#4-fine-tuning-with-mitra","text":"Mitra uses episodic fine-tuning for task-specific adaptation.","title":"4. Fine-Tuning with Mitra"},{"location":"models/mitra/#41-fine-tuning-parameters","text":"tuning_params = { 'device' : 'cuda' , 'epochs' : 3 , # Training epochs 'learning_rate' : 1e-5 , # Optimizer learning rate 'optimizer' : 'adamw' , # Optimizer type # Episodic parameters 'support_size' : 128 , # Support set size 'query_size' : 128 , # Query set size 'n_episodes' : 500 , # Episodes per epoch 'steps_per_epoch' : 50 , # Gradient steps per epoch 'batch_size' : 4 , # Episodes per batch (small!) 'show_progress' : True # Progress bar }","title":"4.1 Fine-Tuning Parameters"},{"location":"models/mitra/#42-key-parameters","text":"Parameter Type Default Description support_size int 128 Context samples per episode query_size int 128 Query samples per episode n_episodes int 500 Total episodes for training steps_per_epoch int 50 Gradient updates per epoch batch_size int 4 Episodes per batch (keep small)","title":"4.2 Key Parameters"},{"location":"models/mitra/#43-why-small-batch-size","text":"Mitra's 2D attention is computationally expensive: - Attention complexity: (O(n^2)) for both rows and columns - Memory grows rapidly with batch size - Empirically: batch_size=4-8 is optimal - Larger: Use gradient accumulation instead","title":"4.3 Why Small Batch Size?"},{"location":"models/mitra/#44-fine-tuning-guidelines","text":"Support/Query Balance : support_size = 128 # Larger context for pattern discovery query_size = 128 # Balance for gradient signal Learning Rate Strategy : - Start: 1e-5 - If converging slowly: increase to 2e-5 - If diverging: decrease to 5e-6 Episode Count : - Small dataset (10K): 500 episodes - Medium dataset (100K): 1000 episodes - Large dataset (500K): 2000 episodes","title":"4.4 Fine-Tuning Guidelines"},{"location":"models/mitra/#5-lora-target-modules","text":"When using PEFT, Mitra targets these modules: target_modules = [ 'x_embedding' , # Feature embedder 'layers' , # Attention layers 'final_layer' # Prediction head ]","title":"5. LoRA Target Modules"},{"location":"models/mitra/#51-default-peft-configuration","text":"peft_config = { 'r' : 8 , 'lora_alpha' : 16 , 'lora_dropout' : 0.05 , 'target_modules' : None # Uses defaults above }","title":"5.1 Default PEFT Configuration"},{"location":"models/mitra/#52-peft-rank-guidelines","text":"Rank Memory Speed Accuracy r=4 Minimal Fast Good r=8 Low Moderate Better r=16 Moderate Slower Best","title":"5.2 PEFT Rank Guidelines"},{"location":"models/mitra/#6-usage-patterns","text":"","title":"6. Usage Patterns"},{"location":"models/mitra/#61-inference-only","text":"from tabtune import TabularPipeline pipeline = TabularPipeline ( model_name = 'Mitra' , tuning_strategy = 'inference' , model_params = { 'd_model' : 64 , 'num_layers' : 2 , 'use_synthetic_prior' : True } ) pipeline . fit ( X_train , y_train ) # Preprocessing only predictions = pipeline . predict ( X_test )","title":"6.1 Inference Only"},{"location":"models/mitra/#62-base-fine-tuning","text":"pipeline = TabularPipeline ( model_name = 'Mitra' , tuning_strategy = 'base-ft' , tuning_params = { 'device' : 'cuda' , 'epochs' : 3 , 'learning_rate' : 1e-5 , 'support_size' : 128 , 'query_size' : 128 , 'batch_size' : 4 , # Small! 'show_progress' : True } ) pipeline . fit ( X_train , y_train ) metrics = pipeline . evaluate ( X_test , y_test )","title":"6.2 Base Fine-Tuning"},{"location":"models/mitra/#63-peft-fine-tuning","text":"pipeline = TabularPipeline ( model_name = 'Mitra' , tuning_strategy = 'peft' , tuning_params = { 'device' : 'cuda' , 'epochs' : 3 , 'learning_rate' : 2e-5 , # Higher for PEFT 'support_size' : 64 , # Reduced for memory 'query_size' : 64 , 'batch_size' : 2 , # Very small 'peft_config' : { 'r' : 8 , 'lora_alpha' : 16 , 'lora_dropout' : 0.05 } } ) pipeline . fit ( X_train , y_train )","title":"6.3 PEFT Fine-Tuning"},{"location":"models/mitra/#64-architecture-customization","text":"pipeline = TabularPipeline ( model_name = 'Mitra' , tuning_strategy = 'base-ft' , model_params = { 'd_model' : 128 , # Larger embeddings 'num_layers' : 4 , # More layers 'num_heads' : 8 , # More heads 'd_ff' : 256 }, tuning_params = { 'epochs' : 5 , 'learning_rate' : 1e-5 } )","title":"6.4 Architecture Customization"},{"location":"models/mitra/#7-complete-examples","text":"","title":"7. Complete Examples"},{"location":"models/mitra/#71-basic-workflow","text":"from tabtune import TabularPipeline from sklearn.model_selection import train_test_split import pandas as pd # Load data df = pd . read_csv ( 'structured_data.csv' ) X = df . drop ( 'target' , axis = 1 ) y = df [ 'target' ] # Split X_train , X_test , y_train , y_test = train_test_split ( X , y , test_size = 0.2 , random_state = 42 ) # Train with Mitra pipeline = TabularPipeline ( model_name = 'Mitra' , tuning_strategy = 'base-ft' , tuning_params = { 'device' : 'cuda' , 'epochs' : 3 , 'support_size' : 128 , 'query_size' : 128 , 'batch_size' : 4 , 'learning_rate' : 1e-5 } ) pipeline . fit ( X_train , y_train ) metrics = pipeline . evaluate ( X_test , y_test ) print ( f \"Accuracy: { metrics [ 'accuracy' ] : .4f } \" ) print ( f \"F1 Score: { metrics [ 'f1_score' ] : .4f } \" )","title":"7.1 Basic Workflow"},{"location":"models/mitra/#72-peft-for-memory-constraints","text":"# Use PEFT when memory is limited pipeline = TabularPipeline ( model_name = 'Mitra' , tuning_strategy = 'peft' , tuning_params = { 'device' : 'cuda' , 'epochs' : 3 , 'learning_rate' : 2e-5 , 'support_size' : 64 , # Reduced 'query_size' : 64 , # Reduced 'batch_size' : 2 , # Very small 'steps_per_epoch' : 30 , # Fewer steps 'peft_config' : { 'r' : 4 , # Lower rank 'lora_alpha' : 8 , 'lora_dropout' : 0.1 } } ) pipeline . fit ( X_train , y_train )","title":"7.2 PEFT for Memory Constraints"},{"location":"models/mitra/#73-architecture-search","text":"from tabtune import TabularLeaderboard # Compare architectures lb = TabularLeaderboard ( X_train , X_test , y_train , y_test ) # Small model lb . add_model ( 'Mitra' , 'base-ft' , name = 'Mitra-Small' , model_params = { 'd_model' : 32 , 'num_layers' : 1 }, tuning_params = { 'epochs' : 3 } ) # Medium model lb . add_model ( 'Mitra' , 'base-ft' , name = 'Mitra-Medium' , model_params = { 'd_model' : 64 , 'num_layers' : 2 }, tuning_params = { 'epochs' : 3 } ) # Large model lb . add_model ( 'Mitra' , 'base-ft' , name = 'Mitra-Large' , model_params = { 'd_model' : 128 , 'num_layers' : 4 }, tuning_params = { 'epochs' : 3 } ) lb . run ( rank_by = 'accuracy' )","title":"7.3 Architecture Search"},{"location":"models/mitra/#74-production-deployment-saving-using-joblib","text":"import joblib # Train optimal model pipeline = TabularPipeline ( model_name = 'Mitra' , tuning_strategy = 'base-ft' , tuning_params = { 'device' : 'cuda' , 'epochs' : 5 , 'support_size' : 128 , 'batch_size' : 4 , 'learning_rate' : 1e-5 } ) pipeline . fit ( X_train , y_train ) metrics = pipeline . evaluate ( X_test , y_test ) # Save for deployment pipeline . save ( 'mitra_production.joblib' ) # In production loaded = TabularPipeline . load ( 'mitra_production.joblib' ) predictions = loaded . predict ( X_new )","title":"7.4 Production Deployment - Saving using joblib"},{"location":"models/mitra/#8-performance-characteristics","text":"","title":"8. Performance Characteristics"},{"location":"models/mitra/#81-speed-benchmarks","text":"Operation Time Notes Inference (batch=1000) 3-5s 2D attention overhead Base FT (3 epochs, 100K) 45-60m Slow but powerful PEFT (3 epochs, 100K) 20-30m Better speed Prediction latency 20-100ms Per sample","title":"8.1 Speed Benchmarks"},{"location":"models/mitra/#82-memory-usage","text":"Scenario Memory GPU VRAM Inference 8-10 GB 6GB minimum Base FT 16-20 GB 12GB recommended PEFT 10-12 GB 6-8GB sufficient Large model Up to 24 GB 16GB+ needed","title":"8.2 Memory Usage"},{"location":"models/mitra/#9-best-practices","text":"","title":"9. Best Practices"},{"location":"models/mitra/#dos","text":"\u2705 Use small batch sizes (2-4) \u2705 Start with medium architecture (d_model=64) \u2705 Monitor memory usage actively \u2705 Use PEFT on constrained systems \u2705 Increase support/query sizes for pattern discovery \u2705 Use synthetic priors (faster convergence)","title":"\u2705 Do's"},{"location":"models/mitra/#donts","text":"\u274c Don't use large batch sizes (causes OOM) \u274c Don't use very large models on small GPUs \u274c Don't skip warmup steps \u274c Don't disable gradient clipping \u274c Don't train for too many epochs (overfit risk)","title":"\u274c Don'ts"},{"location":"models/mitra/#10-troubleshooting","text":"","title":"10. Troubleshooting"},{"location":"models/mitra/#issue-cuda-out-of-memory","text":"Solution 1 : Reduce batch size tuning_params = { 'batch_size' : 2 , # Instead of 4 'support_size' : 64 , # Instead of 128 'query_size' : 64 } Solution 2 : Use PEFT tuning_strategy = 'peft'","title":"Issue: \"CUDA out of memory\""},{"location":"models/mitra/#issue-training-very-slow","text":"Solution : Reduce model size model_params = { 'd_model' : 32 , # Instead of 64 'num_layers' : 1 # Instead of 2 }","title":"Issue: \"Training very slow\""},{"location":"models/mitra/#issue-low-accuracy","text":"Solution : Increase support set size tuning_params = { 'support_size' : 256 , # More context 'query_size' : 256 , 'n_episodes' : 1000 # More training }","title":"Issue: \"Low accuracy\""},{"location":"models/mitra/#issue-overfitting-on-small-datasets","text":"Solution : Use regularization tuning_params = { 'weight_decay' : 0.1 , # Increase regularization 'dropout' : 0.2 # In model_params }","title":"Issue: \"Overfitting on small datasets\""},{"location":"models/mitra/#11-comparison-with-other-models","text":"Aspect Mitra TabICL TabDPT TabBiaxial Speed Slow Fast Slow Medium Memory Very High Moderate High High Accuracy Excellent Good Excellent Very Good Complexity Complex Simple Medium Medium Small Data Good Good Okay Good Large Data Good Good Excellent Good PEFT \u2705 Full \u2705 Full \u2705 Full \u2705 Full","title":"11. Comparison with Other Models"},{"location":"models/mitra/#12-quick-reference","text":"Use Case Config Batch Size Support Small data (10K) d_model=64, layers=2 4 128 Medium data (100K) d_model=64, layers=2 4 256 Large data (500K) d_model=128, layers=4 2 512 Memory limited PEFT, r=4 2 64 Max accuracy d_model=128, layers=4 4 512","title":"12. Quick Reference"},{"location":"models/mitra/#13-next-steps","text":"Model Selection - Compare with other models Tuning Strategies - Fine-tuning details Advanced PEFT - LoRA optimization TabularLeaderboard - Benchmark Mitra Mitra excels at capturing complex 2D patterns in structured tabular data. Use it when maximum accuracy and pattern discovery are priorities!","title":"13. Next Steps"},{"location":"models/overview/","text":"Supported Models Overview \u00b6 TabTune integrates six state-of-the-art tabular foundation models, each with unique architectural properties, strengths, and use cases. This document provides a comprehensive overview of all supported models. 1. Model Ecosystem \u00b6 flowchart TD A[Tabular Foundation Models] --> B[ICL-Based Models] A --> C[Transformer-Based Models] A --> D[PFN-Based Models] B --> E[TabICL] B --> F[TabBiaxial] B --> G[Mitra] B --> H[ContextTab] C --> I[TabDPT] D --> J[TabPFN] 2. Model Comparison Matrix \u00b6 Model Paradigm Architecture Best For Scaling Speed Memory PEFT TabPFN PFN/ICL Approximate Bayesian Small datasets <10K \u2b50\u2b50\u2b50\u2b50\u2b50 \u2b50\u2b50 \u26a0\ufe0f TabICL Scalable ICL Column-Row Attention Balanced 10K-1M \u2b50\u2b50\u2b50\u2b50 \u2b50\u2b50\u2b50 \u2705 TabBiaxial Scalable ICL Biaxial Attention High Accuracy 10K-1M \u2b50\u2b50\u2b50 \u2b50\u2b50\u2b50\u2b50 \u2705 TabDPT Denoising Transformer Large Datasets 100K-5M \u2b50\u2b50\u2b50 \u2b50\u2b50\u2b50\u2b50 \u2705 Mitra 2D Attention Cross-Attention Complex Patterns 10K-500K \u2b50\u2b50 \u2b50\u2b50\u2b50\u2b50\u2b50 \u2705 ContextTab Semantic ICL Text + Embeddings Text-Heavy Features 10K-500K \u2b50\u2b50 \u2b50\u2b50\u2b50 \u26a0\ufe0f 3. Detailed Model Profiles \u00b6 3.1 TabPFN (Prior-Fitted Network) \u00b6 Paradigm : Prior-Fitted Network with approximate Bayesian inference Architecture Overview : - Pre-trained on synthetic task distributions - Approximate Bayesian posterior inference - In-context learning capabilities - Ensemble-based predictions Key Characteristics : - Speed : Extremely fast (no fine-tuning needed) - Scalability : Limited to ~10K rows, ~100 features - Memory : Very lightweight - Uncertainty : Built-in uncertainty quantification Strengths : - \u2b50 Zero-shot performance without training - \u2b50 Excellent for small datasets (<10K) - \u2b50 Bayesian uncertainty estimates - \u2b50 Robust to hyperparameter choices - \u2b50 Fastest inference time Limitations : - \u274c Cannot handle large datasets - \u274c Limited to ~100 features - \u274c PEFT support experimental - \u274c Binary/multi-class classification only Recommended Use Cases : - Quick baseline establishment - Small-scale competition problems - Rapid prototyping Inference Parameters : model_params = { 'n_estimators' : 16 , # Ensemble size 'softmax_temperature' : 0.9 , # Prediction confidence 'average_logits' : True # Average logits vs probabilities } Example : pipeline = TabularPipeline ( model_name = 'TabPFN' , tuning_strategy = 'inference' ) pipeline . fit ( X_train , y_train ) predictions = pipeline . predict ( X_test ) PEFT Status : \u26a0\ufe0f Experimental - Batched inference conflicts with LoRA adapters 3.2 TabICL (In-Context Learning) \u00b6 Paradigm : Scalable In-Context Learning with two-stage attention Architecture Overview : - Column embedder: Processes individual features - Row interactor: Captures feature interactions - ICL predictor: Makes context-aware predictions - Episodic training for adaptation Key Characteristics : - Speed : Fast training and inference - Scalability : 10K - 1M rows effectively - Memory : Moderate requirements - Ensemble : Multiple views for robustness Strengths : - \u2b50 Balanced speed and accuracy - \u2b50 Scales to large datasets - \u2b50 Full PEFT support - \u2b50 Ensemble-based robustness - \u2b50 Episodic training flexibility Limitations : - \u26a0\ufe0f Requires episodic training for adaptation - \u26a0\ufe0f Slower inference with high ensemble size - \u26a0\ufe0f More memory than TabPFN Recommended Use Cases : - General-purpose tabular classification - Medium to large datasets - Production systems - Model adaptation on new tasks Inference Parameters : model_params = { 'n_estimators' : 32 , # Ensemble size 'softmax_temperature' : 0.9 , # Prediction confidence 'average_logits' : True , # Aggregation method 'norm_methods' : [ 'none' , 'power' ], # Feature normalization 'feat_shuffle_method' : 'latin' # Feature permutation } Fine-Tuning Parameters : tuning_params = { 'epochs' : 5 , 'learning_rate' : 2e-5 , 'support_size' : 48 , # Context samples 'query_size' : 32 , # Prediction samples 'n_episodes' : 1000 , # Training episodes 'batch_size' : 8 } PEFT Configuration : peft_config = { 'r' : 8 , 'lora_alpha' : 16 , 'lora_dropout' : 0.05 , 'target_modules' : None # Uses defaults } Example : pipeline = TabularPipeline ( model_name = 'TabICL' , tuning_strategy = 'peft' , tuning_params = { 'epochs' : 5 , 'peft_config' : { 'r' : 8 , 'lora_alpha' : 16 } } ) pipeline . fit ( X_train , y_train ) PEFT Status : \u2705 Full Support 3.3 TabBiaxial \u00b6 Paradigm : Enhanced in-context learning with biaxial attention Architecture Overview : - Extends TabICL with custom biaxial attention mechanisms - Improved feature-feature interactions - Better handling of complex patterns - Advanced context-aware processing Key Characteristics : - Speed : Slower than TabICL - Scalability : 10K - 1M rows - Memory : Higher than TabICL - Accuracy : Higher than TabICL Strengths : - \u2b50 Higher accuracy than TabICL - \u2b50 Better feature interactions - \u2b50 Full PEFT support - \u2b50 Production-grade performance Limitations : - \u274c Slower training than TabICL - \u274c Higher memory requirements - \u274c More hyperparameters to tune Recommended Use Cases : - High-stakes applications (finance, healthcare) - Complex feature interactions - Accuracy-critical tasks - Production models with tuning budget Fine-Tuning Parameters : tuning_params = { 'epochs' : 5 , 'learning_rate' : 2e-5 , 'support_size' : 48 , 'query_size' : 32 , 'n_episodes' : 500 , 'batch_size' : 8 } Example : pipeline = TabularPipeline ( model_name = 'TabBiaxial' , tuning_strategy = 'base-ft' , tuning_params = { 'epochs' : 5 , 'learning_rate' : 2e-5 } ) pipeline . fit ( X_train , y_train ) PEFT Status : \u2705 Full Support 3.4 TabDPT (Denoising Pre-trained Transformer) \u00b6 Paradigm : Denoising pre-training with k-NN context selection Architecture Overview : - Pre-trained on masked feature prediction - k-NN based context selection - Transformer backbone - Robust to noisy features Key Characteristics : - Speed : Moderate training, slower inference - Scalability : 100K - 5M+ rows (best for large) - Memory : High memory requirements - Robustness : Excellent noise handling Strengths : - \u2b50 Scales to very large datasets - \u2b50 Robust to noisy features - \u2b50 Strong generalization - \u2b50 Full PEFT support - \u2b50 Pre-trained on diverse data Limitations : - \u274c Requires large training sets - \u274c Longer training time - \u274c High memory usage - \u274c Complex hyperparameters Recommended Use Cases : - Large-scale production systems - Datasets with noisy/missing features - Long-term deployed models - High-accuracy requirements Inference Parameters : model_params = { 'n_ensembles' : 8 , # Multiple runs 'temperature' : 0.3 , # Prediction confidence 'context_size' : 2048 # k-NN context } Fine-Tuning Parameters : tuning_params = { 'epochs' : 3 , 'learning_rate' : 2e-5 , 'support_size' : 1024 , # Large context 'query_size' : 256 , 'steps_per_epoch' : 15 , 'batch_size' : 32 } PEFT Configuration : peft_config = { 'r' : 8 , 'lora_alpha' : 16 , 'target_modules' : [ 'transformer_encoder' , 'encoder' , 'head' ] } Example : pipeline = TabularPipeline ( model_name = 'TabDPT' , tuning_strategy = 'base-ft' , tuning_params = { 'epochs' : 3 , 'support_size' : 1024 , 'learning_rate' : 1e-5 } ) pipeline . fit ( X_train , y_train ) PEFT Status : \u2705 Full Support 3.5 Mitra (Tab2D) \u00b6 Paradigm : 2D cross-attention with synthetic priors Architecture Overview : - Row embeddings (sample-wise) - Column embeddings (feature-wise) - 2D cross-attention mechanism - Synthetic prior integration Key Characteristics : - Speed : Slowest training - Scalability : 10K - 500K rows - Memory : Highest memory usage - Interaction : Captures row-column dependencies Strengths : - \u2b50 Best for mixed-type features - \u2b50 Captures 2D dependencies - \u2b50 Full PEFT support - \u2b50 Excellent on structured data Limitations : - \u274c Slowest training - \u274c Highest memory consumption - \u274c Small batch sizes required - \u274c Limited scalability Recommended Use Cases : - Structured databases - Scientific datasets - Time-series tabular data - Complex multi-variate relationships Fine-Tuning Parameters : tuning_params = { 'epochs' : 3 , 'learning_rate' : 1e-5 , 'support_size' : 128 , 'query_size' : 128 , 'steps_per_epoch' : 50 , 'batch_size' : 4 # Small batch required } PEFT Configuration : peft_config = { 'r' : 8 , 'lora_alpha' : 16 , 'target_modules' : [ 'x_embedding' , 'layers' , 'final_layer' ] } Example : pipeline = TabularPipeline ( model_name = 'Mitra' , tuning_strategy = 'peft' , tuning_params = { 'epochs' : 3 , 'support_size' : 128 , 'batch_size' : 4 , 'peft_config' : { 'r' : 8 } } ) pipeline . fit ( X_train , y_train ) PEFT Status : \u2705 Full Support 3.6 ContextTab \u00b6 Paradigm : Semantics-aware in-context learning with text embeddings Architecture Overview : - Text encoder for feature names/descriptions - Semantic embeddings integration - Column context understanding - Mixed modality handling Key Characteristics : - Speed : Moderate (embedding overhead) - Scalability : 10K - 500K rows - Memory : Moderate to high - Semantics : Leverages feature semantics Strengths : - \u2b50 Best for text-heavy features - \u2b50 Semantic understanding - \u2b50 Handles heterogeneous data - \u2b50 Pre-trained on diverse corpora Limitations : - \u274c Requires HuggingFace Hub access - \u274c PEFT support experimental - \u274c Slower inference - \u274c Limited feature type support Recommended Use Cases : - Datasets with text columns - Survey/feedback data - Product catalogs - Mixed structured/unstructured Fine-Tuning Parameters : tuning_params = { 'epochs' : 10 , 'learning_rate' : 1e-4 , 'batch_size' : 8 } Note : ContextTab requires gated model access via HuggingFace Example : # Set HF token first import os os . environ [ 'HF_TOKEN' ] = 'your_token_here' pipeline = TabularPipeline ( model_name = 'ContextTab' , tuning_strategy = 'base-ft' , # PEFT not recommended tuning_params = { 'epochs' : 10 , 'learning_rate' : 1e-4 } ) pipeline . fit ( X_train , y_train ) PEFT Status : \u26a0\ufe0f Experimental - Complex embedding pipeline may cause issues 4. Feature Support Matrix \u00b6 Feature TabPFN TabICL TabBiaxial TabDPT Mitra ContextTab Numerical Features \u2705 \u2705 \u2705 \u2705 \u2705 \u2705 Categorical Features \u2705 \u2705 \u2705 \u2705 \u2705 \u2705 Missing Values \u2705 \u2705 \u2705 \u2705 \u2705 \u2705 Text Features \u274c \u274c \u274c \u274c \u274c \u2705 Large Datasets (>1M) \u274c \u2705 \u2705 \u2705 \u274c \u274c Small Datasets (<10K) \u2705 \u2705 \u2705 \u26a0\ufe0f \u2705 \u2705 PEFT Support \u26a0\ufe0f \u2705 \u2705 \u2705 \u2705 \u26a0\ufe0f Multi-GPU Training \u274c \u2705 \u2705 \u2705 \u2705 \u2705 5. Selection Decision Tree \u00b6 What is your dataset size? \u251c\u2500 < 10K rows \u2502 \u2514\u2500 Best: TabPFN (inference) \u2502 Alternative: TabICL (base-ft) \u2502 \u251c\u2500 10K - 100K rows \u2502 \u251c\u2500 Focus: Speed? \u2192 TabICL (PEFT) \u2502 \u251c\u2500 Focus: Accuracy? \u2192 TabBiaxial (base-ft) \u2502 \u2514\u2500 Focus: Complexity \u2192 Mitra (base-ft) \u2502 \u251c\u2500 100K - 1M rows \u2502 \u251c\u2500 Focus: Speed? \u2192 TabICL (base-ft) \u2502 \u251c\u2500 Focus: Accuracy? \u2192 TabDPT (base-ft) \u2502 \u2514\u2500 Focus: Robust? \u2192 TabDPT (PEFT) \u2502 \u2514\u2500 > 1M rows \u2514\u2500 Best: TabDPT (base-ft) 7. Quick Reference \u00b6 If You Need Choose Fastest results TabPFN Best generalization TabDPT Best accuracy TabBiaxial Memory efficient TabICL (PEFT) Complex interactions Mitra Text features ContextTab Production deployment TabBiaxial or TabDPT Small datasets TabPFN or TabICL Large datasets TabDPT 7. Next Steps \u00b6 Model Selection Guide - How to choose models Individual Model Docs - Detailed model documentation Examples - Model-specific examples Each model excels in different scenarios. Use this overview to understand their strengths and select the best fit for your task!","title":"Overview"},{"location":"models/overview/#supported-models-overview","text":"TabTune integrates six state-of-the-art tabular foundation models, each with unique architectural properties, strengths, and use cases. This document provides a comprehensive overview of all supported models.","title":"Supported Models Overview"},{"location":"models/overview/#1-model-ecosystem","text":"flowchart TD A[Tabular Foundation Models] --> B[ICL-Based Models] A --> C[Transformer-Based Models] A --> D[PFN-Based Models] B --> E[TabICL] B --> F[TabBiaxial] B --> G[Mitra] B --> H[ContextTab] C --> I[TabDPT] D --> J[TabPFN]","title":"1. Model Ecosystem"},{"location":"models/overview/#2-model-comparison-matrix","text":"Model Paradigm Architecture Best For Scaling Speed Memory PEFT TabPFN PFN/ICL Approximate Bayesian Small datasets <10K \u2b50\u2b50\u2b50\u2b50\u2b50 \u2b50\u2b50 \u26a0\ufe0f TabICL Scalable ICL Column-Row Attention Balanced 10K-1M \u2b50\u2b50\u2b50\u2b50 \u2b50\u2b50\u2b50 \u2705 TabBiaxial Scalable ICL Biaxial Attention High Accuracy 10K-1M \u2b50\u2b50\u2b50 \u2b50\u2b50\u2b50\u2b50 \u2705 TabDPT Denoising Transformer Large Datasets 100K-5M \u2b50\u2b50\u2b50 \u2b50\u2b50\u2b50\u2b50 \u2705 Mitra 2D Attention Cross-Attention Complex Patterns 10K-500K \u2b50\u2b50 \u2b50\u2b50\u2b50\u2b50\u2b50 \u2705 ContextTab Semantic ICL Text + Embeddings Text-Heavy Features 10K-500K \u2b50\u2b50 \u2b50\u2b50\u2b50 \u26a0\ufe0f","title":"2. Model Comparison Matrix"},{"location":"models/overview/#3-detailed-model-profiles","text":"","title":"3. Detailed Model Profiles"},{"location":"models/overview/#31-tabpfn-prior-fitted-network","text":"Paradigm : Prior-Fitted Network with approximate Bayesian inference Architecture Overview : - Pre-trained on synthetic task distributions - Approximate Bayesian posterior inference - In-context learning capabilities - Ensemble-based predictions Key Characteristics : - Speed : Extremely fast (no fine-tuning needed) - Scalability : Limited to ~10K rows, ~100 features - Memory : Very lightweight - Uncertainty : Built-in uncertainty quantification Strengths : - \u2b50 Zero-shot performance without training - \u2b50 Excellent for small datasets (<10K) - \u2b50 Bayesian uncertainty estimates - \u2b50 Robust to hyperparameter choices - \u2b50 Fastest inference time Limitations : - \u274c Cannot handle large datasets - \u274c Limited to ~100 features - \u274c PEFT support experimental - \u274c Binary/multi-class classification only Recommended Use Cases : - Quick baseline establishment - Small-scale competition problems - Rapid prototyping Inference Parameters : model_params = { 'n_estimators' : 16 , # Ensemble size 'softmax_temperature' : 0.9 , # Prediction confidence 'average_logits' : True # Average logits vs probabilities } Example : pipeline = TabularPipeline ( model_name = 'TabPFN' , tuning_strategy = 'inference' ) pipeline . fit ( X_train , y_train ) predictions = pipeline . predict ( X_test ) PEFT Status : \u26a0\ufe0f Experimental - Batched inference conflicts with LoRA adapters","title":"3.1 TabPFN (Prior-Fitted Network)"},{"location":"models/overview/#32-tabicl-in-context-learning","text":"Paradigm : Scalable In-Context Learning with two-stage attention Architecture Overview : - Column embedder: Processes individual features - Row interactor: Captures feature interactions - ICL predictor: Makes context-aware predictions - Episodic training for adaptation Key Characteristics : - Speed : Fast training and inference - Scalability : 10K - 1M rows effectively - Memory : Moderate requirements - Ensemble : Multiple views for robustness Strengths : - \u2b50 Balanced speed and accuracy - \u2b50 Scales to large datasets - \u2b50 Full PEFT support - \u2b50 Ensemble-based robustness - \u2b50 Episodic training flexibility Limitations : - \u26a0\ufe0f Requires episodic training for adaptation - \u26a0\ufe0f Slower inference with high ensemble size - \u26a0\ufe0f More memory than TabPFN Recommended Use Cases : - General-purpose tabular classification - Medium to large datasets - Production systems - Model adaptation on new tasks Inference Parameters : model_params = { 'n_estimators' : 32 , # Ensemble size 'softmax_temperature' : 0.9 , # Prediction confidence 'average_logits' : True , # Aggregation method 'norm_methods' : [ 'none' , 'power' ], # Feature normalization 'feat_shuffle_method' : 'latin' # Feature permutation } Fine-Tuning Parameters : tuning_params = { 'epochs' : 5 , 'learning_rate' : 2e-5 , 'support_size' : 48 , # Context samples 'query_size' : 32 , # Prediction samples 'n_episodes' : 1000 , # Training episodes 'batch_size' : 8 } PEFT Configuration : peft_config = { 'r' : 8 , 'lora_alpha' : 16 , 'lora_dropout' : 0.05 , 'target_modules' : None # Uses defaults } Example : pipeline = TabularPipeline ( model_name = 'TabICL' , tuning_strategy = 'peft' , tuning_params = { 'epochs' : 5 , 'peft_config' : { 'r' : 8 , 'lora_alpha' : 16 } } ) pipeline . fit ( X_train , y_train ) PEFT Status : \u2705 Full Support","title":"3.2 TabICL (In-Context Learning)"},{"location":"models/overview/#33-tabbiaxial","text":"Paradigm : Enhanced in-context learning with biaxial attention Architecture Overview : - Extends TabICL with custom biaxial attention mechanisms - Improved feature-feature interactions - Better handling of complex patterns - Advanced context-aware processing Key Characteristics : - Speed : Slower than TabICL - Scalability : 10K - 1M rows - Memory : Higher than TabICL - Accuracy : Higher than TabICL Strengths : - \u2b50 Higher accuracy than TabICL - \u2b50 Better feature interactions - \u2b50 Full PEFT support - \u2b50 Production-grade performance Limitations : - \u274c Slower training than TabICL - \u274c Higher memory requirements - \u274c More hyperparameters to tune Recommended Use Cases : - High-stakes applications (finance, healthcare) - Complex feature interactions - Accuracy-critical tasks - Production models with tuning budget Fine-Tuning Parameters : tuning_params = { 'epochs' : 5 , 'learning_rate' : 2e-5 , 'support_size' : 48 , 'query_size' : 32 , 'n_episodes' : 500 , 'batch_size' : 8 } Example : pipeline = TabularPipeline ( model_name = 'TabBiaxial' , tuning_strategy = 'base-ft' , tuning_params = { 'epochs' : 5 , 'learning_rate' : 2e-5 } ) pipeline . fit ( X_train , y_train ) PEFT Status : \u2705 Full Support","title":"3.3 TabBiaxial"},{"location":"models/overview/#34-tabdpt-denoising-pre-trained-transformer","text":"Paradigm : Denoising pre-training with k-NN context selection Architecture Overview : - Pre-trained on masked feature prediction - k-NN based context selection - Transformer backbone - Robust to noisy features Key Characteristics : - Speed : Moderate training, slower inference - Scalability : 100K - 5M+ rows (best for large) - Memory : High memory requirements - Robustness : Excellent noise handling Strengths : - \u2b50 Scales to very large datasets - \u2b50 Robust to noisy features - \u2b50 Strong generalization - \u2b50 Full PEFT support - \u2b50 Pre-trained on diverse data Limitations : - \u274c Requires large training sets - \u274c Longer training time - \u274c High memory usage - \u274c Complex hyperparameters Recommended Use Cases : - Large-scale production systems - Datasets with noisy/missing features - Long-term deployed models - High-accuracy requirements Inference Parameters : model_params = { 'n_ensembles' : 8 , # Multiple runs 'temperature' : 0.3 , # Prediction confidence 'context_size' : 2048 # k-NN context } Fine-Tuning Parameters : tuning_params = { 'epochs' : 3 , 'learning_rate' : 2e-5 , 'support_size' : 1024 , # Large context 'query_size' : 256 , 'steps_per_epoch' : 15 , 'batch_size' : 32 } PEFT Configuration : peft_config = { 'r' : 8 , 'lora_alpha' : 16 , 'target_modules' : [ 'transformer_encoder' , 'encoder' , 'head' ] } Example : pipeline = TabularPipeline ( model_name = 'TabDPT' , tuning_strategy = 'base-ft' , tuning_params = { 'epochs' : 3 , 'support_size' : 1024 , 'learning_rate' : 1e-5 } ) pipeline . fit ( X_train , y_train ) PEFT Status : \u2705 Full Support","title":"3.4 TabDPT (Denoising Pre-trained Transformer)"},{"location":"models/overview/#35-mitra-tab2d","text":"Paradigm : 2D cross-attention with synthetic priors Architecture Overview : - Row embeddings (sample-wise) - Column embeddings (feature-wise) - 2D cross-attention mechanism - Synthetic prior integration Key Characteristics : - Speed : Slowest training - Scalability : 10K - 500K rows - Memory : Highest memory usage - Interaction : Captures row-column dependencies Strengths : - \u2b50 Best for mixed-type features - \u2b50 Captures 2D dependencies - \u2b50 Full PEFT support - \u2b50 Excellent on structured data Limitations : - \u274c Slowest training - \u274c Highest memory consumption - \u274c Small batch sizes required - \u274c Limited scalability Recommended Use Cases : - Structured databases - Scientific datasets - Time-series tabular data - Complex multi-variate relationships Fine-Tuning Parameters : tuning_params = { 'epochs' : 3 , 'learning_rate' : 1e-5 , 'support_size' : 128 , 'query_size' : 128 , 'steps_per_epoch' : 50 , 'batch_size' : 4 # Small batch required } PEFT Configuration : peft_config = { 'r' : 8 , 'lora_alpha' : 16 , 'target_modules' : [ 'x_embedding' , 'layers' , 'final_layer' ] } Example : pipeline = TabularPipeline ( model_name = 'Mitra' , tuning_strategy = 'peft' , tuning_params = { 'epochs' : 3 , 'support_size' : 128 , 'batch_size' : 4 , 'peft_config' : { 'r' : 8 } } ) pipeline . fit ( X_train , y_train ) PEFT Status : \u2705 Full Support","title":"3.5 Mitra (Tab2D)"},{"location":"models/overview/#36-contexttab","text":"Paradigm : Semantics-aware in-context learning with text embeddings Architecture Overview : - Text encoder for feature names/descriptions - Semantic embeddings integration - Column context understanding - Mixed modality handling Key Characteristics : - Speed : Moderate (embedding overhead) - Scalability : 10K - 500K rows - Memory : Moderate to high - Semantics : Leverages feature semantics Strengths : - \u2b50 Best for text-heavy features - \u2b50 Semantic understanding - \u2b50 Handles heterogeneous data - \u2b50 Pre-trained on diverse corpora Limitations : - \u274c Requires HuggingFace Hub access - \u274c PEFT support experimental - \u274c Slower inference - \u274c Limited feature type support Recommended Use Cases : - Datasets with text columns - Survey/feedback data - Product catalogs - Mixed structured/unstructured Fine-Tuning Parameters : tuning_params = { 'epochs' : 10 , 'learning_rate' : 1e-4 , 'batch_size' : 8 } Note : ContextTab requires gated model access via HuggingFace Example : # Set HF token first import os os . environ [ 'HF_TOKEN' ] = 'your_token_here' pipeline = TabularPipeline ( model_name = 'ContextTab' , tuning_strategy = 'base-ft' , # PEFT not recommended tuning_params = { 'epochs' : 10 , 'learning_rate' : 1e-4 } ) pipeline . fit ( X_train , y_train ) PEFT Status : \u26a0\ufe0f Experimental - Complex embedding pipeline may cause issues","title":"3.6 ContextTab"},{"location":"models/overview/#4-feature-support-matrix","text":"Feature TabPFN TabICL TabBiaxial TabDPT Mitra ContextTab Numerical Features \u2705 \u2705 \u2705 \u2705 \u2705 \u2705 Categorical Features \u2705 \u2705 \u2705 \u2705 \u2705 \u2705 Missing Values \u2705 \u2705 \u2705 \u2705 \u2705 \u2705 Text Features \u274c \u274c \u274c \u274c \u274c \u2705 Large Datasets (>1M) \u274c \u2705 \u2705 \u2705 \u274c \u274c Small Datasets (<10K) \u2705 \u2705 \u2705 \u26a0\ufe0f \u2705 \u2705 PEFT Support \u26a0\ufe0f \u2705 \u2705 \u2705 \u2705 \u26a0\ufe0f Multi-GPU Training \u274c \u2705 \u2705 \u2705 \u2705 \u2705","title":"4. Feature Support Matrix"},{"location":"models/overview/#5-selection-decision-tree","text":"What is your dataset size? \u251c\u2500 < 10K rows \u2502 \u2514\u2500 Best: TabPFN (inference) \u2502 Alternative: TabICL (base-ft) \u2502 \u251c\u2500 10K - 100K rows \u2502 \u251c\u2500 Focus: Speed? \u2192 TabICL (PEFT) \u2502 \u251c\u2500 Focus: Accuracy? \u2192 TabBiaxial (base-ft) \u2502 \u2514\u2500 Focus: Complexity \u2192 Mitra (base-ft) \u2502 \u251c\u2500 100K - 1M rows \u2502 \u251c\u2500 Focus: Speed? \u2192 TabICL (base-ft) \u2502 \u251c\u2500 Focus: Accuracy? \u2192 TabDPT (base-ft) \u2502 \u2514\u2500 Focus: Robust? \u2192 TabDPT (PEFT) \u2502 \u2514\u2500 > 1M rows \u2514\u2500 Best: TabDPT (base-ft)","title":"5. Selection Decision Tree"},{"location":"models/overview/#7-quick-reference","text":"If You Need Choose Fastest results TabPFN Best generalization TabDPT Best accuracy TabBiaxial Memory efficient TabICL (PEFT) Complex interactions Mitra Text features ContextTab Production deployment TabBiaxial or TabDPT Small datasets TabPFN or TabICL Large datasets TabDPT","title":"7. Quick Reference"},{"location":"models/overview/#7-next-steps","text":"Model Selection Guide - How to choose models Individual Model Docs - Detailed model documentation Examples - Model-specific examples Each model excels in different scenarios. Use this overview to understand their strengths and select the best fit for your task!","title":"7. Next Steps"},{"location":"models/tabdpt/","text":"TabDPT: Tabular Denoising Pre-trained Transformer \u00b6 TabDPT is a large-scale tabular model pre-trained via denoising objectives on diverse datasets. This document provides comprehensive guidance for using TabDPT with TabTune for maximum scalability and robustness. 1. Introduction \u00b6 What is TabDPT? TabDPT (Tabular Denoising Pre-trained Transformer) is a state-of-the-art model designed for: Large-Scale Learning : Scales to datasets with millions of samples Robust Feature Learning : Pre-trained on denoising objectives Noise Resilience : Handles missing and corrupted features Context-Aware Predictions : k-NN based context selection Strong Generalization : Pre-trained on diverse tabular corpora Key Innovation : Pre-training via masked feature prediction (denoising) enables robust feature representations and strong generalization to new tasks. 2. Architecture \u00b6 2.1 High-Level Design \u00b6 flowchart LR A[Input Features] --> B[Masking Layer] B --> C[Noisy Features] C --> D[Transformer Encoder] D --> E[Hidden Representations] E --> F[k-NN Context Retrieval] F --> G[Context Features] G --> H[Transformer Decoder] H --> I[Reconstructed Features] I --> J[Prediction Head] J --> K[Output] 2.3 Pre-training Strategy \u00b6 Pre-training Phase (on diverse data): 1. Mask random features (30-50%) 2. Encode remaining features 3. Retrieve k-NN context 4. Predict masked features 5. Loss = MSE(predicted, actual) Fine-tuning Phase (on your task): 1. Replace prediction head 2. Fine-tune on task labels 3. Use pre-trained encoder 3. Inference Parameters \u00b6 3.1 Complete Parameter Reference \u00b6 model_params = { # Architecture 'd_model' : 256 , # Embedding dimension 'num_heads' : 8 , # Attention heads 'num_layers' : 4 , # Transformer layers 'hidden_size' : 512 , # Feedforward hidden size 'dropout' : 0.1 , # Dropout probability # Context retrieval 'k_neighbors' : 5 , # Number of neighbors for context 'context_mode' : 'mixed' , # 'mixed' or 'features_only' # Inference behavior 'n_ensembles' : 8 , # Multiple runs 'temperature' : 0.3 , # Output scaling 'mask_ratio' : 0.3 , # Feature masking during inference # Training 'use_pretrain' : True , # Use pre-trained weights 'seed' : 42 # Reproducibility } 3.2 Parameter Descriptions \u00b6 Parameter Type Default Range Description d_model int 256 128-512 Transformer embedding dimension num_heads int 8 4-16 Number of attention heads num_layers int 4 2-8 Number of transformer layers hidden_size int 512 256-1024 Feedforward hidden dimension dropout float 0.1 0.0-0.3 Dropout probability k_neighbors int 5 1-50 k-NN context neighbors context_mode str 'mixed' 'mixed', 'features_only' How to use context n_ensembles int 8 1-16 Number of ensemble runs temperature float 0.3 0.1-1.0 Output temperature use_pretrain bool True True/False Use pre-trained weights 3.3 Architecture Tuning \u00b6 Config Speed Accuracy Memory Best For Small: d=128, layers=2 \u2b50\u2b50\u2b50\u2b50\u2b50 \u2b50\u2b50 \u2b50 Quick baseline Medium: d=256, layers=4 \u2b50\u2b50\u2b50\u2b50 \u2b50\u2b50\u2b50\u2b50 \u2b50\u2b50 Balanced Large: d=512, layers=8 \u2b50\u2b50\u2b50 \u2b50\u2b50\u2b50\u2b50\u2b50 \u2b50\u2b50\u2b50 Max accuracy 3.4 Context Modes \u00b6 context_modes = { 'mixed' : 'Use both context features and their representations' , 'features_only' : 'Use only context features, not representations' } # Typically 'mixed' is better model_params = { 'context_mode' : 'mixed' } 4. Fine-Tuning with TabDPT \u00b6 TabDPT uses episodic fine-tuning with large context windows. 4.1 Fine-Tuning Parameters \u00b6 tuning_params = { 'device' : 'cuda' , 'epochs' : 3 , # Few epochs needed (pre-trained) 'learning_rate' : 2e-5 , # Conservative learning rate 'optimizer' : 'adamw' , # Optimizer type 'scheduler' : 'linear' , # Learning rate scheduler 'warmup_steps' : 500 , # Extended warmup 'weight_decay' : 0.01 , # L2 regularization 'gradient_clip_value' : 1.0 , # Gradient clipping # Large context for TabDPT 'support_size' : 1024 , # Large context 'query_size' : 256 , # Prediction samples 'steps_per_epoch' : 15 , # Gradient steps 'batch_size' : 32 , # Standard batch 'show_progress' : True # Progress bar } 4.2 Key Parameters \u00b6 Parameter Type Default Description support_size int 1024 Large context for k-NN query_size int 256 Query samples per episode steps_per_epoch int 15 Optimization steps batch_size int 32 Samples per batch 4.3 Fine-Tuning Guidelines \u00b6 Large Context Windows : # TabDPT benefits from large context tuning_params = { 'support_size' : 1024 , # Large context (TabDPT strength) 'query_size' : 256 , # Balance for gradients 'batch_size' : 32 # Process in parallel } Learning Rate Strategy : - 1e-5: Conservative, safe - 2e-5: Balanced (default) - 5e-5: Aggressive Pre-training Advantage : - TabDPT needs fewer epochs due to pre-training - Typically 3-5 epochs sufficient - Convergence faster than TabICL 4.4 Dataset Recommendations \u00b6 # TabDPT shines with large datasets dataset_sizes = { '10K' : 'Acceptable, TabICL better' , '100K' : 'Good fit for TabDPT' , '1M' : 'Excellent for TabDPT' , '5M+' : 'Perfect use case' } 5. LoRA Target Modules \u00b6 When using PEFT, TabDPT targets these modules: target_modules = [ 'transformer_encoder' , # Main encoder 'encoder' , # Additional encoder 'y_encoder' , # Label encoder 'head' # Prediction head ] 5.1 Default PEFT Configuration \u00b6 peft_config = { 'r' : 8 , 'lora_alpha' : 16 , 'lora_dropout' : 0.05 , 'target_modules' : None # Uses defaults } 5.2 PEFT for Large Models \u00b6 # PEFT works well with TabDPT's large architecture pipeline = TabularPipeline ( model_name = 'TabDPT' , tuning_strategy = 'peft' , tuning_params = { 'device' : 'cuda' , 'epochs' : 3 , 'learning_rate' : 2e-4 , 'support_size' : 512 , # Still large 'peft_config' : { 'r' : 16 } # Higher rank acceptable } ) 6. Usage Patterns \u00b6 6.1 Inference Only \u00b6 from tabtune import TabularPipeline pipeline = TabularPipeline ( model_name = 'TabDPT' , tuning_strategy = 'inference' , ) pipeline . fit ( X_train , y_train ) predictions = pipeline . predict ( X_test ) 6.2 Base Fine-Tuning on Large Dataset \u00b6 # TabDPT excels with large datasets pipeline = TabularPipeline ( model_name = 'TabDPT' , tuning_strategy = 'base-ft' , tuning_params = { 'device' : 'cuda' , 'epochs' : 3 , 'learning_rate' : 2e-5 , 'support_size' : 1024 , # Large context 'query_size' : 256 , 'steps_per_epoch' : 15 , 'batch_size' : 32 , 'show_progress' : True } ) pipeline . fit ( X_train , y_train ) # 100K+ samples ideal metrics = pipeline . evaluate ( X_test , y_test ) 6.3 PEFT Fine-Tuning \u00b6 pipeline = TabularPipeline ( model_name = 'TabDPT' , tuning_strategy = 'peft' , tuning_params = { 'device' : 'cuda' , 'epochs' : 3 , 'learning_rate' : 2e-4 , 'support_size' : 512 , 'query_size' : 256 , 'peft_config' : { 'r' : 8 , 'lora_alpha' : 16 , 'lora_dropout' : 0.05 } } ) pipeline . fit ( X_train , y_train ) 7. Complete Examples \u00b6 7.1 Large Dataset Workflow \u00b6 from tabtune import TabularPipeline from sklearn.model_selection import train_test_split import pandas as pd # Load large dataset (1M+ rows) df = pd . read_csv ( 'large_dataset.csv' ) # 1M+ rows X = df . drop ( 'target' , axis = 1 ) y = df [ 'target' ] # Split X_train , X_test , y_train , y_test = train_test_split ( X , y , test_size = 0.1 , random_state = 42 ) # Train TabDPT pipeline = TabularPipeline ( model_name = 'TabDPT' , tuning_strategy = 'base-ft' , tuning_params = { 'device' : 'cuda' , 'epochs' : 3 , 'learning_rate' : 2e-5 , 'support_size' : 1024 , 'query_size' : 256 , 'batch_size' : 32 , 'show_progress' : True } ) pipeline . fit ( X_train , y_train ) metrics = pipeline . evaluate ( X_test , y_test ) print ( f \"Accuracy: { metrics [ 'accuracy' ] : .4f } \" ) print ( f \"Training completed on { len ( X_train ) } samples\" ) 7.2 Production Model with PEFT - Saving using joblib \u00b6 # PEFT for production deployment pipeline = TabularPipeline ( model_name = 'TabDPT' , tuning_strategy = 'peft' , tuning_params = { 'device' : 'cuda' , 'epochs' : 3 , 'learning_rate' : 2e-4 , 'support_size' : 512 , 'peft_config' : { 'r' : 8 } } ) pipeline . fit ( X_train , y_train ) metrics = pipeline . evaluate ( X_test , y_test ) # Save for deployment pipeline . save ( 'tabdpt_production.joblib' ) 7.3 Architecture Comparison \u00b6 from tabtune import TabularLeaderboard # Compare architectures lb = TabularLeaderboard ( X_train , X_test , y_train , y_test ) # Small lb . add_model ( 'TabDPT' , 'base-ft' , name = 'TabDPT-Small' , model_params = { 'd_model' : 128 , 'num_layers' : 2 }, tuning_params = { 'epochs' : 3 } ) # Medium lb . add_model ( 'TabDPT' , 'base-ft' , name = 'TabDPT-Medium' , model_params = { 'd_model' : 256 , 'num_layers' : 4 }, tuning_params = { 'epochs' : 3 } ) # Large lb . add_model ( 'TabDPT' , 'base-ft' , name = 'TabDPT-Large' , model_params = { 'd_model' : 512 , 'num_layers' : 8 }, tuning_params = { 'epochs' : 3 } ) lb . run ( rank_by = 'accuracy' ) 9. Best Practices \u00b6 \u2705 Do's \u00b6 \u2705 Use large context windows (support_size >= 512) \u2705 Use on datasets with 100K+ samples \u2705 Leverage pre-trained weights \u2705 Use few epochs (3-5) due to pre-training \u2705 Monitor for overfitting with regularization \u2705 Use PEFT for faster training \u2705 Include k-NN context \u274c Don'ts \u00b6 \u274c Don't use on small datasets (<10K) \u274c Don't use without pre-training \u274c Don't train for too many epochs \u274c Don't use small context windows \u274c Don't disable masking (helps robustness) 10. Troubleshooting \u00b6 Issue: \"Context retrieval slow\" \u00b6 Solution : Reduce k_neighbors or use approximate k-NN model_params = { 'k_neighbors' : 3 , # Instead of 5 'context_mode' : 'features_only' } Issue: \"Out of memory with large support_size\" \u00b6 Solution : Use PEFT or reduce support size tuning_params = { 'support_size' : 512 , # Instead of 1024 'batch_size' : 16 # Smaller batch } Issue: \"Accuracy plateauing\" \u00b6 Solution : Increase training budget tuning_params = { 'epochs' : 5 , # More epochs 'steps_per_epoch' : 20 , # More steps 'warmup_steps' : 1000 # Longer warmup } Issue: \"Prediction latency too high\" \u00b6 Solution : Use smaller ensemble model_params = { 'n_ensembles' : 2 , # Instead of 8 'k_neighbors' : 3 # Fewer neighbors } 11. Comparison with Other Models \u00b6 Aspect TabDPT TabICL Mitra TabPFN Small data (10K) Poor Good Good Excellent Large data (1M) Excellent Good Okay N/A Accuracy Excellent Good Excellent Medium Speed Slow Fast Slow Fastest Memory High Moderate Very High Low Pre-training Yes No No Yes PEFT \u2705 Full \u2705 Full \u2705 Full \u26a0\ufe0f Exp 12. When to Use TabDPT \u00b6 Use TabDPT when : - \u2705 Dataset has 100K+ samples - \u2705 Maximum accuracy is priority - \u2705 Data has missing/noisy values - \u2705 You have sufficient memory - \u2705 Training time is not critical - \u2705 Deployment can handle context retrieval Don't use TabDPT for : - \u274c Small datasets (<50K) - \u274c When prediction speed critical - \u274c Very memory-constrained systems - \u274c When training time is limited 14. Quick Reference \u00b6 Use Case Strategy Config Support Baseline inference default 1024 Production (1M data) base-ft default 1024 Memory limited peft r=8 512 Max accuracy base-ft large model 2048 Fast inference peft r=4 256 15. Next Steps \u00b6 Model Selection - Compare with other models Tuning Strategies - Fine-tuning details Advanced PEFT - LoRA optimization TabularLeaderboard - Benchmark TabDPT TabDPT excels at large-scale tabular learning with pre-trained robustness. Use it for production systems with millions of samples!","title":"TabDPT"},{"location":"models/tabdpt/#tabdpt-tabular-denoising-pre-trained-transformer","text":"TabDPT is a large-scale tabular model pre-trained via denoising objectives on diverse datasets. This document provides comprehensive guidance for using TabDPT with TabTune for maximum scalability and robustness.","title":"TabDPT: Tabular Denoising Pre-trained Transformer"},{"location":"models/tabdpt/#1-introduction","text":"What is TabDPT? TabDPT (Tabular Denoising Pre-trained Transformer) is a state-of-the-art model designed for: Large-Scale Learning : Scales to datasets with millions of samples Robust Feature Learning : Pre-trained on denoising objectives Noise Resilience : Handles missing and corrupted features Context-Aware Predictions : k-NN based context selection Strong Generalization : Pre-trained on diverse tabular corpora Key Innovation : Pre-training via masked feature prediction (denoising) enables robust feature representations and strong generalization to new tasks.","title":"1. Introduction"},{"location":"models/tabdpt/#2-architecture","text":"","title":"2. Architecture"},{"location":"models/tabdpt/#21-high-level-design","text":"flowchart LR A[Input Features] --> B[Masking Layer] B --> C[Noisy Features] C --> D[Transformer Encoder] D --> E[Hidden Representations] E --> F[k-NN Context Retrieval] F --> G[Context Features] G --> H[Transformer Decoder] H --> I[Reconstructed Features] I --> J[Prediction Head] J --> K[Output]","title":"2.1 High-Level Design"},{"location":"models/tabdpt/#23-pre-training-strategy","text":"Pre-training Phase (on diverse data): 1. Mask random features (30-50%) 2. Encode remaining features 3. Retrieve k-NN context 4. Predict masked features 5. Loss = MSE(predicted, actual) Fine-tuning Phase (on your task): 1. Replace prediction head 2. Fine-tune on task labels 3. Use pre-trained encoder","title":"2.3 Pre-training Strategy"},{"location":"models/tabdpt/#3-inference-parameters","text":"","title":"3. Inference Parameters"},{"location":"models/tabdpt/#31-complete-parameter-reference","text":"model_params = { # Architecture 'd_model' : 256 , # Embedding dimension 'num_heads' : 8 , # Attention heads 'num_layers' : 4 , # Transformer layers 'hidden_size' : 512 , # Feedforward hidden size 'dropout' : 0.1 , # Dropout probability # Context retrieval 'k_neighbors' : 5 , # Number of neighbors for context 'context_mode' : 'mixed' , # 'mixed' or 'features_only' # Inference behavior 'n_ensembles' : 8 , # Multiple runs 'temperature' : 0.3 , # Output scaling 'mask_ratio' : 0.3 , # Feature masking during inference # Training 'use_pretrain' : True , # Use pre-trained weights 'seed' : 42 # Reproducibility }","title":"3.1 Complete Parameter Reference"},{"location":"models/tabdpt/#32-parameter-descriptions","text":"Parameter Type Default Range Description d_model int 256 128-512 Transformer embedding dimension num_heads int 8 4-16 Number of attention heads num_layers int 4 2-8 Number of transformer layers hidden_size int 512 256-1024 Feedforward hidden dimension dropout float 0.1 0.0-0.3 Dropout probability k_neighbors int 5 1-50 k-NN context neighbors context_mode str 'mixed' 'mixed', 'features_only' How to use context n_ensembles int 8 1-16 Number of ensemble runs temperature float 0.3 0.1-1.0 Output temperature use_pretrain bool True True/False Use pre-trained weights","title":"3.2 Parameter Descriptions"},{"location":"models/tabdpt/#33-architecture-tuning","text":"Config Speed Accuracy Memory Best For Small: d=128, layers=2 \u2b50\u2b50\u2b50\u2b50\u2b50 \u2b50\u2b50 \u2b50 Quick baseline Medium: d=256, layers=4 \u2b50\u2b50\u2b50\u2b50 \u2b50\u2b50\u2b50\u2b50 \u2b50\u2b50 Balanced Large: d=512, layers=8 \u2b50\u2b50\u2b50 \u2b50\u2b50\u2b50\u2b50\u2b50 \u2b50\u2b50\u2b50 Max accuracy","title":"3.3 Architecture Tuning"},{"location":"models/tabdpt/#34-context-modes","text":"context_modes = { 'mixed' : 'Use both context features and their representations' , 'features_only' : 'Use only context features, not representations' } # Typically 'mixed' is better model_params = { 'context_mode' : 'mixed' }","title":"3.4 Context Modes"},{"location":"models/tabdpt/#4-fine-tuning-with-tabdpt","text":"TabDPT uses episodic fine-tuning with large context windows.","title":"4. Fine-Tuning with TabDPT"},{"location":"models/tabdpt/#41-fine-tuning-parameters","text":"tuning_params = { 'device' : 'cuda' , 'epochs' : 3 , # Few epochs needed (pre-trained) 'learning_rate' : 2e-5 , # Conservative learning rate 'optimizer' : 'adamw' , # Optimizer type 'scheduler' : 'linear' , # Learning rate scheduler 'warmup_steps' : 500 , # Extended warmup 'weight_decay' : 0.01 , # L2 regularization 'gradient_clip_value' : 1.0 , # Gradient clipping # Large context for TabDPT 'support_size' : 1024 , # Large context 'query_size' : 256 , # Prediction samples 'steps_per_epoch' : 15 , # Gradient steps 'batch_size' : 32 , # Standard batch 'show_progress' : True # Progress bar }","title":"4.1 Fine-Tuning Parameters"},{"location":"models/tabdpt/#42-key-parameters","text":"Parameter Type Default Description support_size int 1024 Large context for k-NN query_size int 256 Query samples per episode steps_per_epoch int 15 Optimization steps batch_size int 32 Samples per batch","title":"4.2 Key Parameters"},{"location":"models/tabdpt/#43-fine-tuning-guidelines","text":"Large Context Windows : # TabDPT benefits from large context tuning_params = { 'support_size' : 1024 , # Large context (TabDPT strength) 'query_size' : 256 , # Balance for gradients 'batch_size' : 32 # Process in parallel } Learning Rate Strategy : - 1e-5: Conservative, safe - 2e-5: Balanced (default) - 5e-5: Aggressive Pre-training Advantage : - TabDPT needs fewer epochs due to pre-training - Typically 3-5 epochs sufficient - Convergence faster than TabICL","title":"4.3 Fine-Tuning Guidelines"},{"location":"models/tabdpt/#44-dataset-recommendations","text":"# TabDPT shines with large datasets dataset_sizes = { '10K' : 'Acceptable, TabICL better' , '100K' : 'Good fit for TabDPT' , '1M' : 'Excellent for TabDPT' , '5M+' : 'Perfect use case' }","title":"4.4 Dataset Recommendations"},{"location":"models/tabdpt/#5-lora-target-modules","text":"When using PEFT, TabDPT targets these modules: target_modules = [ 'transformer_encoder' , # Main encoder 'encoder' , # Additional encoder 'y_encoder' , # Label encoder 'head' # Prediction head ]","title":"5. LoRA Target Modules"},{"location":"models/tabdpt/#51-default-peft-configuration","text":"peft_config = { 'r' : 8 , 'lora_alpha' : 16 , 'lora_dropout' : 0.05 , 'target_modules' : None # Uses defaults }","title":"5.1 Default PEFT Configuration"},{"location":"models/tabdpt/#52-peft-for-large-models","text":"# PEFT works well with TabDPT's large architecture pipeline = TabularPipeline ( model_name = 'TabDPT' , tuning_strategy = 'peft' , tuning_params = { 'device' : 'cuda' , 'epochs' : 3 , 'learning_rate' : 2e-4 , 'support_size' : 512 , # Still large 'peft_config' : { 'r' : 16 } # Higher rank acceptable } )","title":"5.2 PEFT for Large Models"},{"location":"models/tabdpt/#6-usage-patterns","text":"","title":"6. Usage Patterns"},{"location":"models/tabdpt/#61-inference-only","text":"from tabtune import TabularPipeline pipeline = TabularPipeline ( model_name = 'TabDPT' , tuning_strategy = 'inference' , ) pipeline . fit ( X_train , y_train ) predictions = pipeline . predict ( X_test )","title":"6.1 Inference Only"},{"location":"models/tabdpt/#62-base-fine-tuning-on-large-dataset","text":"# TabDPT excels with large datasets pipeline = TabularPipeline ( model_name = 'TabDPT' , tuning_strategy = 'base-ft' , tuning_params = { 'device' : 'cuda' , 'epochs' : 3 , 'learning_rate' : 2e-5 , 'support_size' : 1024 , # Large context 'query_size' : 256 , 'steps_per_epoch' : 15 , 'batch_size' : 32 , 'show_progress' : True } ) pipeline . fit ( X_train , y_train ) # 100K+ samples ideal metrics = pipeline . evaluate ( X_test , y_test )","title":"6.2 Base Fine-Tuning on Large Dataset"},{"location":"models/tabdpt/#63-peft-fine-tuning","text":"pipeline = TabularPipeline ( model_name = 'TabDPT' , tuning_strategy = 'peft' , tuning_params = { 'device' : 'cuda' , 'epochs' : 3 , 'learning_rate' : 2e-4 , 'support_size' : 512 , 'query_size' : 256 , 'peft_config' : { 'r' : 8 , 'lora_alpha' : 16 , 'lora_dropout' : 0.05 } } ) pipeline . fit ( X_train , y_train )","title":"6.3 PEFT Fine-Tuning"},{"location":"models/tabdpt/#7-complete-examples","text":"","title":"7. Complete Examples"},{"location":"models/tabdpt/#71-large-dataset-workflow","text":"from tabtune import TabularPipeline from sklearn.model_selection import train_test_split import pandas as pd # Load large dataset (1M+ rows) df = pd . read_csv ( 'large_dataset.csv' ) # 1M+ rows X = df . drop ( 'target' , axis = 1 ) y = df [ 'target' ] # Split X_train , X_test , y_train , y_test = train_test_split ( X , y , test_size = 0.1 , random_state = 42 ) # Train TabDPT pipeline = TabularPipeline ( model_name = 'TabDPT' , tuning_strategy = 'base-ft' , tuning_params = { 'device' : 'cuda' , 'epochs' : 3 , 'learning_rate' : 2e-5 , 'support_size' : 1024 , 'query_size' : 256 , 'batch_size' : 32 , 'show_progress' : True } ) pipeline . fit ( X_train , y_train ) metrics = pipeline . evaluate ( X_test , y_test ) print ( f \"Accuracy: { metrics [ 'accuracy' ] : .4f } \" ) print ( f \"Training completed on { len ( X_train ) } samples\" )","title":"7.1 Large Dataset Workflow"},{"location":"models/tabdpt/#72-production-model-with-peft-saving-using-joblib","text":"# PEFT for production deployment pipeline = TabularPipeline ( model_name = 'TabDPT' , tuning_strategy = 'peft' , tuning_params = { 'device' : 'cuda' , 'epochs' : 3 , 'learning_rate' : 2e-4 , 'support_size' : 512 , 'peft_config' : { 'r' : 8 } } ) pipeline . fit ( X_train , y_train ) metrics = pipeline . evaluate ( X_test , y_test ) # Save for deployment pipeline . save ( 'tabdpt_production.joblib' )","title":"7.2 Production Model with PEFT - Saving using joblib"},{"location":"models/tabdpt/#73-architecture-comparison","text":"from tabtune import TabularLeaderboard # Compare architectures lb = TabularLeaderboard ( X_train , X_test , y_train , y_test ) # Small lb . add_model ( 'TabDPT' , 'base-ft' , name = 'TabDPT-Small' , model_params = { 'd_model' : 128 , 'num_layers' : 2 }, tuning_params = { 'epochs' : 3 } ) # Medium lb . add_model ( 'TabDPT' , 'base-ft' , name = 'TabDPT-Medium' , model_params = { 'd_model' : 256 , 'num_layers' : 4 }, tuning_params = { 'epochs' : 3 } ) # Large lb . add_model ( 'TabDPT' , 'base-ft' , name = 'TabDPT-Large' , model_params = { 'd_model' : 512 , 'num_layers' : 8 }, tuning_params = { 'epochs' : 3 } ) lb . run ( rank_by = 'accuracy' )","title":"7.3 Architecture Comparison"},{"location":"models/tabdpt/#9-best-practices","text":"","title":"9. Best Practices"},{"location":"models/tabdpt/#dos","text":"\u2705 Use large context windows (support_size >= 512) \u2705 Use on datasets with 100K+ samples \u2705 Leverage pre-trained weights \u2705 Use few epochs (3-5) due to pre-training \u2705 Monitor for overfitting with regularization \u2705 Use PEFT for faster training \u2705 Include k-NN context","title":"\u2705 Do's"},{"location":"models/tabdpt/#donts","text":"\u274c Don't use on small datasets (<10K) \u274c Don't use without pre-training \u274c Don't train for too many epochs \u274c Don't use small context windows \u274c Don't disable masking (helps robustness)","title":"\u274c Don'ts"},{"location":"models/tabdpt/#10-troubleshooting","text":"","title":"10. Troubleshooting"},{"location":"models/tabdpt/#issue-context-retrieval-slow","text":"Solution : Reduce k_neighbors or use approximate k-NN model_params = { 'k_neighbors' : 3 , # Instead of 5 'context_mode' : 'features_only' }","title":"Issue: \"Context retrieval slow\""},{"location":"models/tabdpt/#issue-out-of-memory-with-large-support_size","text":"Solution : Use PEFT or reduce support size tuning_params = { 'support_size' : 512 , # Instead of 1024 'batch_size' : 16 # Smaller batch }","title":"Issue: \"Out of memory with large support_size\""},{"location":"models/tabdpt/#issue-accuracy-plateauing","text":"Solution : Increase training budget tuning_params = { 'epochs' : 5 , # More epochs 'steps_per_epoch' : 20 , # More steps 'warmup_steps' : 1000 # Longer warmup }","title":"Issue: \"Accuracy plateauing\""},{"location":"models/tabdpt/#issue-prediction-latency-too-high","text":"Solution : Use smaller ensemble model_params = { 'n_ensembles' : 2 , # Instead of 8 'k_neighbors' : 3 # Fewer neighbors }","title":"Issue: \"Prediction latency too high\""},{"location":"models/tabdpt/#11-comparison-with-other-models","text":"Aspect TabDPT TabICL Mitra TabPFN Small data (10K) Poor Good Good Excellent Large data (1M) Excellent Good Okay N/A Accuracy Excellent Good Excellent Medium Speed Slow Fast Slow Fastest Memory High Moderate Very High Low Pre-training Yes No No Yes PEFT \u2705 Full \u2705 Full \u2705 Full \u26a0\ufe0f Exp","title":"11. Comparison with Other Models"},{"location":"models/tabdpt/#12-when-to-use-tabdpt","text":"Use TabDPT when : - \u2705 Dataset has 100K+ samples - \u2705 Maximum accuracy is priority - \u2705 Data has missing/noisy values - \u2705 You have sufficient memory - \u2705 Training time is not critical - \u2705 Deployment can handle context retrieval Don't use TabDPT for : - \u274c Small datasets (<50K) - \u274c When prediction speed critical - \u274c Very memory-constrained systems - \u274c When training time is limited","title":"12. When to Use TabDPT"},{"location":"models/tabdpt/#14-quick-reference","text":"Use Case Strategy Config Support Baseline inference default 1024 Production (1M data) base-ft default 1024 Memory limited peft r=8 512 Max accuracy base-ft large model 2048 Fast inference peft r=4 256","title":"14. Quick Reference"},{"location":"models/tabdpt/#15-next-steps","text":"Model Selection - Compare with other models Tuning Strategies - Fine-tuning details Advanced PEFT - LoRA optimization TabularLeaderboard - Benchmark TabDPT TabDPT excels at large-scale tabular learning with pre-trained robustness. Use it for production systems with millions of samples!","title":"15. Next Steps"},{"location":"models/tabicl/","text":"TabICL: In-Context Learning for Tabular Data \u00b6 TabICL is a scalable, ensemble-based in-context learning model designed for general-purpose tabular classification. This document provides comprehensive guidance for using TabICL with TabTune. 1. Introduction \u00b6 What is TabICL? TabICL (Tabular In-Context Learning) is a neural model that leverages in-context learning principles adapted for tabular data. Unlike traditional models, TabICL learns to: Process feature relationships dynamically Adapt to task-specific patterns via fine-tuning Generate robust predictions via ensemble methods Handle mixed data types naturally Key Innovation : Two-stage attention mechanism (column \u2192 row) enabling efficient feature processing and interaction modeling. 2. Architecture \u00b6 2.1 High-Level Design \u00b6 flowchart LR A[Input Features] --> B[Column Embedder] B --> C[Feature-wise Embeddings] C --> D[Row Interactor] D --> E[Feature Interactions] E --> F[ICL Predictor] F --> G[Predictions] G --> H[Ensemble Aggregation] H --> I[Final Output] 2.3 Two-Stage Attention \u00b6 Stage 1: Column Attention (Feature-wise) \u2193 Per-feature processing Feature extraction Stage 2: Row Attention (Sample-wise) \u2193 Feature interaction Context modeling 3. Inference Parameters \u00b6 3.1 Complete Parameter Reference \u00b6 model_params = { 'n_estimators' : 32 , # Ensemble size (views) 'softmax_temperature' : 0.9 , # Prediction confidence 'average_logits' : True , # Aggregation method 'norm_methods' : [ 'none' , 'power' ], # Feature normalization 'feat_shuffle_method' : 'latin' , # Feature permutation strategy 'batch_size' : 8 , # Ensemble batch size 'seed' : 42 # Reproducibility } 3.2 Parameter Descriptions \u00b6 Parameter Type Default Range Description n_estimators int 32 4-128 Number of ensemble members; more = robust but slower softmax_temperature float 0.9 0.1-2.0 Scaling before softmax; lower = sharper predictions average_logits bool True True/False Average logits vs probabilities norm_methods list ['none', 'power'] Varies Feature normalization techniques feat_shuffle_method str 'latin' 'random', 'latin', 'sequential' Feature permutation strategy batch_size int 8 1-32 Ensemble members per batch seed int 42 0+ Random seed for reproducibility 3.3 Ensemble Size Effects \u00b6 n_estimators Speed Robustness Memory 4-8 \u2b50\u2b50\u2b50\u2b50\u2b50 \u2b50\u2b50 \u2b50 16-32 \u2b50\u2b50\u2b50\u2b50 \u2b50\u2b50\u2b50\u2b50 \u2b50\u2b50 64-128 \u2b50\u2b50\u2b50 \u2b50\u2b50\u2b50\u2b50\u2b50 \u2b50\u2b50\u2b50 3.4 Feature Normalization Methods \u00b6 norm_methods = [ 'none' , # No normalization 'power' , # Power transformation 'quantile' , # Quantile normalization 'minmax' , # Min-max scaling 'standard' # Standardization ] 3.5 Feature Shuffle Methods \u00b6 feat_shuffle_methods = { 'random' : 'Random permutation each time' , 'latin' : 'Latin square design (balanced)' , 'sequential' : 'Fixed sequential order' } 4. Fine-Tuning with TabICL \u00b6 TabICL supports episodic fine-tuning where training occurs via task-like episodes. 4.1 Episodic Training Parameters \u00b6 tuning_params = { 'device' : 'cuda' , 'epochs' : 5 , # Training epochs 'learning_rate' : 2e-5 , # Optimizer learning rate 'optimizer' : 'adamw' , # Optimizer type # Episodic parameters 'support_size' : 48 , # Support set samples 'query_size' : 32 , # Query set samples 'n_episodes' : 1000 , # Episodes per epoch 'batch_size' : 8 , # Episodes per batch 'show_progress' : True # Progress bar } 4.2 Parameter Descriptions \u00b6 Parameter Type Default Description support_size int 48 Number of samples per support set query_size int 32 Number of samples per query set n_episodes int 1000 Total episodes for training batch_size int 8 Episodes per batch gradient update 4.3 Episodic Training Concept \u00b6 One Episode: \u251c\u2500 Support Set (48 samples) \u2502 \u2514\u2500 Used as training context \u251c\u2500 Query Set (32 samples) \u2502 \u2514\u2500 Used for evaluation \u2514\u2500 Loss computed on Query Epoch = 1000 episodes with gradient updates 4.4 Fine-Tuning Guidelines \u00b6 Support/Query Size Balance : - Larger support \u2192 more context but slower - Larger query \u2192 better gradient signal - Typical: support:query = 3:2 Number of Episodes : - 500-1000: Good for medium datasets - 1000-5000: Better for large datasets - Adjust based on dataset size: (n_{episodes} = \\frac{\\text{dataset_size}}{100}) Learning Rate : - 1e-5: Conservative, safe - 2e-5: Balanced (default) - 5e-5: Aggressive, higher variance 5. Usage Patterns \u00b6 5.1 Inference Only \u00b6 from tabtune import TabularPipeline # Zero-shot with pre-trained weights pipeline = TabularPipeline ( model_name = 'TabICL' , tuning_strategy = 'inference' , model_params = { 'n_estimators' : 32 } ) pipeline . fit ( X_train , y_train ) # Only preprocesses predictions = pipeline . predict ( X_test ) 5.2 Base Fine-Tuning (Full Parameters) \u00b6 pipeline = TabularPipeline ( model_name = 'TabICL' , tuning_strategy = 'base-ft' , tuning_params = { 'device' : 'cuda' , 'epochs' : 5 , 'support_size' : 48 , 'query_size' : 32 , 'n_episodes' : 1000 } ) pipeline . fit ( X_train , y_train ) metrics = pipeline . evaluate ( X_test , y_test ) 5.3 PEFT Fine-Tuning (LoRA Adapters) \u00b6 pipeline = TabularPipeline ( model_name = 'TabICL' , tuning_strategy = 'peft' , tuning_params = { 'device' : 'cuda' , 'epochs' : 5 , 'learning_rate' : 2e-4 , # Higher for PEFT 'support_size' : 24 , # Smaller for memory 'query_size' : 16 , 'peft_config' : { 'r' : 8 , 'lora_alpha' : 16 , 'lora_dropout' : 0.05 } } ) pipeline . fit ( X_train , y_train ) 5.4 Ensemble Configuration \u00b6 # Increase ensemble for robustness pipeline = TabularPipeline ( model_name = 'TabICL' , tuning_strategy = 'inference' , model_params = { 'n_estimators' : 128 , # Large ensemble 'batch_size' : 16 # Parallel processing } ) 6. LoRA Target Modules \u00b6 When using PEFT, TabICL automatically targets these modules: target_modules = [ 'col_embedder.tf_col' , # Column transformer 'col_embedder.in_linear' , # Input projection 'row_interactor' , # Interaction layers 'icl_predictor.tf_icl' , # Prediction transformer 'icl_predictor.decoder' # Decoder head ] Default PEFT Config : peft_config = { 'r' : 8 , 'lora_alpha' : 16 , 'lora_dropout' : 0.05 , 'target_modules' : None # Uses defaults above } 7. Complete Examples \u00b6 7.1 Basic Workflow \u00b6 from tabtune import TabularPipeline from sklearn.model_selection import train_test_split import pandas as pd # Load data df = pd . read_csv ( 'data.csv' ) X = df . drop ( 'target' , axis = 1 ) y = df [ 'target' ] # Split X_train , X_test , y_train , y_test = train_test_split ( X , y , test_size = 0.2 , random_state = 42 ) # Create and train pipeline = TabularPipeline ( model_name = 'TabICL' , tuning_strategy = 'base-ft' , tuning_params = { 'device' : 'cuda' , 'epochs' : 5 , 'learning_rate' : 2e-5 , 'n_episodes' : 1000 } ) pipeline . fit ( X_train , y_train ) metrics = pipeline . evaluate ( X_test , y_test ) print ( f \"Accuracy: { metrics [ 'accuracy' ] : .4f } \" ) print ( f \"F1 Score: { metrics [ 'f1_score' ] : .4f } \" ) 7.2 PEFT for Memory-Constrained Environments \u00b6 # Fit large model in limited memory with PEFT pipeline = TabularPipeline ( model_name = 'TabICL' , tuning_strategy = 'peft' , tuning_params = { 'device' : 'cuda' , 'epochs' : 5 , 'learning_rate' : 2e-4 , 'support_size' : 24 , # Reduced 'query_size' : 16 , # Reduced 'batch_size' : 4 , # Smaller batches 'peft_config' : { 'r' : 4 , # Lower rank 'lora_alpha' : 8 , 'lora_dropout' : 0.1 } } ) pipeline . fit ( X_train , y_train ) 7.3 Hyperparameter Tuning \u00b6 from tabtune import TabularLeaderboard # Compare different configurations lb = TabularLeaderboard ( X_train , X_test , y_train , y_test ) # Ensemble size comparison for n_est in [ 16 , 32 , 64 ]: lb . add_model ( 'TabICL' , 'inference' , name = f 'TabICL-n { n_est } ' , model_params = { 'n_estimators' : n_est } ) # LoRA rank comparison for r in [ 4 , 8 , 16 ]: lb . add_model ( 'TabICL' , 'peft' , name = f 'TabICL-PEFT-r { r } ' , tuning_params = { 'epochs' : 3 , 'peft_config' : { 'r' : r , 'lora_alpha' : 2 * r } } ) lb . run ( rank_by = 'accuracy' ) 9. Troubleshooting \u00b6 Issue: \"Out of memory during training\" \u00b6 Solution 1 : Reduce support/query sizes tuning_params = { 'support_size' : 24 , # Instead of 48 'query_size' : 16 # Instead of 32 } Solution 2 : Use PEFT instead of base-ft tuning_strategy = 'peft' # Lower memory Issue: \"Model not converging\" \u00b6 Solution : Adjust learning rate and epochs tuning_params = { 'learning_rate' : 5e-5 , # Increase 'epochs' : 10 , # More epochs 'n_episodes' : 2000 # More training } Issue: \"Inference too slow\" \u00b6 Solution : Reduce ensemble size model_params = { 'n_estimators' : 8 # Instead of 32 } Issue: \"Low accuracy on small datasets\" \u00b6 Solution : Use larger support set tuning_params = { 'support_size' : 96 , # Larger context 'query_size' : 64 } 10. Advanced Topics \u00b6 11. Quick Reference \u00b6 Use Case Strategy Config Time Quick test inference n_est=16 <1s Rapid proto peft r=8, epochs=3 5-10m Production base-ft epochs=5 20-30m Max accuracy base-ft epochs=10 40-60m Memory limited peft r=4 5-10m 13. Next Steps \u00b6 Model Selection - Compare with other models Tuning Strategies - Deep dive into strategies Advanced PEFT - LoRA deep dive TabularLeaderboard - Benchmark TabICL TabICL offers an excellent balance of speed, accuracy, and scalability. Use it for most tabular classification tasks!","title":"TabICL"},{"location":"models/tabicl/#tabicl-in-context-learning-for-tabular-data","text":"TabICL is a scalable, ensemble-based in-context learning model designed for general-purpose tabular classification. This document provides comprehensive guidance for using TabICL with TabTune.","title":"TabICL: In-Context Learning for Tabular Data"},{"location":"models/tabicl/#1-introduction","text":"What is TabICL? TabICL (Tabular In-Context Learning) is a neural model that leverages in-context learning principles adapted for tabular data. Unlike traditional models, TabICL learns to: Process feature relationships dynamically Adapt to task-specific patterns via fine-tuning Generate robust predictions via ensemble methods Handle mixed data types naturally Key Innovation : Two-stage attention mechanism (column \u2192 row) enabling efficient feature processing and interaction modeling.","title":"1. Introduction"},{"location":"models/tabicl/#2-architecture","text":"","title":"2. Architecture"},{"location":"models/tabicl/#21-high-level-design","text":"flowchart LR A[Input Features] --> B[Column Embedder] B --> C[Feature-wise Embeddings] C --> D[Row Interactor] D --> E[Feature Interactions] E --> F[ICL Predictor] F --> G[Predictions] G --> H[Ensemble Aggregation] H --> I[Final Output]","title":"2.1 High-Level Design"},{"location":"models/tabicl/#23-two-stage-attention","text":"Stage 1: Column Attention (Feature-wise) \u2193 Per-feature processing Feature extraction Stage 2: Row Attention (Sample-wise) \u2193 Feature interaction Context modeling","title":"2.3 Two-Stage Attention"},{"location":"models/tabicl/#3-inference-parameters","text":"","title":"3. Inference Parameters"},{"location":"models/tabicl/#31-complete-parameter-reference","text":"model_params = { 'n_estimators' : 32 , # Ensemble size (views) 'softmax_temperature' : 0.9 , # Prediction confidence 'average_logits' : True , # Aggregation method 'norm_methods' : [ 'none' , 'power' ], # Feature normalization 'feat_shuffle_method' : 'latin' , # Feature permutation strategy 'batch_size' : 8 , # Ensemble batch size 'seed' : 42 # Reproducibility }","title":"3.1 Complete Parameter Reference"},{"location":"models/tabicl/#32-parameter-descriptions","text":"Parameter Type Default Range Description n_estimators int 32 4-128 Number of ensemble members; more = robust but slower softmax_temperature float 0.9 0.1-2.0 Scaling before softmax; lower = sharper predictions average_logits bool True True/False Average logits vs probabilities norm_methods list ['none', 'power'] Varies Feature normalization techniques feat_shuffle_method str 'latin' 'random', 'latin', 'sequential' Feature permutation strategy batch_size int 8 1-32 Ensemble members per batch seed int 42 0+ Random seed for reproducibility","title":"3.2 Parameter Descriptions"},{"location":"models/tabicl/#33-ensemble-size-effects","text":"n_estimators Speed Robustness Memory 4-8 \u2b50\u2b50\u2b50\u2b50\u2b50 \u2b50\u2b50 \u2b50 16-32 \u2b50\u2b50\u2b50\u2b50 \u2b50\u2b50\u2b50\u2b50 \u2b50\u2b50 64-128 \u2b50\u2b50\u2b50 \u2b50\u2b50\u2b50\u2b50\u2b50 \u2b50\u2b50\u2b50","title":"3.3 Ensemble Size Effects"},{"location":"models/tabicl/#34-feature-normalization-methods","text":"norm_methods = [ 'none' , # No normalization 'power' , # Power transformation 'quantile' , # Quantile normalization 'minmax' , # Min-max scaling 'standard' # Standardization ]","title":"3.4 Feature Normalization Methods"},{"location":"models/tabicl/#35-feature-shuffle-methods","text":"feat_shuffle_methods = { 'random' : 'Random permutation each time' , 'latin' : 'Latin square design (balanced)' , 'sequential' : 'Fixed sequential order' }","title":"3.5 Feature Shuffle Methods"},{"location":"models/tabicl/#4-fine-tuning-with-tabicl","text":"TabICL supports episodic fine-tuning where training occurs via task-like episodes.","title":"4. Fine-Tuning with TabICL"},{"location":"models/tabicl/#41-episodic-training-parameters","text":"tuning_params = { 'device' : 'cuda' , 'epochs' : 5 , # Training epochs 'learning_rate' : 2e-5 , # Optimizer learning rate 'optimizer' : 'adamw' , # Optimizer type # Episodic parameters 'support_size' : 48 , # Support set samples 'query_size' : 32 , # Query set samples 'n_episodes' : 1000 , # Episodes per epoch 'batch_size' : 8 , # Episodes per batch 'show_progress' : True # Progress bar }","title":"4.1 Episodic Training Parameters"},{"location":"models/tabicl/#42-parameter-descriptions","text":"Parameter Type Default Description support_size int 48 Number of samples per support set query_size int 32 Number of samples per query set n_episodes int 1000 Total episodes for training batch_size int 8 Episodes per batch gradient update","title":"4.2 Parameter Descriptions"},{"location":"models/tabicl/#43-episodic-training-concept","text":"One Episode: \u251c\u2500 Support Set (48 samples) \u2502 \u2514\u2500 Used as training context \u251c\u2500 Query Set (32 samples) \u2502 \u2514\u2500 Used for evaluation \u2514\u2500 Loss computed on Query Epoch = 1000 episodes with gradient updates","title":"4.3 Episodic Training Concept"},{"location":"models/tabicl/#44-fine-tuning-guidelines","text":"Support/Query Size Balance : - Larger support \u2192 more context but slower - Larger query \u2192 better gradient signal - Typical: support:query = 3:2 Number of Episodes : - 500-1000: Good for medium datasets - 1000-5000: Better for large datasets - Adjust based on dataset size: (n_{episodes} = \\frac{\\text{dataset_size}}{100}) Learning Rate : - 1e-5: Conservative, safe - 2e-5: Balanced (default) - 5e-5: Aggressive, higher variance","title":"4.4 Fine-Tuning Guidelines"},{"location":"models/tabicl/#5-usage-patterns","text":"","title":"5. Usage Patterns"},{"location":"models/tabicl/#51-inference-only","text":"from tabtune import TabularPipeline # Zero-shot with pre-trained weights pipeline = TabularPipeline ( model_name = 'TabICL' , tuning_strategy = 'inference' , model_params = { 'n_estimators' : 32 } ) pipeline . fit ( X_train , y_train ) # Only preprocesses predictions = pipeline . predict ( X_test )","title":"5.1 Inference Only"},{"location":"models/tabicl/#52-base-fine-tuning-full-parameters","text":"pipeline = TabularPipeline ( model_name = 'TabICL' , tuning_strategy = 'base-ft' , tuning_params = { 'device' : 'cuda' , 'epochs' : 5 , 'support_size' : 48 , 'query_size' : 32 , 'n_episodes' : 1000 } ) pipeline . fit ( X_train , y_train ) metrics = pipeline . evaluate ( X_test , y_test )","title":"5.2 Base Fine-Tuning (Full Parameters)"},{"location":"models/tabicl/#53-peft-fine-tuning-lora-adapters","text":"pipeline = TabularPipeline ( model_name = 'TabICL' , tuning_strategy = 'peft' , tuning_params = { 'device' : 'cuda' , 'epochs' : 5 , 'learning_rate' : 2e-4 , # Higher for PEFT 'support_size' : 24 , # Smaller for memory 'query_size' : 16 , 'peft_config' : { 'r' : 8 , 'lora_alpha' : 16 , 'lora_dropout' : 0.05 } } ) pipeline . fit ( X_train , y_train )","title":"5.3 PEFT Fine-Tuning (LoRA Adapters)"},{"location":"models/tabicl/#54-ensemble-configuration","text":"# Increase ensemble for robustness pipeline = TabularPipeline ( model_name = 'TabICL' , tuning_strategy = 'inference' , model_params = { 'n_estimators' : 128 , # Large ensemble 'batch_size' : 16 # Parallel processing } )","title":"5.4 Ensemble Configuration"},{"location":"models/tabicl/#6-lora-target-modules","text":"When using PEFT, TabICL automatically targets these modules: target_modules = [ 'col_embedder.tf_col' , # Column transformer 'col_embedder.in_linear' , # Input projection 'row_interactor' , # Interaction layers 'icl_predictor.tf_icl' , # Prediction transformer 'icl_predictor.decoder' # Decoder head ] Default PEFT Config : peft_config = { 'r' : 8 , 'lora_alpha' : 16 , 'lora_dropout' : 0.05 , 'target_modules' : None # Uses defaults above }","title":"6. LoRA Target Modules"},{"location":"models/tabicl/#7-complete-examples","text":"","title":"7. Complete Examples"},{"location":"models/tabicl/#71-basic-workflow","text":"from tabtune import TabularPipeline from sklearn.model_selection import train_test_split import pandas as pd # Load data df = pd . read_csv ( 'data.csv' ) X = df . drop ( 'target' , axis = 1 ) y = df [ 'target' ] # Split X_train , X_test , y_train , y_test = train_test_split ( X , y , test_size = 0.2 , random_state = 42 ) # Create and train pipeline = TabularPipeline ( model_name = 'TabICL' , tuning_strategy = 'base-ft' , tuning_params = { 'device' : 'cuda' , 'epochs' : 5 , 'learning_rate' : 2e-5 , 'n_episodes' : 1000 } ) pipeline . fit ( X_train , y_train ) metrics = pipeline . evaluate ( X_test , y_test ) print ( f \"Accuracy: { metrics [ 'accuracy' ] : .4f } \" ) print ( f \"F1 Score: { metrics [ 'f1_score' ] : .4f } \" )","title":"7.1 Basic Workflow"},{"location":"models/tabicl/#72-peft-for-memory-constrained-environments","text":"# Fit large model in limited memory with PEFT pipeline = TabularPipeline ( model_name = 'TabICL' , tuning_strategy = 'peft' , tuning_params = { 'device' : 'cuda' , 'epochs' : 5 , 'learning_rate' : 2e-4 , 'support_size' : 24 , # Reduced 'query_size' : 16 , # Reduced 'batch_size' : 4 , # Smaller batches 'peft_config' : { 'r' : 4 , # Lower rank 'lora_alpha' : 8 , 'lora_dropout' : 0.1 } } ) pipeline . fit ( X_train , y_train )","title":"7.2 PEFT for Memory-Constrained Environments"},{"location":"models/tabicl/#73-hyperparameter-tuning","text":"from tabtune import TabularLeaderboard # Compare different configurations lb = TabularLeaderboard ( X_train , X_test , y_train , y_test ) # Ensemble size comparison for n_est in [ 16 , 32 , 64 ]: lb . add_model ( 'TabICL' , 'inference' , name = f 'TabICL-n { n_est } ' , model_params = { 'n_estimators' : n_est } ) # LoRA rank comparison for r in [ 4 , 8 , 16 ]: lb . add_model ( 'TabICL' , 'peft' , name = f 'TabICL-PEFT-r { r } ' , tuning_params = { 'epochs' : 3 , 'peft_config' : { 'r' : r , 'lora_alpha' : 2 * r } } ) lb . run ( rank_by = 'accuracy' )","title":"7.3 Hyperparameter Tuning"},{"location":"models/tabicl/#9-troubleshooting","text":"","title":"9. Troubleshooting"},{"location":"models/tabicl/#issue-out-of-memory-during-training","text":"Solution 1 : Reduce support/query sizes tuning_params = { 'support_size' : 24 , # Instead of 48 'query_size' : 16 # Instead of 32 } Solution 2 : Use PEFT instead of base-ft tuning_strategy = 'peft' # Lower memory","title":"Issue: \"Out of memory during training\""},{"location":"models/tabicl/#issue-model-not-converging","text":"Solution : Adjust learning rate and epochs tuning_params = { 'learning_rate' : 5e-5 , # Increase 'epochs' : 10 , # More epochs 'n_episodes' : 2000 # More training }","title":"Issue: \"Model not converging\""},{"location":"models/tabicl/#issue-inference-too-slow","text":"Solution : Reduce ensemble size model_params = { 'n_estimators' : 8 # Instead of 32 }","title":"Issue: \"Inference too slow\""},{"location":"models/tabicl/#issue-low-accuracy-on-small-datasets","text":"Solution : Use larger support set tuning_params = { 'support_size' : 96 , # Larger context 'query_size' : 64 }","title":"Issue: \"Low accuracy on small datasets\""},{"location":"models/tabicl/#10-advanced-topics","text":"","title":"10. Advanced Topics"},{"location":"models/tabicl/#11-quick-reference","text":"Use Case Strategy Config Time Quick test inference n_est=16 <1s Rapid proto peft r=8, epochs=3 5-10m Production base-ft epochs=5 20-30m Max accuracy base-ft epochs=10 40-60m Memory limited peft r=4 5-10m","title":"11. Quick Reference"},{"location":"models/tabicl/#13-next-steps","text":"Model Selection - Compare with other models Tuning Strategies - Deep dive into strategies Advanced PEFT - LoRA deep dive TabularLeaderboard - Benchmark TabICL TabICL offers an excellent balance of speed, accuracy, and scalability. Use it for most tabular classification tasks!","title":"13. Next Steps"},{"location":"models/tabpfn/","text":"TabPFN: Prior-Fitted Network \u00b6 TabPFN is a revolutionary tabular model that demonstrates strong zero-shot performance without any fine-tuning. This document provides an in-depth guide to using TabPFN with TabTune. 1. Introduction \u00b6 What is TabPFN? TabPFN (Prior-Fitted Network) is a neural network trained via in-context learning on thousands of synthetic datasets. It approximates Bayesian posterior inference, making it uniquely suited for: Quick baseline predictions Small dataset learning Uncertainty quantification Few-shot adaptation Key Innovation : Rather than training on a specific task, TabPFN learns to solve tasks as a sequence-to-sequence problem , making it excel in in-context learning scenarios. 2. Architecture \u00b6 2.1 High-Level Design \u00b6 flowchart LR A[Input Features] --> B[Feature Encoding] B --> C[Support Set Processing] C --> D[Transformer Stack] D --> E[Bayesian Inference] E --> F[Predictions + Uncertainty] 2.2 Core Components \u00b6 Feature Encoder : Converts tabular features to embedding space Support Set Processor : Handles training examples as context Transformer Stack : Self-attention over support + query samples Bayesian Head : Produces mean and variance estimates 2.3 Inference Process \u00b6 1. Encode support set (training data) 2. Encode query point (test sample) 3. Process through transformer layers 4. Output Bayesian posterior (mean + variance) 5. Generate predictions with uncertainty 3. Inference Parameters \u00b6 3.1 Complete Parameter Reference \u00b6 model_params = { 'n_estimators' : 16 , # Ensemble size 'softmax_temperature' : 0.9 , # Prediction confidence 'average_logits' : True , # Aggregation method 'prior_strength' : 1.0 , # Bayesian prior weight 'normalize_input' : True , # Feature normalization 'seed' : 42 # Reproducibility } 3.2 Parameter Descriptions \u00b6 Parameter Type Default Range Description n_estimators int 16 1-32 Number of ensemble members; higher = more robust softmax_temperature float 0.9 0.1-2.0 Scaling of logits before softmax; lower = sharper predictions average_logits bool True True/False Average logits vs probabilities across ensemble prior_strength float 1.0 0.5-2.0 Weight of Bayesian prior relative to data normalize_input bool True True/False Apply input normalization seed int 42 0+ Random seed for reproducibility 3.3 Parameter Tuning Guidelines \u00b6 Ensemble Size ( n_estimators ) : - 8-16 : Fast inference, good uncertainty - 16-32 : Robust predictions, slower Temperature ( softmax_temperature ) : - < 0.5 : Very confident predictions (may overfit) - 0.5 - 1.0 : Default, balanced confidence - > 1.0 : Softer predictions, lower confidence Average Method ( average_logits ) : - True : Better for class imbalance - False : Better for probability calibration 4. Fine-Tuning with TabPFN \u00b6 TabPFN supports full fine-tuning (base-ft strategy) for task adaptation. 4.1 Base Fine-Tuning Parameters \u00b6 tuning_params = { 'device' : 'cuda' , 'epochs' : 3 , 'learning_rate' : 1e-5 , 'batch_size' : 512 , 'optimizer' : 'adamw' , 'scheduler' : 'linear' , 'warmup_steps' : 100 , 'weight_decay' : 0.01 , 'show_progress' : True } 4.2 Fine-Tuning Best Practices \u00b6 Learning Rate : Start with 1e-5, increase if needed Epochs : 3-5 epochs typically sufficient Batch Size : 256-512 works well Warmup : Use 5-10% of total steps Early Stopping : Monitor validation metric 4.3 Fine-Tuning Example \u00b6 from tabtune import TabularPipeline pipeline = TabularPipeline ( model_name = 'TabPFN' , tuning_strategy = 'base-ft' , tuning_params = { 'device' : 'cuda' , 'epochs' : 5 , 'learning_rate' : 2e-5 , 'batch_size' : 256 , 'scheduler' : 'cosine' , 'show_progress' : True } ) # Fine-tune on your data pipeline . fit ( X_train , y_train ) # Evaluate metrics = pipeline . evaluate ( X_test , y_test ) print ( f \"Accuracy: { metrics [ 'accuracy' ] : .4f } \" ) 5. Inference-Only Usage \u00b6 5.1 Zero-Shot Predictions \u00b6 Use TabPFN's pre-trained weights for immediate predictions without training: from tabtune import TabularPipeline # Create pipeline with inference strategy pipeline = TabularPipeline ( model_name = 'TabPFN' , tuning_strategy = 'inference' , model_params = { 'n_estimators' : 16 , 'softmax_temperature' : 0.9 } ) # No training needed - just preprocess and predict pipeline . fit ( X_train , y_train ) # Only does preprocessing predictions = pipeline . predict ( X_test ) uncertainty = pipeline . get_uncertainty ( X_test ) 5.2 Uncertainty Estimation \u00b6 # Get predictions with uncertainty predictions , std_dev = pipeline . predict_with_uncertainty ( X_test ) # Filter predictions by confidence high_conf_idx = std_dev < np . percentile ( std_dev , 25 ) print ( f \"High confidence predictions: { high_conf_idx . sum () } / { len ( predictions ) } \" ) 6. Usage Scenarios \u00b6 6.1 Quick Baseline \u00b6 from tabtune import TabularPipeline # Establish baseline in seconds pipeline = TabularPipeline ( model_name = 'TabPFN' , tuning_strategy = 'inference' ) pipeline . fit ( X_train , y_train ) baseline_score = pipeline . evaluate ( X_test , y_test ) print ( f \"Baseline accuracy: { baseline_score [ 'accuracy' ] : .4f } \" ) 6.2 Small Dataset Learning \u00b6 # For datasets < 10K rows, TabPFN excels X_train , X_test , y_train , y_test = train_test_split ( X , y , test_size = 0.2 , random_state = 42 ) pipeline = TabularPipeline ( model_name = 'TabPFN' , tuning_strategy = 'base-ft' , tuning_params = { 'epochs' : 3 } ) pipeline . fit ( X_train , y_train ) 7. Limitations and Constraints \u00b6 7.1 Data Constraints \u00b6 Constraint Limit Impact Max Rows ~10K Exceeding causes performance degradation Max Features ~100 More features \u2192 longer processing time Min Features 2 Single-feature prediction not supported Max Classes 10 Binary/multi-class up to 10 classes 7.2 Feature Type Constraints \u00b6 Supported : Numerical, categorical, mixed Not Supported : Text, images, time-series Preprocessing : One-hot encoding recommended for categoricals 7.3 Task Type Constraints \u00b6 \u2705 Binary Classification \u2705 Multi-class Classification \u274c Regression \u274c Multi-output \u274c Multi-label 8. PEFT (LoRA) Support \u00b6 8.1 Current Status \u00b6 \u26a0\ufe0f Experimental : LoRA support for TabPFN is experimental due to: - Batched inference engine architecture - Adapter state management conflicts - Potential prediction inconsistencies 8.2 When to Use PEFT \u00b6 Not Recommended for TabPFN. Use base-ft strategy instead: # \u274c Not recommended pipeline = TabularPipeline ( model_name = 'TabPFN' , tuning_strategy = 'peft' # May have issues - will override to base-ft ) # \u2705 Recommended pipeline = TabularPipeline ( model_name = 'TabPFN' , tuning_strategy = 'base-ft' # Fully supported ) 10. Troubleshooting \u00b6 Issue: \"Dataset too large for TabPFN\" \u00b6 Solution : Use TabICL for datasets >10K rows if len ( X_train ) > 10000 : model = 'TabICL' else : model = 'TabPFN' Issue: \"Out of memory during inference\" \u00b6 Solution : Reduce batch size tuning_params = { 'batch_size' : 128 # Instead of 512 } Issue: \"Predictions too confident (low uncertainty)\" \u00b6 Solution : Increase temperature model_params = { 'softmax_temperature' : 1.5 # Instead of 0.9 } Issue: \"PEFT causing prediction errors\" \u00b6 Solution : Use base-ft strategy instead pipeline = TabularPipeline ( model_name = 'TabPFN' , tuning_strategy = 'base-ft' # Not peft ) 11. Complete Example Workflow \u00b6 from tabtune import TabularPipeline , TabularLeaderboard from sklearn.model_selection import train_test_split import pandas as pd # 1. Load data df = pd . read_csv ( 'small_dataset.csv' ) # <10K rows ideal X = df . drop ( 'target' , axis = 1 ) y = df [ 'target' ] # 2. Split X_train , X_test , y_train , y_test = train_test_split ( X , y , test_size = 0.2 , random_state = 42 ) # 3. Strategy 1: Zero-shot baseline print ( \"=== Zero-Shot Baseline ===\" ) baseline = TabularPipeline ( model_name = 'TabPFN' , tuning_strategy = 'inference' ) baseline . fit ( X_train , y_train ) baseline_metrics = baseline . evaluate ( X_test , y_test ) print ( f \"Baseline Accuracy: { baseline_metrics [ 'accuracy' ] : .4f } \" ) # 4. Strategy 2: Fine-tuned print ( \" \\n === Fine-Tuned ===\" ) finetuned = TabularPipeline ( model_name = 'TabPFN' , tuning_strategy = 'base-ft' , tuning_params = { 'device' : 'cuda' , 'epochs' : 5 , 'learning_rate' : 2e-5 , 'show_progress' : True } ) finetuned . fit ( X_train , y_train ) finetuned_metrics = finetuned . evaluate ( X_test , y_test ) print ( f \"Fine-tuned Accuracy: { finetuned_metrics [ 'accuracy' ] : .4f } \" ) # 5. Compare with other models print ( \" \\n === Model Comparison ===\" ) lb = TabularLeaderboard ( X_train , X_test , y_train , y_test ) lb . add_model ( 'TabPFN' , 'inference' , name = 'TabPFN-Inference' ) lb . add_model ( 'TabPFN' , 'base-ft' , name = 'TabPFN-FineTune' , tuning_params = { 'epochs' : 5 }) lb . add_model ( 'TabICL' , 'peft' , name = 'TabICL-PEFT' ) lb . run ( rank_by = 'accuracy' ) 12. Quick Reference \u00b6 Task Strategy Time Accuracy Instant baseline inference <1s Medium Rapid prototyping base-ft + 3 epochs 5m Good Production model base-ft + 5 epochs 15m High Uncertainty estimation inference <1s With uncertainty 13. Next Steps \u00b6 Model Selection - Compare with other models Tuning Strategies - Fine-tuning details TabularLeaderboard - Benchmark TabPFN vs other models API Reference - Complete API docs TabPFN excels at quick learning on small datasets. Use it for rapid experimentation and as a strong baseline!","title":"TabPFN"},{"location":"models/tabpfn/#tabpfn-prior-fitted-network","text":"TabPFN is a revolutionary tabular model that demonstrates strong zero-shot performance without any fine-tuning. This document provides an in-depth guide to using TabPFN with TabTune.","title":"TabPFN: Prior-Fitted Network"},{"location":"models/tabpfn/#1-introduction","text":"What is TabPFN? TabPFN (Prior-Fitted Network) is a neural network trained via in-context learning on thousands of synthetic datasets. It approximates Bayesian posterior inference, making it uniquely suited for: Quick baseline predictions Small dataset learning Uncertainty quantification Few-shot adaptation Key Innovation : Rather than training on a specific task, TabPFN learns to solve tasks as a sequence-to-sequence problem , making it excel in in-context learning scenarios.","title":"1. Introduction"},{"location":"models/tabpfn/#2-architecture","text":"","title":"2. Architecture"},{"location":"models/tabpfn/#21-high-level-design","text":"flowchart LR A[Input Features] --> B[Feature Encoding] B --> C[Support Set Processing] C --> D[Transformer Stack] D --> E[Bayesian Inference] E --> F[Predictions + Uncertainty]","title":"2.1 High-Level Design"},{"location":"models/tabpfn/#22-core-components","text":"Feature Encoder : Converts tabular features to embedding space Support Set Processor : Handles training examples as context Transformer Stack : Self-attention over support + query samples Bayesian Head : Produces mean and variance estimates","title":"2.2 Core Components"},{"location":"models/tabpfn/#23-inference-process","text":"1. Encode support set (training data) 2. Encode query point (test sample) 3. Process through transformer layers 4. Output Bayesian posterior (mean + variance) 5. Generate predictions with uncertainty","title":"2.3 Inference Process"},{"location":"models/tabpfn/#3-inference-parameters","text":"","title":"3. Inference Parameters"},{"location":"models/tabpfn/#31-complete-parameter-reference","text":"model_params = { 'n_estimators' : 16 , # Ensemble size 'softmax_temperature' : 0.9 , # Prediction confidence 'average_logits' : True , # Aggregation method 'prior_strength' : 1.0 , # Bayesian prior weight 'normalize_input' : True , # Feature normalization 'seed' : 42 # Reproducibility }","title":"3.1 Complete Parameter Reference"},{"location":"models/tabpfn/#32-parameter-descriptions","text":"Parameter Type Default Range Description n_estimators int 16 1-32 Number of ensemble members; higher = more robust softmax_temperature float 0.9 0.1-2.0 Scaling of logits before softmax; lower = sharper predictions average_logits bool True True/False Average logits vs probabilities across ensemble prior_strength float 1.0 0.5-2.0 Weight of Bayesian prior relative to data normalize_input bool True True/False Apply input normalization seed int 42 0+ Random seed for reproducibility","title":"3.2 Parameter Descriptions"},{"location":"models/tabpfn/#33-parameter-tuning-guidelines","text":"Ensemble Size ( n_estimators ) : - 8-16 : Fast inference, good uncertainty - 16-32 : Robust predictions, slower Temperature ( softmax_temperature ) : - < 0.5 : Very confident predictions (may overfit) - 0.5 - 1.0 : Default, balanced confidence - > 1.0 : Softer predictions, lower confidence Average Method ( average_logits ) : - True : Better for class imbalance - False : Better for probability calibration","title":"3.3 Parameter Tuning Guidelines"},{"location":"models/tabpfn/#4-fine-tuning-with-tabpfn","text":"TabPFN supports full fine-tuning (base-ft strategy) for task adaptation.","title":"4. Fine-Tuning with TabPFN"},{"location":"models/tabpfn/#41-base-fine-tuning-parameters","text":"tuning_params = { 'device' : 'cuda' , 'epochs' : 3 , 'learning_rate' : 1e-5 , 'batch_size' : 512 , 'optimizer' : 'adamw' , 'scheduler' : 'linear' , 'warmup_steps' : 100 , 'weight_decay' : 0.01 , 'show_progress' : True }","title":"4.1 Base Fine-Tuning Parameters"},{"location":"models/tabpfn/#42-fine-tuning-best-practices","text":"Learning Rate : Start with 1e-5, increase if needed Epochs : 3-5 epochs typically sufficient Batch Size : 256-512 works well Warmup : Use 5-10% of total steps Early Stopping : Monitor validation metric","title":"4.2 Fine-Tuning Best Practices"},{"location":"models/tabpfn/#43-fine-tuning-example","text":"from tabtune import TabularPipeline pipeline = TabularPipeline ( model_name = 'TabPFN' , tuning_strategy = 'base-ft' , tuning_params = { 'device' : 'cuda' , 'epochs' : 5 , 'learning_rate' : 2e-5 , 'batch_size' : 256 , 'scheduler' : 'cosine' , 'show_progress' : True } ) # Fine-tune on your data pipeline . fit ( X_train , y_train ) # Evaluate metrics = pipeline . evaluate ( X_test , y_test ) print ( f \"Accuracy: { metrics [ 'accuracy' ] : .4f } \" )","title":"4.3 Fine-Tuning Example"},{"location":"models/tabpfn/#5-inference-only-usage","text":"","title":"5. Inference-Only Usage"},{"location":"models/tabpfn/#51-zero-shot-predictions","text":"Use TabPFN's pre-trained weights for immediate predictions without training: from tabtune import TabularPipeline # Create pipeline with inference strategy pipeline = TabularPipeline ( model_name = 'TabPFN' , tuning_strategy = 'inference' , model_params = { 'n_estimators' : 16 , 'softmax_temperature' : 0.9 } ) # No training needed - just preprocess and predict pipeline . fit ( X_train , y_train ) # Only does preprocessing predictions = pipeline . predict ( X_test ) uncertainty = pipeline . get_uncertainty ( X_test )","title":"5.1 Zero-Shot Predictions"},{"location":"models/tabpfn/#52-uncertainty-estimation","text":"# Get predictions with uncertainty predictions , std_dev = pipeline . predict_with_uncertainty ( X_test ) # Filter predictions by confidence high_conf_idx = std_dev < np . percentile ( std_dev , 25 ) print ( f \"High confidence predictions: { high_conf_idx . sum () } / { len ( predictions ) } \" )","title":"5.2 Uncertainty Estimation"},{"location":"models/tabpfn/#6-usage-scenarios","text":"","title":"6. Usage Scenarios"},{"location":"models/tabpfn/#61-quick-baseline","text":"from tabtune import TabularPipeline # Establish baseline in seconds pipeline = TabularPipeline ( model_name = 'TabPFN' , tuning_strategy = 'inference' ) pipeline . fit ( X_train , y_train ) baseline_score = pipeline . evaluate ( X_test , y_test ) print ( f \"Baseline accuracy: { baseline_score [ 'accuracy' ] : .4f } \" )","title":"6.1 Quick Baseline"},{"location":"models/tabpfn/#62-small-dataset-learning","text":"# For datasets < 10K rows, TabPFN excels X_train , X_test , y_train , y_test = train_test_split ( X , y , test_size = 0.2 , random_state = 42 ) pipeline = TabularPipeline ( model_name = 'TabPFN' , tuning_strategy = 'base-ft' , tuning_params = { 'epochs' : 3 } ) pipeline . fit ( X_train , y_train )","title":"6.2 Small Dataset Learning"},{"location":"models/tabpfn/#7-limitations-and-constraints","text":"","title":"7. Limitations and Constraints"},{"location":"models/tabpfn/#71-data-constraints","text":"Constraint Limit Impact Max Rows ~10K Exceeding causes performance degradation Max Features ~100 More features \u2192 longer processing time Min Features 2 Single-feature prediction not supported Max Classes 10 Binary/multi-class up to 10 classes","title":"7.1 Data Constraints"},{"location":"models/tabpfn/#72-feature-type-constraints","text":"Supported : Numerical, categorical, mixed Not Supported : Text, images, time-series Preprocessing : One-hot encoding recommended for categoricals","title":"7.2 Feature Type Constraints"},{"location":"models/tabpfn/#73-task-type-constraints","text":"\u2705 Binary Classification \u2705 Multi-class Classification \u274c Regression \u274c Multi-output \u274c Multi-label","title":"7.3 Task Type Constraints"},{"location":"models/tabpfn/#8-peft-lora-support","text":"","title":"8. PEFT (LoRA) Support"},{"location":"models/tabpfn/#81-current-status","text":"\u26a0\ufe0f Experimental : LoRA support for TabPFN is experimental due to: - Batched inference engine architecture - Adapter state management conflicts - Potential prediction inconsistencies","title":"8.1 Current Status"},{"location":"models/tabpfn/#82-when-to-use-peft","text":"Not Recommended for TabPFN. Use base-ft strategy instead: # \u274c Not recommended pipeline = TabularPipeline ( model_name = 'TabPFN' , tuning_strategy = 'peft' # May have issues - will override to base-ft ) # \u2705 Recommended pipeline = TabularPipeline ( model_name = 'TabPFN' , tuning_strategy = 'base-ft' # Fully supported )","title":"8.2 When to Use PEFT"},{"location":"models/tabpfn/#10-troubleshooting","text":"","title":"10. Troubleshooting"},{"location":"models/tabpfn/#issue-dataset-too-large-for-tabpfn","text":"Solution : Use TabICL for datasets >10K rows if len ( X_train ) > 10000 : model = 'TabICL' else : model = 'TabPFN'","title":"Issue: \"Dataset too large for TabPFN\""},{"location":"models/tabpfn/#issue-out-of-memory-during-inference","text":"Solution : Reduce batch size tuning_params = { 'batch_size' : 128 # Instead of 512 }","title":"Issue: \"Out of memory during inference\""},{"location":"models/tabpfn/#issue-predictions-too-confident-low-uncertainty","text":"Solution : Increase temperature model_params = { 'softmax_temperature' : 1.5 # Instead of 0.9 }","title":"Issue: \"Predictions too confident (low uncertainty)\""},{"location":"models/tabpfn/#issue-peft-causing-prediction-errors","text":"Solution : Use base-ft strategy instead pipeline = TabularPipeline ( model_name = 'TabPFN' , tuning_strategy = 'base-ft' # Not peft )","title":"Issue: \"PEFT causing prediction errors\""},{"location":"models/tabpfn/#11-complete-example-workflow","text":"from tabtune import TabularPipeline , TabularLeaderboard from sklearn.model_selection import train_test_split import pandas as pd # 1. Load data df = pd . read_csv ( 'small_dataset.csv' ) # <10K rows ideal X = df . drop ( 'target' , axis = 1 ) y = df [ 'target' ] # 2. Split X_train , X_test , y_train , y_test = train_test_split ( X , y , test_size = 0.2 , random_state = 42 ) # 3. Strategy 1: Zero-shot baseline print ( \"=== Zero-Shot Baseline ===\" ) baseline = TabularPipeline ( model_name = 'TabPFN' , tuning_strategy = 'inference' ) baseline . fit ( X_train , y_train ) baseline_metrics = baseline . evaluate ( X_test , y_test ) print ( f \"Baseline Accuracy: { baseline_metrics [ 'accuracy' ] : .4f } \" ) # 4. Strategy 2: Fine-tuned print ( \" \\n === Fine-Tuned ===\" ) finetuned = TabularPipeline ( model_name = 'TabPFN' , tuning_strategy = 'base-ft' , tuning_params = { 'device' : 'cuda' , 'epochs' : 5 , 'learning_rate' : 2e-5 , 'show_progress' : True } ) finetuned . fit ( X_train , y_train ) finetuned_metrics = finetuned . evaluate ( X_test , y_test ) print ( f \"Fine-tuned Accuracy: { finetuned_metrics [ 'accuracy' ] : .4f } \" ) # 5. Compare with other models print ( \" \\n === Model Comparison ===\" ) lb = TabularLeaderboard ( X_train , X_test , y_train , y_test ) lb . add_model ( 'TabPFN' , 'inference' , name = 'TabPFN-Inference' ) lb . add_model ( 'TabPFN' , 'base-ft' , name = 'TabPFN-FineTune' , tuning_params = { 'epochs' : 5 }) lb . add_model ( 'TabICL' , 'peft' , name = 'TabICL-PEFT' ) lb . run ( rank_by = 'accuracy' )","title":"11. Complete Example Workflow"},{"location":"models/tabpfn/#12-quick-reference","text":"Task Strategy Time Accuracy Instant baseline inference <1s Medium Rapid prototyping base-ft + 3 epochs 5m Good Production model base-ft + 5 epochs 15m High Uncertainty estimation inference <1s With uncertainty","title":"12. Quick Reference"},{"location":"models/tabpfn/#13-next-steps","text":"Model Selection - Compare with other models Tuning Strategies - Fine-tuning details TabularLeaderboard - Benchmark TabPFN vs other models API Reference - Complete API docs TabPFN excels at quick learning on small datasets. Use it for rapid experimentation and as a strong baseline!","title":"13. Next Steps"},{"location":"user-guide/data-processing/","text":"Data Processing \u00b6 The DataProcessor is TabTune's intelligent, model-aware data preparation engine. It automatically handles imputation, scaling, categorical encoding, feature selection, and resampling based on your chosen model's requirements. 1. Overview \u00b6 The DataProcessor integrates two preprocessing pathways: Custom Model-Specific Preprocessing : For models requiring specialized transformations (TabPFN, TabICL, ContextTab, Mitra, TabDPT, TabBiaxial, APT). Standard Preprocessing Pipeline : For general tabular models using scikit-learn-compatible transformations. 2. Architecture \u00b6 flowchart TD A[Raw DataFrame] --> B{Model-Specific?} B -->|Yes| C[Custom Preprocessor] B -->|No| D[Standard Pipeline] C --> E[Transformed Data] D --> F[Imputation] F --> G[Categorical Encoding] G --> H[Scaling] H --> I[Feature Selection] I --> J[Resampling] J --> E 3. Initialization Parameters \u00b6 DataProcessor ( model_name = None , override_types = None , imputation_strategy = 'mean' , categorical_encoding = 'onehot' , scaling_strategy = 'standard' , resampling_strategy = None , feature_selection_strategy = None , feature_selection_k = 10 , model_params = None ) Parameter Reference \u00b6 Parameter Type Default Description model_name str None Model identifier to trigger custom preprocessing override_types dict None Manual column type specifications imputation_strategy str 'mean' Strategy for missing value imputation categorical_encoding str 'onehot' Categorical encoding method scaling_strategy str 'standard' Numerical feature scaling method resampling_strategy str None Class imbalance correction method feature_selection_strategy str None Feature selection algorithm feature_selection_k int 10 Number of features to select model_params dict None Additional model-specific parameters 4. Model-Aware Defaults \u00b6 When model_name is specified, the processor automatically configures optimal preprocessing: Model Categorical Encoding Special Handling TabPFN tabpfn_special Integer encoding, limited feature count TabICL tabicl_special Normalization + shuffling support TabBiaxial tab_biaxial_special Custom biaxial attention preprocessing TabDPT tabdpt_special Context-aware transformations Mitra mitra_special 2D attention preprocessing ContextTab contexttab_special Text embedding integration APT apt_special Patch-based encoding (requires d_patch ) 5. Standard Preprocessing Pipeline \u00b6 5.1 Missing Value Imputation \u00b6 Available strategies: mean : Replace with column mean (default) median : Replace with column median iterative : Multivariate imputation using IterativeImputer knn : K-Nearest Neighbors imputation processor = DataProcessor ( imputation_strategy = 'median' ) 5.2 Categorical Encoding \u00b6 Supported methods: onehot : One-hot encoding (default, handles unknown categories) ordinal : Ordinal encoding with unknown value handling target : Target encoding (uses target statistics) hashing : Hashing encoder for high-cardinality features binary : Binary encoding for memory efficiency processor = DataProcessor ( categorical_encoding = 'target' ) 5.3 Numerical Scaling \u00b6 Available scalers: standard : Standardization (zero mean, unit variance) minmax : Min-max normalization to [0, 1] robust : Robust scaling using median and IQR power_transform : Power transformation for normality processor = DataProcessor ( scaling_strategy = 'robust' ) 5.4 Feature Selection \u00b6 Methods: variance : Remove low-variance features select_k_best_anova : ANOVA F-test for classification select_k_best_chi2 : Chi-squared test (requires non-negative features) correlation : Remove highly correlated features (threshold=0.9) processor = DataProcessor ( feature_selection_strategy = 'select_k_best_anova' , feature_selection_k = 20 ) 5.5 Class Resampling \u00b6 Imbalanced data handling: smote : Synthetic Minority Over-sampling Technique random_over : Random oversampling of minority class random_under : Random undersampling of majority class tomek : Tomek links removal kmeans : Cluster centroids undersampling knn : Neighborhood cleaning rule processor = DataProcessor ( resampling_strategy = 'smote' ) 6. get_processing_summary() function \u00b6 Returns a detailed report of all applied transformations. summary = processor . get_processing_summary () print ( summary ) Example Output : --- Data Processing Summary --- [Standard Preprocessing Pipeline] 1. Imputation (Strategy: 'median') - Applied to 12 numerical features: `age`, `salary`, ... 2. Categorical Encoding (Strategy: 'onehot') - Applied to 5 categorical features: `city`, `occupation`, ... 3. Scaling (Strategy: 'standard') - Applied to 27 features (original numerical + encoded categorical). 4. Feature Selection (Strategy: 'select_k_best_anova') - Removed 7 features: `feature_3`, `feature_8`, ... [Resampling] - Strategy: 'smote' applied to the training data. 7. Custom Model Preprocessing \u00b6 For models with special requirements, TabTune automatically loads the appropriate custom preprocessor: TabPFN Preprocessing \u00b6 Converts all features to integer codes Limits feature count (max 100) Handles categorical and numerical separately TabICL/TabBiaxial Preprocessing \u00b6 Applies multiple normalization methods Supports feature shuffling strategies Prepares data for episodic training ContextTab Preprocessing \u00b6 Generates text embeddings for categorical features Integrates semantic information Handles mixed-type features TabDPT Preprocessing \u00b6 Context-based transformations Supports k-NN context selection Prepares permuted features Mitra Preprocessing \u00b6 2D attention-compatible format Row and column embeddings Synthetic prior integration 8. Usage Examples \u00b6 Example 1: Basic Standard Pipeline \u00b6 from tabtune import DataProcessor processor = DataProcessor ( imputation_strategy = 'median' , categorical_encoding = 'onehot' , scaling_strategy = 'standard' ) # Fit on training data processor . fit ( X_train , y_train ) # Transform training and test data X_train_processed , y_train_processed = processor . transform ( X_train , y_train ) X_test_processed = processor . transform ( X_test ) Example 3: Handling Imbalanced Data \u00b6 processor = DataProcessor ( imputation_strategy = 'mean' , categorical_encoding = 'target' , scaling_strategy = 'robust' , resampling_strategy = 'smote' ) # Resampling is applied during fit_transform X_resampled , y_resampled = processor . fit_transform ( X_train , y_train ) 9. Best Practices \u00b6 Always fit on training data only : Never fit on test data to avoid data leakage. Use model-specific preprocessing : Let model_name trigger optimal defaults. Check processing summary : Verify applied transformations before training. Handle missing values explicitly : Choose imputation strategy based on data distribution. Scale after encoding : Standard pipeline handles this automatically. Test resampling impact : Compare with and without resampling for imbalanced tasks. 10. Troubleshooting \u00b6 Issue: \"Must call fit() before calling transform()\" \u00b6 Solution : Ensure .fit() is called on training data before .transform() . Issue: Feature count mismatch after encoding \u00b6 Solution : Use handle_unknown='ignore' in encoder or ensure test data has same categories. Issue: NaN values after transformation \u00b6 Solution : Check imputation strategy; use 'iterative' for complex missing patterns. Issue: Memory errors with large datasets \u00b6 Solution : Use 'hashing' or 'binary' encoding; avoid one-hot for high cardinality. 11. Next Steps \u00b6 Tuning Strategies - Learn about training workflows Model Selection - Choose the right model API Reference - Complete DataProcessor API The DataProcessor ensures your data is optimally prepared for any TabTune model with minimal configuration.","title":"Data Processing"},{"location":"user-guide/data-processing/#data-processing","text":"The DataProcessor is TabTune's intelligent, model-aware data preparation engine. It automatically handles imputation, scaling, categorical encoding, feature selection, and resampling based on your chosen model's requirements.","title":"Data Processing"},{"location":"user-guide/data-processing/#1-overview","text":"The DataProcessor integrates two preprocessing pathways: Custom Model-Specific Preprocessing : For models requiring specialized transformations (TabPFN, TabICL, ContextTab, Mitra, TabDPT, TabBiaxial, APT). Standard Preprocessing Pipeline : For general tabular models using scikit-learn-compatible transformations.","title":"1. Overview"},{"location":"user-guide/data-processing/#2-architecture","text":"flowchart TD A[Raw DataFrame] --> B{Model-Specific?} B -->|Yes| C[Custom Preprocessor] B -->|No| D[Standard Pipeline] C --> E[Transformed Data] D --> F[Imputation] F --> G[Categorical Encoding] G --> H[Scaling] H --> I[Feature Selection] I --> J[Resampling] J --> E","title":"2. Architecture"},{"location":"user-guide/data-processing/#3-initialization-parameters","text":"DataProcessor ( model_name = None , override_types = None , imputation_strategy = 'mean' , categorical_encoding = 'onehot' , scaling_strategy = 'standard' , resampling_strategy = None , feature_selection_strategy = None , feature_selection_k = 10 , model_params = None )","title":"3. Initialization Parameters"},{"location":"user-guide/data-processing/#parameter-reference","text":"Parameter Type Default Description model_name str None Model identifier to trigger custom preprocessing override_types dict None Manual column type specifications imputation_strategy str 'mean' Strategy for missing value imputation categorical_encoding str 'onehot' Categorical encoding method scaling_strategy str 'standard' Numerical feature scaling method resampling_strategy str None Class imbalance correction method feature_selection_strategy str None Feature selection algorithm feature_selection_k int 10 Number of features to select model_params dict None Additional model-specific parameters","title":"Parameter Reference"},{"location":"user-guide/data-processing/#4-model-aware-defaults","text":"When model_name is specified, the processor automatically configures optimal preprocessing: Model Categorical Encoding Special Handling TabPFN tabpfn_special Integer encoding, limited feature count TabICL tabicl_special Normalization + shuffling support TabBiaxial tab_biaxial_special Custom biaxial attention preprocessing TabDPT tabdpt_special Context-aware transformations Mitra mitra_special 2D attention preprocessing ContextTab contexttab_special Text embedding integration APT apt_special Patch-based encoding (requires d_patch )","title":"4. Model-Aware Defaults"},{"location":"user-guide/data-processing/#5-standard-preprocessing-pipeline","text":"","title":"5. Standard Preprocessing Pipeline"},{"location":"user-guide/data-processing/#51-missing-value-imputation","text":"Available strategies: mean : Replace with column mean (default) median : Replace with column median iterative : Multivariate imputation using IterativeImputer knn : K-Nearest Neighbors imputation processor = DataProcessor ( imputation_strategy = 'median' )","title":"5.1 Missing Value Imputation"},{"location":"user-guide/data-processing/#52-categorical-encoding","text":"Supported methods: onehot : One-hot encoding (default, handles unknown categories) ordinal : Ordinal encoding with unknown value handling target : Target encoding (uses target statistics) hashing : Hashing encoder for high-cardinality features binary : Binary encoding for memory efficiency processor = DataProcessor ( categorical_encoding = 'target' )","title":"5.2 Categorical Encoding"},{"location":"user-guide/data-processing/#53-numerical-scaling","text":"Available scalers: standard : Standardization (zero mean, unit variance) minmax : Min-max normalization to [0, 1] robust : Robust scaling using median and IQR power_transform : Power transformation for normality processor = DataProcessor ( scaling_strategy = 'robust' )","title":"5.3 Numerical Scaling"},{"location":"user-guide/data-processing/#54-feature-selection","text":"Methods: variance : Remove low-variance features select_k_best_anova : ANOVA F-test for classification select_k_best_chi2 : Chi-squared test (requires non-negative features) correlation : Remove highly correlated features (threshold=0.9) processor = DataProcessor ( feature_selection_strategy = 'select_k_best_anova' , feature_selection_k = 20 )","title":"5.4 Feature Selection"},{"location":"user-guide/data-processing/#55-class-resampling","text":"Imbalanced data handling: smote : Synthetic Minority Over-sampling Technique random_over : Random oversampling of minority class random_under : Random undersampling of majority class tomek : Tomek links removal kmeans : Cluster centroids undersampling knn : Neighborhood cleaning rule processor = DataProcessor ( resampling_strategy = 'smote' )","title":"5.5 Class Resampling"},{"location":"user-guide/data-processing/#6-get_processing_summary-function","text":"Returns a detailed report of all applied transformations. summary = processor . get_processing_summary () print ( summary ) Example Output : --- Data Processing Summary --- [Standard Preprocessing Pipeline] 1. Imputation (Strategy: 'median') - Applied to 12 numerical features: `age`, `salary`, ... 2. Categorical Encoding (Strategy: 'onehot') - Applied to 5 categorical features: `city`, `occupation`, ... 3. Scaling (Strategy: 'standard') - Applied to 27 features (original numerical + encoded categorical). 4. Feature Selection (Strategy: 'select_k_best_anova') - Removed 7 features: `feature_3`, `feature_8`, ... [Resampling] - Strategy: 'smote' applied to the training data.","title":"6. get_processing_summary() function"},{"location":"user-guide/data-processing/#7-custom-model-preprocessing","text":"For models with special requirements, TabTune automatically loads the appropriate custom preprocessor:","title":"7. Custom Model Preprocessing"},{"location":"user-guide/data-processing/#tabpfn-preprocessing","text":"Converts all features to integer codes Limits feature count (max 100) Handles categorical and numerical separately","title":"TabPFN Preprocessing"},{"location":"user-guide/data-processing/#tabicltabbiaxial-preprocessing","text":"Applies multiple normalization methods Supports feature shuffling strategies Prepares data for episodic training","title":"TabICL/TabBiaxial Preprocessing"},{"location":"user-guide/data-processing/#contexttab-preprocessing","text":"Generates text embeddings for categorical features Integrates semantic information Handles mixed-type features","title":"ContextTab Preprocessing"},{"location":"user-guide/data-processing/#tabdpt-preprocessing","text":"Context-based transformations Supports k-NN context selection Prepares permuted features","title":"TabDPT Preprocessing"},{"location":"user-guide/data-processing/#mitra-preprocessing","text":"2D attention-compatible format Row and column embeddings Synthetic prior integration","title":"Mitra Preprocessing"},{"location":"user-guide/data-processing/#8-usage-examples","text":"","title":"8. Usage Examples"},{"location":"user-guide/data-processing/#example-1-basic-standard-pipeline","text":"from tabtune import DataProcessor processor = DataProcessor ( imputation_strategy = 'median' , categorical_encoding = 'onehot' , scaling_strategy = 'standard' ) # Fit on training data processor . fit ( X_train , y_train ) # Transform training and test data X_train_processed , y_train_processed = processor . transform ( X_train , y_train ) X_test_processed = processor . transform ( X_test )","title":"Example 1: Basic Standard Pipeline"},{"location":"user-guide/data-processing/#example-3-handling-imbalanced-data","text":"processor = DataProcessor ( imputation_strategy = 'mean' , categorical_encoding = 'target' , scaling_strategy = 'robust' , resampling_strategy = 'smote' ) # Resampling is applied during fit_transform X_resampled , y_resampled = processor . fit_transform ( X_train , y_train )","title":"Example 3: Handling Imbalanced Data"},{"location":"user-guide/data-processing/#9-best-practices","text":"Always fit on training data only : Never fit on test data to avoid data leakage. Use model-specific preprocessing : Let model_name trigger optimal defaults. Check processing summary : Verify applied transformations before training. Handle missing values explicitly : Choose imputation strategy based on data distribution. Scale after encoding : Standard pipeline handles this automatically. Test resampling impact : Compare with and without resampling for imbalanced tasks.","title":"9. Best Practices"},{"location":"user-guide/data-processing/#10-troubleshooting","text":"","title":"10. Troubleshooting"},{"location":"user-guide/data-processing/#issue-must-call-fit-before-calling-transform","text":"Solution : Ensure .fit() is called on training data before .transform() .","title":"Issue: \"Must call fit() before calling transform()\""},{"location":"user-guide/data-processing/#issue-feature-count-mismatch-after-encoding","text":"Solution : Use handle_unknown='ignore' in encoder or ensure test data has same categories.","title":"Issue: Feature count mismatch after encoding"},{"location":"user-guide/data-processing/#issue-nan-values-after-transformation","text":"Solution : Check imputation strategy; use 'iterative' for complex missing patterns.","title":"Issue: NaN values after transformation"},{"location":"user-guide/data-processing/#issue-memory-errors-with-large-datasets","text":"Solution : Use 'hashing' or 'binary' encoding; avoid one-hot for high cardinality.","title":"Issue: Memory errors with large datasets"},{"location":"user-guide/data-processing/#11-next-steps","text":"Tuning Strategies - Learn about training workflows Model Selection - Choose the right model API Reference - Complete DataProcessor API The DataProcessor ensures your data is optimally prepared for any TabTune model with minimal configuration.","title":"11. Next Steps"},{"location":"user-guide/leaderboard/","text":"Model Comparison with TabularLeaderboard \u00b6 The TabularLeaderboard is a powerful benchmarking tool that enables systematic comparison of multiple models and tuning strategies on your dataset. This guide shows how to use it effectively. 1. Overview \u00b6 TabularLeaderboard simplifies the process of comparing different TabTune models and strategies: \u2705 Compare multiple models simultaneously \u2705 Test different tuning strategies (inference, base-ft, peft) \u2705 Rank results by any evaluation metric \u2705 Export results for analysis \u2705 Track experiment metadata \u2705 Visualize performance comparisons 2. Core Concepts \u00b6 2.1 Workflow \u00b6 flowchart LR A[Initialize Leaderboard] --> B[Add Models] B --> C[Configure Strategies] C --> D[Run Benchmarks] D --> E[Evaluate & Rank] E --> F[Export Results] 2.2 Key Components \u00b6 Leaderboard : Container for managing multiple model configurations Model Entry : Single model + strategy + hyperparameter combination Benchmark : Complete evaluation run across all added models Results : Ranked comparison with metrics and metadata 3. Basic Usage \u00b6 3.1 Initialize Leaderboard \u00b6 from tabtune import TabularLeaderboard from sklearn.model_selection import train_test_split # Prepare data splits X , y = load_your_data () X_train , X_test , y_train , y_test = train_test_split ( X , y , test_size = 0.2 , random_state = 42 ) # Initialize leaderboard leaderboard = TabularLeaderboard ( X_train = X_train , X_test = X_test , y_train = y_train , y_test = y_test , task_type = 'classification' ) 3.2 Add Models to Leaderboard \u00b6 # Add single model with default settings leaderboard . add_model ( model_name = 'TabPFN' , tuning_strategy = 'inference' ) # Add model with custom tuning parameters leaderboard . add_model ( model_name = 'TabICL' , tuning_strategy = 'base-ft' , model_params = { 'n_estimators' : 16 }, tuning_params = { 'epochs' : 5 , 'learning_rate' : 2e-5 } ) # Add same model with different strategy leaderboard . add_model ( model_name = 'TabICL' , tuning_strategy = 'peft' , model_params = { 'n_estimators' : 16 }, tuning_params = { 'epochs' : 5 , 'learning_rate' : 2e-4 , 'peft_config' : { 'r' : 8 , 'lora_alpha' : 16 } } ) 3.3 Run Benchmarks \u00b6 # Run all added models results = leaderboard . run () # Run with custom settings results = leaderboard . run ( rank_by = 'roc_auc_score' , # Metric to rank by verbose = True , # Print progress n_jobs = 1 # Parallel jobs (1 = sequential) ) 4. Complete Example \u00b6 4.1 Full Comparison Workflow \u00b6 from tabtune import TabularLeaderboard import pandas as pd from sklearn.model_selection import train_test_split # 1. Load data df = pd . read_csv ( 'dataset.csv' ) X = df . drop ( 'target' , axis = 1 ) y = df [ 'target' ] # 2. Split data X_train , X_test , y_train , y_test = train_test_split ( X , y , test_size = 0.2 ) # 3. Initialize leaderboard leaderboard = TabularLeaderboard ( X_train , X_test , y_train , y_test ) # 4. Add models - Inference Baseline leaderboard . add_model ( 'TabPFN' , 'inference' , name = 'TabPFN-Inference' ) # 5. Add models - PEFT Strategies for model in [ 'TabICL' , 'TabBiaxial' , 'Mitra' ]: leaderboard . add_model ( model , 'peft' , name = f ' { model } -PEFT' , tuning_params = { 'epochs' : 3 , 'peft_config' : { 'r' : 8 , 'lora_alpha' : 16 } } ) # 6. Add models - Base Fine-Tuning (for comparison) leaderboard . add_model ( 'TabICL' , 'base-ft' , name = 'TabICL-BaseFT' , tuning_params = { 'epochs' : 5 , 'learning_rate' : 2e-5 } ) # 7. Run benchmarks results = leaderboard . run ( rank_by = 'accuracy' , verbose = True ) 5. Class Reference \u00b6 5.1 TabularLeaderboard Constructor \u00b6 TabularLeaderboard ( X_train : pd . DataFrame , X_test : pd . DataFrame , y_train : pd . Series , y_test : pd . Series , task_type : str = 'classification' , validation_split : float = 0.1 , random_state : int = 42 ) Parameters : Parameter Type Default Description X_train DataFrame Required Training features X_test DataFrame Required Test features y_train Series Required Training labels y_test Series Required Test labels task_type str 'classification' 'classification' or 'regression' validation_split float 0.1 Fraction for validation random_state int 42 Random seed for reproducibility 5.2 add_model() Method \u00b6 leaderboard . add_model ( model_name : str , tuning_strategy : str , name : str = None , model_params : dict = None , tuning_params : dict = None , processor_params : dict = None ) Parameters : Parameter Type Default Description model_name str Required Model to add (TabPFN, TabICL, etc.) tuning_strategy str Required 'inference', 'base-ft', or 'peft' name str None Custom name for leaderboard (auto-generated if None) model_params dict None Model-specific hyperparameters tuning_params dict None Training hyperparameters processor_params dict None Preprocessing parameters 5.3 run() Method \u00b6 results = leaderboard . run ( rank_by : str = 'accuracy' , verbose : bool = True , timeout : float = None ) Parameters : Parameter Type Default Description rank_by str 'accuracy' Metric to rank models verbose bool True Print progress timeout float None Timeout per model in seconds Return : LeaderboardResults object with all benchmarks 6. Advanced Usage \u00b6 6.1 Custom Model Names \u00b6 # Add with meaningful names leaderboard . add_model ( 'TabICL' , 'peft' , name = 'TabICL-PEFT-r8' , tuning_params = { 'peft_config' : { 'r' : 8 }} ) leaderboard . add_model ( 'TabICL' , 'peft' , name = 'TabICL-PEFT-r16' , tuning_params = { 'peft_config' : { 'r' : 16 }} ) results = leaderboard . run ( rank_by = 'accuracy' ) 6.2 Hyperparameter Grid \u00b6 Test multiple hyperparameter combinations: from itertools import product # Define hyperparameter grid learning_rates = [ 1e-5 , 2e-5 , 5e-5 ] ranks = [ 4 , 8 , 16 ] # Grid search for lr , r in product ( learning_rates , ranks ): leaderboard . add_model ( 'TabICL' , 'peft' , name = f 'TabICL-lr { lr } -r { r } ' , tuning_params = { 'learning_rate' : lr , 'peft_config' : { 'r' : r , 'lora_alpha' : 2 * r } } ) results = leaderboard . run ( rank_by = 'f1_score' , verbose = True ) print ( leaderboard . get_ranking ()) 6.3 Strategy Comparison \u00b6 Compare all three strategies for a model: model = 'TabDPT' # Inference baseline leaderboard . add_model ( model , 'inference' , name = f ' { model } -Inference' ) # Base fine-tuning leaderboard . add_model ( model , 'base-ft' , name = f ' { model } -BaseFT' , tuning_params = { 'epochs' : 5 } ) # PEFT fine-tuning leaderboard . add_model ( model , 'peft' , name = f ' { model } -PEFT' , tuning_params = { 'epochs' : 5 , 'peft_config' : { 'r' : 8 }} ) results = leaderboard . run () # Extract strategy comparison for name , score in results . items (): strategy = name . split ( '-' )[ - 1 ] print ( f \" { strategy } : { score [ 'accuracy' ] : .4f } \" ) 6.4 Cross-Validation \u00b6 Test stability across multiple folds: from sklearn.model_selection import KFold kf = KFold ( n_splits = 5 ) fold_results = [] for fold_idx , ( train_idx , test_idx ) in enumerate ( kf . split ( X )): X_train_fold , X_test_fold = X . iloc [ train_idx ], X . iloc [ test_idx ] y_train_fold , y_test_fold = y . iloc [ train_idx ], y . iloc [ test_idx ] # Create leaderboard for this fold lb = TabularLeaderboard ( X_train_fold , X_test_fold , y_train_fold , y_test_fold ) # Add models lb . add_model ( 'TabICL' , 'peft' ) lb . add_model ( 'TabBiaxial' , 'base-ft' ) # Run and store results = lb . run () fold_results . append ( results ) # Aggregate results import pandas as pd all_results = pd . concat ( fold_results ) print ( all_results . groupby ( 'model' ) . mean ()) 7. Evaluation Metrics \u00b6 Supported metrics for ranking: Classification Metrics : - accuracy : Fraction of correct predictions - f1_score : Harmonic mean of precision and recall (weighted) - roc_auc_score : Area under ROC curve - precision_score : True positives / (TP + FP) - recall_score : True positives / (TP + FN) - balanced_accuracy : Average per-class accuracy Regression Metrics : - mse : Mean squared error (lower is better) - rmse : Root mean squared error - mae : Mean absolute error - r2_score : Coefficient of determination # Rank by different metrics results_acc = leaderboard . run ( rank_by = 'accuracy' ) results_f1 = leaderboard . run ( rank_by = 'f1_score' ) results_auc = leaderboard . run ( rank_by = 'roc_auc_score' ) 8. Best Practices \u00b6 \u2705 Do's \u00b6 \u2705 Use consistent data splits across all models \u2705 Fix random seed for reproducibility \u2705 Include inference baseline for comparison \u2705 Test multiple strategies for each model \u2705 Save results to disk \u2705 Use reasonable timeout values \u2705 Export results as CSV for further analysis \u274c Don'ts \u00b6 \u274c Don't change data between runs \u274c Don't use training data for validation \u274c Don't tune hyperparameters on test set \u274c Don't mix different task types in one leaderboard \u274c Don't run without timeout protection \u274c Don't forget to save best model config 9. Complete Workflow Example \u00b6 from tabtune import TabularLeaderboard import pandas as pd from sklearn.model_selection import train_test_split # Step 1: Load and prepare data print ( \"Loading data...\" ) df = pd . read_csv ( 'data.csv' ) X = df . drop ( 'target' , axis = 1 ) y = df [ 'target' ] X_train , X_test , y_train , y_test = train_test_split ( X , y , test_size = 0.2 , random_state = 42 ) # Step 2: Initialize leaderboard print ( \"Initializing leaderboard...\" ) lb = TabularLeaderboard ( X_train , X_test , y_train , y_test ) # Step 3: Add baseline print ( \"Adding inference baseline...\" ) lb . add_model ( 'TabPFN' , 'inference' ) # Step 4: Add PEFT models print ( \"Adding PEFT models...\" ) for model in [ 'TabICL' , 'TabBiaxial' , 'TabDPT' ]: lb . add_model ( model , 'peft' , tuning_params = { 'epochs' : 3 , 'peft_config' : { 'r' : 8 }} ) # Step 5: Run benchmarks print ( \"Running benchmarks...\" ) results = lb . run ( rank_by = 'accuracy' , verbose = True , n_jobs =- 1 ) 10. Next Steps \u00b6 Model Selection - Guide for choosing models Tuning Strategies - Deep dive into strategies Examples - More examples The TabularLeaderboard streamlines model selection by enabling systematic, reproducible benchmarking!","title":"Model Comparison"},{"location":"user-guide/leaderboard/#model-comparison-with-tabularleaderboard","text":"The TabularLeaderboard is a powerful benchmarking tool that enables systematic comparison of multiple models and tuning strategies on your dataset. This guide shows how to use it effectively.","title":"Model Comparison with TabularLeaderboard"},{"location":"user-guide/leaderboard/#1-overview","text":"TabularLeaderboard simplifies the process of comparing different TabTune models and strategies: \u2705 Compare multiple models simultaneously \u2705 Test different tuning strategies (inference, base-ft, peft) \u2705 Rank results by any evaluation metric \u2705 Export results for analysis \u2705 Track experiment metadata \u2705 Visualize performance comparisons","title":"1. Overview"},{"location":"user-guide/leaderboard/#2-core-concepts","text":"","title":"2. Core Concepts"},{"location":"user-guide/leaderboard/#21-workflow","text":"flowchart LR A[Initialize Leaderboard] --> B[Add Models] B --> C[Configure Strategies] C --> D[Run Benchmarks] D --> E[Evaluate & Rank] E --> F[Export Results]","title":"2.1 Workflow"},{"location":"user-guide/leaderboard/#22-key-components","text":"Leaderboard : Container for managing multiple model configurations Model Entry : Single model + strategy + hyperparameter combination Benchmark : Complete evaluation run across all added models Results : Ranked comparison with metrics and metadata","title":"2.2 Key Components"},{"location":"user-guide/leaderboard/#3-basic-usage","text":"","title":"3. Basic Usage"},{"location":"user-guide/leaderboard/#31-initialize-leaderboard","text":"from tabtune import TabularLeaderboard from sklearn.model_selection import train_test_split # Prepare data splits X , y = load_your_data () X_train , X_test , y_train , y_test = train_test_split ( X , y , test_size = 0.2 , random_state = 42 ) # Initialize leaderboard leaderboard = TabularLeaderboard ( X_train = X_train , X_test = X_test , y_train = y_train , y_test = y_test , task_type = 'classification' )","title":"3.1 Initialize Leaderboard"},{"location":"user-guide/leaderboard/#32-add-models-to-leaderboard","text":"# Add single model with default settings leaderboard . add_model ( model_name = 'TabPFN' , tuning_strategy = 'inference' ) # Add model with custom tuning parameters leaderboard . add_model ( model_name = 'TabICL' , tuning_strategy = 'base-ft' , model_params = { 'n_estimators' : 16 }, tuning_params = { 'epochs' : 5 , 'learning_rate' : 2e-5 } ) # Add same model with different strategy leaderboard . add_model ( model_name = 'TabICL' , tuning_strategy = 'peft' , model_params = { 'n_estimators' : 16 }, tuning_params = { 'epochs' : 5 , 'learning_rate' : 2e-4 , 'peft_config' : { 'r' : 8 , 'lora_alpha' : 16 } } )","title":"3.2 Add Models to Leaderboard"},{"location":"user-guide/leaderboard/#33-run-benchmarks","text":"# Run all added models results = leaderboard . run () # Run with custom settings results = leaderboard . run ( rank_by = 'roc_auc_score' , # Metric to rank by verbose = True , # Print progress n_jobs = 1 # Parallel jobs (1 = sequential) )","title":"3.3 Run Benchmarks"},{"location":"user-guide/leaderboard/#4-complete-example","text":"","title":"4. Complete Example"},{"location":"user-guide/leaderboard/#41-full-comparison-workflow","text":"from tabtune import TabularLeaderboard import pandas as pd from sklearn.model_selection import train_test_split # 1. Load data df = pd . read_csv ( 'dataset.csv' ) X = df . drop ( 'target' , axis = 1 ) y = df [ 'target' ] # 2. Split data X_train , X_test , y_train , y_test = train_test_split ( X , y , test_size = 0.2 ) # 3. Initialize leaderboard leaderboard = TabularLeaderboard ( X_train , X_test , y_train , y_test ) # 4. Add models - Inference Baseline leaderboard . add_model ( 'TabPFN' , 'inference' , name = 'TabPFN-Inference' ) # 5. Add models - PEFT Strategies for model in [ 'TabICL' , 'TabBiaxial' , 'Mitra' ]: leaderboard . add_model ( model , 'peft' , name = f ' { model } -PEFT' , tuning_params = { 'epochs' : 3 , 'peft_config' : { 'r' : 8 , 'lora_alpha' : 16 } } ) # 6. Add models - Base Fine-Tuning (for comparison) leaderboard . add_model ( 'TabICL' , 'base-ft' , name = 'TabICL-BaseFT' , tuning_params = { 'epochs' : 5 , 'learning_rate' : 2e-5 } ) # 7. Run benchmarks results = leaderboard . run ( rank_by = 'accuracy' , verbose = True )","title":"4.1 Full Comparison Workflow"},{"location":"user-guide/leaderboard/#5-class-reference","text":"","title":"5. Class Reference"},{"location":"user-guide/leaderboard/#51-tabularleaderboard-constructor","text":"TabularLeaderboard ( X_train : pd . DataFrame , X_test : pd . DataFrame , y_train : pd . Series , y_test : pd . Series , task_type : str = 'classification' , validation_split : float = 0.1 , random_state : int = 42 ) Parameters : Parameter Type Default Description X_train DataFrame Required Training features X_test DataFrame Required Test features y_train Series Required Training labels y_test Series Required Test labels task_type str 'classification' 'classification' or 'regression' validation_split float 0.1 Fraction for validation random_state int 42 Random seed for reproducibility","title":"5.1 TabularLeaderboard Constructor"},{"location":"user-guide/leaderboard/#52-add_model-method","text":"leaderboard . add_model ( model_name : str , tuning_strategy : str , name : str = None , model_params : dict = None , tuning_params : dict = None , processor_params : dict = None ) Parameters : Parameter Type Default Description model_name str Required Model to add (TabPFN, TabICL, etc.) tuning_strategy str Required 'inference', 'base-ft', or 'peft' name str None Custom name for leaderboard (auto-generated if None) model_params dict None Model-specific hyperparameters tuning_params dict None Training hyperparameters processor_params dict None Preprocessing parameters","title":"5.2 add_model() Method"},{"location":"user-guide/leaderboard/#53-run-method","text":"results = leaderboard . run ( rank_by : str = 'accuracy' , verbose : bool = True , timeout : float = None ) Parameters : Parameter Type Default Description rank_by str 'accuracy' Metric to rank models verbose bool True Print progress timeout float None Timeout per model in seconds Return : LeaderboardResults object with all benchmarks","title":"5.3 run() Method"},{"location":"user-guide/leaderboard/#6-advanced-usage","text":"","title":"6. Advanced Usage"},{"location":"user-guide/leaderboard/#61-custom-model-names","text":"# Add with meaningful names leaderboard . add_model ( 'TabICL' , 'peft' , name = 'TabICL-PEFT-r8' , tuning_params = { 'peft_config' : { 'r' : 8 }} ) leaderboard . add_model ( 'TabICL' , 'peft' , name = 'TabICL-PEFT-r16' , tuning_params = { 'peft_config' : { 'r' : 16 }} ) results = leaderboard . run ( rank_by = 'accuracy' )","title":"6.1 Custom Model Names"},{"location":"user-guide/leaderboard/#62-hyperparameter-grid","text":"Test multiple hyperparameter combinations: from itertools import product # Define hyperparameter grid learning_rates = [ 1e-5 , 2e-5 , 5e-5 ] ranks = [ 4 , 8 , 16 ] # Grid search for lr , r in product ( learning_rates , ranks ): leaderboard . add_model ( 'TabICL' , 'peft' , name = f 'TabICL-lr { lr } -r { r } ' , tuning_params = { 'learning_rate' : lr , 'peft_config' : { 'r' : r , 'lora_alpha' : 2 * r } } ) results = leaderboard . run ( rank_by = 'f1_score' , verbose = True ) print ( leaderboard . get_ranking ())","title":"6.2 Hyperparameter Grid"},{"location":"user-guide/leaderboard/#63-strategy-comparison","text":"Compare all three strategies for a model: model = 'TabDPT' # Inference baseline leaderboard . add_model ( model , 'inference' , name = f ' { model } -Inference' ) # Base fine-tuning leaderboard . add_model ( model , 'base-ft' , name = f ' { model } -BaseFT' , tuning_params = { 'epochs' : 5 } ) # PEFT fine-tuning leaderboard . add_model ( model , 'peft' , name = f ' { model } -PEFT' , tuning_params = { 'epochs' : 5 , 'peft_config' : { 'r' : 8 }} ) results = leaderboard . run () # Extract strategy comparison for name , score in results . items (): strategy = name . split ( '-' )[ - 1 ] print ( f \" { strategy } : { score [ 'accuracy' ] : .4f } \" )","title":"6.3 Strategy Comparison"},{"location":"user-guide/leaderboard/#64-cross-validation","text":"Test stability across multiple folds: from sklearn.model_selection import KFold kf = KFold ( n_splits = 5 ) fold_results = [] for fold_idx , ( train_idx , test_idx ) in enumerate ( kf . split ( X )): X_train_fold , X_test_fold = X . iloc [ train_idx ], X . iloc [ test_idx ] y_train_fold , y_test_fold = y . iloc [ train_idx ], y . iloc [ test_idx ] # Create leaderboard for this fold lb = TabularLeaderboard ( X_train_fold , X_test_fold , y_train_fold , y_test_fold ) # Add models lb . add_model ( 'TabICL' , 'peft' ) lb . add_model ( 'TabBiaxial' , 'base-ft' ) # Run and store results = lb . run () fold_results . append ( results ) # Aggregate results import pandas as pd all_results = pd . concat ( fold_results ) print ( all_results . groupby ( 'model' ) . mean ())","title":"6.4 Cross-Validation"},{"location":"user-guide/leaderboard/#7-evaluation-metrics","text":"Supported metrics for ranking: Classification Metrics : - accuracy : Fraction of correct predictions - f1_score : Harmonic mean of precision and recall (weighted) - roc_auc_score : Area under ROC curve - precision_score : True positives / (TP + FP) - recall_score : True positives / (TP + FN) - balanced_accuracy : Average per-class accuracy Regression Metrics : - mse : Mean squared error (lower is better) - rmse : Root mean squared error - mae : Mean absolute error - r2_score : Coefficient of determination # Rank by different metrics results_acc = leaderboard . run ( rank_by = 'accuracy' ) results_f1 = leaderboard . run ( rank_by = 'f1_score' ) results_auc = leaderboard . run ( rank_by = 'roc_auc_score' )","title":"7. Evaluation Metrics"},{"location":"user-guide/leaderboard/#8-best-practices","text":"","title":"8. Best Practices"},{"location":"user-guide/leaderboard/#dos","text":"\u2705 Use consistent data splits across all models \u2705 Fix random seed for reproducibility \u2705 Include inference baseline for comparison \u2705 Test multiple strategies for each model \u2705 Save results to disk \u2705 Use reasonable timeout values \u2705 Export results as CSV for further analysis","title":"\u2705 Do's"},{"location":"user-guide/leaderboard/#donts","text":"\u274c Don't change data between runs \u274c Don't use training data for validation \u274c Don't tune hyperparameters on test set \u274c Don't mix different task types in one leaderboard \u274c Don't run without timeout protection \u274c Don't forget to save best model config","title":"\u274c Don'ts"},{"location":"user-guide/leaderboard/#9-complete-workflow-example","text":"from tabtune import TabularLeaderboard import pandas as pd from sklearn.model_selection import train_test_split # Step 1: Load and prepare data print ( \"Loading data...\" ) df = pd . read_csv ( 'data.csv' ) X = df . drop ( 'target' , axis = 1 ) y = df [ 'target' ] X_train , X_test , y_train , y_test = train_test_split ( X , y , test_size = 0.2 , random_state = 42 ) # Step 2: Initialize leaderboard print ( \"Initializing leaderboard...\" ) lb = TabularLeaderboard ( X_train , X_test , y_train , y_test ) # Step 3: Add baseline print ( \"Adding inference baseline...\" ) lb . add_model ( 'TabPFN' , 'inference' ) # Step 4: Add PEFT models print ( \"Adding PEFT models...\" ) for model in [ 'TabICL' , 'TabBiaxial' , 'TabDPT' ]: lb . add_model ( model , 'peft' , tuning_params = { 'epochs' : 3 , 'peft_config' : { 'r' : 8 }} ) # Step 5: Run benchmarks print ( \"Running benchmarks...\" ) results = lb . run ( rank_by = 'accuracy' , verbose = True , n_jobs =- 1 )","title":"9. Complete Workflow Example"},{"location":"user-guide/leaderboard/#10-next-steps","text":"Model Selection - Guide for choosing models Tuning Strategies - Deep dive into strategies Examples - More examples The TabularLeaderboard streamlines model selection by enabling systematic, reproducible benchmarking!","title":"10. Next Steps"},{"location":"user-guide/model-selection/","text":"Model Selection \u00b6 Choosing the right model for your tabular task is crucial for achieving optimal performance. This guide helps you navigate TabTune's model ecosystem and select the best model for your specific use case. 1. Model Overview \u00b6 Model Family Best For Dataset Size PEFT Support Training Speed TabPFN PFN/ICL Small datasets, quick experiments <10K rows \u26a0\ufe0f Experimental \u2b50\u2b50\u2b50\u2b50\u2b50 TabICL Scalable ICL General tabular, balanced performance 10K-1M rows \u2705 Full \u2b50\u2b50\u2b50\u2b50 TabBiaxial Scalable ICL High-accuracy scenarios 10K-1M rows \u2705 Full \u2b50\u2b50\u2b50 TabDPT Denoising Large datasets, robust features 100K-5M rows \u2705 Full \u2b50\u2b50\u2b50 Mitra 2D Attention Complex patterns, mixed types 10K-500K rows \u2705 Full \u2b50\u2b50 ContextTab Semantic ICL Text-heavy features, semantics 10K-500K rows \u26a0\ufe0f Experimental \u2b50\u2b50 2. Decision Framework \u00b6 2.1 By Dataset Size \u00b6 flowchart TD A[Dataset Size?] --> B{< 10K rows} A --> C{10K - 100K rows} A --> D{100K - 1M rows} A --> E{> 1M rows} B --> F[TabPFN] C --> G[TabICL or Mitra] D --> H[TabBiaxial or TabDPT] E --> I[TabDPT] Small (<10K rows) - Recommended : TabPFN, Mitra - Alternative : TabICL with small n_estimators Medium (10K-100K rows) - Recommended : TabICL - Alternatives : Mitra, TabBiaxial Large (100K-1M rows) - Recommended : TabBiaxial, TabDPT - Alternative : TabICL with larger n_estimators Very Large (>1M rows) - Recommended : TabDPT - Alternative : TabBiaxial with chunked training 2.2 By Feature Types \u00b6 Primarily Numerical - Best : TabDPT, TabICL - Reason : Efficient scaling and normalization pipelines Primarily Categorical - Best : TabPFN (if small), ContextTab - Reason : Specialized categorical encoding Mixed (Numerical + Categorical) - Best : TabICL, TabBiaxial, Mitra - Reason : Balanced handling of both types Text/Semantic Features - Best : ContextTab - Reason : Built-in text embedding support 2.3 By Computational Budget \u00b6 Limited Resources (<8GB GPU) - Recommended : TabPFN (inference), TabICL (PEFT) - Strategy : Use peft tuning strategy Moderate Resources (8-16GB GPU) - Recommended : TabICL, TabBiaxial - Strategy : base-ft or peft Ample Resources (>16GB GPU) - Recommended : TabDPT, Mitra, TabBiaxial - Strategy : base-ft with mixed precision 2.4 By Use Case \u00b6 Quick Prototyping - Model : TabPFN, TabICL - Strategy : inference - Reason : Zero-shot predictions, instant results Production Deployment - Model : TabBiaxial, TabDPT - Strategy : base-ft - Reason : Highest accuracy, stable performance Research/Experimentation - Model : Any with peft - Strategy : peft - Reason : Fast iteration, low cost High Accuracy Priority - Model : TabBiaxial, TabDPT - Strategy : base-ft with extensive tuning - Reason : State-of-the-art performance 3. Detailed Model Profiles \u00b6 3.1 TabPFN \u00b6 Architecture : Prior-Fitted Network with approximate Bayesian inference Strengths : - \u2b50 Extremely fast inference - \u2b50 No training required for small datasets - \u2b50 Robust to hyperparameter choices - \u2b50 Good uncertainty estimates Limitations : - \u26a0\ufe0f Limited to ~10K training samples - \u26a0\ufe0f Maximum ~100 features - \u26a0\ufe0f PEFT support experimental - \u26a0\ufe0f Binary and multi-class classification only Ideal Use Cases : - Quick baseline comparisons - Small-scale classification tasks - Kaggle competitions with small data - A/B testing with limited samples Example Configuration : from tabtune import TabularPipeline pipeline = TabularPipeline ( model_name = 'TabPFN' , tuning_strategy = 'inference' , # or 'base-ft' for adaptation ) 3.2 TabICL \u00b6 Architecture : Two-stage in-context learning (column \u2192 row attention) Strengths : - \u2b50 Balanced speed and accuracy - \u2b50 Scales to 1M+ rows - \u2b50 Full PEFT support - \u2b50 Ensemble-based robustness Limitations : - \u26a0\ufe0f Requires episodic training for fine-tuning - \u26a0\ufe0f More memory than TabPFN - \u26a0\ufe0f Slower inference with high n_estimators Ideal Use Cases : - General-purpose tabular classification - Medium to large datasets - Tasks requiring model adaptation - Ensemble predictions for robustness Example Configuration : pipeline = TabularPipeline ( model_name = 'TabICL' , tuning_strategy = 'peft' , model_params = { 'n_estimators' : 32 , 'softmax_temperature' : 0.9 }, tuning_params = { 'device' : 'cuda' , 'epochs' : 5 , 'support_size' : 48 , 'query_size' : 32 , 'n_episodes' : 1000 , 'peft_config' : { 'r' : 8 , 'lora_alpha' : 16 } } ) 3.3 TabBiaxial \u00b6 Architecture : Custom variant of TabICL with biaxial attention mechanisms Strengths : Limitations : Ideal Use Cases : - High-stakes applications (finance, healthcare) - Complex feature interactions - When accuracy > speed - Production models with tuning budget Example Configuration : pipeline = TabularPipeline ( model_name = 'TabBiaxial' , tuning_strategy = 'base-ft' , model_params = { 'n_estimators' : 32 }, tuning_params = { 'device' : 'cuda' , 'epochs' : 5 , 'support_size' : 48 , 'query_size' : 32 , 'learning_rate' : 2e-5 } ) 3.4 TabDPT \u00b6 Architecture : Denoising pre-trained transformer with k-NN context selection Strengths : - \u2b50 Scales to very large datasets (5M+ rows) - \u2b50 Robust to noisy features - \u2b50 Strong generalization - \u2b50 Full PEFT support Limitations : - \u26a0\ufe0f Requires large training sets for best performance - \u26a0\ufe0f Longer training time - \u26a0\ufe0f Memory-intensive for large context sizes Ideal Use Cases : - Large-scale production systems - Datasets with noisy/missing features - Long-term deployed models - High-accuracy requirements Example Configuration : pipeline = TabularPipeline ( model_name = 'TabDPT' , tuning_strategy = 'base-ft' , model_params = { 'n_ensembles' : 8 , 'temperature' : 0.3 , 'context_size' : 2048 }, tuning_params = { 'device' : 'cuda' , 'epochs' : 3 , 'support_size' : 1024 , 'query_size' : 256 , 'steps_per_epoch' : 15 } ) 3.5 Mitra \u00b6 Architecture : 2D cross-attention (Tab2D) with synthetic priors Strengths : - \u2b50 Excellent for mixed-type features - \u2b50 Captures row and column dependencies - \u2b50 Full PEFT support - \u2b50 Strong on structured data Limitations : - \u26a0\ufe0f Slowest training among ICL models - \u26a0\ufe0f High memory usage - \u26a0\ufe0f Requires careful hyperparameter tuning - \u26a0\ufe0f Small batch sizes needed Ideal Use Cases : - Structured databases (SQL-like tables) - Scientific datasets with meaningful columns - Time-series tabular data - Complex multi-variate relationships Example Configuration : pipeline = TabularPipeline ( model_name = 'Mitra' , tuning_strategy = 'peft' , tuning_params = { 'device' : 'cuda' , 'epochs' : 3 , 'support_size' : 128 , 'query_size' : 128 , 'steps_per_epoch' : 50 , 'batch_size' : 4 , 'peft_config' : { 'r' : 8 , 'lora_alpha' : 16 } } ) 3.6 ContextTab \u00b6 Architecture : Semantics-aware ICL with modality-specific embeddings Strengths : - \u2b50 Best for text-heavy features - \u2b50 Semantic understanding of column names - \u2b50 Handles heterogeneous data types - \u2b50 Pre-trained on diverse tabular corpora Limitations : - \u26a0\ufe0f Requires HuggingFace Hub access - \u26a0\ufe0f PEFT support experimental - \u26a0\ufe0f Slower inference due to embedding computation - \u26a0\ufe0f Limited to specific feature types Ideal Use Cases : - Datasets with free-text columns - Survey data with semantic features - Product catalogs, reviews - Mixed structured/unstructured data Example Configuration : # Requires HF_TOKEN environment variable pipeline = TabularPipeline ( model_name = 'ContextTab' , tuning_strategy = 'base-ft' , # Use base-ft, not peft tuning_params = { 'device' : 'cuda' , 'epochs' : 10 , 'learning_rate' : 1e-4 , 'batch_size' : 8 } ) 4. Model Selection Checklist \u00b6 Use this checklist to guide your decision: Dataset Characteristics - [ ] How many rows? (<10K, 10K-100K, 100K-1M, >1M) - [ ] How many features? (<50, 50-100, >100) - [ ] Feature types? (Numerical, Categorical, Mixed, Text) - [ ] Class balance? (Balanced, Imbalanced) - [ ] Missing values? (None, Few, Many) Requirements - [ ] Priority: Speed vs. Accuracy? - [ ] GPU available? (None, <8GB, 8-16GB, >16GB) - [ ] Training time budget? (Minutes, Hours, Days) - [ ] Deployment constraints? (Model size, inference latency) Recommendations Based on Checklist If dataset < 10K rows \u2192 TabPFN If dataset 10K-100K rows AND balanced types \u2192 TabICL If dataset 10K-100K rows AND high accuracy needed \u2192 TabBiaxial If dataset > 100K rows \u2192 TabDPT If text features present \u2192 ContextTab If complex patterns + mixed types \u2192 Mitra If GPU < 8GB \u2192 TabPFN or TabICL with PEFT If speed critical \u2192 TabPFN (inference) If accuracy critical \u2192 TabBiaxial or TabDPT (base-ft) 5. Hybrid Approaches \u00b6 Ensemble Multiple Models \u00b6 from tabtune import TabularLeaderboard leaderboard = TabularLeaderboard ( X_train , X_test , y_train , y_test ) # Add multiple models leaderboard . add_model ( 'TabPFN' , 'inference' ) leaderboard . add_model ( 'TabICL' , 'peft' , tuning_params = { 'epochs' : 5 }) leaderboard . add_model ( 'TabBiaxial' , 'base-ft' , tuning_params = { 'epochs' : 5 }) # Run and compare results = leaderboard . run ( rank_by = 'roc_auc_score' ) # Ensemble predictions (average probabilities) from sklearn.ensemble import VotingClassifier # Use predictions from top 3 models for ensemble 6. Advanced Selection Criteria \u00b6 6.1 Explainability Requirements \u00b6 High Explainability : TabPFN (inherent uncertainty), TabICL (attention weights) Moderate Explainability : TabDPT (feature importance) Low Explainability : Mitra, ContextTab (complex architectures) 6.2 Regulatory Compliance \u00b6 Medical/Financial : TabBiaxial, TabDPT (reproducible, auditable) General : Any model with saved checkpoints and logged hyperparameters 6.3 Transfer Learning \u00b6 Best for Transfer : TabDPT (large pre-training corpus) Moderate Transfer : TabICL, ContextTab Limited Transfer : TabPFN (task-specific priors) 7. Next Steps \u00b6 Pipeline Overview - Learn the TabularPipeline API Model Documentation - Detailed model specifications TabularLeaderboard - Compare models systematically Select your model wisely, and iterate based on your specific requirements and constraints!","title":"Model Selection"},{"location":"user-guide/model-selection/#model-selection","text":"Choosing the right model for your tabular task is crucial for achieving optimal performance. This guide helps you navigate TabTune's model ecosystem and select the best model for your specific use case.","title":"Model Selection"},{"location":"user-guide/model-selection/#1-model-overview","text":"Model Family Best For Dataset Size PEFT Support Training Speed TabPFN PFN/ICL Small datasets, quick experiments <10K rows \u26a0\ufe0f Experimental \u2b50\u2b50\u2b50\u2b50\u2b50 TabICL Scalable ICL General tabular, balanced performance 10K-1M rows \u2705 Full \u2b50\u2b50\u2b50\u2b50 TabBiaxial Scalable ICL High-accuracy scenarios 10K-1M rows \u2705 Full \u2b50\u2b50\u2b50 TabDPT Denoising Large datasets, robust features 100K-5M rows \u2705 Full \u2b50\u2b50\u2b50 Mitra 2D Attention Complex patterns, mixed types 10K-500K rows \u2705 Full \u2b50\u2b50 ContextTab Semantic ICL Text-heavy features, semantics 10K-500K rows \u26a0\ufe0f Experimental \u2b50\u2b50","title":"1. Model Overview"},{"location":"user-guide/model-selection/#2-decision-framework","text":"","title":"2. Decision Framework"},{"location":"user-guide/model-selection/#21-by-dataset-size","text":"flowchart TD A[Dataset Size?] --> B{< 10K rows} A --> C{10K - 100K rows} A --> D{100K - 1M rows} A --> E{> 1M rows} B --> F[TabPFN] C --> G[TabICL or Mitra] D --> H[TabBiaxial or TabDPT] E --> I[TabDPT] Small (<10K rows) - Recommended : TabPFN, Mitra - Alternative : TabICL with small n_estimators Medium (10K-100K rows) - Recommended : TabICL - Alternatives : Mitra, TabBiaxial Large (100K-1M rows) - Recommended : TabBiaxial, TabDPT - Alternative : TabICL with larger n_estimators Very Large (>1M rows) - Recommended : TabDPT - Alternative : TabBiaxial with chunked training","title":"2.1 By Dataset Size"},{"location":"user-guide/model-selection/#22-by-feature-types","text":"Primarily Numerical - Best : TabDPT, TabICL - Reason : Efficient scaling and normalization pipelines Primarily Categorical - Best : TabPFN (if small), ContextTab - Reason : Specialized categorical encoding Mixed (Numerical + Categorical) - Best : TabICL, TabBiaxial, Mitra - Reason : Balanced handling of both types Text/Semantic Features - Best : ContextTab - Reason : Built-in text embedding support","title":"2.2 By Feature Types"},{"location":"user-guide/model-selection/#23-by-computational-budget","text":"Limited Resources (<8GB GPU) - Recommended : TabPFN (inference), TabICL (PEFT) - Strategy : Use peft tuning strategy Moderate Resources (8-16GB GPU) - Recommended : TabICL, TabBiaxial - Strategy : base-ft or peft Ample Resources (>16GB GPU) - Recommended : TabDPT, Mitra, TabBiaxial - Strategy : base-ft with mixed precision","title":"2.3 By Computational Budget"},{"location":"user-guide/model-selection/#24-by-use-case","text":"Quick Prototyping - Model : TabPFN, TabICL - Strategy : inference - Reason : Zero-shot predictions, instant results Production Deployment - Model : TabBiaxial, TabDPT - Strategy : base-ft - Reason : Highest accuracy, stable performance Research/Experimentation - Model : Any with peft - Strategy : peft - Reason : Fast iteration, low cost High Accuracy Priority - Model : TabBiaxial, TabDPT - Strategy : base-ft with extensive tuning - Reason : State-of-the-art performance","title":"2.4 By Use Case"},{"location":"user-guide/model-selection/#3-detailed-model-profiles","text":"","title":"3. Detailed Model Profiles"},{"location":"user-guide/model-selection/#31-tabpfn","text":"Architecture : Prior-Fitted Network with approximate Bayesian inference Strengths : - \u2b50 Extremely fast inference - \u2b50 No training required for small datasets - \u2b50 Robust to hyperparameter choices - \u2b50 Good uncertainty estimates Limitations : - \u26a0\ufe0f Limited to ~10K training samples - \u26a0\ufe0f Maximum ~100 features - \u26a0\ufe0f PEFT support experimental - \u26a0\ufe0f Binary and multi-class classification only Ideal Use Cases : - Quick baseline comparisons - Small-scale classification tasks - Kaggle competitions with small data - A/B testing with limited samples Example Configuration : from tabtune import TabularPipeline pipeline = TabularPipeline ( model_name = 'TabPFN' , tuning_strategy = 'inference' , # or 'base-ft' for adaptation )","title":"3.1 TabPFN"},{"location":"user-guide/model-selection/#32-tabicl","text":"Architecture : Two-stage in-context learning (column \u2192 row attention) Strengths : - \u2b50 Balanced speed and accuracy - \u2b50 Scales to 1M+ rows - \u2b50 Full PEFT support - \u2b50 Ensemble-based robustness Limitations : - \u26a0\ufe0f Requires episodic training for fine-tuning - \u26a0\ufe0f More memory than TabPFN - \u26a0\ufe0f Slower inference with high n_estimators Ideal Use Cases : - General-purpose tabular classification - Medium to large datasets - Tasks requiring model adaptation - Ensemble predictions for robustness Example Configuration : pipeline = TabularPipeline ( model_name = 'TabICL' , tuning_strategy = 'peft' , model_params = { 'n_estimators' : 32 , 'softmax_temperature' : 0.9 }, tuning_params = { 'device' : 'cuda' , 'epochs' : 5 , 'support_size' : 48 , 'query_size' : 32 , 'n_episodes' : 1000 , 'peft_config' : { 'r' : 8 , 'lora_alpha' : 16 } } )","title":"3.2 TabICL"},{"location":"user-guide/model-selection/#33-tabbiaxial","text":"Architecture : Custom variant of TabICL with biaxial attention mechanisms Strengths : Limitations : Ideal Use Cases : - High-stakes applications (finance, healthcare) - Complex feature interactions - When accuracy > speed - Production models with tuning budget Example Configuration : pipeline = TabularPipeline ( model_name = 'TabBiaxial' , tuning_strategy = 'base-ft' , model_params = { 'n_estimators' : 32 }, tuning_params = { 'device' : 'cuda' , 'epochs' : 5 , 'support_size' : 48 , 'query_size' : 32 , 'learning_rate' : 2e-5 } )","title":"3.3 TabBiaxial"},{"location":"user-guide/model-selection/#34-tabdpt","text":"Architecture : Denoising pre-trained transformer with k-NN context selection Strengths : - \u2b50 Scales to very large datasets (5M+ rows) - \u2b50 Robust to noisy features - \u2b50 Strong generalization - \u2b50 Full PEFT support Limitations : - \u26a0\ufe0f Requires large training sets for best performance - \u26a0\ufe0f Longer training time - \u26a0\ufe0f Memory-intensive for large context sizes Ideal Use Cases : - Large-scale production systems - Datasets with noisy/missing features - Long-term deployed models - High-accuracy requirements Example Configuration : pipeline = TabularPipeline ( model_name = 'TabDPT' , tuning_strategy = 'base-ft' , model_params = { 'n_ensembles' : 8 , 'temperature' : 0.3 , 'context_size' : 2048 }, tuning_params = { 'device' : 'cuda' , 'epochs' : 3 , 'support_size' : 1024 , 'query_size' : 256 , 'steps_per_epoch' : 15 } )","title":"3.4 TabDPT"},{"location":"user-guide/model-selection/#35-mitra","text":"Architecture : 2D cross-attention (Tab2D) with synthetic priors Strengths : - \u2b50 Excellent for mixed-type features - \u2b50 Captures row and column dependencies - \u2b50 Full PEFT support - \u2b50 Strong on structured data Limitations : - \u26a0\ufe0f Slowest training among ICL models - \u26a0\ufe0f High memory usage - \u26a0\ufe0f Requires careful hyperparameter tuning - \u26a0\ufe0f Small batch sizes needed Ideal Use Cases : - Structured databases (SQL-like tables) - Scientific datasets with meaningful columns - Time-series tabular data - Complex multi-variate relationships Example Configuration : pipeline = TabularPipeline ( model_name = 'Mitra' , tuning_strategy = 'peft' , tuning_params = { 'device' : 'cuda' , 'epochs' : 3 , 'support_size' : 128 , 'query_size' : 128 , 'steps_per_epoch' : 50 , 'batch_size' : 4 , 'peft_config' : { 'r' : 8 , 'lora_alpha' : 16 } } )","title":"3.5 Mitra"},{"location":"user-guide/model-selection/#36-contexttab","text":"Architecture : Semantics-aware ICL with modality-specific embeddings Strengths : - \u2b50 Best for text-heavy features - \u2b50 Semantic understanding of column names - \u2b50 Handles heterogeneous data types - \u2b50 Pre-trained on diverse tabular corpora Limitations : - \u26a0\ufe0f Requires HuggingFace Hub access - \u26a0\ufe0f PEFT support experimental - \u26a0\ufe0f Slower inference due to embedding computation - \u26a0\ufe0f Limited to specific feature types Ideal Use Cases : - Datasets with free-text columns - Survey data with semantic features - Product catalogs, reviews - Mixed structured/unstructured data Example Configuration : # Requires HF_TOKEN environment variable pipeline = TabularPipeline ( model_name = 'ContextTab' , tuning_strategy = 'base-ft' , # Use base-ft, not peft tuning_params = { 'device' : 'cuda' , 'epochs' : 10 , 'learning_rate' : 1e-4 , 'batch_size' : 8 } )","title":"3.6 ContextTab"},{"location":"user-guide/model-selection/#4-model-selection-checklist","text":"Use this checklist to guide your decision: Dataset Characteristics - [ ] How many rows? (<10K, 10K-100K, 100K-1M, >1M) - [ ] How many features? (<50, 50-100, >100) - [ ] Feature types? (Numerical, Categorical, Mixed, Text) - [ ] Class balance? (Balanced, Imbalanced) - [ ] Missing values? (None, Few, Many) Requirements - [ ] Priority: Speed vs. Accuracy? - [ ] GPU available? (None, <8GB, 8-16GB, >16GB) - [ ] Training time budget? (Minutes, Hours, Days) - [ ] Deployment constraints? (Model size, inference latency) Recommendations Based on Checklist If dataset < 10K rows \u2192 TabPFN If dataset 10K-100K rows AND balanced types \u2192 TabICL If dataset 10K-100K rows AND high accuracy needed \u2192 TabBiaxial If dataset > 100K rows \u2192 TabDPT If text features present \u2192 ContextTab If complex patterns + mixed types \u2192 Mitra If GPU < 8GB \u2192 TabPFN or TabICL with PEFT If speed critical \u2192 TabPFN (inference) If accuracy critical \u2192 TabBiaxial or TabDPT (base-ft)","title":"4. Model Selection Checklist"},{"location":"user-guide/model-selection/#5-hybrid-approaches","text":"","title":"5. Hybrid Approaches"},{"location":"user-guide/model-selection/#ensemble-multiple-models","text":"from tabtune import TabularLeaderboard leaderboard = TabularLeaderboard ( X_train , X_test , y_train , y_test ) # Add multiple models leaderboard . add_model ( 'TabPFN' , 'inference' ) leaderboard . add_model ( 'TabICL' , 'peft' , tuning_params = { 'epochs' : 5 }) leaderboard . add_model ( 'TabBiaxial' , 'base-ft' , tuning_params = { 'epochs' : 5 }) # Run and compare results = leaderboard . run ( rank_by = 'roc_auc_score' ) # Ensemble predictions (average probabilities) from sklearn.ensemble import VotingClassifier # Use predictions from top 3 models for ensemble","title":"Ensemble Multiple Models"},{"location":"user-guide/model-selection/#6-advanced-selection-criteria","text":"","title":"6. Advanced Selection Criteria"},{"location":"user-guide/model-selection/#61-explainability-requirements","text":"High Explainability : TabPFN (inherent uncertainty), TabICL (attention weights) Moderate Explainability : TabDPT (feature importance) Low Explainability : Mitra, ContextTab (complex architectures)","title":"6.1 Explainability Requirements"},{"location":"user-guide/model-selection/#62-regulatory-compliance","text":"Medical/Financial : TabBiaxial, TabDPT (reproducible, auditable) General : Any model with saved checkpoints and logged hyperparameters","title":"6.2 Regulatory Compliance"},{"location":"user-guide/model-selection/#63-transfer-learning","text":"Best for Transfer : TabDPT (large pre-training corpus) Moderate Transfer : TabICL, ContextTab Limited Transfer : TabPFN (task-specific priors)","title":"6.3 Transfer Learning"},{"location":"user-guide/model-selection/#7-next-steps","text":"Pipeline Overview - Learn the TabularPipeline API Model Documentation - Detailed model specifications TabularLeaderboard - Compare models systematically Select your model wisely, and iterate based on your specific requirements and constraints!","title":"7. Next Steps"},{"location":"user-guide/pipeline-overview/","text":"TabularPipeline Overview \u00b6 This document provides an in-depth look at the TabularPipeline class\u2014the central entry point for using TabTune\u2019s end-to-end workflows. 1. Architecture Diagram \u00b6 flowchart LR A[Raw Data: Pandas DataFrame] --> B[DataProcessor] B --> C[Transformed Data: Dataset / DataLoader] C --> D[TuningManager] D --> E[Model Training / Inference] E --> F[Predictions / Metrics] DataProcessor : Prepares input features and labels for models. TuningManager : Executes the specified strategy ( inference , base-ft , peft ). Model : Underlying tabular foundation model (e.g., TabPFN, TabICL). Output : Predictions and evaluation metrics. 2. Class Signature \u00b6 class TabularPipeline : def __init__ ( self , model_name : str , task_type : str = 'classification' , tuning_strategy : str = 'inference' , tuning_params : dict | None = None , processor_params : dict | None = None , model_params : dict | None = None , model_checkpoint_path : str | None = None ) model_name : Name of the model to instantiate (e.g., TabPFN , TabICL ). task_type : Either classification or regression . tuning_strategy : One of inference , base-ft , or peft . tuning_params : Hyperparameters for training/inference (device, epochs, learning_rate, etc.). processor_params : Options for data preprocessing (imputation, scaling, encoding). model_params : Direct parameters for the model constructor. model_checkpoint_path : Path to load pre-trained weights. 3. Core Methods \u00b6 3.1 .fit(X, y) \u00b6 pipeline . fit ( X_train : DataFrame , y_train : Series ) -> None - Description : Runs preprocessing and training. - Workflow : 1. Fit DataProcessor on X_train, y_train . 2. Create PyTorch Dataset and DataLoader . 3. Execute TuningManager training loop if strategy != inference . 3.2 .predict(X) \u00b6 predictions = pipeline . predict ( X_test : DataFrame ) -> np . ndarray - Description : Preprocesses X_test and returns model predictions. - Notes : In inference mode, loads pre-trained model and runs forward passes. 3.3 .evaluate(X, y) \u00b6 metrics = pipeline . evaluate ( X_test : DataFrame , y_test : Series ) -> dict - Description : Calls .predict() , then computes specified metrics. - Default Metrics : Accuracy, Weighted F1, ROC AUC. 3.4 .save(path) \u00b6 pipeline . save ( 'pipeline.joblib' ) - Description : Serializes the pipeline including processor, model state, and config. 3.5 .load(path) \u00b6 loaded = TabularPipeline . load ( 'pipeline.joblib' ) - Description : Loads a saved pipeline for inference or continued training. 4. Configuration Best Practices \u00b6 Use config files : YAML/JSON to record experiments reproducibly. Version control : Commit tuning_params and processor_params along with code. Checkpointing : Save intermediate checkpoints via model_checkpoint_path for long runs. 5. Example Usage \u00b6 from tabtune import TabularPipeline # Setup pipeline pipeline = TabularPipeline ( model_name = 'TabICL' , tuning_strategy = 'peft' , tuning_params = { 'device' : 'cuda' , 'epochs' : 5 , 'learning_rate' : 2e-4 }, processor_params = { 'imputation_strategy' : 'median' }, model_params = { 'n_estimators' : 16 } ) # Train pipeline . fit ( X_train , y_train ) # Predict & evaluate metrics = pipeline . evaluate ( X_test , y_test ) print ( metrics ) 6. Error Handling \u00b6 Invalid model_name : Raises ValueError with supported model list. Unsupported strategy : Raises ValueError advising available strategies. Missing data : Raises DataValidationError if nulls present after imputation. 7. Extending TabularPipeline \u00b6 Custom pipeline : Subclass TabularPipeline and override _setup_processors or _setup_manager . Custom metrics : Pass metrics=[your_metric_fn] to .evaluate() . Callbacks : Integrate PyTorch Lightning callbacks via tuning_params . This overview should help you understand and utilize TabularPipeline for all your tabular modeling needs.","title":"TabularPipeline Overview"},{"location":"user-guide/pipeline-overview/#tabularpipeline-overview","text":"This document provides an in-depth look at the TabularPipeline class\u2014the central entry point for using TabTune\u2019s end-to-end workflows.","title":"TabularPipeline Overview"},{"location":"user-guide/pipeline-overview/#1-architecture-diagram","text":"flowchart LR A[Raw Data: Pandas DataFrame] --> B[DataProcessor] B --> C[Transformed Data: Dataset / DataLoader] C --> D[TuningManager] D --> E[Model Training / Inference] E --> F[Predictions / Metrics] DataProcessor : Prepares input features and labels for models. TuningManager : Executes the specified strategy ( inference , base-ft , peft ). Model : Underlying tabular foundation model (e.g., TabPFN, TabICL). Output : Predictions and evaluation metrics.","title":"1. Architecture Diagram"},{"location":"user-guide/pipeline-overview/#2-class-signature","text":"class TabularPipeline : def __init__ ( self , model_name : str , task_type : str = 'classification' , tuning_strategy : str = 'inference' , tuning_params : dict | None = None , processor_params : dict | None = None , model_params : dict | None = None , model_checkpoint_path : str | None = None ) model_name : Name of the model to instantiate (e.g., TabPFN , TabICL ). task_type : Either classification or regression . tuning_strategy : One of inference , base-ft , or peft . tuning_params : Hyperparameters for training/inference (device, epochs, learning_rate, etc.). processor_params : Options for data preprocessing (imputation, scaling, encoding). model_params : Direct parameters for the model constructor. model_checkpoint_path : Path to load pre-trained weights.","title":"2. Class Signature"},{"location":"user-guide/pipeline-overview/#3-core-methods","text":"","title":"3. Core Methods"},{"location":"user-guide/pipeline-overview/#31-fitx-y","text":"pipeline . fit ( X_train : DataFrame , y_train : Series ) -> None - Description : Runs preprocessing and training. - Workflow : 1. Fit DataProcessor on X_train, y_train . 2. Create PyTorch Dataset and DataLoader . 3. Execute TuningManager training loop if strategy != inference .","title":"3.1 .fit(X, y)"},{"location":"user-guide/pipeline-overview/#32-predictx","text":"predictions = pipeline . predict ( X_test : DataFrame ) -> np . ndarray - Description : Preprocesses X_test and returns model predictions. - Notes : In inference mode, loads pre-trained model and runs forward passes.","title":"3.2 .predict(X)"},{"location":"user-guide/pipeline-overview/#33-evaluatex-y","text":"metrics = pipeline . evaluate ( X_test : DataFrame , y_test : Series ) -> dict - Description : Calls .predict() , then computes specified metrics. - Default Metrics : Accuracy, Weighted F1, ROC AUC.","title":"3.3 .evaluate(X, y)"},{"location":"user-guide/pipeline-overview/#34-savepath","text":"pipeline . save ( 'pipeline.joblib' ) - Description : Serializes the pipeline including processor, model state, and config.","title":"3.4 .save(path)"},{"location":"user-guide/pipeline-overview/#35-loadpath","text":"loaded = TabularPipeline . load ( 'pipeline.joblib' ) - Description : Loads a saved pipeline for inference or continued training.","title":"3.5 .load(path)"},{"location":"user-guide/pipeline-overview/#4-configuration-best-practices","text":"Use config files : YAML/JSON to record experiments reproducibly. Version control : Commit tuning_params and processor_params along with code. Checkpointing : Save intermediate checkpoints via model_checkpoint_path for long runs.","title":"4. Configuration Best Practices"},{"location":"user-guide/pipeline-overview/#5-example-usage","text":"from tabtune import TabularPipeline # Setup pipeline pipeline = TabularPipeline ( model_name = 'TabICL' , tuning_strategy = 'peft' , tuning_params = { 'device' : 'cuda' , 'epochs' : 5 , 'learning_rate' : 2e-4 }, processor_params = { 'imputation_strategy' : 'median' }, model_params = { 'n_estimators' : 16 } ) # Train pipeline . fit ( X_train , y_train ) # Predict & evaluate metrics = pipeline . evaluate ( X_test , y_test ) print ( metrics )","title":"5. Example Usage"},{"location":"user-guide/pipeline-overview/#6-error-handling","text":"Invalid model_name : Raises ValueError with supported model list. Unsupported strategy : Raises ValueError advising available strategies. Missing data : Raises DataValidationError if nulls present after imputation.","title":"6. Error Handling"},{"location":"user-guide/pipeline-overview/#7-extending-tabularpipeline","text":"Custom pipeline : Subclass TabularPipeline and override _setup_processors or _setup_manager . Custom metrics : Pass metrics=[your_metric_fn] to .evaluate() . Callbacks : Integrate PyTorch Lightning callbacks via tuning_params . This overview should help you understand and utilize TabularPipeline for all your tabular modeling needs.","title":"7. Extending TabularPipeline"},{"location":"user-guide/saving-loading/","text":"Saving and Loading Pipelines \u00b6 This guide explains how to persist TabTune pipelines for production deployment, reproducibility, and continued training. 1. Overview \u00b6 TabTune supports multiple serialization formats for different use cases: Format Extension Use Case Size Compatibility joblib .joblib Production, complete pipeline ~100-500MB Python only PyTorch .pt Model weights only ~50-200MB PyTorch ecosystems Checkpoint .pt Training resume, intermediate states ~50-200MB Development 2. Pipeline Serialization (joblib) \u00b6 2.1 Saving a Pipeline \u00b6 Save the complete fitted pipeline including model, preprocessor, and configuration: from tabtune import TabularPipeline # Train pipeline pipeline = TabularPipeline ( model_name = 'TabICL' , tuning_strategy = 'peft' , tuning_params = { 'device' : 'cuda' , 'epochs' : 5 } ) pipeline . fit ( X_train , y_train ) # Save entire pipeline pipeline . save ( 'my_pipeline.joblib' ) What Gets Saved : - \u2705 Trained model weights - \u2705 DataProcessor state (imputer, encoder, scaler) - \u2705 Model configuration - \u2705 Tuning parameters - \u2705 Label encoder for target variable - \u2705 PEFT adapter weights (if applicable) 2.2 Loading a Pipeline \u00b6 Load a saved pipeline for inference or continued training: from tabtune import TabularPipeline # Load pipeline loaded_pipeline = TabularPipeline . load ( 'my_pipeline.joblib' ) # Make predictions immediately predictions = loaded_pipeline . predict ( X_test ) # Or continue training with new data loaded_pipeline . fit ( X_new_train , y_new_train ) 2.3 Size Optimization \u00b6 If pipeline file is too large, compress it: import joblib # Save with compression joblib . dump ( pipeline , 'my_pipeline.joblib' , compress = 3 ) # compression=0-9 # Load compressed pipeline pipeline = joblib . load ( 'my_pipeline.joblib' ) 3. Model-Only Serialization (PyTorch) \u00b6 For minimal storage or deployment, save only model weights: 3.1 Saving Model Weights \u00b6 import torch # Save model weights only torch . save ( pipeline . model . state_dict (), 'model_weights.pt' ) 3.2 Loading Model Weights \u00b6 import torch from tabtune import TabularPipeline # Create new pipeline with same config new_pipeline = TabularPipeline ( model_name = 'TabICL' , tuning_strategy = 'inference' ) # Load weights into model state_dict = torch . load ( 'model_weights.pt' ) new_pipeline . model . load_state_dict ( state_dict ) # Ready for inference predictions = new_pipeline . predict ( X_test ) Advantages : - \u2705 Smaller file size (50-200MB) - \u2705 Faster loading - \u2705 Cross-platform compatible Disadvantages : - \u274c Requires manual DataProcessor setup - \u274c Must know original configuration - \u274c No automatic version compatibility 4. Checkpoint Management \u00b6 For long training runs, save intermediate checkpoints: 4.1 Automatic Checkpointing During Training \u00b6 pipeline = TabularPipeline ( model_name = 'TabDPT' , tuning_strategy = 'base-ft' , tuning_params = { 'device' : 'cuda' , 'epochs' : 20 , 'save_checkpoint_path' : 'checkpoints/best_model.pt' , 'save_every_n_epochs' : 5 # Save every 5 epochs } ) pipeline . fit ( X_train , y_train ) # Checkpoints saved as: # checkpoints/best_model.pt (best validation score) # checkpoints/checkpoint_epoch_5.pt # checkpoints/checkpoint_epoch_10.pt # checkpoints/checkpoint_epoch_15.pt # checkpoints/checkpoint_epoch_20.pt 4.2 Best Checkpoint Tracking \u00b6 import os from pathlib import Path checkpoint_dir = Path ( 'checkpoints' ) checkpoint_dir . mkdir ( exist_ok = True ) # Find best checkpoint best_checkpoint = None best_score = 0 for checkpoint_file in checkpoint_dir . glob ( '*.pt' ): pipeline = TabularPipeline . load ( str ( checkpoint_file )) score = pipeline . evaluate ( X_val , y_val )[ 'accuracy' ] if score > best_score : best_score = score best_checkpoint = checkpoint_file print ( f \"Best checkpoint: { best_checkpoint } with score { best_score : .4f } \" ) 5. PEFT-Specific Serialization \u00b6 When using PEFT (LoRA), save and manage adapter weights: 5.1 Save with PEFT Adapters \u00b6 # Train with PEFT pipeline = TabularPipeline ( model_name = 'TabICL' , tuning_strategy = 'peft' , tuning_params = { 'peft_config' : { 'r' : 8 , 'lora_alpha' : 16 }, 'save_checkpoint_path' : 'peft_model.joblib' } ) pipeline . fit ( X_train , y_train ) # Save includes both base model and LoRA adapters pipeline . save ( 'peft_pipeline.joblib' ) 5.2 Load PEFT Pipeline \u00b6 # Load includes automatic adapter reconstruction pipeline = TabularPipeline . load ( 'peft_pipeline.joblib' ) # Adapters are already injected predictions = pipeline . predict ( X_test ) 5.3 Extract LoRA Adapters Only \u00b6 import torch # Save only adapter weights (minimal size) if hasattr ( pipeline . model , 'lora_A' ) and hasattr ( pipeline . model , 'lora_B' ): adapter_state = { 'lora_A' : pipeline . model . lora_A . state_dict (), 'lora_B' : pipeline . model . lora_B . state_dict () } torch . save ( adapter_state , 'adapters.pt' ) 6. Configuration Serialization \u00b6 Save pipeline configuration for reproducibility: 6.1 Save Configuration to YAML \u00b6 import yaml config = { 'model_name' : pipeline . model_name , 'task_type' : pipeline . task_type , 'tuning_strategy' : pipeline . tuning_strategy , 'tuning_params' : pipeline . tuning_params , 'processor_params' : pipeline . processor_params , 'model_params' : pipeline . model_params } with open ( 'pipeline_config.yaml' , 'w' ) as f : yaml . dump ( config , f ) Example Config File : model_name : TabICL task_type : classification tuning_strategy : peft tuning_params : device : cuda epochs : 5 learning_rate : 2e-4 peft_config : r : 8 lora_alpha : 16 lora_dropout : 0.05 processor_params : imputation_strategy : median categorical_encoding : onehot scaling_strategy : standard model_params : n_estimators : 16 6.2 Load Pipeline from Configuration \u00b6 import yaml from tabtune import TabularPipeline # Load config with open ( 'pipeline_config.yaml' , 'r' ) as f : config = yaml . safe_load ( f ) # Recreate pipeline pipeline = TabularPipeline ( ** config ) 7. Best Practices \u00b6 \u2705 Do's \u00b6 \u2705 Save full pipelines for production deployment \u2705 Use checkpoints for long training runs \u2705 Store configuration separately for reproducibility \u2705 Version control YAML/JSON configs \u2705 Keep multiple backups at different timestamps \u2705 Document model metadata and creation date \u2705 Test loading on different systems \u274c Don'ts \u00b6 \u274c Don't save only model weights without config \u274c Don't overwrite checkpoints without backup \u274c Don't forget to compress large pipelines \u274c Don't hardcode paths in production code \u274c Don't commit large .joblib files to git (use git-lfs) 8. Troubleshooting \u00b6 Issue: \"ModuleNotFoundError when loading pipeline\" \u00b6 Solution : Ensure TabTune is installed in the target environment pip install tabtune Issue: \"Pickle protocol version mismatch\" \u00b6 Solution : Use compatible Python and PyTorch versions # Save with older protocol for compatibility import joblib joblib . dump ( pipeline , 'compatible.joblib' , protocol = 2 ) Issue: \"CUDA error when loading on CPU\" \u00b6 Solution : Specify device when loading pipeline = TabularPipeline . load ( 'pipeline.joblib' ) pipeline . model = pipeline . model . to ( 'cpu' ) Issue: \"Pipeline file corrupted or incomplete\" \u00b6 Solution : Restore from backup restored = TabularPipeline . load ( 'backups/pipeline_backup_20250101_120000.joblib' ) 9. Summary Table \u00b6 Task Method File Size Compatibility Full pipeline backup .save() ~300MB joblib only Model weights only torch.save(state_dict) ~100MB PyTorch Configuration only YAML/JSON <1MB Any language Training checkpoint .pt ~150MB Development Production package .joblib + config ~300MB+ Python 10. Next Steps \u00b6 Advanced Topics - Optimize for deployment API Reference - Complete API documentation Examples - Real-world examples Properly saving and loading pipelines ensures reproducibility, enables production deployment, and protects your trained models!","title":"Saving and Loading"},{"location":"user-guide/saving-loading/#saving-and-loading-pipelines","text":"This guide explains how to persist TabTune pipelines for production deployment, reproducibility, and continued training.","title":"Saving and Loading Pipelines"},{"location":"user-guide/saving-loading/#1-overview","text":"TabTune supports multiple serialization formats for different use cases: Format Extension Use Case Size Compatibility joblib .joblib Production, complete pipeline ~100-500MB Python only PyTorch .pt Model weights only ~50-200MB PyTorch ecosystems Checkpoint .pt Training resume, intermediate states ~50-200MB Development","title":"1. Overview"},{"location":"user-guide/saving-loading/#2-pipeline-serialization-joblib","text":"","title":"2. Pipeline Serialization (joblib)"},{"location":"user-guide/saving-loading/#21-saving-a-pipeline","text":"Save the complete fitted pipeline including model, preprocessor, and configuration: from tabtune import TabularPipeline # Train pipeline pipeline = TabularPipeline ( model_name = 'TabICL' , tuning_strategy = 'peft' , tuning_params = { 'device' : 'cuda' , 'epochs' : 5 } ) pipeline . fit ( X_train , y_train ) # Save entire pipeline pipeline . save ( 'my_pipeline.joblib' ) What Gets Saved : - \u2705 Trained model weights - \u2705 DataProcessor state (imputer, encoder, scaler) - \u2705 Model configuration - \u2705 Tuning parameters - \u2705 Label encoder for target variable - \u2705 PEFT adapter weights (if applicable)","title":"2.1 Saving a Pipeline"},{"location":"user-guide/saving-loading/#22-loading-a-pipeline","text":"Load a saved pipeline for inference or continued training: from tabtune import TabularPipeline # Load pipeline loaded_pipeline = TabularPipeline . load ( 'my_pipeline.joblib' ) # Make predictions immediately predictions = loaded_pipeline . predict ( X_test ) # Or continue training with new data loaded_pipeline . fit ( X_new_train , y_new_train )","title":"2.2 Loading a Pipeline"},{"location":"user-guide/saving-loading/#23-size-optimization","text":"If pipeline file is too large, compress it: import joblib # Save with compression joblib . dump ( pipeline , 'my_pipeline.joblib' , compress = 3 ) # compression=0-9 # Load compressed pipeline pipeline = joblib . load ( 'my_pipeline.joblib' )","title":"2.3 Size Optimization"},{"location":"user-guide/saving-loading/#3-model-only-serialization-pytorch","text":"For minimal storage or deployment, save only model weights:","title":"3. Model-Only Serialization (PyTorch)"},{"location":"user-guide/saving-loading/#31-saving-model-weights","text":"import torch # Save model weights only torch . save ( pipeline . model . state_dict (), 'model_weights.pt' )","title":"3.1 Saving Model Weights"},{"location":"user-guide/saving-loading/#32-loading-model-weights","text":"import torch from tabtune import TabularPipeline # Create new pipeline with same config new_pipeline = TabularPipeline ( model_name = 'TabICL' , tuning_strategy = 'inference' ) # Load weights into model state_dict = torch . load ( 'model_weights.pt' ) new_pipeline . model . load_state_dict ( state_dict ) # Ready for inference predictions = new_pipeline . predict ( X_test ) Advantages : - \u2705 Smaller file size (50-200MB) - \u2705 Faster loading - \u2705 Cross-platform compatible Disadvantages : - \u274c Requires manual DataProcessor setup - \u274c Must know original configuration - \u274c No automatic version compatibility","title":"3.2 Loading Model Weights"},{"location":"user-guide/saving-loading/#4-checkpoint-management","text":"For long training runs, save intermediate checkpoints:","title":"4. Checkpoint Management"},{"location":"user-guide/saving-loading/#41-automatic-checkpointing-during-training","text":"pipeline = TabularPipeline ( model_name = 'TabDPT' , tuning_strategy = 'base-ft' , tuning_params = { 'device' : 'cuda' , 'epochs' : 20 , 'save_checkpoint_path' : 'checkpoints/best_model.pt' , 'save_every_n_epochs' : 5 # Save every 5 epochs } ) pipeline . fit ( X_train , y_train ) # Checkpoints saved as: # checkpoints/best_model.pt (best validation score) # checkpoints/checkpoint_epoch_5.pt # checkpoints/checkpoint_epoch_10.pt # checkpoints/checkpoint_epoch_15.pt # checkpoints/checkpoint_epoch_20.pt","title":"4.1 Automatic Checkpointing During Training"},{"location":"user-guide/saving-loading/#42-best-checkpoint-tracking","text":"import os from pathlib import Path checkpoint_dir = Path ( 'checkpoints' ) checkpoint_dir . mkdir ( exist_ok = True ) # Find best checkpoint best_checkpoint = None best_score = 0 for checkpoint_file in checkpoint_dir . glob ( '*.pt' ): pipeline = TabularPipeline . load ( str ( checkpoint_file )) score = pipeline . evaluate ( X_val , y_val )[ 'accuracy' ] if score > best_score : best_score = score best_checkpoint = checkpoint_file print ( f \"Best checkpoint: { best_checkpoint } with score { best_score : .4f } \" )","title":"4.2 Best Checkpoint Tracking"},{"location":"user-guide/saving-loading/#5-peft-specific-serialization","text":"When using PEFT (LoRA), save and manage adapter weights:","title":"5. PEFT-Specific Serialization"},{"location":"user-guide/saving-loading/#51-save-with-peft-adapters","text":"# Train with PEFT pipeline = TabularPipeline ( model_name = 'TabICL' , tuning_strategy = 'peft' , tuning_params = { 'peft_config' : { 'r' : 8 , 'lora_alpha' : 16 }, 'save_checkpoint_path' : 'peft_model.joblib' } ) pipeline . fit ( X_train , y_train ) # Save includes both base model and LoRA adapters pipeline . save ( 'peft_pipeline.joblib' )","title":"5.1 Save with PEFT Adapters"},{"location":"user-guide/saving-loading/#52-load-peft-pipeline","text":"# Load includes automatic adapter reconstruction pipeline = TabularPipeline . load ( 'peft_pipeline.joblib' ) # Adapters are already injected predictions = pipeline . predict ( X_test )","title":"5.2 Load PEFT Pipeline"},{"location":"user-guide/saving-loading/#53-extract-lora-adapters-only","text":"import torch # Save only adapter weights (minimal size) if hasattr ( pipeline . model , 'lora_A' ) and hasattr ( pipeline . model , 'lora_B' ): adapter_state = { 'lora_A' : pipeline . model . lora_A . state_dict (), 'lora_B' : pipeline . model . lora_B . state_dict () } torch . save ( adapter_state , 'adapters.pt' )","title":"5.3 Extract LoRA Adapters Only"},{"location":"user-guide/saving-loading/#6-configuration-serialization","text":"Save pipeline configuration for reproducibility:","title":"6. Configuration Serialization"},{"location":"user-guide/saving-loading/#61-save-configuration-to-yaml","text":"import yaml config = { 'model_name' : pipeline . model_name , 'task_type' : pipeline . task_type , 'tuning_strategy' : pipeline . tuning_strategy , 'tuning_params' : pipeline . tuning_params , 'processor_params' : pipeline . processor_params , 'model_params' : pipeline . model_params } with open ( 'pipeline_config.yaml' , 'w' ) as f : yaml . dump ( config , f ) Example Config File : model_name : TabICL task_type : classification tuning_strategy : peft tuning_params : device : cuda epochs : 5 learning_rate : 2e-4 peft_config : r : 8 lora_alpha : 16 lora_dropout : 0.05 processor_params : imputation_strategy : median categorical_encoding : onehot scaling_strategy : standard model_params : n_estimators : 16","title":"6.1 Save Configuration to YAML"},{"location":"user-guide/saving-loading/#62-load-pipeline-from-configuration","text":"import yaml from tabtune import TabularPipeline # Load config with open ( 'pipeline_config.yaml' , 'r' ) as f : config = yaml . safe_load ( f ) # Recreate pipeline pipeline = TabularPipeline ( ** config )","title":"6.2 Load Pipeline from Configuration"},{"location":"user-guide/saving-loading/#7-best-practices","text":"","title":"7. Best Practices"},{"location":"user-guide/saving-loading/#dos","text":"\u2705 Save full pipelines for production deployment \u2705 Use checkpoints for long training runs \u2705 Store configuration separately for reproducibility \u2705 Version control YAML/JSON configs \u2705 Keep multiple backups at different timestamps \u2705 Document model metadata and creation date \u2705 Test loading on different systems","title":"\u2705 Do's"},{"location":"user-guide/saving-loading/#donts","text":"\u274c Don't save only model weights without config \u274c Don't overwrite checkpoints without backup \u274c Don't forget to compress large pipelines \u274c Don't hardcode paths in production code \u274c Don't commit large .joblib files to git (use git-lfs)","title":"\u274c Don'ts"},{"location":"user-guide/saving-loading/#8-troubleshooting","text":"","title":"8. Troubleshooting"},{"location":"user-guide/saving-loading/#issue-modulenotfounderror-when-loading-pipeline","text":"Solution : Ensure TabTune is installed in the target environment pip install tabtune","title":"Issue: \"ModuleNotFoundError when loading pipeline\""},{"location":"user-guide/saving-loading/#issue-pickle-protocol-version-mismatch","text":"Solution : Use compatible Python and PyTorch versions # Save with older protocol for compatibility import joblib joblib . dump ( pipeline , 'compatible.joblib' , protocol = 2 )","title":"Issue: \"Pickle protocol version mismatch\""},{"location":"user-guide/saving-loading/#issue-cuda-error-when-loading-on-cpu","text":"Solution : Specify device when loading pipeline = TabularPipeline . load ( 'pipeline.joblib' ) pipeline . model = pipeline . model . to ( 'cpu' )","title":"Issue: \"CUDA error when loading on CPU\""},{"location":"user-guide/saving-loading/#issue-pipeline-file-corrupted-or-incomplete","text":"Solution : Restore from backup restored = TabularPipeline . load ( 'backups/pipeline_backup_20250101_120000.joblib' )","title":"Issue: \"Pipeline file corrupted or incomplete\""},{"location":"user-guide/saving-loading/#9-summary-table","text":"Task Method File Size Compatibility Full pipeline backup .save() ~300MB joblib only Model weights only torch.save(state_dict) ~100MB PyTorch Configuration only YAML/JSON <1MB Any language Training checkpoint .pt ~150MB Development Production package .joblib + config ~300MB+ Python","title":"9. Summary Table"},{"location":"user-guide/saving-loading/#10-next-steps","text":"Advanced Topics - Optimize for deployment API Reference - Complete API documentation Examples - Real-world examples Properly saving and loading pipelines ensures reproducibility, enables production deployment, and protects your trained models!","title":"10. Next Steps"},{"location":"user-guide/tuning-strategies/","text":"Tuning Strategies \u00b6 TabTune provides three distinct tuning strategies to accommodate different use cases, computational budgets, and performance requirements. This guide explains each strategy in detail, including when to use them and their tradeoffs. 1. Overview \u00b6 Strategy Training Use Case Memory Speed Accuracy inference None Baseline, zero-shot Minimal Fast Baseline base-ft Full params High accuracy, ample resources High Slow Highest peft LoRA adapters Memory-constrained, iteration Low Medium High 2. Inference Strategy \u00b6 Definition \u00b6 Zero-shot inference using pre-trained model weights without any training on your data. Use Cases \u00b6 Quick baseline comparisons Evaluating out-of-the-box model performance Time-constrained scenarios Testing data preprocessing pipeline Workflow \u00b6 flowchart LR A[Raw Data] --> B[DataProcessor] B --> C[Load Pre-trained Model] C --> D[Forward Pass Only] D --> E[Predictions] Implementation \u00b6 from tabtune import TabularPipeline # No training occurs pipeline = TabularPipeline ( model_name = 'TabPFN' , task_type = 'classification' , tuning_strategy = 'inference' , tuning_params = { 'device' : 'cuda' } ) # fit() only applies preprocessing; no model training pipeline . fit ( X_train , y_train ) # Direct prediction on test data predictions = pipeline . predict ( X_test ) metrics = pipeline . evaluate ( X_test , y_test ) Advantages \u00b6 \u2705 No training time needed \u2705 Minimal memory footprint \u2705 Immediate results \u2705 Good for baseline comparisons Disadvantages \u00b6 \u274c Generic pre-trained weights may not fit your data \u274c Typically lower accuracy than fine-tuned models \u274c Cannot adapt to task-specific patterns Performance Profile \u00b6 Training time : 0 seconds Memory usage : 2-4 GB (model + data) Inference latency : 10-50 ms per batch Example with All Models \u00b6 from tabtune import TabularPipeline models = [ 'TabPFN' , 'TabICL' , 'TabDPT' , 'Mitra' , 'ContextTab' , 'TabBiaxial' ] for model in models : pipeline = TabularPipeline ( model_name = model , tuning_strategy = 'inference' ) pipeline . fit ( X_train , y_train ) metrics = pipeline . evaluate ( X_test , y_test ) print ( f \" { model } - Accuracy: { metrics [ 'accuracy' ] : .4f } \" ) 3. Base Fine-Tuning Strategy ( base-ft ) \u00b6 Definition \u00b6 Full-parameter fine-tuning where all model weights are updated during training. Use Cases \u00b6 Maximum accuracy is priority Abundant computational resources (GPU, RAM) Large training datasets (>100K samples) Production models requiring best performance Transfer learning from related domains Workflow \u00b6 flowchart LR A[Raw Data] --> B[DataProcessor] B --> C[Load Pre-trained Model] C --> D[Update ALL Parameters] D --> E[Training Loop] E --> F[Fine-tuned Model] F --> G[Predictions] Implementation \u00b6 from tabtune import TabularPipeline pipeline = TabularPipeline ( model_name = 'TabICL' , task_type = 'classification' , tuning_strategy = 'base-ft' , tuning_params = { 'device' : 'cuda' , 'epochs' : 5 , 'learning_rate' : 2e-5 , 'batch_size' : 32 , 'show_progress' : True , 'gradient_accumulation_steps' : 2 , # optional 'mixed_precision' : 'fp16' # optional } ) # Full training occurs during fit() pipeline . fit ( X_train , y_train ) # Use fine-tuned model for predictions predictions = pipeline . predict ( X_test ) metrics = pipeline . evaluate ( X_test , y_test ) Supported Parameters \u00b6 Parameter Type Default Description device str 'cpu' 'cuda' or 'cpu' epochs int 3 Number of training epochs learning_rate float 2e-5 Optimizer learning rate batch_size int 32 Samples per batch optimizer str 'adamw' 'adamw' or 'sgd' show_progress bool True Display training progress bar Advantages \u00b6 \u2705 Highest accuracy potential \u2705 Fully adapts to task-specific patterns \u2705 Works with any dataset size \u2705 Best for production models \u2705 Supports all hyperparameter tuning Disadvantages \u00b6 \u274c High memory consumption (8-16GB+) \u274c Long training time (hours for large models) \u274c Risk of overfitting on small datasets \u274c Requires careful hyperparameter tuning \u274c GPU memory can become bottleneck Performance Profile \u00b6 Training time : 30 minutes - 2 hours (depending on dataset) Memory usage : 12-24 GB (full model + gradients + optimizer states) Inference latency : 10-50 ms per batch Training Loop Details \u00b6 The training process follows this pattern: Initialize optimizer (AdamW with weight decay) For each epoch : Shuffle training data For each batch : Forward pass through model Compute loss Backward pass (compute gradients) Clip gradients if specified Update weights Update learning rate scheduler Validate on development set (if available) Save best checkpoint based on validation metric Return fine-tuned model Example: Full Training Pipeline \u00b6 from tabtune import TabularPipeline from sklearn.model_selection import train_test_split # Load and split data X , y = load_your_dataset () X_train , X_val , y_train , y_val = train_test_split ( X , y , test_size = 0.2 ) # Configure base fine-tuning pipeline = TabularPipeline ( model_name = 'TabDPT' , task_type = 'classification' , tuning_strategy = 'base-ft' , tuning_params = { 'device' : 'cuda' , 'epochs' : 10 , 'learning_rate' : 1e-5 , 'batch_size' : 64 , 'scheduler' : 'cosine' , 'warmup_steps' : 500 , 'mixed_precision' : 'fp16' , 'show_progress' : True , 'save_checkpoint_path' : 'best_model.pt' } ) # Train on training data pipeline . fit ( X_train , y_train ) # Evaluate on validation set val_metrics = pipeline . evaluate ( X_val , y_val ) print ( f \"Validation Accuracy: { val_metrics [ 'accuracy' ] : .4f } \" ) # Save for later use pipeline . save ( 'fintuned_pipeline.joblib' ) 4. PEFT Fine-Tuning Strategy ( peft ) \u00b6 Definition \u00b6 Parameter-Efficient Fine-Tuning using LoRA (Low-Rank Adaptation) where only small adapter weights are trained while base model is frozen. How LoRA Works \u00b6 Use Cases \u00b6 Limited GPU memory (< 8 GB) Quick iteration cycles Fine-tuning multiple models simultaneously Rapid experimentation Deployment with minimal storage Workflow \u00b6 flowchart LR A[Raw Data] --> B[DataProcessor] B --> C[Load Pre-trained Model] C --> D[Inject LoRA Adapters] D --> E[Update ONLY Adapters] E --> F[Training Loop] F --> G[Model + LoRA Weights] G --> H[Predictions] Implementation \u00b6 from tabtune import TabularPipeline pipeline = TabularPipeline ( model_name = 'Mitra' , task_type = 'classification' , tuning_strategy = 'peft' , tuning_params = { 'device' : 'cuda' , 'epochs' : 5 , 'learning_rate' : 2e-4 , 'peft_config' : { 'r' : 8 , 'lora_alpha' : 16 , 'lora_dropout' : 0.05 , 'target_modules' : None # Uses model defaults if None }, 'show_progress' : True } ) # Training with LoRA adapters pipeline . fit ( X_train , y_train ) # Predictions with adapted model predictions = pipeline . predict ( X_test ) metrics = pipeline . evaluate ( X_test , y_test ) PEFT Parameters \u00b6 Parameter Type Default Description r int 8 LoRA rank (lower = more compression) lora_alpha int 16 Scaling factor for LoRA output lora_dropout float 0.05 Dropout applied to LoRA input target_modules list None Which linear layers to adapt (None = use defaults) Model-Specific LoRA Targets \u00b6 TabTune pre-configures optimal target modules per model: TabICL/TabBiaxial : col_embedder.tf_col, row_interactor, icl_predictor.tf_icl, icl_predictor.decoder TabDPT : transformer_encoder, encoder, y_encoder, head Mitra : x_embedding, layers, final_layer ContextTab : in_context_encoder, dense, output_head, embeddings TabPFN (\u26a0\ufe0f Experimental): encoder.5.layer, y_encoder.2.layer, transformer_encoder.layers, decoder_dict.standard Advantages \u00b6 \u2705 90% memory reduction vs base-ft \u2705 2-3x faster training \u2705 Only stores small adapter weights \u2705 Can run on 4GB GPUs \u2705 Fast iteration for experimentation Disadvantages \u00b6 \u274c Slightly lower accuracy than base-ft (~2-5% in practice) \u274c Not all model layers adapted (frozen backbone limits flexibility) \u274c May struggle with very different tasks \u274c Experimental support on TabPFN and ContextTab Performance Profile \u00b6 Training time : 10-30 minutes Memory usage : 3-6 GB (adapters + activations only) Inference latency : 10-50 ms per batch Model size : Original model size + 1-2% (adapters) Parameter Tuning Guidelines \u00b6 Rank Selection : r = 4 \u2192 Highest compression, faster, lower accuracy r = 8 \u2192 Good balance (default) r = 16 \u2192 More expressive, slower, higher accuracy r = 32 \u2192 Close to base-ft, but still compressed Alpha Selection : lora_alpha should typically be 2x the rank r=8 \u2192 lora_alpha=16 r=16 \u2192 lora_alpha=32 Dropout Selection : lora_dropout=0.0 \u2192 No regularization lora_dropout=0.05 \u2192 Light regularization (default) lora_dropout=0.1 \u2192 Strong regularization Example: PEFT Training with Hyperparameter Tuning \u00b6 from tabtune import TabularPipeline # Experiment with different LoRA ranks for r in [ 4 , 8 , 16 ]: pipeline = TabularPipeline ( model_name = 'TabICL' , tuning_strategy = 'peft' , tuning_params = { 'device' : 'cuda' , 'epochs' : 5 , 'learning_rate' : 2e-4 , 'peft_config' : { 'r' : r , 'lora_alpha' : 2 * r , 'lora_dropout' : 0.05 } } ) pipeline . fit ( X_train , y_train ) metrics = pipeline . evaluate ( X_test , y_test ) print ( f \"Rank { r } : Accuracy = { metrics [ 'accuracy' ] : .4f } \" ) 5. Strategy Comparison & Decision Tree \u00b6 Quick Comparison Table \u00b6 Aspect inference base-ft peft Training No Yes, all params Yes, adapters only Memory \u2b50\u2b50 \u2b50\u2b50\u2b50\u2b50\u2b50 \u2b50\u2b50\u2b50 Speed \u2b50\u2b50\u2b50\u2b50\u2b50 \u2b50 \u2b50\u2b50\u2b50 Accuracy \u2b50\u2b50\u2b50 \u2b50\u2b50\u2b50\u2b50\u2b50 \u2b50\u2b50\u2b50\u2b50 Cost Free High GPU cost Low GPU cost Production \u274c \u2705 \u2705 Decision Tree \u00b6 Start: Which strategy? \u2502 \u251c\u2500 \"I want instant results, no training\" \u2192 inference \u2502 \u2514\u2500 Best for: Baseline, quick exploration \u2502 \u251c\u2500 \"I have limited resources (<8GB GPU)\" \u2192 peft \u2502 \u2514\u2500 Best for: Rapid iteration, memory-constrained \u2502 \u2514\u2500 \"I need best accuracy, have resources\" \u2192 base-ft \u2514\u2500 Best for: Production, large datasets, high accuracy 6. Best Practices \u00b6 Start with inference to establish baseline Use PEFT for exploration when resources are limited Switch to base-ft for production models Monitor for overfitting on small datasets Save checkpoints for long training runs Use validation set to track progress Start with default hyperparameters then tune 7. Troubleshooting \u00b6 Issue: \"CUDA out of memory\" \u00b6 Solution : Reduce batch size or use PEFT strategy Issue: \"Accuracy decreasing during training\" \u00b6 Solution : Lower learning rate, reduce epochs, use regularization Issue: \"Model not improving after training\" \u00b6 Solution : Increase learning rate, use different scheduler, increase epochs Issue: \"PEFT not significantly faster\" \u00b6 Solution : Use lower rank (r=4), verify LoRA is actually applied 8. Next Steps \u00b6 PEFT & LoRA Details - Deep dive into LoRA theory Hyperparameter Tuning - Optimize model performance Model Selection - Choose right model for your task Choose the right strategy for your use case and resource constraints!","title":"Tuning Strategies"},{"location":"user-guide/tuning-strategies/#tuning-strategies","text":"TabTune provides three distinct tuning strategies to accommodate different use cases, computational budgets, and performance requirements. This guide explains each strategy in detail, including when to use them and their tradeoffs.","title":"Tuning Strategies"},{"location":"user-guide/tuning-strategies/#1-overview","text":"Strategy Training Use Case Memory Speed Accuracy inference None Baseline, zero-shot Minimal Fast Baseline base-ft Full params High accuracy, ample resources High Slow Highest peft LoRA adapters Memory-constrained, iteration Low Medium High","title":"1. Overview"},{"location":"user-guide/tuning-strategies/#2-inference-strategy","text":"","title":"2. Inference Strategy"},{"location":"user-guide/tuning-strategies/#definition","text":"Zero-shot inference using pre-trained model weights without any training on your data.","title":"Definition"},{"location":"user-guide/tuning-strategies/#use-cases","text":"Quick baseline comparisons Evaluating out-of-the-box model performance Time-constrained scenarios Testing data preprocessing pipeline","title":"Use Cases"},{"location":"user-guide/tuning-strategies/#workflow","text":"flowchart LR A[Raw Data] --> B[DataProcessor] B --> C[Load Pre-trained Model] C --> D[Forward Pass Only] D --> E[Predictions]","title":"Workflow"},{"location":"user-guide/tuning-strategies/#implementation","text":"from tabtune import TabularPipeline # No training occurs pipeline = TabularPipeline ( model_name = 'TabPFN' , task_type = 'classification' , tuning_strategy = 'inference' , tuning_params = { 'device' : 'cuda' } ) # fit() only applies preprocessing; no model training pipeline . fit ( X_train , y_train ) # Direct prediction on test data predictions = pipeline . predict ( X_test ) metrics = pipeline . evaluate ( X_test , y_test )","title":"Implementation"},{"location":"user-guide/tuning-strategies/#advantages","text":"\u2705 No training time needed \u2705 Minimal memory footprint \u2705 Immediate results \u2705 Good for baseline comparisons","title":"Advantages"},{"location":"user-guide/tuning-strategies/#disadvantages","text":"\u274c Generic pre-trained weights may not fit your data \u274c Typically lower accuracy than fine-tuned models \u274c Cannot adapt to task-specific patterns","title":"Disadvantages"},{"location":"user-guide/tuning-strategies/#performance-profile","text":"Training time : 0 seconds Memory usage : 2-4 GB (model + data) Inference latency : 10-50 ms per batch","title":"Performance Profile"},{"location":"user-guide/tuning-strategies/#example-with-all-models","text":"from tabtune import TabularPipeline models = [ 'TabPFN' , 'TabICL' , 'TabDPT' , 'Mitra' , 'ContextTab' , 'TabBiaxial' ] for model in models : pipeline = TabularPipeline ( model_name = model , tuning_strategy = 'inference' ) pipeline . fit ( X_train , y_train ) metrics = pipeline . evaluate ( X_test , y_test ) print ( f \" { model } - Accuracy: { metrics [ 'accuracy' ] : .4f } \" )","title":"Example with All Models"},{"location":"user-guide/tuning-strategies/#3-base-fine-tuning-strategy-base-ft","text":"","title":"3. Base Fine-Tuning Strategy (base-ft)"},{"location":"user-guide/tuning-strategies/#definition_1","text":"Full-parameter fine-tuning where all model weights are updated during training.","title":"Definition"},{"location":"user-guide/tuning-strategies/#use-cases_1","text":"Maximum accuracy is priority Abundant computational resources (GPU, RAM) Large training datasets (>100K samples) Production models requiring best performance Transfer learning from related domains","title":"Use Cases"},{"location":"user-guide/tuning-strategies/#workflow_1","text":"flowchart LR A[Raw Data] --> B[DataProcessor] B --> C[Load Pre-trained Model] C --> D[Update ALL Parameters] D --> E[Training Loop] E --> F[Fine-tuned Model] F --> G[Predictions]","title":"Workflow"},{"location":"user-guide/tuning-strategies/#implementation_1","text":"from tabtune import TabularPipeline pipeline = TabularPipeline ( model_name = 'TabICL' , task_type = 'classification' , tuning_strategy = 'base-ft' , tuning_params = { 'device' : 'cuda' , 'epochs' : 5 , 'learning_rate' : 2e-5 , 'batch_size' : 32 , 'show_progress' : True , 'gradient_accumulation_steps' : 2 , # optional 'mixed_precision' : 'fp16' # optional } ) # Full training occurs during fit() pipeline . fit ( X_train , y_train ) # Use fine-tuned model for predictions predictions = pipeline . predict ( X_test ) metrics = pipeline . evaluate ( X_test , y_test )","title":"Implementation"},{"location":"user-guide/tuning-strategies/#supported-parameters","text":"Parameter Type Default Description device str 'cpu' 'cuda' or 'cpu' epochs int 3 Number of training epochs learning_rate float 2e-5 Optimizer learning rate batch_size int 32 Samples per batch optimizer str 'adamw' 'adamw' or 'sgd' show_progress bool True Display training progress bar","title":"Supported Parameters"},{"location":"user-guide/tuning-strategies/#advantages_1","text":"\u2705 Highest accuracy potential \u2705 Fully adapts to task-specific patterns \u2705 Works with any dataset size \u2705 Best for production models \u2705 Supports all hyperparameter tuning","title":"Advantages"},{"location":"user-guide/tuning-strategies/#disadvantages_1","text":"\u274c High memory consumption (8-16GB+) \u274c Long training time (hours for large models) \u274c Risk of overfitting on small datasets \u274c Requires careful hyperparameter tuning \u274c GPU memory can become bottleneck","title":"Disadvantages"},{"location":"user-guide/tuning-strategies/#performance-profile_1","text":"Training time : 30 minutes - 2 hours (depending on dataset) Memory usage : 12-24 GB (full model + gradients + optimizer states) Inference latency : 10-50 ms per batch","title":"Performance Profile"},{"location":"user-guide/tuning-strategies/#training-loop-details","text":"The training process follows this pattern: Initialize optimizer (AdamW with weight decay) For each epoch : Shuffle training data For each batch : Forward pass through model Compute loss Backward pass (compute gradients) Clip gradients if specified Update weights Update learning rate scheduler Validate on development set (if available) Save best checkpoint based on validation metric Return fine-tuned model","title":"Training Loop Details"},{"location":"user-guide/tuning-strategies/#example-full-training-pipeline","text":"from tabtune import TabularPipeline from sklearn.model_selection import train_test_split # Load and split data X , y = load_your_dataset () X_train , X_val , y_train , y_val = train_test_split ( X , y , test_size = 0.2 ) # Configure base fine-tuning pipeline = TabularPipeline ( model_name = 'TabDPT' , task_type = 'classification' , tuning_strategy = 'base-ft' , tuning_params = { 'device' : 'cuda' , 'epochs' : 10 , 'learning_rate' : 1e-5 , 'batch_size' : 64 , 'scheduler' : 'cosine' , 'warmup_steps' : 500 , 'mixed_precision' : 'fp16' , 'show_progress' : True , 'save_checkpoint_path' : 'best_model.pt' } ) # Train on training data pipeline . fit ( X_train , y_train ) # Evaluate on validation set val_metrics = pipeline . evaluate ( X_val , y_val ) print ( f \"Validation Accuracy: { val_metrics [ 'accuracy' ] : .4f } \" ) # Save for later use pipeline . save ( 'fintuned_pipeline.joblib' )","title":"Example: Full Training Pipeline"},{"location":"user-guide/tuning-strategies/#4-peft-fine-tuning-strategy-peft","text":"","title":"4. PEFT Fine-Tuning Strategy (peft)"},{"location":"user-guide/tuning-strategies/#definition_2","text":"Parameter-Efficient Fine-Tuning using LoRA (Low-Rank Adaptation) where only small adapter weights are trained while base model is frozen.","title":"Definition"},{"location":"user-guide/tuning-strategies/#how-lora-works","text":"","title":"How LoRA Works"},{"location":"user-guide/tuning-strategies/#use-cases_2","text":"Limited GPU memory (< 8 GB) Quick iteration cycles Fine-tuning multiple models simultaneously Rapid experimentation Deployment with minimal storage","title":"Use Cases"},{"location":"user-guide/tuning-strategies/#workflow_2","text":"flowchart LR A[Raw Data] --> B[DataProcessor] B --> C[Load Pre-trained Model] C --> D[Inject LoRA Adapters] D --> E[Update ONLY Adapters] E --> F[Training Loop] F --> G[Model + LoRA Weights] G --> H[Predictions]","title":"Workflow"},{"location":"user-guide/tuning-strategies/#implementation_2","text":"from tabtune import TabularPipeline pipeline = TabularPipeline ( model_name = 'Mitra' , task_type = 'classification' , tuning_strategy = 'peft' , tuning_params = { 'device' : 'cuda' , 'epochs' : 5 , 'learning_rate' : 2e-4 , 'peft_config' : { 'r' : 8 , 'lora_alpha' : 16 , 'lora_dropout' : 0.05 , 'target_modules' : None # Uses model defaults if None }, 'show_progress' : True } ) # Training with LoRA adapters pipeline . fit ( X_train , y_train ) # Predictions with adapted model predictions = pipeline . predict ( X_test ) metrics = pipeline . evaluate ( X_test , y_test )","title":"Implementation"},{"location":"user-guide/tuning-strategies/#peft-parameters","text":"Parameter Type Default Description r int 8 LoRA rank (lower = more compression) lora_alpha int 16 Scaling factor for LoRA output lora_dropout float 0.05 Dropout applied to LoRA input target_modules list None Which linear layers to adapt (None = use defaults)","title":"PEFT Parameters"},{"location":"user-guide/tuning-strategies/#model-specific-lora-targets","text":"TabTune pre-configures optimal target modules per model: TabICL/TabBiaxial : col_embedder.tf_col, row_interactor, icl_predictor.tf_icl, icl_predictor.decoder TabDPT : transformer_encoder, encoder, y_encoder, head Mitra : x_embedding, layers, final_layer ContextTab : in_context_encoder, dense, output_head, embeddings TabPFN (\u26a0\ufe0f Experimental): encoder.5.layer, y_encoder.2.layer, transformer_encoder.layers, decoder_dict.standard","title":"Model-Specific LoRA Targets"},{"location":"user-guide/tuning-strategies/#advantages_2","text":"\u2705 90% memory reduction vs base-ft \u2705 2-3x faster training \u2705 Only stores small adapter weights \u2705 Can run on 4GB GPUs \u2705 Fast iteration for experimentation","title":"Advantages"},{"location":"user-guide/tuning-strategies/#disadvantages_2","text":"\u274c Slightly lower accuracy than base-ft (~2-5% in practice) \u274c Not all model layers adapted (frozen backbone limits flexibility) \u274c May struggle with very different tasks \u274c Experimental support on TabPFN and ContextTab","title":"Disadvantages"},{"location":"user-guide/tuning-strategies/#performance-profile_2","text":"Training time : 10-30 minutes Memory usage : 3-6 GB (adapters + activations only) Inference latency : 10-50 ms per batch Model size : Original model size + 1-2% (adapters)","title":"Performance Profile"},{"location":"user-guide/tuning-strategies/#parameter-tuning-guidelines","text":"Rank Selection : r = 4 \u2192 Highest compression, faster, lower accuracy r = 8 \u2192 Good balance (default) r = 16 \u2192 More expressive, slower, higher accuracy r = 32 \u2192 Close to base-ft, but still compressed Alpha Selection : lora_alpha should typically be 2x the rank r=8 \u2192 lora_alpha=16 r=16 \u2192 lora_alpha=32 Dropout Selection : lora_dropout=0.0 \u2192 No regularization lora_dropout=0.05 \u2192 Light regularization (default) lora_dropout=0.1 \u2192 Strong regularization","title":"Parameter Tuning Guidelines"},{"location":"user-guide/tuning-strategies/#example-peft-training-with-hyperparameter-tuning","text":"from tabtune import TabularPipeline # Experiment with different LoRA ranks for r in [ 4 , 8 , 16 ]: pipeline = TabularPipeline ( model_name = 'TabICL' , tuning_strategy = 'peft' , tuning_params = { 'device' : 'cuda' , 'epochs' : 5 , 'learning_rate' : 2e-4 , 'peft_config' : { 'r' : r , 'lora_alpha' : 2 * r , 'lora_dropout' : 0.05 } } ) pipeline . fit ( X_train , y_train ) metrics = pipeline . evaluate ( X_test , y_test ) print ( f \"Rank { r } : Accuracy = { metrics [ 'accuracy' ] : .4f } \" )","title":"Example: PEFT Training with Hyperparameter Tuning"},{"location":"user-guide/tuning-strategies/#5-strategy-comparison-decision-tree","text":"","title":"5. Strategy Comparison &amp; Decision Tree"},{"location":"user-guide/tuning-strategies/#quick-comparison-table","text":"Aspect inference base-ft peft Training No Yes, all params Yes, adapters only Memory \u2b50\u2b50 \u2b50\u2b50\u2b50\u2b50\u2b50 \u2b50\u2b50\u2b50 Speed \u2b50\u2b50\u2b50\u2b50\u2b50 \u2b50 \u2b50\u2b50\u2b50 Accuracy \u2b50\u2b50\u2b50 \u2b50\u2b50\u2b50\u2b50\u2b50 \u2b50\u2b50\u2b50\u2b50 Cost Free High GPU cost Low GPU cost Production \u274c \u2705 \u2705","title":"Quick Comparison Table"},{"location":"user-guide/tuning-strategies/#decision-tree","text":"Start: Which strategy? \u2502 \u251c\u2500 \"I want instant results, no training\" \u2192 inference \u2502 \u2514\u2500 Best for: Baseline, quick exploration \u2502 \u251c\u2500 \"I have limited resources (<8GB GPU)\" \u2192 peft \u2502 \u2514\u2500 Best for: Rapid iteration, memory-constrained \u2502 \u2514\u2500 \"I need best accuracy, have resources\" \u2192 base-ft \u2514\u2500 Best for: Production, large datasets, high accuracy","title":"Decision Tree"},{"location":"user-guide/tuning-strategies/#6-best-practices","text":"Start with inference to establish baseline Use PEFT for exploration when resources are limited Switch to base-ft for production models Monitor for overfitting on small datasets Save checkpoints for long training runs Use validation set to track progress Start with default hyperparameters then tune","title":"6. Best Practices"},{"location":"user-guide/tuning-strategies/#7-troubleshooting","text":"","title":"7. Troubleshooting"},{"location":"user-guide/tuning-strategies/#issue-cuda-out-of-memory","text":"Solution : Reduce batch size or use PEFT strategy","title":"Issue: \"CUDA out of memory\""},{"location":"user-guide/tuning-strategies/#issue-accuracy-decreasing-during-training","text":"Solution : Lower learning rate, reduce epochs, use regularization","title":"Issue: \"Accuracy decreasing during training\""},{"location":"user-guide/tuning-strategies/#issue-model-not-improving-after-training","text":"Solution : Increase learning rate, use different scheduler, increase epochs","title":"Issue: \"Model not improving after training\""},{"location":"user-guide/tuning-strategies/#issue-peft-not-significantly-faster","text":"Solution : Use lower rank (r=4), verify LoRA is actually applied","title":"Issue: \"PEFT not significantly faster\""},{"location":"user-guide/tuning-strategies/#8-next-steps","text":"PEFT & LoRA Details - Deep dive into LoRA theory Hyperparameter Tuning - Optimize model performance Model Selection - Choose right model for your task Choose the right strategy for your use case and resource constraints!","title":"8. Next Steps"}]}