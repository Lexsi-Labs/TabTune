{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"TabTune is a powerful and flexible Python library designed to simplify the training and fine-tuning of modern foundation models on tabular data. It provides a high-level, scikit-learn-compatible API that abstracts away the complexities of data preprocessing, model-specific training loops, and benchmarking, letting you focus on delivering results. Whether you are a practitioner aiming for production-grade pipelines or a researcher exploring advanced architectures, TabTune streamlines your workflow for tabular deep learning. \ud83d\ude80 TabTune v1.0 - First Release \u00b6 Welcome to TabTune! This initial release provides a complete, production-ready framework for tabular foundation models. Core Components: Unified API ( TabularPipeline ): Single, scikit-learn-compatible interface for all models with .fit() , .predict() , .save() , and .load() methods. Smart Data Processing ( DataProcessor ): Model-aware preprocessing that automatically handles imputation, scaling, categorical encoding, and feature transformations for each model. Flexible Tuning ( TuningManager ): Three tuning strategies\u2014zero-shot inference , supervised fine-tuning ( base-ft ) with full parameter updates, and memory-efficient peft (LoRA) adapters. Supports episodic meta-learning for ICL models. Model Comparison ( TabularLeaderboard ): Systematic benchmarking tool for comparing multiple models and strategies on your datasets. Supported Models (7 Models): TabPFN-v2: Fast Bayesian inference for small datasets (<10K rows) with experimental PEFT support. TabICL: Scalable ICL with two-stage attention, full PEFT support, ideal for 10K-1M row datasets. OrionMSP: Multi-scale prior ICL for balanced generalization on 50K-2M+ row datasets, full PEFT support. OrionBix: Biaxial interaction expert for high-accuracy scenarios (50K-2M+ rows), full PEFT support. TabDPT: Denoising transformer for very large datasets (100K-5M rows), full PEFT support. Mitra: 2D cross-attention model for complex patterns and mixed data types, full PEFT support. ContextTab: Semantics-aware ICL with text embedding integration, experimental PEFT support. Key Capabilities: \u2705 Multiple Training Paradigms: Supports supervised fine-tuning (SFT) with full parameter updates, episodic meta-learning for in-context learning models, and parameter-efficient PEFT strategies. \u2705 PEFT (LoRA) Support: Parameter-efficient fine-tuning for 5 out of 7 models (TabICL, OrionMSP, OrionBix, TabDPT, Mitra) with full support. \u2705 Meta-Learning Integration: Episodic training with support/query sets for ICL models (TabICL, OrionMSP, OrionBix, Mitra) enabling fast task adaptation. \u2705 Comprehensive Documentation: Extensive guides, API references, troubleshooting, and model-specific documentation. \u2705 Production Ready: Model serialization, reproducible training, and deployment-ready pipelines. \u2705 Extensible Architecture: Modular design for easy integration of custom processors and models. \u2b50 Core Features \u00b6 Unified API: Single interface for model training, inference, and evaluation across multiple tabular model families. Automated Preprocessing: Model-aware data processing for feature scaling, encoding, imputation, and transformation. Flexible Fine-Tuning: Choose between zero-shot inference, full fine-tuning, or memory-efficient PEFT strategies. Model Comparison: Built-in leaderboard for systematic benchmarking and strategy evaluation. Extensible Design: Modular codebase for easy integration of custom data processors and models. \ud83d\udce6 Supported Models \u00b6 Model Family / Paradigm Key Innovation PEFT Support TabPFN-v2 PFN / ICL Bayesian approximation on synthetic \u26a0\ufe0f Experimental TabICL Scalable ICL Two-stage column-row attention \u2705 Full Support OrionMSP Scalable ICL Multi-Scale Prior (MSP) ICL \u2705 Full Support Orion BIX Scalable ICL Biaxial Interaction eXpert \u2705 Full Support Mitra Scalable ICL 2D attention, synthetic priors \u2705 Full Support ContextTab Semantics-Aware ICL Modality-specific embeddings \u26a0\ufe0f Experimental TabDPT Denoising Transformer Denoising pre-trained transformer \u2705 Full Support \u2705 Full Support: Reliable LoRA integration \u26a0\ufe0f Experimental: Known issues may occur; use base-ft if unstable \u26a1 Quick Start \u00b6 import pandas as pd from sklearn.model_selection import train_test_split import openml from tabtune import TabularPipeline # Load dataset dataset = openml . datasets . get_dataset ( 42178 ) X , y , _ , _ = dataset . get_data ( target = dataset . default_target_attribute ) X_train , X_test , y_train , y_test = train_test_split ( X , y , test_size = 0.25 , random_state = 42 ) # Init and fit pipeline pipeline = TabularPipeline ( model_name = \"TabPFN\" , task_type = \"classification\" , tuning_strategy = \"base-ft\" , tuning_params = { \"device\" : \"cpu\" } ) pipeline . fit ( X_train , y_train ) # Save and load pipeline for prediction pipeline . save ( \"churn_pipeline.joblib\" ) loaded_pipeline = TabularPipeline . load ( \"churn_pipeline.joblib\" ) predictions = loaded_pipeline . predict ( X_test ) metrics = pipeline . evaluate ( X_test , y_test ) print ( metrics ) \ud83d\udcdd Why TabTune? \u00b6 No Boilerplate: Avoids repetitive code for model-specific data loading, training, and inference. Consistent Results: Automates best practices for tabular DL research and model selection. Fast Iteration: Easily compare new models with your data, using the same consistent API. Production Ready: Model and config serialization for robust deployment and reproducibility. Community-Driven: Extensible design and open contribution policy. \ud83d\udcd1 Explore the Documentation \u00b6 Getting Started : Installation, setup, and basic usage. User Guide : In-depth tutorials for each component. Supported Models : Model details and design notes. Advanced Topics : PEFT/LoRA, custom preprocessing, and more. API Reference : Complete Python API and class/method details. Examples & Benchmarks : End-to-end code notebooks. \ud83c\udfc6 Example Notebooks \u00b6 Name Task Type Colab Link TabPFN Inference Inference & Fine-Tune Open In Colab Mitra Inference/FineTune Inference & Fine-Tune Open In Colab OrionMSP/BIX Examples Inference & Fine-Tune Coming soon TabDPT Large Dataset Large-Scale Training Coming soon PEFT Fine-Tuning Guide Parameter-Efficient FT Coming soon Leaderboard Benchmarking Model Comparison Coming soon \ud83d\udcc2 Project Structure \u00b6 tabtune/ \u251c\u2500\u2500 Dataprocess/ \u251c\u2500\u2500 models/ \u251c\u2500\u2500 TabularPipeline/ \u251c\u2500\u2500 TuningManager/ \u251c\u2500\u2500 TabularLeaderboard/ \u251c\u2500\u2500 benchmarking/ \u251c\u2500\u2500 data/ \u251c\u2500\u2500 logger.py \u2514\u2500\u2500 run.py See User Guide for a full file/module breakdown. \ud83c\udfe2 Developed by Lexsi Labs \u00b6 Created by the team at Lexsi Labs , TabTune extends frontier AI research into the tabular domain. \ud83d\uddc3\ufe0f License \u00b6 This project is released under the MIT License. Please cite appropriately if used in academic or production projects. BibTeX Citation: @misc { tanna2025tabtuneunifiedlibraryinference , title = {TabTune: A Unified Library for Inference and Fine-Tuning Tabular Foundation Models} , author = {Aditya Tanna and Pratinav Seth and Mohamed Bouadi and Utsav Avaiya and Vinay Kumar Sankarapu} , year = {2025} , eprint = {2511.02802} , archivePrefix = {arXiv} , primaryClass = {cs.LG} , url = {https://arxiv.org/abs/2511.02802} , } \ud83d\udceb Join Community / Contribute \u00b6 Issues and discussions are welcomed on the GitHub issue tracker . Please see the Contributing section for contribution standards, code reviews, and documentation tips. Get started with TabTune and accelerate your tabular deep learning workflows today!","title":"Home"},{"location":"#tabtune-v10-first-release","text":"Welcome to TabTune! This initial release provides a complete, production-ready framework for tabular foundation models. Core Components: Unified API ( TabularPipeline ): Single, scikit-learn-compatible interface for all models with .fit() , .predict() , .save() , and .load() methods. Smart Data Processing ( DataProcessor ): Model-aware preprocessing that automatically handles imputation, scaling, categorical encoding, and feature transformations for each model. Flexible Tuning ( TuningManager ): Three tuning strategies\u2014zero-shot inference , supervised fine-tuning ( base-ft ) with full parameter updates, and memory-efficient peft (LoRA) adapters. Supports episodic meta-learning for ICL models. Model Comparison ( TabularLeaderboard ): Systematic benchmarking tool for comparing multiple models and strategies on your datasets. Supported Models (7 Models): TabPFN-v2: Fast Bayesian inference for small datasets (<10K rows) with experimental PEFT support. TabICL: Scalable ICL with two-stage attention, full PEFT support, ideal for 10K-1M row datasets. OrionMSP: Multi-scale prior ICL for balanced generalization on 50K-2M+ row datasets, full PEFT support. OrionBix: Biaxial interaction expert for high-accuracy scenarios (50K-2M+ rows), full PEFT support. TabDPT: Denoising transformer for very large datasets (100K-5M rows), full PEFT support. Mitra: 2D cross-attention model for complex patterns and mixed data types, full PEFT support. ContextTab: Semantics-aware ICL with text embedding integration, experimental PEFT support. Key Capabilities: \u2705 Multiple Training Paradigms: Supports supervised fine-tuning (SFT) with full parameter updates, episodic meta-learning for in-context learning models, and parameter-efficient PEFT strategies. \u2705 PEFT (LoRA) Support: Parameter-efficient fine-tuning for 5 out of 7 models (TabICL, OrionMSP, OrionBix, TabDPT, Mitra) with full support. \u2705 Meta-Learning Integration: Episodic training with support/query sets for ICL models (TabICL, OrionMSP, OrionBix, Mitra) enabling fast task adaptation. \u2705 Comprehensive Documentation: Extensive guides, API references, troubleshooting, and model-specific documentation. \u2705 Production Ready: Model serialization, reproducible training, and deployment-ready pipelines. \u2705 Extensible Architecture: Modular design for easy integration of custom processors and models.","title":"\ud83d\ude80 TabTune v1.0 - First Release"},{"location":"#core-features","text":"Unified API: Single interface for model training, inference, and evaluation across multiple tabular model families. Automated Preprocessing: Model-aware data processing for feature scaling, encoding, imputation, and transformation. Flexible Fine-Tuning: Choose between zero-shot inference, full fine-tuning, or memory-efficient PEFT strategies. Model Comparison: Built-in leaderboard for systematic benchmarking and strategy evaluation. Extensible Design: Modular codebase for easy integration of custom data processors and models.","title":"\u2b50 Core Features"},{"location":"#supported-models","text":"Model Family / Paradigm Key Innovation PEFT Support TabPFN-v2 PFN / ICL Bayesian approximation on synthetic \u26a0\ufe0f Experimental TabICL Scalable ICL Two-stage column-row attention \u2705 Full Support OrionMSP Scalable ICL Multi-Scale Prior (MSP) ICL \u2705 Full Support Orion BIX Scalable ICL Biaxial Interaction eXpert \u2705 Full Support Mitra Scalable ICL 2D attention, synthetic priors \u2705 Full Support ContextTab Semantics-Aware ICL Modality-specific embeddings \u26a0\ufe0f Experimental TabDPT Denoising Transformer Denoising pre-trained transformer \u2705 Full Support \u2705 Full Support: Reliable LoRA integration \u26a0\ufe0f Experimental: Known issues may occur; use base-ft if unstable","title":"\ud83d\udce6 Supported Models"},{"location":"#quick-start","text":"import pandas as pd from sklearn.model_selection import train_test_split import openml from tabtune import TabularPipeline # Load dataset dataset = openml . datasets . get_dataset ( 42178 ) X , y , _ , _ = dataset . get_data ( target = dataset . default_target_attribute ) X_train , X_test , y_train , y_test = train_test_split ( X , y , test_size = 0.25 , random_state = 42 ) # Init and fit pipeline pipeline = TabularPipeline ( model_name = \"TabPFN\" , task_type = \"classification\" , tuning_strategy = \"base-ft\" , tuning_params = { \"device\" : \"cpu\" } ) pipeline . fit ( X_train , y_train ) # Save and load pipeline for prediction pipeline . save ( \"churn_pipeline.joblib\" ) loaded_pipeline = TabularPipeline . load ( \"churn_pipeline.joblib\" ) predictions = loaded_pipeline . predict ( X_test ) metrics = pipeline . evaluate ( X_test , y_test ) print ( metrics )","title":"\u26a1 Quick Start"},{"location":"#why-tabtune","text":"No Boilerplate: Avoids repetitive code for model-specific data loading, training, and inference. Consistent Results: Automates best practices for tabular DL research and model selection. Fast Iteration: Easily compare new models with your data, using the same consistent API. Production Ready: Model and config serialization for robust deployment and reproducibility. Community-Driven: Extensible design and open contribution policy.","title":"\ud83d\udcdd Why TabTune?"},{"location":"#explore-the-documentation","text":"Getting Started : Installation, setup, and basic usage. User Guide : In-depth tutorials for each component. Supported Models : Model details and design notes. Advanced Topics : PEFT/LoRA, custom preprocessing, and more. API Reference : Complete Python API and class/method details. Examples & Benchmarks : End-to-end code notebooks.","title":"\ud83d\udcd1 Explore the Documentation"},{"location":"#example-notebooks","text":"Name Task Type Colab Link TabPFN Inference Inference & Fine-Tune Open In Colab Mitra Inference/FineTune Inference & Fine-Tune Open In Colab OrionMSP/BIX Examples Inference & Fine-Tune Coming soon TabDPT Large Dataset Large-Scale Training Coming soon PEFT Fine-Tuning Guide Parameter-Efficient FT Coming soon Leaderboard Benchmarking Model Comparison Coming soon","title":"\ud83c\udfc6 Example Notebooks"},{"location":"#project-structure","text":"tabtune/ \u251c\u2500\u2500 Dataprocess/ \u251c\u2500\u2500 models/ \u251c\u2500\u2500 TabularPipeline/ \u251c\u2500\u2500 TuningManager/ \u251c\u2500\u2500 TabularLeaderboard/ \u251c\u2500\u2500 benchmarking/ \u251c\u2500\u2500 data/ \u251c\u2500\u2500 logger.py \u2514\u2500\u2500 run.py See User Guide for a full file/module breakdown.","title":"\ud83d\udcc2 Project Structure"},{"location":"#developed-by-lexsi-labs","text":"Created by the team at Lexsi Labs , TabTune extends frontier AI research into the tabular domain.","title":"\ud83c\udfe2 Developed by Lexsi Labs"},{"location":"#license","text":"This project is released under the MIT License. Please cite appropriately if used in academic or production projects. BibTeX Citation: @misc { tanna2025tabtuneunifiedlibraryinference , title = {TabTune: A Unified Library for Inference and Fine-Tuning Tabular Foundation Models} , author = {Aditya Tanna and Pratinav Seth and Mohamed Bouadi and Utsav Avaiya and Vinay Kumar Sankarapu} , year = {2025} , eprint = {2511.02802} , archivePrefix = {arXiv} , primaryClass = {cs.LG} , url = {https://arxiv.org/abs/2511.02802} , }","title":"\ud83d\uddc3\ufe0f License"},{"location":"#join-community-contribute","text":"Issues and discussions are welcomed on the GitHub issue tracker . Please see the Contributing section for contribution standards, code reviews, and documentation tips. Get started with TabTune and accelerate your tabular deep learning workflows today!","title":"\ud83d\udceb Join Community / Contribute"},{"location":"about/faq/","text":"FAQ \u00b6 Frequently asked questions about TabTune, covering installation, usage, model selection, and troubleshooting. Installation & Setup \u00b6 Which Python versions are supported? \u00b6 Python 3.10+ is required. Python 3.11+ is recommended for best performance. Do I need a GPU? \u00b6 No, TabTune works on CPU for many models. However, a GPU is strongly recommended for: - Training/fine-tuning (base-ft and peft strategies) - Large datasets (>100K rows) - Faster inference Models like TabPFN and TabICL can run on CPU for inference, but training will be significantly slower. How do I install TabTune with GPU support? \u00b6 Install PyTorch with CUDA support first, then install TabTune: # Install PyTorch with CUDA (check your CUDA version first with nvidia-smi) pip install torch torchvision --index-url https://download.pytorch.org/whl/cu118 # Then install TabTune pip install -r requirements.txt pip install -e . Tasks & Models \u00b6 Does TabTune support regression? \u00b6 Not yet; regression support is planned for future releases. Currently, TabTune focuses on classification tasks (binary and multi-class). Which models support PEFT (LoRA)? \u00b6 Full PEFT Support: - TabICL - OrionMSP - OrionBix - TabDPT - Mitra Experimental PEFT Support: - TabPFN (may have stability issues) - ContextTab (may have stability issues) If you encounter issues with experimental models, use base-ft strategy instead. How do I choose the right model for my dataset? \u00b6 See the Model Selection Guide for detailed guidance. Quick reference: <10K rows : TabPFN (inference) or TabICL 10K-100K rows : TabICL or Mitra 100K-1M rows : OrionBix, OrionMSP, or TabDPT >1M rows : TabDPT Text-heavy features : ContextTab High accuracy needed : OrionBix or TabDPT with base-ft Usage & Workflow \u00b6 What's the difference between inference, base-ft, and peft strategies? \u00b6 inference : Zero-shot predictions using pre-trained weights. No training occurs. Fastest, lowest accuracy. base-ft : Full fine-tuning of all model parameters. Slowest, highest accuracy, requires most memory. peft : Parameter-efficient fine-tuning using LoRA adapters. Faster than base-ft, uses less memory, high accuracy. See Tuning Strategies for detailed comparisons. How do I save and load models? \u00b6 # Save pipeline (includes preprocessing and model state) pipeline . save ( \"my_pipeline.joblib\" ) # Load pipeline loaded_pipeline = TabularPipeline . load ( \"my_pipeline.joblib\" ) predictions = loaded_pipeline . predict ( X_test ) Note : Saved pipelines include the DataProcessor state, so preprocessing is automatically applied. What file formats are supported for input data? \u00b6 TabTune accepts pandas DataFrames and Series . You can load data from: - CSV files: pd.read_csv() - Excel files: pd.read_excel() - Parquet files: pd.read_parquet() - Any format that pandas supports How do I handle missing values? \u00b6 The DataProcessor automatically handles missing values based on your configuration: pipeline = TabularPipeline ( model_name = \"TabICL\" , processor_params = { \"imputation_strategy\" : \"mean\" # Options: 'mean', 'median', 'mode', 'knn' } ) Most models have sensible defaults, so you often don't need to specify this. Data & Preprocessing \u00b6 Can I use my own custom preprocessing? \u00b6 Yes! See Custom Preprocessing for details on: - Creating custom preprocessors - Extending the data pipeline - Integrating domain-specific transformations What are the memory requirements? \u00b6 Memory usage varies by model and dataset size: TabPFN : ~2-4 GB (small datasets) TabICL : ~4-8 GB (medium datasets) OrionBix/OrionMSP : ~8-16 GB (large datasets) TabDPT : ~12-24 GB (very large datasets) Mitra : ~16-32 GB (complex datasets) PEFT strategy reduces memory by 40-60% compared to base-ft. Training & Performance \u00b6 How long does training take? \u00b6 Training time depends on: - Dataset size (rows and features) - Model choice - Strategy (inference: 0s, peft: fast, base-ft: slower) - Hardware (GPU vs CPU) Rough estimates: - Inference : Instant (no training) - PEFT : 5-30 minutes for medium datasets - Base-ft : 30 minutes to several hours for large datasets How do I debug training issues? \u00b6 Check logs : TabTune uses structured logging - enable verbose mode Reduce dataset size : Test with a smaller subset first Use CPU : Test on CPU to rule out GPU-specific issues Lower batch size : Reduce memory pressure Check data quality : Ensure no invalid values or type mismatches See Troubleshooting for detailed solutions. Why is my model overfitting? \u00b6 Common causes and solutions: Too many epochs : Reduce epochs in tuning_params Too high learning rate : Lower learning_rate (try 1e-5 to 2e-5) Dataset too small : Use more data or a simpler model Try PEFT : LoRA adapters often generalize better Technical Issues \u00b6 I get \"CUDA out of memory\" errors. How do I fix this? \u00b6 Solutions: 1. Use peft strategy instead of base-ft (40-60% less memory) 2. Reduce batch_size in tuning_params 3. Use a smaller model (TabICL instead of TabDPT) 4. Process data in chunks 5. Use CPU instead of GPU (slower but no memory limits) ModuleNotFoundError: No module named 'tabtune' \u00b6 Solution : Install TabTune in development mode: cd TabTune_Internal pip install -e . Import errors or version conflicts \u00b6 Solution : Use a virtual environment: python -m venv tabtune-env source tabtune-env/bin/activate # Linux/macOS # tabtune-env\\Scripts\\activate # Windows pip install -r requirements.txt pip install -e . Model predictions are all the same class \u00b6 Possible causes: - Model not trained (using inference with poor pre-trained weights) - Data preprocessing issue (check DataProcessor summary) - Severe class imbalance (use resampling strategies) - Wrong model for dataset size Solution : Try fine-tuning with base-ft or peft strategy. Comparison & Evaluation \u00b6 How do I compare multiple models? \u00b6 Use TabularLeaderboard : from tabtune import TabularLeaderboard leaderboard = TabularLeaderboard ( X_train , X_test , y_train , y_test ) leaderboard . add_model ( \"TabICL\" , \"base-ft\" ) leaderboard . add_model ( \"OrionBix\" , \"peft\" ) results = leaderboard . run ( rank_by = \"roc_auc_score\" ) See Model Comparison for detailed examples. What evaluation metrics are available? \u00b6 Default metrics in .evaluate() : - Accuracy : Overall correctness - Weighted F1 Score : Class-balanced F1 - ROC AUC Score : Binary and multi-class supported - Precision : Weighted average - Recall : Weighted average - MCC : Matthews Correlation Coefficient How do I interpret the evaluation metrics? \u00b6 Accuracy : Simple but can be misleading with imbalanced classes F1 Score : Better for imbalanced datasets (weighted average) ROC AUC : Best for ranking/model comparison, works with imbalanced data MCC : Comprehensive metric that accounts for all confusion matrix values Advanced Topics \u00b6 Can I use TabTune for production deployment? \u00b6 Yes! TabTune pipelines are production-ready: - Save complete pipelines with .save() - Includes all preprocessing transformations - Reproducible results - Handles new data automatically Best practices: - Use base-ft or peft for best accuracy - Save checkpoints during training - Log hyperparameters and preprocessing config - Test on validation sets before deployment How do I fine-tune hyperparameters? \u00b6 See Hyperparameter Tuning for: - Search strategies (grid, random, Bayesian) - Hyperparameter spaces for each model - Integration with Optuna and other tools - Best practices and validation strategies Can I use multiple GPUs? \u00b6 Yes, for supported models. See Multi-GPU Training for configuration details. Support & Community \u00b6 Where can I get help? \u00b6 GitHub Issues : TabTune_Internal Issues Documentation : Browse the User Guide FAQ : This page! How do I report a bug? \u00b6 Open an issue on GitHub with: - TabTune version - Python version - Error message and traceback - Minimal reproducible example - System information (OS, GPU if applicable) Can I contribute to TabTune? \u00b6 Yes! See the Contributing Guide for: - Development setup - Code standards - How to add new models - Documentation guidelines","title":"FAQ"},{"location":"about/faq/#faq","text":"Frequently asked questions about TabTune, covering installation, usage, model selection, and troubleshooting.","title":"FAQ"},{"location":"about/faq/#installation-setup","text":"","title":"Installation &amp; Setup"},{"location":"about/faq/#which-python-versions-are-supported","text":"Python 3.10+ is required. Python 3.11+ is recommended for best performance.","title":"Which Python versions are supported?"},{"location":"about/faq/#do-i-need-a-gpu","text":"No, TabTune works on CPU for many models. However, a GPU is strongly recommended for: - Training/fine-tuning (base-ft and peft strategies) - Large datasets (>100K rows) - Faster inference Models like TabPFN and TabICL can run on CPU for inference, but training will be significantly slower.","title":"Do I need a GPU?"},{"location":"about/faq/#how-do-i-install-tabtune-with-gpu-support","text":"Install PyTorch with CUDA support first, then install TabTune: # Install PyTorch with CUDA (check your CUDA version first with nvidia-smi) pip install torch torchvision --index-url https://download.pytorch.org/whl/cu118 # Then install TabTune pip install -r requirements.txt pip install -e .","title":"How do I install TabTune with GPU support?"},{"location":"about/faq/#tasks-models","text":"","title":"Tasks &amp; Models"},{"location":"about/faq/#does-tabtune-support-regression","text":"Not yet; regression support is planned for future releases. Currently, TabTune focuses on classification tasks (binary and multi-class).","title":"Does TabTune support regression?"},{"location":"about/faq/#which-models-support-peft-lora","text":"Full PEFT Support: - TabICL - OrionMSP - OrionBix - TabDPT - Mitra Experimental PEFT Support: - TabPFN (may have stability issues) - ContextTab (may have stability issues) If you encounter issues with experimental models, use base-ft strategy instead.","title":"Which models support PEFT (LoRA)?"},{"location":"about/faq/#how-do-i-choose-the-right-model-for-my-dataset","text":"See the Model Selection Guide for detailed guidance. Quick reference: <10K rows : TabPFN (inference) or TabICL 10K-100K rows : TabICL or Mitra 100K-1M rows : OrionBix, OrionMSP, or TabDPT >1M rows : TabDPT Text-heavy features : ContextTab High accuracy needed : OrionBix or TabDPT with base-ft","title":"How do I choose the right model for my dataset?"},{"location":"about/faq/#usage-workflow","text":"","title":"Usage &amp; Workflow"},{"location":"about/faq/#whats-the-difference-between-inference-base-ft-and-peft-strategies","text":"inference : Zero-shot predictions using pre-trained weights. No training occurs. Fastest, lowest accuracy. base-ft : Full fine-tuning of all model parameters. Slowest, highest accuracy, requires most memory. peft : Parameter-efficient fine-tuning using LoRA adapters. Faster than base-ft, uses less memory, high accuracy. See Tuning Strategies for detailed comparisons.","title":"What's the difference between inference, base-ft, and peft strategies?"},{"location":"about/faq/#how-do-i-save-and-load-models","text":"# Save pipeline (includes preprocessing and model state) pipeline . save ( \"my_pipeline.joblib\" ) # Load pipeline loaded_pipeline = TabularPipeline . load ( \"my_pipeline.joblib\" ) predictions = loaded_pipeline . predict ( X_test ) Note : Saved pipelines include the DataProcessor state, so preprocessing is automatically applied.","title":"How do I save and load models?"},{"location":"about/faq/#what-file-formats-are-supported-for-input-data","text":"TabTune accepts pandas DataFrames and Series . You can load data from: - CSV files: pd.read_csv() - Excel files: pd.read_excel() - Parquet files: pd.read_parquet() - Any format that pandas supports","title":"What file formats are supported for input data?"},{"location":"about/faq/#how-do-i-handle-missing-values","text":"The DataProcessor automatically handles missing values based on your configuration: pipeline = TabularPipeline ( model_name = \"TabICL\" , processor_params = { \"imputation_strategy\" : \"mean\" # Options: 'mean', 'median', 'mode', 'knn' } ) Most models have sensible defaults, so you often don't need to specify this.","title":"How do I handle missing values?"},{"location":"about/faq/#data-preprocessing","text":"","title":"Data &amp; Preprocessing"},{"location":"about/faq/#can-i-use-my-own-custom-preprocessing","text":"Yes! See Custom Preprocessing for details on: - Creating custom preprocessors - Extending the data pipeline - Integrating domain-specific transformations","title":"Can I use my own custom preprocessing?"},{"location":"about/faq/#what-are-the-memory-requirements","text":"Memory usage varies by model and dataset size: TabPFN : ~2-4 GB (small datasets) TabICL : ~4-8 GB (medium datasets) OrionBix/OrionMSP : ~8-16 GB (large datasets) TabDPT : ~12-24 GB (very large datasets) Mitra : ~16-32 GB (complex datasets) PEFT strategy reduces memory by 40-60% compared to base-ft.","title":"What are the memory requirements?"},{"location":"about/faq/#training-performance","text":"","title":"Training &amp; Performance"},{"location":"about/faq/#how-long-does-training-take","text":"Training time depends on: - Dataset size (rows and features) - Model choice - Strategy (inference: 0s, peft: fast, base-ft: slower) - Hardware (GPU vs CPU) Rough estimates: - Inference : Instant (no training) - PEFT : 5-30 minutes for medium datasets - Base-ft : 30 minutes to several hours for large datasets","title":"How long does training take?"},{"location":"about/faq/#how-do-i-debug-training-issues","text":"Check logs : TabTune uses structured logging - enable verbose mode Reduce dataset size : Test with a smaller subset first Use CPU : Test on CPU to rule out GPU-specific issues Lower batch size : Reduce memory pressure Check data quality : Ensure no invalid values or type mismatches See Troubleshooting for detailed solutions.","title":"How do I debug training issues?"},{"location":"about/faq/#why-is-my-model-overfitting","text":"Common causes and solutions: Too many epochs : Reduce epochs in tuning_params Too high learning rate : Lower learning_rate (try 1e-5 to 2e-5) Dataset too small : Use more data or a simpler model Try PEFT : LoRA adapters often generalize better","title":"Why is my model overfitting?"},{"location":"about/faq/#technical-issues","text":"","title":"Technical Issues"},{"location":"about/faq/#i-get-cuda-out-of-memory-errors-how-do-i-fix-this","text":"Solutions: 1. Use peft strategy instead of base-ft (40-60% less memory) 2. Reduce batch_size in tuning_params 3. Use a smaller model (TabICL instead of TabDPT) 4. Process data in chunks 5. Use CPU instead of GPU (slower but no memory limits)","title":"I get \"CUDA out of memory\" errors. How do I fix this?"},{"location":"about/faq/#modulenotfounderror-no-module-named-tabtune","text":"Solution : Install TabTune in development mode: cd TabTune_Internal pip install -e .","title":"ModuleNotFoundError: No module named 'tabtune'"},{"location":"about/faq/#import-errors-or-version-conflicts","text":"Solution : Use a virtual environment: python -m venv tabtune-env source tabtune-env/bin/activate # Linux/macOS # tabtune-env\\Scripts\\activate # Windows pip install -r requirements.txt pip install -e .","title":"Import errors or version conflicts"},{"location":"about/faq/#model-predictions-are-all-the-same-class","text":"Possible causes: - Model not trained (using inference with poor pre-trained weights) - Data preprocessing issue (check DataProcessor summary) - Severe class imbalance (use resampling strategies) - Wrong model for dataset size Solution : Try fine-tuning with base-ft or peft strategy.","title":"Model predictions are all the same class"},{"location":"about/faq/#comparison-evaluation","text":"","title":"Comparison &amp; Evaluation"},{"location":"about/faq/#how-do-i-compare-multiple-models","text":"Use TabularLeaderboard : from tabtune import TabularLeaderboard leaderboard = TabularLeaderboard ( X_train , X_test , y_train , y_test ) leaderboard . add_model ( \"TabICL\" , \"base-ft\" ) leaderboard . add_model ( \"OrionBix\" , \"peft\" ) results = leaderboard . run ( rank_by = \"roc_auc_score\" ) See Model Comparison for detailed examples.","title":"How do I compare multiple models?"},{"location":"about/faq/#what-evaluation-metrics-are-available","text":"Default metrics in .evaluate() : - Accuracy : Overall correctness - Weighted F1 Score : Class-balanced F1 - ROC AUC Score : Binary and multi-class supported - Precision : Weighted average - Recall : Weighted average - MCC : Matthews Correlation Coefficient","title":"What evaluation metrics are available?"},{"location":"about/faq/#how-do-i-interpret-the-evaluation-metrics","text":"Accuracy : Simple but can be misleading with imbalanced classes F1 Score : Better for imbalanced datasets (weighted average) ROC AUC : Best for ranking/model comparison, works with imbalanced data MCC : Comprehensive metric that accounts for all confusion matrix values","title":"How do I interpret the evaluation metrics?"},{"location":"about/faq/#advanced-topics","text":"","title":"Advanced Topics"},{"location":"about/faq/#can-i-use-tabtune-for-production-deployment","text":"Yes! TabTune pipelines are production-ready: - Save complete pipelines with .save() - Includes all preprocessing transformations - Reproducible results - Handles new data automatically Best practices: - Use base-ft or peft for best accuracy - Save checkpoints during training - Log hyperparameters and preprocessing config - Test on validation sets before deployment","title":"Can I use TabTune for production deployment?"},{"location":"about/faq/#how-do-i-fine-tune-hyperparameters","text":"See Hyperparameter Tuning for: - Search strategies (grid, random, Bayesian) - Hyperparameter spaces for each model - Integration with Optuna and other tools - Best practices and validation strategies","title":"How do I fine-tune hyperparameters?"},{"location":"about/faq/#can-i-use-multiple-gpus","text":"Yes, for supported models. See Multi-GPU Training for configuration details.","title":"Can I use multiple GPUs?"},{"location":"about/faq/#support-community","text":"","title":"Support &amp; Community"},{"location":"about/faq/#where-can-i-get-help","text":"GitHub Issues : TabTune_Internal Issues Documentation : Browse the User Guide FAQ : This page!","title":"Where can I get help?"},{"location":"about/faq/#how-do-i-report-a-bug","text":"Open an issue on GitHub with: - TabTune version - Python version - Error message and traceback - Minimal reproducible example - System information (OS, GPU if applicable)","title":"How do I report a bug?"},{"location":"about/faq/#can-i-contribute-to-tabtune","text":"Yes! See the Contributing Guide for: - Development setup - Code standards - How to add new models - Documentation guidelines","title":"Can I contribute to TabTune?"},{"location":"about/license/","text":"License \u00b6 This project is licensed under the MIT License. See the LICENSE file in the repository root if present, or the package metadata.","title":"License"},{"location":"about/license/#license","text":"This project is licensed under the MIT License. See the LICENSE file in the repository root if present, or the package metadata.","title":"License"},{"location":"about/release-notes/","text":"Release Notes \u00b6 0.1.0 \u00b6 Initial alpha release TabularPipeline, DataProcessor, TuningManager, TabularLeaderboard Docs site with getting started, user guide, models, API","title":"Release Notes"},{"location":"about/release-notes/#release-notes","text":"","title":"Release Notes"},{"location":"about/release-notes/#010","text":"Initial alpha release TabularPipeline, DataProcessor, TuningManager, TabularLeaderboard Docs site with getting started, user guide, models, API","title":"0.1.0"},{"location":"about/roadmap/","text":"Roadmap \u00b6 Regression support Expanded benchmarking suite Additional models and preprocessors Improved PEFT/LoRA integrations Tutorials and advanced guides","title":"Roadmap"},{"location":"about/roadmap/#roadmap","text":"Regression support Expanded benchmarking suite Additional models and preprocessors Improved PEFT/LoRA integrations Tutorials and advanced guides","title":"Roadmap"},{"location":"advanced/custom-preprocessing/","text":"Custom Preprocessing: Extending TabTune's Data Pipeline \u00b6 This document explains how to create custom preprocessors, extend the data pipeline, and integrate domain-specific transformations with TabTune. 1. Introduction \u00b6 While TabTune provides comprehensive automatic preprocessing, you may need custom transformations for: Domain-specific feature engineering Specialized encoding for your data Integration with existing pipelines Research and experimentation Non-standard data types This guide shows how to extend TabTune's preprocessing architecture. 2. Preprocessing Architecture \u00b6 2.1 Class Hierarchy \u00b6 BasePreprocessor (Abstract) \u251c\u2500\u2500 StandardPreprocessor (Default) \u251c\u2500\u2500 TabPFNPreprocessor \u251c\u2500\u2500 TabICLPreprocessor \u251c\u2500\u2500 MitraPreprocessor \u251c\u2500\u2500 ContextTabPreprocessor \u251c\u2500\u2500 TabDPTPreprocessor \u251c\u2500\u2500 OrionBixPreprocessor \u251c\u2500\u2500 OrionMSPPreprocessor \u2514\u2500\u2500 YourCustomPreprocessor 2.2 Base Class Interface \u00b6 from abc import ABC , abstractmethod import pandas as pd class BasePreprocessor ( ABC ): \"\"\"Abstract base for all preprocessors.\"\"\" def __init__ ( self , ** kwargs ): self . config = kwargs @abstractmethod def fit ( self , X : pd . DataFrame , y : pd . Series = None ): \"\"\"Learn preprocessing parameters from data.\"\"\" pass @abstractmethod def transform ( self , X : pd . DataFrame ) -> pd . DataFrame : \"\"\"Apply preprocessing to data.\"\"\" pass def fit_transform ( self , X : pd . DataFrame , y : pd . Series = None ): \"\"\"Fit and transform in one call.\"\"\" self . fit ( X , y ) return self . transform ( X ) def get_config ( self ) -> dict : \"\"\"Return preprocessing configuration.\"\"\" return self . config 3. Creating Custom Preprocessors \u00b6 3.1 Simple Custom Preprocessor \u00b6 Create a basic preprocessor for specialized transformations: import pandas as pd import numpy as np from tabtune.preprocessing.base import BasePreprocessor from sklearn.preprocessing import StandardScaler class CustomFeatureEngineeringPreprocessor ( BasePreprocessor ): \"\"\"Custom preprocessor with feature engineering.\"\"\" def __init__ ( self , scale_numericals = True , create_interactions = True ): super () . __init__ ( scale_numericals = scale_numericals , create_interactions = create_interactions ) self . scaler = StandardScaler () self . numerical_cols = None self . categorical_cols = None def fit ( self , X : pd . DataFrame , y : pd . Series = None ): \"\"\"Learn column types and scaling.\"\"\" # Identify numerical and categorical columns self . numerical_cols = X . select_dtypes ( include = [ np . number ] ) . columns . tolist () self . categorical_cols = X . select_dtypes ( include = [ 'object' , 'category' ] ) . columns . tolist () # Fit scaler on numerical columns if self . config [ 'scale_numericals' ]: self . scaler . fit ( X [ self . numerical_cols ]) def transform ( self , X : pd . DataFrame ) -> pd . DataFrame : \"\"\"Apply transformations.\"\"\" X_transformed = X . copy () # Scale numerical features if self . config [ 'scale_numericals' ]: X_transformed [ self . numerical_cols ] = self . scaler . transform ( X [ self . numerical_cols ] ) # Create interaction features if self . config [ 'create_interactions' ]: X_transformed = self . _create_interactions ( X_transformed ) return X_transformed def _create_interactions ( self , X : pd . DataFrame ) -> pd . DataFrame : \"\"\"Create polynomial interaction features.\"\"\" if len ( self . numerical_cols ) >= 2 : # Create pairwise interactions for i , col1 in enumerate ( self . numerical_cols ): for col2 in self . numerical_cols [ i + 1 :]: X [ f ' { col1 } _x_ { col2 } ' ] = X [ col1 ] * X [ col2 ] return X 3.2 Domain-Specific Preprocessor (Finance Example) \u00b6 import pandas as pd import numpy as np from tabtune.preprocessing.base import BasePreprocessor class FinancialDataPreprocessor ( BasePreprocessor ): \"\"\"Specialized preprocessor for financial data.\"\"\" def __init__ ( self , handle_outliers = True , create_ratios = True , normalize_by_scale = True ): super () . __init__ ( handle_outliers = handle_outliers , create_ratios = create_ratios , normalize_by_scale = normalize_by_scale ) self . outlier_bounds = {} self . scaling_factors = {} def fit ( self , X : pd . DataFrame , y : pd . Series = None ): \"\"\"Learn financial data patterns.\"\"\" # Detect outliers (IQR method) if self . config [ 'handle_outliers' ]: for col in X . select_dtypes ( include = [ np . number ]): Q1 = X [ col ] . quantile ( 0.25 ) Q3 = X [ col ] . quantile ( 0.75 ) IQR = Q3 - Q1 self . outlier_bounds [ col ] = { 'lower' : Q1 - 1.5 * IQR , 'upper' : Q3 + 1.5 * IQR } # Learn scaling factors by sector if self . config [ 'normalize_by_scale' ]: if 'sector' in X . columns : for sector in X [ 'sector' ] . unique (): sector_data = X [ X [ 'sector' ] == sector ] self . scaling_factors [ sector ] = sector_data . mean () def transform ( self , X : pd . DataFrame ) -> pd . DataFrame : \"\"\"Apply financial transformations.\"\"\" X_transformed = X . copy () # Handle outliers with clipping if self . config [ 'handle_outliers' ]: for col , bounds in self . outlier_bounds . items (): X_transformed [ col ] = X_transformed [ col ] . clip ( lower = bounds [ 'lower' ], upper = bounds [ 'upper' ] ) # Create financial ratios if self . config [ 'create_ratios' ]: if 'revenue' in X_transformed . columns and 'costs' in X_transformed . columns : X_transformed [ 'profit_margin' ] = ( X_transformed [ 'revenue' ] - X_transformed [ 'costs' ] ) / X_transformed [ 'revenue' ] # Normalize by sector if self . config [ 'normalize_by_scale' ]: if 'sector' in X_transformed . columns : for sector , factors in self . scaling_factors . items (): sector_mask = X_transformed [ 'sector' ] == sector # Normalize numeric columns by sector mean for col in X_transformed . select_dtypes ( include = [ np . number ]): X_transformed . loc [ sector_mask , col ] /= factors [ col ] return X_transformed 4. Integrating Custom Preprocessors \u00b6 4.1 Register Custom Preprocessor \u00b6 # In your codebase or configuration file from tabtune.data_processor import DataProcessor from your_module import CustomFeatureEngineeringPreprocessor # Register custom preprocessor DataProcessor . register_preprocessor ( 'custom_features' , CustomFeatureEngineeringPreprocessor ) 4.2 Use Custom Preprocessor with Pipeline \u00b6 from tabtune import TabularPipeline # Use custom preprocessor pipeline = TabularPipeline ( model_name = 'TabICL' , tuning_strategy = 'base-ft' , processor_params = { 'preprocessor_type' : 'custom_features' , 'scale_numericals' : True , 'create_interactions' : True } ) pipeline . fit ( X_train , y_train ) 4.3 Chaining Preprocessors \u00b6 Combine multiple preprocessors: class ChainedPreprocessor ( BasePreprocessor ): \"\"\"Sequentially apply multiple preprocessors.\"\"\" def __init__ ( self , preprocessors : list ): super () . __init__ ( preprocessors = preprocessors ) self . preprocessor_chain = preprocessors def fit ( self , X : pd . DataFrame , y : pd . Series = None ): \"\"\"Fit each preprocessor in chain.\"\"\" for preprocessor in self . preprocessor_chain : preprocessor . fit ( X , y ) X = preprocessor . transform ( X ) def transform ( self , X : pd . DataFrame ) -> pd . DataFrame : \"\"\"Apply all preprocessors in sequence.\"\"\" for preprocessor in self . preprocessor_chain : X = preprocessor . transform ( X ) return X # Usage preprocessors = [ CustomFeatureEngineeringPreprocessor ( create_interactions = True ), FinancialDataPreprocessor ( handle_outliers = True ), YourSpecializedPreprocessor () ] chained = ChainedPreprocessor ( preprocessors ) X_transformed = chained . fit_transform ( X_train , y_train ) 5. Feature Engineering Examples \u00b6 5.1 Polynomial Features \u00b6 from sklearn.preprocessing import PolynomialFeatures class PolynomialPreprocessor ( BasePreprocessor ): \"\"\"Add polynomial features.\"\"\" def __init__ ( self , degree = 2 , include_bias = False ): super () . __init__ ( degree = degree , include_bias = include_bias ) self . poly = PolynomialFeatures ( degree = degree , include_bias = include_bias ) self . feature_names = None def fit ( self , X : pd . DataFrame , y : pd . Series = None ): \"\"\"Fit polynomial transformer.\"\"\" numerical = X . select_dtypes ( include = [ np . number ]) self . poly . fit ( numerical ) self . feature_names = self . poly . get_feature_names_out ( numerical . columns ) def transform ( self , X : pd . DataFrame ) -> pd . DataFrame : \"\"\"Transform to polynomial features.\"\"\" numerical = X . select_dtypes ( include = [ np . number ]) poly_features = self . poly . transform ( numerical ) return pd . DataFrame ( poly_features , columns = self . feature_names , index = X . index ) 5.2 Statistical Features \u00b6 class StatisticalFeaturePreprocessor ( BasePreprocessor ): \"\"\"Extract statistical features from groups.\"\"\" def __init__ ( self , groupby_col = None , agg_functions = None ): super () . __init__ ( groupby_col = groupby_col , agg_functions = agg_functions or [ 'mean' , 'std' , 'min' , 'max' ] ) def fit ( self , X : pd . DataFrame , y : pd . Series = None ): \"\"\"No fitting needed for statistical features.\"\"\" pass def transform ( self , X : pd . DataFrame ) -> pd . DataFrame : \"\"\"Create statistical aggregates.\"\"\" X_transformed = X . copy () if self . config [ 'groupby_col' ] and self . config [ 'groupby_col' ] in X : groupby_col = self . config [ 'groupby_col' ] # Aggregate statistics for col in X . select_dtypes ( include = [ np . number ]): for func in self . config [ 'agg_functions' ]: agg_values = X . groupby ( groupby_col )[ col ] . agg ( func ) X_transformed [ f ' { col } _ { func } _by_ { groupby_col } ' ] = ( X [ groupby_col ] . map ( agg_values ) ) return X_transformed 5.3 Text Feature Extraction \u00b6 from sklearn.feature_extraction.text import TfidfVectorizer class TextFeaturePreprocessor ( BasePreprocessor ): \"\"\"Extract features from text columns.\"\"\" def __init__ ( self , text_columns = None , max_features = 100 ): super () . __init__ ( text_columns = text_columns , max_features = max_features ) self . vectorizers = {} def fit ( self , X : pd . DataFrame , y : pd . Series = None ): \"\"\"Fit TF-IDF vectorizers.\"\"\" text_cols = self . config [ 'text_columns' ] or [ col for col in X . columns if X [ col ] . dtype == 'object' ] for col in text_cols : vectorizer = TfidfVectorizer ( max_features = self . config [ 'max_features' ], lowercase = True ) vectorizer . fit ( X [ col ] . astype ( str )) self . vectorizers [ col ] = vectorizer def transform ( self , X : pd . DataFrame ) -> pd . DataFrame : \"\"\"Transform text to features.\"\"\" X_transformed = X . copy () for col , vectorizer in self . vectorizers . items (): tfidf_matrix = vectorizer . transform ( X [ col ] . astype ( str )) feature_names = vectorizer . get_feature_names_out () for i , fname in enumerate ( feature_names ): X_transformed [ f ' { col } _tfidf_ { fname } ' ] = ( tfidf_matrix [:, i ] . toarray () . flatten () ) return X_transformed 6. Integration with scikit-learn \u00b6 6.1 Use scikit-learn Transformers \u00b6 from sklearn.pipeline import Pipeline from sklearn.preprocessing import StandardScaler , RobustScaler from sklearn.decomposition import PCA class SklearnPreprocessor ( BasePreprocessor ): \"\"\"Wrap scikit-learn preprocessing pipeline.\"\"\" def __init__ ( self , steps = None ): super () . __init__ ( steps = steps or []) self . pipeline = Pipeline ( steps or []) def fit ( self , X : pd . DataFrame , y : pd . Series = None ): \"\"\"Fit sklearn pipeline.\"\"\" self . pipeline . fit ( X , y ) def transform ( self , X : pd . DataFrame ) -> pd . DataFrame : \"\"\"Transform using sklearn pipeline.\"\"\" result = self . pipeline . transform ( X ) return pd . DataFrame ( result , index = X . index ) # Usage with sklearn pipeline sklearn_steps = [ ( 'scaler' , RobustScaler ()), ( 'pca' , PCA ( n_components = 50 )) ] preprocessor = SklearnPreprocessor ( steps = sklearn_steps ) pipeline = TabularPipeline ( model_name = 'TabICL' , processor_params = { 'custom_preprocessor' : preprocessor } ) 7. Validation and Monitoring \u00b6 7.1 Validation Framework \u00b6 class ValidatingPreprocessor ( BasePreprocessor ): \"\"\"Preprocessor with validation.\"\"\" def __init__ ( self , validators = None ): super () . __init__ ( validators = validators or []) self . validators = validators def validate ( self , X : pd . DataFrame ) -> dict : \"\"\"Run validators and return results.\"\"\" results = {} for validator in self . validators : results [ validator . name ] = validator ( X ) return results def fit ( self , X : pd . DataFrame , y : pd . Series = None ): \"\"\"Fit with validation.\"\"\" validation_results = self . validate ( X ) for name , passed in validation_results . items (): if not passed : print ( f \"\u26a0\ufe0f Validation failed: { name } \" ) 7.2 Data Quality Checks \u00b6 class DataQualityValidator : \"\"\"Validate data quality before preprocessing.\"\"\" def __init__ ( self , name , check_func ): self . name = name self . check_func = check_func def __call__ ( self , X : pd . DataFrame ) -> bool : \"\"\"Run validation check.\"\"\" return self . check_func ( X ) # Define checks no_all_nulls = DataQualityValidator ( 'no_all_nulls' , lambda X : not X . isnull () . all () . any () ) sufficient_samples = DataQualityValidator ( 'sufficient_samples' , lambda X : len ( X ) >= 100 ) numeric_columns_exist = DataQualityValidator ( 'numeric_columns' , lambda X : X . select_dtypes ( include = [ np . number ]) . shape [ 1 ] > 0 ) 8. Performance Optimization \u00b6 8.1 Caching Transformed Data \u00b6 from functools import lru_cache import hashlib class CachedPreprocessor ( BasePreprocessor ): \"\"\"Cache preprocessor outputs.\"\"\" def __init__ ( self , cache_size = 128 ): super () . __init__ ( cache_size = cache_size ) self . cache = {} self . cache_size = cache_size def _get_cache_key ( self , X : pd . DataFrame ) -> str : \"\"\"Generate cache key from data hash.\"\"\" data_hash = hashlib . md5 ( pd . util . hash_pandas_object ( X ) . values ) . hexdigest () return data_hash def transform ( self , X : pd . DataFrame ) -> pd . DataFrame : \"\"\"Transform with caching.\"\"\" cache_key = self . _get_cache_key ( X ) if cache_key in self . cache : return self . cache [ cache_key ] # Perform transformation result = self . _transform_impl ( X ) # Cache result if space available if len ( self . cache ) < self . cache_size : self . cache [ cache_key ] = result return result def _transform_impl ( self , X : pd . DataFrame ) -> pd . DataFrame : \"\"\"Override this for your transformation.\"\"\" return X 8.2 Parallel Processing \u00b6 from joblib import Parallel , delayed class ParallelPreprocessor ( BasePreprocessor ): \"\"\"Apply preprocessing in parallel.\"\"\" def __init__ ( self , n_jobs =- 1 , batch_size = 1000 ): super () . __init__ ( n_jobs = n_jobs , batch_size = batch_size ) def transform ( self , X : pd . DataFrame ) -> pd . DataFrame : \"\"\"Transform in parallel batches.\"\"\" n_jobs = self . config [ 'n_jobs' ] batch_size = self . config [ 'batch_size' ] # Split into batches batches = [ X . iloc [ i : i + batch_size ] for i in range ( 0 , len ( X ), batch_size ) ] # Process in parallel results = Parallel ( n_jobs = n_jobs )( delayed ( self . _transform_batch )( batch ) for batch in batches ) return pd . concat ( results , ignore_index = True ) def _transform_batch ( self , batch : pd . DataFrame ) -> pd . DataFrame : \"\"\"Override for batch transformation.\"\"\" return batch 9. Testing Custom Preprocessors \u00b6 9.1 Unit Tests \u00b6 import unittest import pandas as pd import numpy as np class TestCustomPreprocessor ( unittest . TestCase ): \"\"\"Test custom preprocessor.\"\"\" def setUp ( self ): \"\"\"Create test data.\"\"\" self . X = pd . DataFrame ({ 'A' : [ 1 , 2 , 3 , 4 , 5 ], 'B' : [ 10 , 20 , 30 , 40 , 50 ], 'C' : [ 'x' , 'y' , 'x' , 'y' , 'z' ] }) self . y = pd . Series ([ 0 , 1 , 0 , 1 , 0 ]) def test_fit_transform ( self ): \"\"\"Test fit_transform.\"\"\" preprocessor = CustomFeatureEngineeringPreprocessor () result = preprocessor . fit_transform ( self . X , self . y ) self . assertEqual ( len ( result ), len ( self . X )) self . assertGreater ( result . shape [ 1 ], self . X . shape [ 1 ]) def test_transform_consistency ( self ): \"\"\"Test consistency across calls.\"\"\" preprocessor = CustomFeatureEngineeringPreprocessor () preprocessor . fit ( self . X , self . y ) result1 = preprocessor . transform ( self . X ) result2 = preprocessor . transform ( self . X ) pd . testing . assert_frame_equal ( result1 , result2 ) def test_no_data_leakage ( self ): \"\"\"Test train/test independence.\"\"\" X_train = self . X . iloc [: 3 ] X_test = self . X . iloc [ 3 :] preprocessor = CustomFeatureEngineeringPreprocessor () preprocessor . fit ( X_train ) result_test = preprocessor . transform ( X_test ) self . assertEqual ( len ( result_test ), len ( X_test )) if __name__ == '__main__' : unittest . main () 10. Best Practices \u00b6 \u2705 Do's \u00b6 \u2705 Inherit from BasePreprocessor \u2705 Implement fit() and transform() \u2705 Prevent data leakage (fit on train only) \u2705 Return DataFrames with proper indices \u2705 Document configuration parameters \u2705 Handle edge cases (empty data, NaNs) \u2705 Test thoroughly \u2705 Cache expensive computations \u274c Don'ts \u00b6 \u274c Don't modify input data in-place \u274c Don't fit on test data \u274c Don't hardcode column names \u274c Don't ignore NaN values silently \u274c Don't create state in transform() \u274c Don't forget to preserve index \u274c Don't skip error handling 11. Real-World Example: Complete Custom Pipeline \u00b6 class ComprehensivePreprocessor ( BasePreprocessor ): \"\"\"Complete preprocessing pipeline.\"\"\" def __init__ ( self ): super () . __init__ () self . numerical_cols = None self . categorical_cols = None self . transformers = {} def fit ( self , X : pd . DataFrame , y : pd . Series = None ): \"\"\"Fit all transformers.\"\"\" self . numerical_cols = X . select_dtypes ( include = [ np . number ] ) . columns . tolist () self . categorical_cols = X . select_dtypes ( include = [ 'object' , 'category' ] ) . columns . tolist () # Fit numerical transformer from sklearn.preprocessing import StandardScaler self . transformers [ 'scaler' ] = StandardScaler () self . transformers [ 'scaler' ] . fit ( X [ self . numerical_cols ]) # Fit categorical transformer from sklearn.preprocessing import LabelEncoder self . transformers [ 'encoders' ] = {} for col in self . categorical_cols : le = LabelEncoder () le . fit ( X [ col ] . astype ( str )) self . transformers [ 'encoders' ][ col ] = le def transform ( self , X : pd . DataFrame ) -> pd . DataFrame : \"\"\"Apply all transformations.\"\"\" X_transformed = pd . DataFrame ( index = X . index ) # Transform numerical scaled = self . transformers [ 'scaler' ] . transform ( X [ self . numerical_cols ] ) X_transformed [ self . numerical_cols ] = scaled # Transform categorical for col in self . categorical_cols : encoded = self . transformers [ 'encoders' ][ col ] . transform ( X [ col ] . astype ( str ) ) X_transformed [ col ] = encoded return X_transformed # Usage preprocessor = ComprehensivePreprocessor () preprocessor . fit ( X_train , y_train ) X_train_processed = preprocessor . transform ( X_train ) X_test_processed = preprocessor . transform ( X_test ) pipeline = TabularPipeline ( model_name = 'TabICL' , processor_params = { 'custom_preprocessor' : preprocessor } ) 12. Next Steps \u00b6 Data Processing - Standard preprocessing API Reference - DataProcessor API Examples - Full examples with custom preprocessing Extend TabTune's preprocessing with custom transformers tailored to your domain!","title":"Custom Preprocessing"},{"location":"advanced/custom-preprocessing/#custom-preprocessing-extending-tabtunes-data-pipeline","text":"This document explains how to create custom preprocessors, extend the data pipeline, and integrate domain-specific transformations with TabTune.","title":"Custom Preprocessing: Extending TabTune's Data Pipeline"},{"location":"advanced/custom-preprocessing/#1-introduction","text":"While TabTune provides comprehensive automatic preprocessing, you may need custom transformations for: Domain-specific feature engineering Specialized encoding for your data Integration with existing pipelines Research and experimentation Non-standard data types This guide shows how to extend TabTune's preprocessing architecture.","title":"1. Introduction"},{"location":"advanced/custom-preprocessing/#2-preprocessing-architecture","text":"","title":"2. Preprocessing Architecture"},{"location":"advanced/custom-preprocessing/#21-class-hierarchy","text":"BasePreprocessor (Abstract) \u251c\u2500\u2500 StandardPreprocessor (Default) \u251c\u2500\u2500 TabPFNPreprocessor \u251c\u2500\u2500 TabICLPreprocessor \u251c\u2500\u2500 MitraPreprocessor \u251c\u2500\u2500 ContextTabPreprocessor \u251c\u2500\u2500 TabDPTPreprocessor \u251c\u2500\u2500 OrionBixPreprocessor \u251c\u2500\u2500 OrionMSPPreprocessor \u2514\u2500\u2500 YourCustomPreprocessor","title":"2.1 Class Hierarchy"},{"location":"advanced/custom-preprocessing/#22-base-class-interface","text":"from abc import ABC , abstractmethod import pandas as pd class BasePreprocessor ( ABC ): \"\"\"Abstract base for all preprocessors.\"\"\" def __init__ ( self , ** kwargs ): self . config = kwargs @abstractmethod def fit ( self , X : pd . DataFrame , y : pd . Series = None ): \"\"\"Learn preprocessing parameters from data.\"\"\" pass @abstractmethod def transform ( self , X : pd . DataFrame ) -> pd . DataFrame : \"\"\"Apply preprocessing to data.\"\"\" pass def fit_transform ( self , X : pd . DataFrame , y : pd . Series = None ): \"\"\"Fit and transform in one call.\"\"\" self . fit ( X , y ) return self . transform ( X ) def get_config ( self ) -> dict : \"\"\"Return preprocessing configuration.\"\"\" return self . config","title":"2.2 Base Class Interface"},{"location":"advanced/custom-preprocessing/#3-creating-custom-preprocessors","text":"","title":"3. Creating Custom Preprocessors"},{"location":"advanced/custom-preprocessing/#31-simple-custom-preprocessor","text":"Create a basic preprocessor for specialized transformations: import pandas as pd import numpy as np from tabtune.preprocessing.base import BasePreprocessor from sklearn.preprocessing import StandardScaler class CustomFeatureEngineeringPreprocessor ( BasePreprocessor ): \"\"\"Custom preprocessor with feature engineering.\"\"\" def __init__ ( self , scale_numericals = True , create_interactions = True ): super () . __init__ ( scale_numericals = scale_numericals , create_interactions = create_interactions ) self . scaler = StandardScaler () self . numerical_cols = None self . categorical_cols = None def fit ( self , X : pd . DataFrame , y : pd . Series = None ): \"\"\"Learn column types and scaling.\"\"\" # Identify numerical and categorical columns self . numerical_cols = X . select_dtypes ( include = [ np . number ] ) . columns . tolist () self . categorical_cols = X . select_dtypes ( include = [ 'object' , 'category' ] ) . columns . tolist () # Fit scaler on numerical columns if self . config [ 'scale_numericals' ]: self . scaler . fit ( X [ self . numerical_cols ]) def transform ( self , X : pd . DataFrame ) -> pd . DataFrame : \"\"\"Apply transformations.\"\"\" X_transformed = X . copy () # Scale numerical features if self . config [ 'scale_numericals' ]: X_transformed [ self . numerical_cols ] = self . scaler . transform ( X [ self . numerical_cols ] ) # Create interaction features if self . config [ 'create_interactions' ]: X_transformed = self . _create_interactions ( X_transformed ) return X_transformed def _create_interactions ( self , X : pd . DataFrame ) -> pd . DataFrame : \"\"\"Create polynomial interaction features.\"\"\" if len ( self . numerical_cols ) >= 2 : # Create pairwise interactions for i , col1 in enumerate ( self . numerical_cols ): for col2 in self . numerical_cols [ i + 1 :]: X [ f ' { col1 } _x_ { col2 } ' ] = X [ col1 ] * X [ col2 ] return X","title":"3.1 Simple Custom Preprocessor"},{"location":"advanced/custom-preprocessing/#32-domain-specific-preprocessor-finance-example","text":"import pandas as pd import numpy as np from tabtune.preprocessing.base import BasePreprocessor class FinancialDataPreprocessor ( BasePreprocessor ): \"\"\"Specialized preprocessor for financial data.\"\"\" def __init__ ( self , handle_outliers = True , create_ratios = True , normalize_by_scale = True ): super () . __init__ ( handle_outliers = handle_outliers , create_ratios = create_ratios , normalize_by_scale = normalize_by_scale ) self . outlier_bounds = {} self . scaling_factors = {} def fit ( self , X : pd . DataFrame , y : pd . Series = None ): \"\"\"Learn financial data patterns.\"\"\" # Detect outliers (IQR method) if self . config [ 'handle_outliers' ]: for col in X . select_dtypes ( include = [ np . number ]): Q1 = X [ col ] . quantile ( 0.25 ) Q3 = X [ col ] . quantile ( 0.75 ) IQR = Q3 - Q1 self . outlier_bounds [ col ] = { 'lower' : Q1 - 1.5 * IQR , 'upper' : Q3 + 1.5 * IQR } # Learn scaling factors by sector if self . config [ 'normalize_by_scale' ]: if 'sector' in X . columns : for sector in X [ 'sector' ] . unique (): sector_data = X [ X [ 'sector' ] == sector ] self . scaling_factors [ sector ] = sector_data . mean () def transform ( self , X : pd . DataFrame ) -> pd . DataFrame : \"\"\"Apply financial transformations.\"\"\" X_transformed = X . copy () # Handle outliers with clipping if self . config [ 'handle_outliers' ]: for col , bounds in self . outlier_bounds . items (): X_transformed [ col ] = X_transformed [ col ] . clip ( lower = bounds [ 'lower' ], upper = bounds [ 'upper' ] ) # Create financial ratios if self . config [ 'create_ratios' ]: if 'revenue' in X_transformed . columns and 'costs' in X_transformed . columns : X_transformed [ 'profit_margin' ] = ( X_transformed [ 'revenue' ] - X_transformed [ 'costs' ] ) / X_transformed [ 'revenue' ] # Normalize by sector if self . config [ 'normalize_by_scale' ]: if 'sector' in X_transformed . columns : for sector , factors in self . scaling_factors . items (): sector_mask = X_transformed [ 'sector' ] == sector # Normalize numeric columns by sector mean for col in X_transformed . select_dtypes ( include = [ np . number ]): X_transformed . loc [ sector_mask , col ] /= factors [ col ] return X_transformed","title":"3.2 Domain-Specific Preprocessor (Finance Example)"},{"location":"advanced/custom-preprocessing/#4-integrating-custom-preprocessors","text":"","title":"4. Integrating Custom Preprocessors"},{"location":"advanced/custom-preprocessing/#41-register-custom-preprocessor","text":"# In your codebase or configuration file from tabtune.data_processor import DataProcessor from your_module import CustomFeatureEngineeringPreprocessor # Register custom preprocessor DataProcessor . register_preprocessor ( 'custom_features' , CustomFeatureEngineeringPreprocessor )","title":"4.1 Register Custom Preprocessor"},{"location":"advanced/custom-preprocessing/#42-use-custom-preprocessor-with-pipeline","text":"from tabtune import TabularPipeline # Use custom preprocessor pipeline = TabularPipeline ( model_name = 'TabICL' , tuning_strategy = 'base-ft' , processor_params = { 'preprocessor_type' : 'custom_features' , 'scale_numericals' : True , 'create_interactions' : True } ) pipeline . fit ( X_train , y_train )","title":"4.2 Use Custom Preprocessor with Pipeline"},{"location":"advanced/custom-preprocessing/#43-chaining-preprocessors","text":"Combine multiple preprocessors: class ChainedPreprocessor ( BasePreprocessor ): \"\"\"Sequentially apply multiple preprocessors.\"\"\" def __init__ ( self , preprocessors : list ): super () . __init__ ( preprocessors = preprocessors ) self . preprocessor_chain = preprocessors def fit ( self , X : pd . DataFrame , y : pd . Series = None ): \"\"\"Fit each preprocessor in chain.\"\"\" for preprocessor in self . preprocessor_chain : preprocessor . fit ( X , y ) X = preprocessor . transform ( X ) def transform ( self , X : pd . DataFrame ) -> pd . DataFrame : \"\"\"Apply all preprocessors in sequence.\"\"\" for preprocessor in self . preprocessor_chain : X = preprocessor . transform ( X ) return X # Usage preprocessors = [ CustomFeatureEngineeringPreprocessor ( create_interactions = True ), FinancialDataPreprocessor ( handle_outliers = True ), YourSpecializedPreprocessor () ] chained = ChainedPreprocessor ( preprocessors ) X_transformed = chained . fit_transform ( X_train , y_train )","title":"4.3 Chaining Preprocessors"},{"location":"advanced/custom-preprocessing/#5-feature-engineering-examples","text":"","title":"5. Feature Engineering Examples"},{"location":"advanced/custom-preprocessing/#51-polynomial-features","text":"from sklearn.preprocessing import PolynomialFeatures class PolynomialPreprocessor ( BasePreprocessor ): \"\"\"Add polynomial features.\"\"\" def __init__ ( self , degree = 2 , include_bias = False ): super () . __init__ ( degree = degree , include_bias = include_bias ) self . poly = PolynomialFeatures ( degree = degree , include_bias = include_bias ) self . feature_names = None def fit ( self , X : pd . DataFrame , y : pd . Series = None ): \"\"\"Fit polynomial transformer.\"\"\" numerical = X . select_dtypes ( include = [ np . number ]) self . poly . fit ( numerical ) self . feature_names = self . poly . get_feature_names_out ( numerical . columns ) def transform ( self , X : pd . DataFrame ) -> pd . DataFrame : \"\"\"Transform to polynomial features.\"\"\" numerical = X . select_dtypes ( include = [ np . number ]) poly_features = self . poly . transform ( numerical ) return pd . DataFrame ( poly_features , columns = self . feature_names , index = X . index )","title":"5.1 Polynomial Features"},{"location":"advanced/custom-preprocessing/#52-statistical-features","text":"class StatisticalFeaturePreprocessor ( BasePreprocessor ): \"\"\"Extract statistical features from groups.\"\"\" def __init__ ( self , groupby_col = None , agg_functions = None ): super () . __init__ ( groupby_col = groupby_col , agg_functions = agg_functions or [ 'mean' , 'std' , 'min' , 'max' ] ) def fit ( self , X : pd . DataFrame , y : pd . Series = None ): \"\"\"No fitting needed for statistical features.\"\"\" pass def transform ( self , X : pd . DataFrame ) -> pd . DataFrame : \"\"\"Create statistical aggregates.\"\"\" X_transformed = X . copy () if self . config [ 'groupby_col' ] and self . config [ 'groupby_col' ] in X : groupby_col = self . config [ 'groupby_col' ] # Aggregate statistics for col in X . select_dtypes ( include = [ np . number ]): for func in self . config [ 'agg_functions' ]: agg_values = X . groupby ( groupby_col )[ col ] . agg ( func ) X_transformed [ f ' { col } _ { func } _by_ { groupby_col } ' ] = ( X [ groupby_col ] . map ( agg_values ) ) return X_transformed","title":"5.2 Statistical Features"},{"location":"advanced/custom-preprocessing/#53-text-feature-extraction","text":"from sklearn.feature_extraction.text import TfidfVectorizer class TextFeaturePreprocessor ( BasePreprocessor ): \"\"\"Extract features from text columns.\"\"\" def __init__ ( self , text_columns = None , max_features = 100 ): super () . __init__ ( text_columns = text_columns , max_features = max_features ) self . vectorizers = {} def fit ( self , X : pd . DataFrame , y : pd . Series = None ): \"\"\"Fit TF-IDF vectorizers.\"\"\" text_cols = self . config [ 'text_columns' ] or [ col for col in X . columns if X [ col ] . dtype == 'object' ] for col in text_cols : vectorizer = TfidfVectorizer ( max_features = self . config [ 'max_features' ], lowercase = True ) vectorizer . fit ( X [ col ] . astype ( str )) self . vectorizers [ col ] = vectorizer def transform ( self , X : pd . DataFrame ) -> pd . DataFrame : \"\"\"Transform text to features.\"\"\" X_transformed = X . copy () for col , vectorizer in self . vectorizers . items (): tfidf_matrix = vectorizer . transform ( X [ col ] . astype ( str )) feature_names = vectorizer . get_feature_names_out () for i , fname in enumerate ( feature_names ): X_transformed [ f ' { col } _tfidf_ { fname } ' ] = ( tfidf_matrix [:, i ] . toarray () . flatten () ) return X_transformed","title":"5.3 Text Feature Extraction"},{"location":"advanced/custom-preprocessing/#6-integration-with-scikit-learn","text":"","title":"6. Integration with scikit-learn"},{"location":"advanced/custom-preprocessing/#61-use-scikit-learn-transformers","text":"from sklearn.pipeline import Pipeline from sklearn.preprocessing import StandardScaler , RobustScaler from sklearn.decomposition import PCA class SklearnPreprocessor ( BasePreprocessor ): \"\"\"Wrap scikit-learn preprocessing pipeline.\"\"\" def __init__ ( self , steps = None ): super () . __init__ ( steps = steps or []) self . pipeline = Pipeline ( steps or []) def fit ( self , X : pd . DataFrame , y : pd . Series = None ): \"\"\"Fit sklearn pipeline.\"\"\" self . pipeline . fit ( X , y ) def transform ( self , X : pd . DataFrame ) -> pd . DataFrame : \"\"\"Transform using sklearn pipeline.\"\"\" result = self . pipeline . transform ( X ) return pd . DataFrame ( result , index = X . index ) # Usage with sklearn pipeline sklearn_steps = [ ( 'scaler' , RobustScaler ()), ( 'pca' , PCA ( n_components = 50 )) ] preprocessor = SklearnPreprocessor ( steps = sklearn_steps ) pipeline = TabularPipeline ( model_name = 'TabICL' , processor_params = { 'custom_preprocessor' : preprocessor } )","title":"6.1 Use scikit-learn Transformers"},{"location":"advanced/custom-preprocessing/#7-validation-and-monitoring","text":"","title":"7. Validation and Monitoring"},{"location":"advanced/custom-preprocessing/#71-validation-framework","text":"class ValidatingPreprocessor ( BasePreprocessor ): \"\"\"Preprocessor with validation.\"\"\" def __init__ ( self , validators = None ): super () . __init__ ( validators = validators or []) self . validators = validators def validate ( self , X : pd . DataFrame ) -> dict : \"\"\"Run validators and return results.\"\"\" results = {} for validator in self . validators : results [ validator . name ] = validator ( X ) return results def fit ( self , X : pd . DataFrame , y : pd . Series = None ): \"\"\"Fit with validation.\"\"\" validation_results = self . validate ( X ) for name , passed in validation_results . items (): if not passed : print ( f \"\u26a0\ufe0f Validation failed: { name } \" )","title":"7.1 Validation Framework"},{"location":"advanced/custom-preprocessing/#72-data-quality-checks","text":"class DataQualityValidator : \"\"\"Validate data quality before preprocessing.\"\"\" def __init__ ( self , name , check_func ): self . name = name self . check_func = check_func def __call__ ( self , X : pd . DataFrame ) -> bool : \"\"\"Run validation check.\"\"\" return self . check_func ( X ) # Define checks no_all_nulls = DataQualityValidator ( 'no_all_nulls' , lambda X : not X . isnull () . all () . any () ) sufficient_samples = DataQualityValidator ( 'sufficient_samples' , lambda X : len ( X ) >= 100 ) numeric_columns_exist = DataQualityValidator ( 'numeric_columns' , lambda X : X . select_dtypes ( include = [ np . number ]) . shape [ 1 ] > 0 )","title":"7.2 Data Quality Checks"},{"location":"advanced/custom-preprocessing/#8-performance-optimization","text":"","title":"8. Performance Optimization"},{"location":"advanced/custom-preprocessing/#81-caching-transformed-data","text":"from functools import lru_cache import hashlib class CachedPreprocessor ( BasePreprocessor ): \"\"\"Cache preprocessor outputs.\"\"\" def __init__ ( self , cache_size = 128 ): super () . __init__ ( cache_size = cache_size ) self . cache = {} self . cache_size = cache_size def _get_cache_key ( self , X : pd . DataFrame ) -> str : \"\"\"Generate cache key from data hash.\"\"\" data_hash = hashlib . md5 ( pd . util . hash_pandas_object ( X ) . values ) . hexdigest () return data_hash def transform ( self , X : pd . DataFrame ) -> pd . DataFrame : \"\"\"Transform with caching.\"\"\" cache_key = self . _get_cache_key ( X ) if cache_key in self . cache : return self . cache [ cache_key ] # Perform transformation result = self . _transform_impl ( X ) # Cache result if space available if len ( self . cache ) < self . cache_size : self . cache [ cache_key ] = result return result def _transform_impl ( self , X : pd . DataFrame ) -> pd . DataFrame : \"\"\"Override this for your transformation.\"\"\" return X","title":"8.1 Caching Transformed Data"},{"location":"advanced/custom-preprocessing/#82-parallel-processing","text":"from joblib import Parallel , delayed class ParallelPreprocessor ( BasePreprocessor ): \"\"\"Apply preprocessing in parallel.\"\"\" def __init__ ( self , n_jobs =- 1 , batch_size = 1000 ): super () . __init__ ( n_jobs = n_jobs , batch_size = batch_size ) def transform ( self , X : pd . DataFrame ) -> pd . DataFrame : \"\"\"Transform in parallel batches.\"\"\" n_jobs = self . config [ 'n_jobs' ] batch_size = self . config [ 'batch_size' ] # Split into batches batches = [ X . iloc [ i : i + batch_size ] for i in range ( 0 , len ( X ), batch_size ) ] # Process in parallel results = Parallel ( n_jobs = n_jobs )( delayed ( self . _transform_batch )( batch ) for batch in batches ) return pd . concat ( results , ignore_index = True ) def _transform_batch ( self , batch : pd . DataFrame ) -> pd . DataFrame : \"\"\"Override for batch transformation.\"\"\" return batch","title":"8.2 Parallel Processing"},{"location":"advanced/custom-preprocessing/#9-testing-custom-preprocessors","text":"","title":"9. Testing Custom Preprocessors"},{"location":"advanced/custom-preprocessing/#91-unit-tests","text":"import unittest import pandas as pd import numpy as np class TestCustomPreprocessor ( unittest . TestCase ): \"\"\"Test custom preprocessor.\"\"\" def setUp ( self ): \"\"\"Create test data.\"\"\" self . X = pd . DataFrame ({ 'A' : [ 1 , 2 , 3 , 4 , 5 ], 'B' : [ 10 , 20 , 30 , 40 , 50 ], 'C' : [ 'x' , 'y' , 'x' , 'y' , 'z' ] }) self . y = pd . Series ([ 0 , 1 , 0 , 1 , 0 ]) def test_fit_transform ( self ): \"\"\"Test fit_transform.\"\"\" preprocessor = CustomFeatureEngineeringPreprocessor () result = preprocessor . fit_transform ( self . X , self . y ) self . assertEqual ( len ( result ), len ( self . X )) self . assertGreater ( result . shape [ 1 ], self . X . shape [ 1 ]) def test_transform_consistency ( self ): \"\"\"Test consistency across calls.\"\"\" preprocessor = CustomFeatureEngineeringPreprocessor () preprocessor . fit ( self . X , self . y ) result1 = preprocessor . transform ( self . X ) result2 = preprocessor . transform ( self . X ) pd . testing . assert_frame_equal ( result1 , result2 ) def test_no_data_leakage ( self ): \"\"\"Test train/test independence.\"\"\" X_train = self . X . iloc [: 3 ] X_test = self . X . iloc [ 3 :] preprocessor = CustomFeatureEngineeringPreprocessor () preprocessor . fit ( X_train ) result_test = preprocessor . transform ( X_test ) self . assertEqual ( len ( result_test ), len ( X_test )) if __name__ == '__main__' : unittest . main ()","title":"9.1 Unit Tests"},{"location":"advanced/custom-preprocessing/#10-best-practices","text":"","title":"10. Best Practices"},{"location":"advanced/custom-preprocessing/#dos","text":"\u2705 Inherit from BasePreprocessor \u2705 Implement fit() and transform() \u2705 Prevent data leakage (fit on train only) \u2705 Return DataFrames with proper indices \u2705 Document configuration parameters \u2705 Handle edge cases (empty data, NaNs) \u2705 Test thoroughly \u2705 Cache expensive computations","title":"\u2705 Do's"},{"location":"advanced/custom-preprocessing/#donts","text":"\u274c Don't modify input data in-place \u274c Don't fit on test data \u274c Don't hardcode column names \u274c Don't ignore NaN values silently \u274c Don't create state in transform() \u274c Don't forget to preserve index \u274c Don't skip error handling","title":"\u274c Don'ts"},{"location":"advanced/custom-preprocessing/#11-real-world-example-complete-custom-pipeline","text":"class ComprehensivePreprocessor ( BasePreprocessor ): \"\"\"Complete preprocessing pipeline.\"\"\" def __init__ ( self ): super () . __init__ () self . numerical_cols = None self . categorical_cols = None self . transformers = {} def fit ( self , X : pd . DataFrame , y : pd . Series = None ): \"\"\"Fit all transformers.\"\"\" self . numerical_cols = X . select_dtypes ( include = [ np . number ] ) . columns . tolist () self . categorical_cols = X . select_dtypes ( include = [ 'object' , 'category' ] ) . columns . tolist () # Fit numerical transformer from sklearn.preprocessing import StandardScaler self . transformers [ 'scaler' ] = StandardScaler () self . transformers [ 'scaler' ] . fit ( X [ self . numerical_cols ]) # Fit categorical transformer from sklearn.preprocessing import LabelEncoder self . transformers [ 'encoders' ] = {} for col in self . categorical_cols : le = LabelEncoder () le . fit ( X [ col ] . astype ( str )) self . transformers [ 'encoders' ][ col ] = le def transform ( self , X : pd . DataFrame ) -> pd . DataFrame : \"\"\"Apply all transformations.\"\"\" X_transformed = pd . DataFrame ( index = X . index ) # Transform numerical scaled = self . transformers [ 'scaler' ] . transform ( X [ self . numerical_cols ] ) X_transformed [ self . numerical_cols ] = scaled # Transform categorical for col in self . categorical_cols : encoded = self . transformers [ 'encoders' ][ col ] . transform ( X [ col ] . astype ( str ) ) X_transformed [ col ] = encoded return X_transformed # Usage preprocessor = ComprehensivePreprocessor () preprocessor . fit ( X_train , y_train ) X_train_processed = preprocessor . transform ( X_train ) X_test_processed = preprocessor . transform ( X_test ) pipeline = TabularPipeline ( model_name = 'TabICL' , processor_params = { 'custom_preprocessor' : preprocessor } )","title":"11. Real-World Example: Complete Custom Pipeline"},{"location":"advanced/custom-preprocessing/#12-next-steps","text":"Data Processing - Standard preprocessing API Reference - DataProcessor API Examples - Full examples with custom preprocessing Extend TabTune's preprocessing with custom transformers tailored to your domain!","title":"12. Next Steps"},{"location":"advanced/hyperparameter-tuning/","text":"Hyperparameter Tuning: Optimizing TabTune Model Performance \u00b6 This document provides comprehensive guidance on hyperparameter tuning for TabTune models, including strategies, tools, and best practices for finding optimal configurations. 1. Introduction \u00b6 Hyperparameter tuning is the process of systematically searching for the best model configuration to maximize performance on your specific task. This guide covers: Search Strategies : Grid search, random search, Bayesian optimization Hyperparameter Spaces : Ranges and distributions for each model Tuning Tools : Integration with Optuna, scikit-optimize, and hyperopt Best Practices : Efficient tuning workflows and validation strategies 2. Hyperparameter Landscape \u00b6 2.1 Tunable Hyperparameters by Model \u00b6 Model Critical Important Minor TabPFN epochs, lr temperature batch_size TabICL n_episodes, lr support_size, query_size, n_estimators norm_methods OrionMSP n_episodes, lr support_size, query_size n_estimators OrionBix n_episodes, lr support_size, query_size n_estimators TabDPT support_size, lr k_neighbors, num_layers temperature Mitra support_size, lr batch_size, num_layers d_model ContextTab epochs, lr warmup_steps text_encoder 2.2 Shared Hyperparameters \u00b6 shared_hparams = { 'learning_rate' : [ 1e-5 , 5e-5 , 1e-4 , 5e-4 ], 'epochs' : [ 1 , 3 , 5 , 10 ], 'batch_size' : [ 8 , 16 , 32 , 64 ], 'weight_decay' : [ 0.0 , 0.01 , 0.1 ], 'warmup_steps' : [ 0 , 100 , 500 , 1000 ] } # PEFT-specific peft_hparams = { 'r' : [ 2 , 4 , 8 , 16 ], 'lora_alpha' : [ 4 , 8 , 16 , 32 ], 'lora_dropout' : [ 0.0 , 0.05 , 0.1 , 0.2 ] } 3. Search Strategies \u00b6 3.1 Grid Search \u00b6 Systematic evaluation of all parameter combinations. Advantages : - \u2705 Exhaustive coverage - \u2705 Parallelize easily - \u2705 Reproducible Disadvantages : - \u274c Exponential complexity - \u274c Wasteful on large spaces - \u274c Poor scaling from sklearn.model_selection import ParameterGrid from tabtune import TabularPipeline # Define grid param_grid = { 'learning_rate' : [ 1e-5 , 2e-5 , 5e-5 ], 'epochs' : [ 3 , 5 ], 'batch_size' : [ 16 , 32 ] } # Grid search best_score = 0 best_params = None for params in ParameterGrid ( param_grid ): pipeline = TabularPipeline ( model_name = 'TabICL' , tuning_strategy = 'base-ft' , tuning_params = params ) pipeline . fit ( X_train , y_train ) score = pipeline . evaluate ( X_val , y_val )[ 'accuracy' ] if score > best_score : best_score = score best_params = params print ( f \"New best: { score : .4f } with { params } \" ) print ( f \" \\n Best parameters: { best_params } \" ) print ( f \"Best score: { best_score : .4f } \" ) 3.2 Random Search \u00b6 Random sampling from parameter distributions. Advantages : - \u2705 Covers parameter space more uniformly - \u2705 Scales well to large spaces - \u2705 Simple parallelization Disadvantages : - \u274c May miss optimal region - \u274c Less reproducible import numpy as np from scipy.stats import uniform , randint # Define distributions param_distributions = { 'learning_rate' : uniform ( 1e-5 , 1e-3 ), 'epochs' : randint ( 1 , 20 ), 'batch_size' : randint ( 8 , 128 ), 'weight_decay' : uniform ( 0.0 , 0.1 ) } # Random search n_iter = 20 best_score = 0 best_params = None for i in range ( n_iter ): # Sample random parameters params = { key : dist . rvs () for key , dist in param_distributions . items () } # Convert to integers params [ 'epochs' ] = int ( params [ 'epochs' ]) params [ 'batch_size' ] = int ( params [ 'batch_size' ]) pipeline = TabularPipeline ( model_name = 'TabICL' , tuning_strategy = 'base-ft' , tuning_params = params ) pipeline . fit ( X_train , y_train ) score = pipeline . evaluate ( X_val , y_val )[ 'accuracy' ] if score > best_score : best_score = score best_params = params print ( f \"Iteration { i + 1 } / { n_iter } : { score : .4f } \" ) print ( f \" \\n Best parameters: { best_params } \" ) 3.3 Bayesian Optimization with Optuna \u00b6 Intelligent search using Gaussian processes. Advantages : - \u2705 Intelligent sampling - \u2705 Few evaluations needed - \u2705 Adaptively explores promising regions Disadvantages : - \u274c More complex - \u274c Slower per iteration - \u274c Requires more setup import optuna from optuna.pruners import MedianPruner def objective ( trial ): \"\"\"Optuna objective function.\"\"\" # Suggest hyperparameters learning_rate = trial . suggest_float ( 'learning_rate' , 1e-5 , 1e-3 , log = True ) epochs = trial . suggest_int ( 'epochs' , 1 , 20 ) batch_size = trial . suggest_int ( 'batch_size' , 8 , 128 ) weight_decay = trial . suggest_float ( 'weight_decay' , 0.0 , 0.1 ) tuning_params = { 'device' : 'cuda' , 'learning_rate' : learning_rate , 'epochs' : epochs , 'batch_size' : batch_size , 'weight_decay' : weight_decay } # Train and evaluate pipeline = TabularPipeline ( model_name = 'TabICL' , tuning_strategy = 'base-ft' , tuning_params = tuning_params ) pipeline . fit ( X_train , y_train ) score = pipeline . evaluate ( X_val , y_val )[ 'accuracy' ] return score # Create study study = optuna . create_study ( direction = 'maximize' , pruner = MedianPruner () ) # Optimize study . optimize ( objective , n_trials = 50 , n_jobs = 1 ) # Get results print ( f \"Best score: { study . best_value : .4f } \" ) print ( f \"Best parameters: { study . best_params } \" ) 4. Model-Specific Tuning \u00b6 4.1 TabPFN Tuning \u00b6 Focus on inference parameters since base-ft is not primary use case: # Key hyperparameters tabpfn_hparams = { 'n_estimators' : [ 8 , 16 , 32 ], # Ensemble size 'softmax_temperature' : [ 0.5 , 0.9 , 1.5 ], # Confidence 'epochs' : [ 1 , 3 , 5 ], # If fine-tuning 'learning_rate' : [ 1e-5 , 2e-5 , 5e-5 ] # If fine-tuning } # Fine-tune only if needed pipeline = TabularPipeline ( model_name = 'TabPFN' , tuning_strategy = 'base-ft' , tuning_params = { 'device' : 'cuda' , 'epochs' : 3 , 'learning_rate' : 2e-5 } ) 4.2 TabICL/OrionMSP/OrionBix Tuning \u00b6 Optimize episodic training parameters: # Key hyperparameters tabicl_hparams = { 'support_size' : [ 24 , 48 , 96 ], # Context size 'query_size' : [ 16 , 32 , 64 ], # Query size 'n_episodes' : [ 500 , 1000 , 2000 ], # Training episodes 'learning_rate' : [ 1e-5 , 2e-5 , 5e-5 ], 'n_estimators' : [ 16 , 32 , 64 ] # Ensemble } # Recommended defaults best_config = { 'support_size' : 48 , 'query_size' : 32 , 'n_episodes' : 1000 , 'learning_rate' : 2e-5 , 'epochs' : 5 } 4.3 TabDPT Tuning \u00b6 Leverage pre-training and large context: # Key hyperparameters tabdpt_hparams = { 'support_size' : [ 512 , 1024 , 2048 ], # Large context 'query_size' : [ 128 , 256 , 512 ], 'k_neighbors' : [ 3 , 5 , 10 ], # k-NN context 'num_layers' : [ 2 , 4 , 8 ], # Architecture 'learning_rate' : [ 1e-5 , 2e-5 , 5e-5 ] } # Recommended for large datasets best_config = { 'support_size' : 1024 , 'query_size' : 256 , 'k_neighbors' : 5 , 'learning_rate' : 2e-5 , 'epochs' : 3 # Few due to pre-training } 4.4 Mitra Tuning \u00b6 Optimize 2D attention parameters: # Key hyperparameters mitra_hparams = { 'support_size' : [ 64 , 128 , 256 ], 'query_size' : [ 64 , 128 , 256 ], 'd_model' : [ 32 , 64 , 128 ], # Embedding dim 'num_layers' : [ 1 , 2 , 4 ], 'batch_size' : [ 2 , 4 , 8 ], # Must be small 'learning_rate' : [ 1e-5 , 2e-5 , 5e-5 ] } # Recommended best_config = { 'support_size' : 128 , 'query_size' : 128 , 'd_model' : 64 , 'num_layers' : 2 , 'batch_size' : 4 , # Critical: keep small 'learning_rate' : 1e-5 } 4.5 PEFT Tuning \u00b6 Optimize LoRA parameters: # LoRA hyperparameters peft_hparams = { 'r' : [ 2 , 4 , 8 , 16 ], 'lora_alpha' : [ 4 , 8 , 16 , 32 ], # Usually 2x rank 'lora_dropout' : [ 0.05 , 0.1 , 0.2 ], 'learning_rate' : [ 1e-4 , 2e-4 , 5e-4 ] # Higher than base-ft } # Recommended best_peft_config = { 'r' : 8 , 'lora_alpha' : 16 , 'lora_dropout' : 0.05 , 'learning_rate' : 2e-4 # 10x base-ft } 5. Cross-Validation Strategy \u00b6 5.1 k-Fold Cross-Validation \u00b6 from sklearn.model_selection import KFold import numpy as np def cross_validate_hyperparams ( X , y , model_name , params , k = 5 ): \"\"\"Evaluate hyperparameters via k-fold CV.\"\"\" kf = KFold ( n_splits = k , shuffle = True , random_state = 42 ) scores = [] for fold_idx , ( train_idx , val_idx ) in enumerate ( kf . split ( X )): X_train_fold = X . iloc [ train_idx ] X_val_fold = X . iloc [ val_idx ] y_train_fold = y . iloc [ train_idx ] y_val_fold = y . iloc [ val_idx ] # Train and evaluate pipeline = TabularPipeline ( model_name = model_name , tuning_strategy = 'base-ft' , tuning_params = params ) pipeline . fit ( X_train_fold , y_train_fold ) score = pipeline . evaluate ( X_val_fold , y_val_fold )[ 'accuracy' ] scores . append ( score ) print ( f \"Fold { fold_idx + 1 } / { k } : { score : .4f } \" ) mean_score = np . mean ( scores ) std_score = np . std ( scores ) print ( f \" \\n Mean: { mean_score : .4f } \u00b1 { std_score : .4f } \" ) return mean_score , std_score # Usage mean , std = cross_validate_hyperparams ( X_train , y_train , model_name = 'TabICL' , params = { 'epochs' : 5 , 'learning_rate' : 2e-5 }, k = 5 ) 5.2 Stratified k-Fold \u00b6 For imbalanced classification: from sklearn.model_selection import StratifiedKFold # Ensures class distribution preserved skf = StratifiedKFold ( n_splits = 5 , shuffle = True , random_state = 42 ) for train_idx , val_idx in skf . split ( X , y ): X_train_fold = X . iloc [ train_idx ] X_val_fold = X . iloc [ val_idx ] # ... training code 6. Efficient Tuning Workflows \u00b6 6.1 Cascading Search \u00b6 Start coarse, then fine-grained: # Stage 1: Coarse grid search stage1_params = { 'learning_rate' : [ 1e-5 , 5e-5 , 1e-4 ], 'epochs' : [ 3 , 5 , 10 ] } # Find best in stage 1 best_params_stage1 = grid_search ( stage1_params ) # Stage 2: Fine-grained around best stage2_params = { 'learning_rate' : [ best_params_stage1 [ 'learning_rate' ] / 2 , best_params_stage1 [ 'learning_rate' ], best_params_stage1 [ 'learning_rate' ] * 2 ], 'epochs' : [ best_params_stage1 [ 'epochs' ] - 1 , best_params_stage1 [ 'epochs' ], best_params_stage1 [ 'epochs' ] + 1 ] } # Find best in stage 2 best_params_stage2 = grid_search ( stage2_params ) 6.2 Early Stopping During Tuning \u00b6 class EarlyStoppingTuner : \"\"\"Tuner with early stopping.\"\"\" def __init__ ( self , patience = 3 ): self . patience = patience self . best_score = 0 self . no_improve_count = 0 def should_stop ( self , score ): \"\"\"Check if tuning should stop.\"\"\" if score > self . best_score : self . best_score = score self . no_improve_count = 0 return False else : self . no_improve_count += 1 return self . no_improve_count >= self . patience # Usage tuner = EarlyStoppingTuner ( patience = 5 ) for params in param_grid : # ... train and evaluate ... if tuner . should_stop ( score ): print ( f \"Early stopping after { tuner . no_improve_count } iterations\" ) break 7. Complete Tuning Examples \u00b6 7.1 Simple Grid Search with Leaderboard \u00b6 from tabtune import TabularLeaderboard # Define parameter grid param_grid = { 'learning_rate' : [ 1e-5 , 2e-5 , 5e-5 ], 'epochs' : [ 3 , 5 ], 'batch_size' : [ 16 , 32 ] } # Generate all combinations from itertools import product param_combinations = [ dict ( zip ( param_grid . keys (), values )) for values in product ( * param_grid . values ()) ] # Use leaderboard for comparison leaderboard = TabularLeaderboard ( X_train , X_val , y_train , y_val ) for i , params in enumerate ( param_combinations ): config_name = f \"Config_ { i + 1 } _ { params [ 'learning_rate' ] } _ { params [ 'epochs' ] } \" leaderboard . add_model ( 'TabICL' , 'base-ft' , name = config_name , tuning_params = params ) results = leaderboard . run ( rank_by = 'accuracy' ) print ( leaderboard . get_ranking ()) 7.2 Bayesian Optimization with Pruning \u00b6 import optuna def objective_with_pruning ( trial ): \"\"\"Objective with early pruning.\"\"\" learning_rate = trial . suggest_float ( 'lr' , 1e-5 , 1e-3 , log = True ) epochs = trial . suggest_int ( 'epochs' , 1 , 20 ) pipeline = TabularPipeline ( model_name = 'TabICL' , tuning_strategy = 'base-ft' , tuning_params = { 'learning_rate' : learning_rate , 'epochs' : epochs } ) # Train with intermediate reporting for epoch in range ( epochs ): # ... train for one epoch ... # Evaluate score = pipeline . evaluate ( X_val , y_val )[ 'accuracy' ] # Report intermediate value trial . report ( score , epoch ) # Prune if not promising if trial . should_prune (): raise optuna . TrialPruned () return score # Create study with pruning study = optuna . create_study ( direction = 'maximize' , pruner = optuna . pruners . MedianPruner () ) study . optimize ( objective_with_pruning , n_trials = 30 ) 7.3 Parallel Tuning with Ray \u00b6 import ray from ray import tune # Initialize Ray ray . init () def train_model ( config ): \"\"\"Trainable function for Ray.\"\"\" pipeline = TabularPipeline ( model_name = 'TabICL' , tuning_strategy = 'base-ft' , tuning_params = config ) pipeline . fit ( X_train , y_train ) metrics = pipeline . evaluate ( X_val , y_val ) return metrics # Parallel tuning results = tune . run ( train_model , config = { 'learning_rate' : tune . loguniform ( 1e-5 , 1e-3 ), 'epochs' : tune . randint ( 1 , 20 ), 'batch_size' : tune . choice ([ 16 , 32 , 64 ]) }, num_samples = 30 , verbose = 1 ) ray . shutdown () 8. Tuning Summary & Defaults \u00b6 8.1 Quick Reference Table \u00b6 Model Learning Rate Epochs Support Size Key Parameter TabPFN 2e-5 3-5 N/A n_estimators TabICL 2e-5 5 48 n_episodes OrionMSP 2e-5 5 1024 n_episodes OrionBix 2e-5 5 48 n_episodes TabDPT 2e-5 3 1024 k_neighbors Mitra 1e-5 3 128 batch_size ContextTab 1e-4 10 N/A warmup_steps PEFT 2e-4 5 48 rank (r) 8.2 Tuning Priority \u00b6 Learning Rate : 80% of impact Epochs : 10% of impact Batch/Support Size : 5% of impact Other : 5% of impact 9. Best Practices \u00b6 \u2705 Do's \u00b6 \u2705 Start with default hyperparameters \u2705 Use cross-validation for robustness \u2705 Tune on validation set, evaluate on test set \u2705 Focus on high-impact hyperparameters first \u2705 Use multiple seeds for stability \u2705 Log all experiments \u2705 Parallelize when possible \u274c Don'ts \u00b6 \u274c Don't tune on test set \u274c Don't use learning rate as only tunable parameter \u274c Don't ignore data size when choosing ranges \u274c Don't forget to freeze random seed \u274c Don't tune without validation set \u274c Don't skip early stopping 10. Common Pitfalls & Solutions \u00b6 Issue: \"Tuning is too slow\" \u00b6 Solution : - Use Bayesian optimization instead of grid search - Parallelize across cores - Use early stopping Issue: \"Best tuned model still overfits\" \u00b6 Solution : - Increase regularization (weight decay) - Use PEFT instead of base-ft - Reduce learning rate - Add dropout Issue: \"Tuning results don't transfer to test set\" \u00b6 Solution : - Use larger validation set - Use cross-validation - Don't overfit to validation set - Use proper hyperparameter ranges 11. Next Steps \u00b6 Tuning Strategies - Strategy details Model Selection - Choosing models TabularLeaderboard - Systematic comparison PEFT & LoRA - PEFT-specific tuning Systematic hyperparameter tuning unlocks the full potential of TabTune models!","title":"Hyperparameter Tuning"},{"location":"advanced/hyperparameter-tuning/#hyperparameter-tuning-optimizing-tabtune-model-performance","text":"This document provides comprehensive guidance on hyperparameter tuning for TabTune models, including strategies, tools, and best practices for finding optimal configurations.","title":"Hyperparameter Tuning: Optimizing TabTune Model Performance"},{"location":"advanced/hyperparameter-tuning/#1-introduction","text":"Hyperparameter tuning is the process of systematically searching for the best model configuration to maximize performance on your specific task. This guide covers: Search Strategies : Grid search, random search, Bayesian optimization Hyperparameter Spaces : Ranges and distributions for each model Tuning Tools : Integration with Optuna, scikit-optimize, and hyperopt Best Practices : Efficient tuning workflows and validation strategies","title":"1. Introduction"},{"location":"advanced/hyperparameter-tuning/#2-hyperparameter-landscape","text":"","title":"2. Hyperparameter Landscape"},{"location":"advanced/hyperparameter-tuning/#21-tunable-hyperparameters-by-model","text":"Model Critical Important Minor TabPFN epochs, lr temperature batch_size TabICL n_episodes, lr support_size, query_size, n_estimators norm_methods OrionMSP n_episodes, lr support_size, query_size n_estimators OrionBix n_episodes, lr support_size, query_size n_estimators TabDPT support_size, lr k_neighbors, num_layers temperature Mitra support_size, lr batch_size, num_layers d_model ContextTab epochs, lr warmup_steps text_encoder","title":"2.1 Tunable Hyperparameters by Model"},{"location":"advanced/hyperparameter-tuning/#22-shared-hyperparameters","text":"shared_hparams = { 'learning_rate' : [ 1e-5 , 5e-5 , 1e-4 , 5e-4 ], 'epochs' : [ 1 , 3 , 5 , 10 ], 'batch_size' : [ 8 , 16 , 32 , 64 ], 'weight_decay' : [ 0.0 , 0.01 , 0.1 ], 'warmup_steps' : [ 0 , 100 , 500 , 1000 ] } # PEFT-specific peft_hparams = { 'r' : [ 2 , 4 , 8 , 16 ], 'lora_alpha' : [ 4 , 8 , 16 , 32 ], 'lora_dropout' : [ 0.0 , 0.05 , 0.1 , 0.2 ] }","title":"2.2 Shared Hyperparameters"},{"location":"advanced/hyperparameter-tuning/#3-search-strategies","text":"","title":"3. Search Strategies"},{"location":"advanced/hyperparameter-tuning/#31-grid-search","text":"Systematic evaluation of all parameter combinations. Advantages : - \u2705 Exhaustive coverage - \u2705 Parallelize easily - \u2705 Reproducible Disadvantages : - \u274c Exponential complexity - \u274c Wasteful on large spaces - \u274c Poor scaling from sklearn.model_selection import ParameterGrid from tabtune import TabularPipeline # Define grid param_grid = { 'learning_rate' : [ 1e-5 , 2e-5 , 5e-5 ], 'epochs' : [ 3 , 5 ], 'batch_size' : [ 16 , 32 ] } # Grid search best_score = 0 best_params = None for params in ParameterGrid ( param_grid ): pipeline = TabularPipeline ( model_name = 'TabICL' , tuning_strategy = 'base-ft' , tuning_params = params ) pipeline . fit ( X_train , y_train ) score = pipeline . evaluate ( X_val , y_val )[ 'accuracy' ] if score > best_score : best_score = score best_params = params print ( f \"New best: { score : .4f } with { params } \" ) print ( f \" \\n Best parameters: { best_params } \" ) print ( f \"Best score: { best_score : .4f } \" )","title":"3.1 Grid Search"},{"location":"advanced/hyperparameter-tuning/#32-random-search","text":"Random sampling from parameter distributions. Advantages : - \u2705 Covers parameter space more uniformly - \u2705 Scales well to large spaces - \u2705 Simple parallelization Disadvantages : - \u274c May miss optimal region - \u274c Less reproducible import numpy as np from scipy.stats import uniform , randint # Define distributions param_distributions = { 'learning_rate' : uniform ( 1e-5 , 1e-3 ), 'epochs' : randint ( 1 , 20 ), 'batch_size' : randint ( 8 , 128 ), 'weight_decay' : uniform ( 0.0 , 0.1 ) } # Random search n_iter = 20 best_score = 0 best_params = None for i in range ( n_iter ): # Sample random parameters params = { key : dist . rvs () for key , dist in param_distributions . items () } # Convert to integers params [ 'epochs' ] = int ( params [ 'epochs' ]) params [ 'batch_size' ] = int ( params [ 'batch_size' ]) pipeline = TabularPipeline ( model_name = 'TabICL' , tuning_strategy = 'base-ft' , tuning_params = params ) pipeline . fit ( X_train , y_train ) score = pipeline . evaluate ( X_val , y_val )[ 'accuracy' ] if score > best_score : best_score = score best_params = params print ( f \"Iteration { i + 1 } / { n_iter } : { score : .4f } \" ) print ( f \" \\n Best parameters: { best_params } \" )","title":"3.2 Random Search"},{"location":"advanced/hyperparameter-tuning/#33-bayesian-optimization-with-optuna","text":"Intelligent search using Gaussian processes. Advantages : - \u2705 Intelligent sampling - \u2705 Few evaluations needed - \u2705 Adaptively explores promising regions Disadvantages : - \u274c More complex - \u274c Slower per iteration - \u274c Requires more setup import optuna from optuna.pruners import MedianPruner def objective ( trial ): \"\"\"Optuna objective function.\"\"\" # Suggest hyperparameters learning_rate = trial . suggest_float ( 'learning_rate' , 1e-5 , 1e-3 , log = True ) epochs = trial . suggest_int ( 'epochs' , 1 , 20 ) batch_size = trial . suggest_int ( 'batch_size' , 8 , 128 ) weight_decay = trial . suggest_float ( 'weight_decay' , 0.0 , 0.1 ) tuning_params = { 'device' : 'cuda' , 'learning_rate' : learning_rate , 'epochs' : epochs , 'batch_size' : batch_size , 'weight_decay' : weight_decay } # Train and evaluate pipeline = TabularPipeline ( model_name = 'TabICL' , tuning_strategy = 'base-ft' , tuning_params = tuning_params ) pipeline . fit ( X_train , y_train ) score = pipeline . evaluate ( X_val , y_val )[ 'accuracy' ] return score # Create study study = optuna . create_study ( direction = 'maximize' , pruner = MedianPruner () ) # Optimize study . optimize ( objective , n_trials = 50 , n_jobs = 1 ) # Get results print ( f \"Best score: { study . best_value : .4f } \" ) print ( f \"Best parameters: { study . best_params } \" )","title":"3.3 Bayesian Optimization with Optuna"},{"location":"advanced/hyperparameter-tuning/#4-model-specific-tuning","text":"","title":"4. Model-Specific Tuning"},{"location":"advanced/hyperparameter-tuning/#41-tabpfn-tuning","text":"Focus on inference parameters since base-ft is not primary use case: # Key hyperparameters tabpfn_hparams = { 'n_estimators' : [ 8 , 16 , 32 ], # Ensemble size 'softmax_temperature' : [ 0.5 , 0.9 , 1.5 ], # Confidence 'epochs' : [ 1 , 3 , 5 ], # If fine-tuning 'learning_rate' : [ 1e-5 , 2e-5 , 5e-5 ] # If fine-tuning } # Fine-tune only if needed pipeline = TabularPipeline ( model_name = 'TabPFN' , tuning_strategy = 'base-ft' , tuning_params = { 'device' : 'cuda' , 'epochs' : 3 , 'learning_rate' : 2e-5 } )","title":"4.1 TabPFN Tuning"},{"location":"advanced/hyperparameter-tuning/#42-tabiclorionmsporionbix-tuning","text":"Optimize episodic training parameters: # Key hyperparameters tabicl_hparams = { 'support_size' : [ 24 , 48 , 96 ], # Context size 'query_size' : [ 16 , 32 , 64 ], # Query size 'n_episodes' : [ 500 , 1000 , 2000 ], # Training episodes 'learning_rate' : [ 1e-5 , 2e-5 , 5e-5 ], 'n_estimators' : [ 16 , 32 , 64 ] # Ensemble } # Recommended defaults best_config = { 'support_size' : 48 , 'query_size' : 32 , 'n_episodes' : 1000 , 'learning_rate' : 2e-5 , 'epochs' : 5 }","title":"4.2 TabICL/OrionMSP/OrionBix Tuning"},{"location":"advanced/hyperparameter-tuning/#43-tabdpt-tuning","text":"Leverage pre-training and large context: # Key hyperparameters tabdpt_hparams = { 'support_size' : [ 512 , 1024 , 2048 ], # Large context 'query_size' : [ 128 , 256 , 512 ], 'k_neighbors' : [ 3 , 5 , 10 ], # k-NN context 'num_layers' : [ 2 , 4 , 8 ], # Architecture 'learning_rate' : [ 1e-5 , 2e-5 , 5e-5 ] } # Recommended for large datasets best_config = { 'support_size' : 1024 , 'query_size' : 256 , 'k_neighbors' : 5 , 'learning_rate' : 2e-5 , 'epochs' : 3 # Few due to pre-training }","title":"4.3 TabDPT Tuning"},{"location":"advanced/hyperparameter-tuning/#44-mitra-tuning","text":"Optimize 2D attention parameters: # Key hyperparameters mitra_hparams = { 'support_size' : [ 64 , 128 , 256 ], 'query_size' : [ 64 , 128 , 256 ], 'd_model' : [ 32 , 64 , 128 ], # Embedding dim 'num_layers' : [ 1 , 2 , 4 ], 'batch_size' : [ 2 , 4 , 8 ], # Must be small 'learning_rate' : [ 1e-5 , 2e-5 , 5e-5 ] } # Recommended best_config = { 'support_size' : 128 , 'query_size' : 128 , 'd_model' : 64 , 'num_layers' : 2 , 'batch_size' : 4 , # Critical: keep small 'learning_rate' : 1e-5 }","title":"4.4 Mitra Tuning"},{"location":"advanced/hyperparameter-tuning/#45-peft-tuning","text":"Optimize LoRA parameters: # LoRA hyperparameters peft_hparams = { 'r' : [ 2 , 4 , 8 , 16 ], 'lora_alpha' : [ 4 , 8 , 16 , 32 ], # Usually 2x rank 'lora_dropout' : [ 0.05 , 0.1 , 0.2 ], 'learning_rate' : [ 1e-4 , 2e-4 , 5e-4 ] # Higher than base-ft } # Recommended best_peft_config = { 'r' : 8 , 'lora_alpha' : 16 , 'lora_dropout' : 0.05 , 'learning_rate' : 2e-4 # 10x base-ft }","title":"4.5 PEFT Tuning"},{"location":"advanced/hyperparameter-tuning/#5-cross-validation-strategy","text":"","title":"5. Cross-Validation Strategy"},{"location":"advanced/hyperparameter-tuning/#51-k-fold-cross-validation","text":"from sklearn.model_selection import KFold import numpy as np def cross_validate_hyperparams ( X , y , model_name , params , k = 5 ): \"\"\"Evaluate hyperparameters via k-fold CV.\"\"\" kf = KFold ( n_splits = k , shuffle = True , random_state = 42 ) scores = [] for fold_idx , ( train_idx , val_idx ) in enumerate ( kf . split ( X )): X_train_fold = X . iloc [ train_idx ] X_val_fold = X . iloc [ val_idx ] y_train_fold = y . iloc [ train_idx ] y_val_fold = y . iloc [ val_idx ] # Train and evaluate pipeline = TabularPipeline ( model_name = model_name , tuning_strategy = 'base-ft' , tuning_params = params ) pipeline . fit ( X_train_fold , y_train_fold ) score = pipeline . evaluate ( X_val_fold , y_val_fold )[ 'accuracy' ] scores . append ( score ) print ( f \"Fold { fold_idx + 1 } / { k } : { score : .4f } \" ) mean_score = np . mean ( scores ) std_score = np . std ( scores ) print ( f \" \\n Mean: { mean_score : .4f } \u00b1 { std_score : .4f } \" ) return mean_score , std_score # Usage mean , std = cross_validate_hyperparams ( X_train , y_train , model_name = 'TabICL' , params = { 'epochs' : 5 , 'learning_rate' : 2e-5 }, k = 5 )","title":"5.1 k-Fold Cross-Validation"},{"location":"advanced/hyperparameter-tuning/#52-stratified-k-fold","text":"For imbalanced classification: from sklearn.model_selection import StratifiedKFold # Ensures class distribution preserved skf = StratifiedKFold ( n_splits = 5 , shuffle = True , random_state = 42 ) for train_idx , val_idx in skf . split ( X , y ): X_train_fold = X . iloc [ train_idx ] X_val_fold = X . iloc [ val_idx ] # ... training code","title":"5.2 Stratified k-Fold"},{"location":"advanced/hyperparameter-tuning/#6-efficient-tuning-workflows","text":"","title":"6. Efficient Tuning Workflows"},{"location":"advanced/hyperparameter-tuning/#61-cascading-search","text":"Start coarse, then fine-grained: # Stage 1: Coarse grid search stage1_params = { 'learning_rate' : [ 1e-5 , 5e-5 , 1e-4 ], 'epochs' : [ 3 , 5 , 10 ] } # Find best in stage 1 best_params_stage1 = grid_search ( stage1_params ) # Stage 2: Fine-grained around best stage2_params = { 'learning_rate' : [ best_params_stage1 [ 'learning_rate' ] / 2 , best_params_stage1 [ 'learning_rate' ], best_params_stage1 [ 'learning_rate' ] * 2 ], 'epochs' : [ best_params_stage1 [ 'epochs' ] - 1 , best_params_stage1 [ 'epochs' ], best_params_stage1 [ 'epochs' ] + 1 ] } # Find best in stage 2 best_params_stage2 = grid_search ( stage2_params )","title":"6.1 Cascading Search"},{"location":"advanced/hyperparameter-tuning/#62-early-stopping-during-tuning","text":"class EarlyStoppingTuner : \"\"\"Tuner with early stopping.\"\"\" def __init__ ( self , patience = 3 ): self . patience = patience self . best_score = 0 self . no_improve_count = 0 def should_stop ( self , score ): \"\"\"Check if tuning should stop.\"\"\" if score > self . best_score : self . best_score = score self . no_improve_count = 0 return False else : self . no_improve_count += 1 return self . no_improve_count >= self . patience # Usage tuner = EarlyStoppingTuner ( patience = 5 ) for params in param_grid : # ... train and evaluate ... if tuner . should_stop ( score ): print ( f \"Early stopping after { tuner . no_improve_count } iterations\" ) break","title":"6.2 Early Stopping During Tuning"},{"location":"advanced/hyperparameter-tuning/#7-complete-tuning-examples","text":"","title":"7. Complete Tuning Examples"},{"location":"advanced/hyperparameter-tuning/#71-simple-grid-search-with-leaderboard","text":"from tabtune import TabularLeaderboard # Define parameter grid param_grid = { 'learning_rate' : [ 1e-5 , 2e-5 , 5e-5 ], 'epochs' : [ 3 , 5 ], 'batch_size' : [ 16 , 32 ] } # Generate all combinations from itertools import product param_combinations = [ dict ( zip ( param_grid . keys (), values )) for values in product ( * param_grid . values ()) ] # Use leaderboard for comparison leaderboard = TabularLeaderboard ( X_train , X_val , y_train , y_val ) for i , params in enumerate ( param_combinations ): config_name = f \"Config_ { i + 1 } _ { params [ 'learning_rate' ] } _ { params [ 'epochs' ] } \" leaderboard . add_model ( 'TabICL' , 'base-ft' , name = config_name , tuning_params = params ) results = leaderboard . run ( rank_by = 'accuracy' ) print ( leaderboard . get_ranking ())","title":"7.1 Simple Grid Search with Leaderboard"},{"location":"advanced/hyperparameter-tuning/#72-bayesian-optimization-with-pruning","text":"import optuna def objective_with_pruning ( trial ): \"\"\"Objective with early pruning.\"\"\" learning_rate = trial . suggest_float ( 'lr' , 1e-5 , 1e-3 , log = True ) epochs = trial . suggest_int ( 'epochs' , 1 , 20 ) pipeline = TabularPipeline ( model_name = 'TabICL' , tuning_strategy = 'base-ft' , tuning_params = { 'learning_rate' : learning_rate , 'epochs' : epochs } ) # Train with intermediate reporting for epoch in range ( epochs ): # ... train for one epoch ... # Evaluate score = pipeline . evaluate ( X_val , y_val )[ 'accuracy' ] # Report intermediate value trial . report ( score , epoch ) # Prune if not promising if trial . should_prune (): raise optuna . TrialPruned () return score # Create study with pruning study = optuna . create_study ( direction = 'maximize' , pruner = optuna . pruners . MedianPruner () ) study . optimize ( objective_with_pruning , n_trials = 30 )","title":"7.2 Bayesian Optimization with Pruning"},{"location":"advanced/hyperparameter-tuning/#73-parallel-tuning-with-ray","text":"import ray from ray import tune # Initialize Ray ray . init () def train_model ( config ): \"\"\"Trainable function for Ray.\"\"\" pipeline = TabularPipeline ( model_name = 'TabICL' , tuning_strategy = 'base-ft' , tuning_params = config ) pipeline . fit ( X_train , y_train ) metrics = pipeline . evaluate ( X_val , y_val ) return metrics # Parallel tuning results = tune . run ( train_model , config = { 'learning_rate' : tune . loguniform ( 1e-5 , 1e-3 ), 'epochs' : tune . randint ( 1 , 20 ), 'batch_size' : tune . choice ([ 16 , 32 , 64 ]) }, num_samples = 30 , verbose = 1 ) ray . shutdown ()","title":"7.3 Parallel Tuning with Ray"},{"location":"advanced/hyperparameter-tuning/#8-tuning-summary-defaults","text":"","title":"8. Tuning Summary &amp; Defaults"},{"location":"advanced/hyperparameter-tuning/#81-quick-reference-table","text":"Model Learning Rate Epochs Support Size Key Parameter TabPFN 2e-5 3-5 N/A n_estimators TabICL 2e-5 5 48 n_episodes OrionMSP 2e-5 5 1024 n_episodes OrionBix 2e-5 5 48 n_episodes TabDPT 2e-5 3 1024 k_neighbors Mitra 1e-5 3 128 batch_size ContextTab 1e-4 10 N/A warmup_steps PEFT 2e-4 5 48 rank (r)","title":"8.1 Quick Reference Table"},{"location":"advanced/hyperparameter-tuning/#82-tuning-priority","text":"Learning Rate : 80% of impact Epochs : 10% of impact Batch/Support Size : 5% of impact Other : 5% of impact","title":"8.2 Tuning Priority"},{"location":"advanced/hyperparameter-tuning/#9-best-practices","text":"","title":"9. Best Practices"},{"location":"advanced/hyperparameter-tuning/#dos","text":"\u2705 Start with default hyperparameters \u2705 Use cross-validation for robustness \u2705 Tune on validation set, evaluate on test set \u2705 Focus on high-impact hyperparameters first \u2705 Use multiple seeds for stability \u2705 Log all experiments \u2705 Parallelize when possible","title":"\u2705 Do's"},{"location":"advanced/hyperparameter-tuning/#donts","text":"\u274c Don't tune on test set \u274c Don't use learning rate as only tunable parameter \u274c Don't ignore data size when choosing ranges \u274c Don't forget to freeze random seed \u274c Don't tune without validation set \u274c Don't skip early stopping","title":"\u274c Don'ts"},{"location":"advanced/hyperparameter-tuning/#10-common-pitfalls-solutions","text":"","title":"10. Common Pitfalls &amp; Solutions"},{"location":"advanced/hyperparameter-tuning/#issue-tuning-is-too-slow","text":"Solution : - Use Bayesian optimization instead of grid search - Parallelize across cores - Use early stopping","title":"Issue: \"Tuning is too slow\""},{"location":"advanced/hyperparameter-tuning/#issue-best-tuned-model-still-overfits","text":"Solution : - Increase regularization (weight decay) - Use PEFT instead of base-ft - Reduce learning rate - Add dropout","title":"Issue: \"Best tuned model still overfits\""},{"location":"advanced/hyperparameter-tuning/#issue-tuning-results-dont-transfer-to-test-set","text":"Solution : - Use larger validation set - Use cross-validation - Don't overfit to validation set - Use proper hyperparameter ranges","title":"Issue: \"Tuning results don't transfer to test set\""},{"location":"advanced/hyperparameter-tuning/#11-next-steps","text":"Tuning Strategies - Strategy details Model Selection - Choosing models TabularLeaderboard - Systematic comparison PEFT & LoRA - PEFT-specific tuning Systematic hyperparameter tuning unlocks the full potential of TabTune models!","title":"11. Next Steps"},{"location":"advanced/memory-optimization/","text":"Memory Optimization: Techniques for Training Large Models \u00b6 This document provides comprehensive strategies for optimizing memory usage when training TabTune models, enabling efficient use of limited GPU/CPU resources. 1. Introduction \u00b6 Memory optimization is critical for: Training on GPUs with limited VRAM (< 8GB) Processing large datasets (100K+ rows) Using large model architectures Running multiple experiments simultaneously Deploying in resource-constrained environments This guide covers techniques, tools, and trade-offs for memory efficiency. 2. Memory Profiling \u00b6 2.1 Understanding Memory Usage \u00b6 Memory Breakdown (typical forward/backward pass): Model weights: 40-50% (frozen in PEFT) Optimizer states: 20-30% (momentum, variance) Gradients: 10-15% Activations: 10-20% (intermediate values) DataLoader buffers: 5-10% \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Total: 100% 2.2 Profiling Tools \u00b6 PyTorch Memory Profiler \u00b6 import torch from torch.profiler import profile , record_function , ProfilerActivity # Basic memory measurement torch . cuda . empty_cache () torch . cuda . reset_peak_memory_stats () # Your training code pipeline . fit ( X_train , y_train ) # Print memory stats print ( f \"Peak memory: { torch . cuda . max_memory_allocated () / 1e9 : .2f } GB\" ) print ( f \"Current memory: { torch . cuda . memory_allocated () / 1e9 : .2f } GB\" ) # Detailed profiling with profile ( activities = [ ProfilerActivity . CPU , ProfilerActivity . CUDA ], record_shapes = True ) as prof : pipeline . fit ( X_train , y_train ) print ( prof . key_averages () . table ( sort_by = \"cuda_memory_usage\" )) Memory Monitoring Script \u00b6 import psutil import nvidia_smi class MemoryMonitor : \"\"\"Monitor memory usage during training.\"\"\" def __init__ ( self ): self . peak_gpu = 0 self . peak_cpu = 0 def record ( self ): \"\"\"Record current memory usage.\"\"\" try : nvidia_smi . nvmlInit () handle = nvidia_smi . nvmlDeviceGetHandleByIndex ( 0 ) info = nvidia_smi . nvmlDeviceGetMemoryInfo ( handle ) gpu_mem = info . used / 1e9 self . peak_gpu = max ( self . peak_gpu , gpu_mem ) except : gpu_mem = 0 # CPU memory process = psutil . Process () cpu_mem = process . memory_info () . rss / 1e9 self . peak_cpu = max ( self . peak_cpu , cpu_mem ) return gpu_mem , cpu_mem def report ( self ): \"\"\"Print memory report.\"\"\" print ( f \"Peak GPU: { self . peak_gpu : .2f } GB\" ) print ( f \"Peak CPU: { self . peak_cpu : .2f } GB\" ) # Usage monitor = MemoryMonitor () # Periodically call during training for epoch in range ( num_epochs ): gpu_mem , cpu_mem = monitor . record () print ( f \"Epoch { epoch } : GPU { gpu_mem : .2f } GB, CPU { cpu_mem : .2f } GB\" ) monitor . report () 3. Optimization Techniques \u00b6 3.1 PEFT (LoRA) - Primary Technique \u00b6 Impact : 60-90% memory reduction # Base fine-tuning: high memory pipeline_base = TabularPipeline ( model_name = 'TabICL' , tuning_strategy = 'base-ft' , tuning_params = { 'device' : 'cuda' , 'epochs' : 5 } ) # Memory: ~12 GB for 100K samples # PEFT fine-tuning: low memory pipeline_peft = TabularPipeline ( model_name = 'TabICL' , tuning_strategy = 'peft' , tuning_params = { 'device' : 'cuda' , 'epochs' : 5 , 'peft_config' : { 'r' : 8 } } ) # Memory: ~4 GB for same task Choosing PEFT config for memory : # Ultra-constrained (2GB GPU) peft_config = { 'r' : 2 , 'lora_alpha' : 4 , 'lora_dropout' : 0.2 } # Memory-constrained (4GB GPU) peft_config = { 'r' : 4 , 'lora_alpha' : 8 , 'lora_dropout' : 0.1 } # Moderate constraint (6GB GPU) peft_config = { 'r' : 8 , 'lora_alpha' : 16 , 'lora_dropout' : 0.05 } 3.2 Batch Size Reduction \u00b6 Impact : 20-40% memory reduction per halving # Large batch (high memory) tuning_params = { 'batch_size' : 64 , 'support_size' : 512 } # Memory: ~12 GB # Reduced batch (medium memory) tuning_params = { 'batch_size' : 32 , 'support_size' : 256 } # Memory: ~6 GB # Small batch (low memory) tuning_params = { 'batch_size' : 8 , 'support_size' : 64 } # Memory: ~2 GB Trade-offs : - Smaller batch: More gradient noise, longer convergence - Larger batch: Faster convergence, higher memory 3.3 Gradient Accumulation \u00b6 Simulate larger batch without increased memory: tuning_params = { 'batch_size' : 8 , # Actual batch 'gradient_accumulation_steps' : 4 , # Accumulate 4x # Effective batch = 32 'device' : 'cuda' } # Memory cost: Similar to batch_size=8 # Effective batch size benefit: batch_size=32 3.4 Mixed Precision Training \u00b6 Impact : 20-30% memory reduction # Standard (float32): high memory tuning_params = { 'device' : 'cuda' , 'mixed_precision' : None # Full precision } # Half precision (float16): lower memory tuning_params = { 'device' : 'cuda' , 'mixed_precision' : 'fp16' # Use 16-bit floats } # BFloat16 (better stability) tuning_params = { 'device' : 'cuda' , 'mixed_precision' : 'bf16' } Implementation : import torch from torch.cuda.amp import autocast , GradScaler # Setup for mixed precision scaler = GradScaler () # In training loop with autocast ( dtype = torch . float16 ): loss = compute_loss ( ... ) scaler . scale ( loss ) . backward () scaler . step ( optimizer ) scaler . update () 3.5 Gradient Checkpointing \u00b6 Trade computation for memory: tuning_params = { 'device' : 'cuda' , 'gradient_checkpoint' : True # Save memory at cost of computation } # Memory reduction: ~30-50% # Computation increase: ~20-30% (recompute activations) 3.6 Data Loading Optimization \u00b6 # Efficient data loading loader_config = { 'batch_size' : 32 , 'num_workers' : 4 , # Parallel loading 'pin_memory' : True , # Faster CPU\u2192GPU transfer 'persistent_workers' : True , # Keep workers alive 'prefetch_factor' : 2 # Prefetch next batches } # Or with TabTune tuning_params = { 'num_workers' : 4 , 'pin_memory' : True , 'device' : 'cuda' } 3.7 Model Architecture Reduction \u00b6 Reduce model complexity: # Large model: high memory model_params = { 'd_model' : 256 , # Embedding dimension 'num_layers' : 8 , # Transformer layers 'num_heads' : 16 # Attention heads } # Medium model: medium memory model_params = { 'd_model' : 128 , 'num_layers' : 4 , 'num_heads' : 8 } # Small model: low memory model_params = { 'd_model' : 64 , 'num_layers' : 2 , 'num_heads' : 4 } 4. Optimization Strategies by Constraint \u00b6 4.1 Severe Constraint (2GB GPU) \u00b6 pipeline = TabularPipeline ( model_name = 'TabICL' , tuning_strategy = 'peft' , model_params = { 'n_estimators' : 8 # Reduce ensemble }, tuning_params = { 'device' : 'cuda' , 'epochs' : 3 , 'learning_rate' : 2e-4 , 'batch_size' : 4 , # Very small 'support_size' : 32 , # Small context 'query_size' : 16 , 'mixed_precision' : 'fp16' , # Use half precision 'gradient_checkpoint' : True , 'num_workers' : 0 , # No parallel loading 'peft_config' : { 'r' : 2 , # Very low rank 'lora_alpha' : 4 , 'lora_dropout' : 0.2 } } ) 4.2 Moderate Constraint (4GB GPU) \u00b6 pipeline = TabularPipeline ( model_name = 'TabICL' , tuning_strategy = 'peft' , tuning_params = { 'device' : 'cuda' , 'epochs' : 5 , 'learning_rate' : 2e-4 , 'batch_size' : 8 , 'support_size' : 64 , 'query_size' : 32 , 'mixed_precision' : 'fp16' , 'num_workers' : 2 , 'peft_config' : { 'r' : 4 , 'lora_alpha' : 8 , 'lora_dropout' : 0.1 } } ) 4.3 Comfortable (8GB+ GPU) \u00b6 pipeline = TabularPipeline ( model_name = 'TabICL' , tuning_strategy = 'base-ft' , # Full fine-tuning possible tuning_params = { 'device' : 'cuda' , 'epochs' : 5 , 'learning_rate' : 2e-5 , 'batch_size' : 32 , 'support_size' : 128 , 'query_size' : 64 , 'num_workers' : 4 , 'gradient_accumulation_steps' : 2 } ) 5. Advanced Techniques \u00b6 5.1 Activation Checkpointing \u00b6 import torch.utils.checkpoint as checkpoint class CheckpointedModel ( torch . nn . Module ): \"\"\"Wrap model with checkpointing.\"\"\" def __init__ ( self , model ): super () . __init__ () self . model = model def forward ( self , x ): # Checkpoint during forward pass return checkpoint . checkpoint ( self . model , x , use_reentrant = False ) 5.2 Quantization \u00b6 Reduce model precision: import torch.quantization as quantization def quantize_model ( model , backend = 'fbgemm' ): \"\"\"Quantize model to int8.\"\"\" model . qconfig = quantization . get_default_qconfig ( backend ) quantization . prepare ( model , inplace = True ) # Calibrate on data... quantization . convert ( model , inplace = True ) return model # Usage quantized_pipeline = quantize_model ( pipeline . model ) 5.3 Parameter Sharing \u00b6 Share weights across layers: class ParameterSharingModel ( torch . nn . Module ): \"\"\"Model with shared parameters.\"\"\" def __init__ ( self , shared_layer , num_repeats ): super () . __init__ () self . shared_layer = shared_layer self . num_repeats = num_repeats def forward ( self , x ): for _ in range ( self . num_repeats ): x = self . shared_layer ( x ) return x 5.4 Knowledge Distillation \u00b6 Train smaller student model from larger teacher: # Large teacher model (high accuracy) teacher = TabularPipeline ( model_name = 'TabDPT' , tuning_strategy = 'base-ft' ) teacher . fit ( X_train , y_train ) # Small student model (low memory) student = TabularPipeline ( model_name = 'TabPFN' , tuning_strategy = 'peft' ) # Distillation: train student to match teacher for batch in dataloader : teacher_logits = teacher . model ( batch ) student_logits = student . model ( batch ) # KL divergence loss loss = nn . KLDivLoss ()( nn . log_softmax ( student_logits ), nn . softmax ( teacher_logits ) ) # ... optimize ... 6. Memory-Time Trade-offs \u00b6 6.1 Time-Memory Pareto Frontier \u00b6 configurations = [ { 'name' : 'Max Speed' , 'batch_size' : 64 , 'precision' : 'fp32' , 'time' : 30 , # minutes 'memory' : 16 # GB }, { 'name' : 'Balanced' , 'batch_size' : 32 , 'precision' : 'fp32' , 'time' : 45 , 'memory' : 12 }, { 'name' : 'Efficient' , 'batch_size' : 16 , 'precision' : 'fp16' , 'time' : 60 , 'memory' : 6 }, { 'name' : 'Ultra-Efficient' , 'batch_size' : 8 , 'precision' : 'fp16' , 'peft' : True , 'time' : 90 , 'memory' : 3 } ] 6.2 Choosing Configuration \u00b6 def choose_config ( gpu_memory_gb , time_budget_hours ): \"\"\"Choose config based on constraints.\"\"\" if gpu_memory_gb < 4 : return 'peft' # Must use PEFT elif gpu_memory_gb < 8 : return 'fp16_batch16' elif gpu_memory_gb < 16 : return 'fp16_batch32' else : return 'base_ft' # Usage config = choose_config ( gpu_memory_gb = 6 , time_budget = 4 ) 7. Monitoring During Training \u00b6 7.1 Real-time Memory Tracking \u00b6 import logging from datetime import datetime class MemoryTracker : \"\"\"Track memory throughout training.\"\"\" def __init__ ( self , log_interval = 100 ): self . log_interval = log_interval self . iteration = 0 self . history = [] def step ( self ): \"\"\"Call after each training step.\"\"\" self . iteration += 1 if self . iteration % self . log_interval == 0 : peak = torch . cuda . max_memory_allocated () / 1e9 current = torch . cuda . memory_allocated () / 1e9 self . history . append ({ 'iteration' : self . iteration , 'peak' : peak , 'current' : current , 'time' : datetime . now () }) print ( f \"[ { self . iteration } ] Peak: { peak : .2f } GB, Current: { current : .2f } GB\" ) def plot ( self ): \"\"\"Plot memory over time.\"\"\" import matplotlib.pyplot as plt iterations = [ h [ 'iteration' ] for h in self . history ] peaks = [ h [ 'peak' ] for h in self . history ] plt . plot ( iterations , peaks ) plt . xlabel ( 'Iteration' ) plt . ylabel ( 'Peak Memory (GB)' ) plt . title ( 'GPU Memory Usage Over Time' ) plt . show () # Usage tracker = MemoryTracker () for batch in dataloader : # ... training step ... tracker . step () tracker . plot () 8. Debugging Memory Issues \u00b6 8.1 OOM (Out of Memory) Error Handling \u00b6 def safe_train ( model , dataloader , max_retries = 3 ): \"\"\"Train with automatic memory adaptation.\"\"\" batch_size = 32 for attempt in range ( max_retries ): try : # Try training with current batch size for batch in dataloader : # ... training code ... pass return # Success except RuntimeError as e : if 'out of memory' in str ( e ) . lower (): # Reduce batch size and retry batch_size //= 2 print ( f \"OOM detected. Retrying with batch_size= { batch_size } \" ) # Clear cache torch . cuda . empty_cache () # Recreate dataloader with smaller batch dataloader = DataLoader ( dataset , batch_size = batch_size ) else : raise raise RuntimeError ( f \"Failed after { max_retries } attempts\" ) 8.2 Memory Leak Detection \u00b6 import tracemalloc tracemalloc . start () # Your training code pipeline . fit ( X_train , y_train ) current , peak = tracemalloc . get_traced_memory () print ( f \"Current: { current / 1e9 : .2f } GB; Peak: { peak / 1e9 : .2f } GB\" ) # Find top memory allocations snapshot = tracemalloc . take_snapshot () top_stats = snapshot . statistics ( 'lineno' ) print ( \"[ Top 10 ]\" ) for stat in top_stats [: 10 ]: print ( stat ) 9. Complete Example: Memory-Optimized Training \u00b6 from tabtune import TabularPipeline import torch def memory_optimized_training ( X_train , y_train , X_val , y_val , gpu_memory_gb = 4 ): \"\"\"Train with automatic memory optimization.\"\"\" # Determine configuration if gpu_memory_gb < 4 : use_peft = True batch_size = 4 support_size = 32 mixed_precision = 'fp16' rank = 2 elif gpu_memory_gb < 8 : use_peft = True batch_size = 8 support_size = 64 mixed_precision = 'fp16' rank = 4 else : use_peft = False batch_size = 32 support_size = 128 mixed_precision = None rank = 8 # Create pipeline strategy = 'peft' if use_peft else 'base-ft' tuning_params = { 'device' : 'cuda' , 'epochs' : 5 , 'learning_rate' : 2e-4 if use_peft else 2e-5 , 'batch_size' : batch_size , 'support_size' : support_size , 'num_workers' : 0 , 'pin_memory' : False if gpu_memory_gb < 4 else True } if mixed_precision : tuning_params [ 'mixed_precision' ] = mixed_precision if use_peft : tuning_params [ 'peft_config' ] = { 'r' : rank , 'lora_alpha' : 2 * rank , 'lora_dropout' : 0.05 } pipeline = TabularPipeline ( model_name = 'TabICL' , tuning_strategy = strategy , tuning_params = tuning_params ) # Train with monitoring print ( f \"Training with strategy= { strategy } , batch_size= { batch_size } \" ) pipeline . fit ( X_train , y_train ) # Evaluate metrics = pipeline . evaluate ( X_val , y_val ) print ( f \"Validation accuracy: { metrics [ 'accuracy' ] : .4f } \" ) return pipeline # Usage pipeline = memory_optimized_training ( X_train , y_train , X_val , y_val , gpu_memory_gb = 4 ) 10. Quick Reference \u00b6 Memory Reduction Techniques (by impact) \u00b6 Technique Memory Saving Time Overhead Effort PEFT 60-90% None Low Batch size \u00f72 50% None Low Mixed precision 20-30% 20-30% Medium Gradient accumulation 0% 0% Low Gradient checkpoint 30-50% 20-30% Medium Quantization 75% 5-10% High 11. Best Practices \u00b6 \u2705 Do's \u00b6 \u2705 Profile memory before optimization \u2705 Use PEFT for memory-constrained environments \u2705 Start with small batch sizes \u2705 Use mixed precision when possible \u2705 Monitor memory during training \u2705 Empty cache between experiments \u2705 Use gradient accumulation for large effective batches \u274c Don'ts \u00b6 \u274c Don't forget to clear cache between runs \u274c Don't use full precision when half works \u274c Don't load entire dataset to memory \u274c Don't tune without monitoring memory \u274c Don't ignore OOM errors 12. Next Steps \u00b6 Tuning Strategies - PEFT details PEFT & LoRA - Memory-efficient fine-tuning Advanced Topics - Advanced optimizations Multi-GPU - Distributed training Optimize memory strategically to train powerful models on limited resources!","title":"Memory Optimization"},{"location":"advanced/memory-optimization/#memory-optimization-techniques-for-training-large-models","text":"This document provides comprehensive strategies for optimizing memory usage when training TabTune models, enabling efficient use of limited GPU/CPU resources.","title":"Memory Optimization: Techniques for Training Large Models"},{"location":"advanced/memory-optimization/#1-introduction","text":"Memory optimization is critical for: Training on GPUs with limited VRAM (< 8GB) Processing large datasets (100K+ rows) Using large model architectures Running multiple experiments simultaneously Deploying in resource-constrained environments This guide covers techniques, tools, and trade-offs for memory efficiency.","title":"1. Introduction"},{"location":"advanced/memory-optimization/#2-memory-profiling","text":"","title":"2. Memory Profiling"},{"location":"advanced/memory-optimization/#21-understanding-memory-usage","text":"Memory Breakdown (typical forward/backward pass): Model weights: 40-50% (frozen in PEFT) Optimizer states: 20-30% (momentum, variance) Gradients: 10-15% Activations: 10-20% (intermediate values) DataLoader buffers: 5-10% \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Total: 100%","title":"2.1 Understanding Memory Usage"},{"location":"advanced/memory-optimization/#22-profiling-tools","text":"","title":"2.2 Profiling Tools"},{"location":"advanced/memory-optimization/#pytorch-memory-profiler","text":"import torch from torch.profiler import profile , record_function , ProfilerActivity # Basic memory measurement torch . cuda . empty_cache () torch . cuda . reset_peak_memory_stats () # Your training code pipeline . fit ( X_train , y_train ) # Print memory stats print ( f \"Peak memory: { torch . cuda . max_memory_allocated () / 1e9 : .2f } GB\" ) print ( f \"Current memory: { torch . cuda . memory_allocated () / 1e9 : .2f } GB\" ) # Detailed profiling with profile ( activities = [ ProfilerActivity . CPU , ProfilerActivity . CUDA ], record_shapes = True ) as prof : pipeline . fit ( X_train , y_train ) print ( prof . key_averages () . table ( sort_by = \"cuda_memory_usage\" ))","title":"PyTorch Memory Profiler"},{"location":"advanced/memory-optimization/#memory-monitoring-script","text":"import psutil import nvidia_smi class MemoryMonitor : \"\"\"Monitor memory usage during training.\"\"\" def __init__ ( self ): self . peak_gpu = 0 self . peak_cpu = 0 def record ( self ): \"\"\"Record current memory usage.\"\"\" try : nvidia_smi . nvmlInit () handle = nvidia_smi . nvmlDeviceGetHandleByIndex ( 0 ) info = nvidia_smi . nvmlDeviceGetMemoryInfo ( handle ) gpu_mem = info . used / 1e9 self . peak_gpu = max ( self . peak_gpu , gpu_mem ) except : gpu_mem = 0 # CPU memory process = psutil . Process () cpu_mem = process . memory_info () . rss / 1e9 self . peak_cpu = max ( self . peak_cpu , cpu_mem ) return gpu_mem , cpu_mem def report ( self ): \"\"\"Print memory report.\"\"\" print ( f \"Peak GPU: { self . peak_gpu : .2f } GB\" ) print ( f \"Peak CPU: { self . peak_cpu : .2f } GB\" ) # Usage monitor = MemoryMonitor () # Periodically call during training for epoch in range ( num_epochs ): gpu_mem , cpu_mem = monitor . record () print ( f \"Epoch { epoch } : GPU { gpu_mem : .2f } GB, CPU { cpu_mem : .2f } GB\" ) monitor . report ()","title":"Memory Monitoring Script"},{"location":"advanced/memory-optimization/#3-optimization-techniques","text":"","title":"3. Optimization Techniques"},{"location":"advanced/memory-optimization/#31-peft-lora-primary-technique","text":"Impact : 60-90% memory reduction # Base fine-tuning: high memory pipeline_base = TabularPipeline ( model_name = 'TabICL' , tuning_strategy = 'base-ft' , tuning_params = { 'device' : 'cuda' , 'epochs' : 5 } ) # Memory: ~12 GB for 100K samples # PEFT fine-tuning: low memory pipeline_peft = TabularPipeline ( model_name = 'TabICL' , tuning_strategy = 'peft' , tuning_params = { 'device' : 'cuda' , 'epochs' : 5 , 'peft_config' : { 'r' : 8 } } ) # Memory: ~4 GB for same task Choosing PEFT config for memory : # Ultra-constrained (2GB GPU) peft_config = { 'r' : 2 , 'lora_alpha' : 4 , 'lora_dropout' : 0.2 } # Memory-constrained (4GB GPU) peft_config = { 'r' : 4 , 'lora_alpha' : 8 , 'lora_dropout' : 0.1 } # Moderate constraint (6GB GPU) peft_config = { 'r' : 8 , 'lora_alpha' : 16 , 'lora_dropout' : 0.05 }","title":"3.1 PEFT (LoRA) - Primary Technique"},{"location":"advanced/memory-optimization/#32-batch-size-reduction","text":"Impact : 20-40% memory reduction per halving # Large batch (high memory) tuning_params = { 'batch_size' : 64 , 'support_size' : 512 } # Memory: ~12 GB # Reduced batch (medium memory) tuning_params = { 'batch_size' : 32 , 'support_size' : 256 } # Memory: ~6 GB # Small batch (low memory) tuning_params = { 'batch_size' : 8 , 'support_size' : 64 } # Memory: ~2 GB Trade-offs : - Smaller batch: More gradient noise, longer convergence - Larger batch: Faster convergence, higher memory","title":"3.2 Batch Size Reduction"},{"location":"advanced/memory-optimization/#33-gradient-accumulation","text":"Simulate larger batch without increased memory: tuning_params = { 'batch_size' : 8 , # Actual batch 'gradient_accumulation_steps' : 4 , # Accumulate 4x # Effective batch = 32 'device' : 'cuda' } # Memory cost: Similar to batch_size=8 # Effective batch size benefit: batch_size=32","title":"3.3 Gradient Accumulation"},{"location":"advanced/memory-optimization/#34-mixed-precision-training","text":"Impact : 20-30% memory reduction # Standard (float32): high memory tuning_params = { 'device' : 'cuda' , 'mixed_precision' : None # Full precision } # Half precision (float16): lower memory tuning_params = { 'device' : 'cuda' , 'mixed_precision' : 'fp16' # Use 16-bit floats } # BFloat16 (better stability) tuning_params = { 'device' : 'cuda' , 'mixed_precision' : 'bf16' } Implementation : import torch from torch.cuda.amp import autocast , GradScaler # Setup for mixed precision scaler = GradScaler () # In training loop with autocast ( dtype = torch . float16 ): loss = compute_loss ( ... ) scaler . scale ( loss ) . backward () scaler . step ( optimizer ) scaler . update ()","title":"3.4 Mixed Precision Training"},{"location":"advanced/memory-optimization/#35-gradient-checkpointing","text":"Trade computation for memory: tuning_params = { 'device' : 'cuda' , 'gradient_checkpoint' : True # Save memory at cost of computation } # Memory reduction: ~30-50% # Computation increase: ~20-30% (recompute activations)","title":"3.5 Gradient Checkpointing"},{"location":"advanced/memory-optimization/#36-data-loading-optimization","text":"# Efficient data loading loader_config = { 'batch_size' : 32 , 'num_workers' : 4 , # Parallel loading 'pin_memory' : True , # Faster CPU\u2192GPU transfer 'persistent_workers' : True , # Keep workers alive 'prefetch_factor' : 2 # Prefetch next batches } # Or with TabTune tuning_params = { 'num_workers' : 4 , 'pin_memory' : True , 'device' : 'cuda' }","title":"3.6 Data Loading Optimization"},{"location":"advanced/memory-optimization/#37-model-architecture-reduction","text":"Reduce model complexity: # Large model: high memory model_params = { 'd_model' : 256 , # Embedding dimension 'num_layers' : 8 , # Transformer layers 'num_heads' : 16 # Attention heads } # Medium model: medium memory model_params = { 'd_model' : 128 , 'num_layers' : 4 , 'num_heads' : 8 } # Small model: low memory model_params = { 'd_model' : 64 , 'num_layers' : 2 , 'num_heads' : 4 }","title":"3.7 Model Architecture Reduction"},{"location":"advanced/memory-optimization/#4-optimization-strategies-by-constraint","text":"","title":"4. Optimization Strategies by Constraint"},{"location":"advanced/memory-optimization/#41-severe-constraint-2gb-gpu","text":"pipeline = TabularPipeline ( model_name = 'TabICL' , tuning_strategy = 'peft' , model_params = { 'n_estimators' : 8 # Reduce ensemble }, tuning_params = { 'device' : 'cuda' , 'epochs' : 3 , 'learning_rate' : 2e-4 , 'batch_size' : 4 , # Very small 'support_size' : 32 , # Small context 'query_size' : 16 , 'mixed_precision' : 'fp16' , # Use half precision 'gradient_checkpoint' : True , 'num_workers' : 0 , # No parallel loading 'peft_config' : { 'r' : 2 , # Very low rank 'lora_alpha' : 4 , 'lora_dropout' : 0.2 } } )","title":"4.1 Severe Constraint (2GB GPU)"},{"location":"advanced/memory-optimization/#42-moderate-constraint-4gb-gpu","text":"pipeline = TabularPipeline ( model_name = 'TabICL' , tuning_strategy = 'peft' , tuning_params = { 'device' : 'cuda' , 'epochs' : 5 , 'learning_rate' : 2e-4 , 'batch_size' : 8 , 'support_size' : 64 , 'query_size' : 32 , 'mixed_precision' : 'fp16' , 'num_workers' : 2 , 'peft_config' : { 'r' : 4 , 'lora_alpha' : 8 , 'lora_dropout' : 0.1 } } )","title":"4.2 Moderate Constraint (4GB GPU)"},{"location":"advanced/memory-optimization/#43-comfortable-8gb-gpu","text":"pipeline = TabularPipeline ( model_name = 'TabICL' , tuning_strategy = 'base-ft' , # Full fine-tuning possible tuning_params = { 'device' : 'cuda' , 'epochs' : 5 , 'learning_rate' : 2e-5 , 'batch_size' : 32 , 'support_size' : 128 , 'query_size' : 64 , 'num_workers' : 4 , 'gradient_accumulation_steps' : 2 } )","title":"4.3 Comfortable (8GB+ GPU)"},{"location":"advanced/memory-optimization/#5-advanced-techniques","text":"","title":"5. Advanced Techniques"},{"location":"advanced/memory-optimization/#51-activation-checkpointing","text":"import torch.utils.checkpoint as checkpoint class CheckpointedModel ( torch . nn . Module ): \"\"\"Wrap model with checkpointing.\"\"\" def __init__ ( self , model ): super () . __init__ () self . model = model def forward ( self , x ): # Checkpoint during forward pass return checkpoint . checkpoint ( self . model , x , use_reentrant = False )","title":"5.1 Activation Checkpointing"},{"location":"advanced/memory-optimization/#52-quantization","text":"Reduce model precision: import torch.quantization as quantization def quantize_model ( model , backend = 'fbgemm' ): \"\"\"Quantize model to int8.\"\"\" model . qconfig = quantization . get_default_qconfig ( backend ) quantization . prepare ( model , inplace = True ) # Calibrate on data... quantization . convert ( model , inplace = True ) return model # Usage quantized_pipeline = quantize_model ( pipeline . model )","title":"5.2 Quantization"},{"location":"advanced/memory-optimization/#53-parameter-sharing","text":"Share weights across layers: class ParameterSharingModel ( torch . nn . Module ): \"\"\"Model with shared parameters.\"\"\" def __init__ ( self , shared_layer , num_repeats ): super () . __init__ () self . shared_layer = shared_layer self . num_repeats = num_repeats def forward ( self , x ): for _ in range ( self . num_repeats ): x = self . shared_layer ( x ) return x","title":"5.3 Parameter Sharing"},{"location":"advanced/memory-optimization/#54-knowledge-distillation","text":"Train smaller student model from larger teacher: # Large teacher model (high accuracy) teacher = TabularPipeline ( model_name = 'TabDPT' , tuning_strategy = 'base-ft' ) teacher . fit ( X_train , y_train ) # Small student model (low memory) student = TabularPipeline ( model_name = 'TabPFN' , tuning_strategy = 'peft' ) # Distillation: train student to match teacher for batch in dataloader : teacher_logits = teacher . model ( batch ) student_logits = student . model ( batch ) # KL divergence loss loss = nn . KLDivLoss ()( nn . log_softmax ( student_logits ), nn . softmax ( teacher_logits ) ) # ... optimize ...","title":"5.4 Knowledge Distillation"},{"location":"advanced/memory-optimization/#6-memory-time-trade-offs","text":"","title":"6. Memory-Time Trade-offs"},{"location":"advanced/memory-optimization/#61-time-memory-pareto-frontier","text":"configurations = [ { 'name' : 'Max Speed' , 'batch_size' : 64 , 'precision' : 'fp32' , 'time' : 30 , # minutes 'memory' : 16 # GB }, { 'name' : 'Balanced' , 'batch_size' : 32 , 'precision' : 'fp32' , 'time' : 45 , 'memory' : 12 }, { 'name' : 'Efficient' , 'batch_size' : 16 , 'precision' : 'fp16' , 'time' : 60 , 'memory' : 6 }, { 'name' : 'Ultra-Efficient' , 'batch_size' : 8 , 'precision' : 'fp16' , 'peft' : True , 'time' : 90 , 'memory' : 3 } ]","title":"6.1 Time-Memory Pareto Frontier"},{"location":"advanced/memory-optimization/#62-choosing-configuration","text":"def choose_config ( gpu_memory_gb , time_budget_hours ): \"\"\"Choose config based on constraints.\"\"\" if gpu_memory_gb < 4 : return 'peft' # Must use PEFT elif gpu_memory_gb < 8 : return 'fp16_batch16' elif gpu_memory_gb < 16 : return 'fp16_batch32' else : return 'base_ft' # Usage config = choose_config ( gpu_memory_gb = 6 , time_budget = 4 )","title":"6.2 Choosing Configuration"},{"location":"advanced/memory-optimization/#7-monitoring-during-training","text":"","title":"7. Monitoring During Training"},{"location":"advanced/memory-optimization/#71-real-time-memory-tracking","text":"import logging from datetime import datetime class MemoryTracker : \"\"\"Track memory throughout training.\"\"\" def __init__ ( self , log_interval = 100 ): self . log_interval = log_interval self . iteration = 0 self . history = [] def step ( self ): \"\"\"Call after each training step.\"\"\" self . iteration += 1 if self . iteration % self . log_interval == 0 : peak = torch . cuda . max_memory_allocated () / 1e9 current = torch . cuda . memory_allocated () / 1e9 self . history . append ({ 'iteration' : self . iteration , 'peak' : peak , 'current' : current , 'time' : datetime . now () }) print ( f \"[ { self . iteration } ] Peak: { peak : .2f } GB, Current: { current : .2f } GB\" ) def plot ( self ): \"\"\"Plot memory over time.\"\"\" import matplotlib.pyplot as plt iterations = [ h [ 'iteration' ] for h in self . history ] peaks = [ h [ 'peak' ] for h in self . history ] plt . plot ( iterations , peaks ) plt . xlabel ( 'Iteration' ) plt . ylabel ( 'Peak Memory (GB)' ) plt . title ( 'GPU Memory Usage Over Time' ) plt . show () # Usage tracker = MemoryTracker () for batch in dataloader : # ... training step ... tracker . step () tracker . plot ()","title":"7.1 Real-time Memory Tracking"},{"location":"advanced/memory-optimization/#8-debugging-memory-issues","text":"","title":"8. Debugging Memory Issues"},{"location":"advanced/memory-optimization/#81-oom-out-of-memory-error-handling","text":"def safe_train ( model , dataloader , max_retries = 3 ): \"\"\"Train with automatic memory adaptation.\"\"\" batch_size = 32 for attempt in range ( max_retries ): try : # Try training with current batch size for batch in dataloader : # ... training code ... pass return # Success except RuntimeError as e : if 'out of memory' in str ( e ) . lower (): # Reduce batch size and retry batch_size //= 2 print ( f \"OOM detected. Retrying with batch_size= { batch_size } \" ) # Clear cache torch . cuda . empty_cache () # Recreate dataloader with smaller batch dataloader = DataLoader ( dataset , batch_size = batch_size ) else : raise raise RuntimeError ( f \"Failed after { max_retries } attempts\" )","title":"8.1 OOM (Out of Memory) Error Handling"},{"location":"advanced/memory-optimization/#82-memory-leak-detection","text":"import tracemalloc tracemalloc . start () # Your training code pipeline . fit ( X_train , y_train ) current , peak = tracemalloc . get_traced_memory () print ( f \"Current: { current / 1e9 : .2f } GB; Peak: { peak / 1e9 : .2f } GB\" ) # Find top memory allocations snapshot = tracemalloc . take_snapshot () top_stats = snapshot . statistics ( 'lineno' ) print ( \"[ Top 10 ]\" ) for stat in top_stats [: 10 ]: print ( stat )","title":"8.2 Memory Leak Detection"},{"location":"advanced/memory-optimization/#9-complete-example-memory-optimized-training","text":"from tabtune import TabularPipeline import torch def memory_optimized_training ( X_train , y_train , X_val , y_val , gpu_memory_gb = 4 ): \"\"\"Train with automatic memory optimization.\"\"\" # Determine configuration if gpu_memory_gb < 4 : use_peft = True batch_size = 4 support_size = 32 mixed_precision = 'fp16' rank = 2 elif gpu_memory_gb < 8 : use_peft = True batch_size = 8 support_size = 64 mixed_precision = 'fp16' rank = 4 else : use_peft = False batch_size = 32 support_size = 128 mixed_precision = None rank = 8 # Create pipeline strategy = 'peft' if use_peft else 'base-ft' tuning_params = { 'device' : 'cuda' , 'epochs' : 5 , 'learning_rate' : 2e-4 if use_peft else 2e-5 , 'batch_size' : batch_size , 'support_size' : support_size , 'num_workers' : 0 , 'pin_memory' : False if gpu_memory_gb < 4 else True } if mixed_precision : tuning_params [ 'mixed_precision' ] = mixed_precision if use_peft : tuning_params [ 'peft_config' ] = { 'r' : rank , 'lora_alpha' : 2 * rank , 'lora_dropout' : 0.05 } pipeline = TabularPipeline ( model_name = 'TabICL' , tuning_strategy = strategy , tuning_params = tuning_params ) # Train with monitoring print ( f \"Training with strategy= { strategy } , batch_size= { batch_size } \" ) pipeline . fit ( X_train , y_train ) # Evaluate metrics = pipeline . evaluate ( X_val , y_val ) print ( f \"Validation accuracy: { metrics [ 'accuracy' ] : .4f } \" ) return pipeline # Usage pipeline = memory_optimized_training ( X_train , y_train , X_val , y_val , gpu_memory_gb = 4 )","title":"9. Complete Example: Memory-Optimized Training"},{"location":"advanced/memory-optimization/#10-quick-reference","text":"","title":"10. Quick Reference"},{"location":"advanced/memory-optimization/#memory-reduction-techniques-by-impact","text":"Technique Memory Saving Time Overhead Effort PEFT 60-90% None Low Batch size \u00f72 50% None Low Mixed precision 20-30% 20-30% Medium Gradient accumulation 0% 0% Low Gradient checkpoint 30-50% 20-30% Medium Quantization 75% 5-10% High","title":"Memory Reduction Techniques (by impact)"},{"location":"advanced/memory-optimization/#11-best-practices","text":"","title":"11. Best Practices"},{"location":"advanced/memory-optimization/#dos","text":"\u2705 Profile memory before optimization \u2705 Use PEFT for memory-constrained environments \u2705 Start with small batch sizes \u2705 Use mixed precision when possible \u2705 Monitor memory during training \u2705 Empty cache between experiments \u2705 Use gradient accumulation for large effective batches","title":"\u2705 Do's"},{"location":"advanced/memory-optimization/#donts","text":"\u274c Don't forget to clear cache between runs \u274c Don't use full precision when half works \u274c Don't load entire dataset to memory \u274c Don't tune without monitoring memory \u274c Don't ignore OOM errors","title":"\u274c Don'ts"},{"location":"advanced/memory-optimization/#12-next-steps","text":"Tuning Strategies - PEFT details PEFT & LoRA - Memory-efficient fine-tuning Advanced Topics - Advanced optimizations Multi-GPU - Distributed training Optimize memory strategically to train powerful models on limited resources!","title":"12. Next Steps"},{"location":"advanced/multi-gpu/","text":"Multi-GPU Training: Scaling TabTune Across Multiple GPUs \u00b6 This document provides comprehensive guidance on leveraging multiple GPUs for distributed training with TabTune, enabling faster training and handling of larger models and datasets. 1. Introduction \u00b6 Multi-GPU training accelerates TabTune workflows through: Data Parallelism : Distribute data across GPUs Model Parallelism : Distribute model across GPUs Distributed Optimization : Synchronized gradient updates Scaling : Near-linear speedup with multiple GPUs This guide covers setup, strategies, and best practices. 2. Multi-GPU Fundamentals \u00b6 2.1 Parallelism Strategies \u00b6 flowchart TD A[Multi-GPU Training] --> B[Data Parallelism] A --> C[Model Parallelism] A --> D[Pipeline Parallelism] B --> B1[\"Each GPU gets data batch\"] B --> B2[\"Model replicated on each GPU\"] B --> B3[\"Gradients averaged across GPUs\"] C --> C1[\"Model layers split across GPUs\"] C --> C2[\"Each GPU processes different layers\"] C --> C3[\"Communication between GPUs\"] D --> D1[\"Stages of model pipeline\"] D --> D2[\"Different stages on different GPUs\"] D --> D3[\"Micro-batching for efficiency\"] 2.2 Data Parallelism (Most Common) \u00b6 Recommended for TabTune - simplest and most effective: # Single GPU training model = TabularPipeline ( model_name = 'TabICL' ) # Loss: 100% on GPU 0 # Data parallel (2 GPUs) # Batch split into 2 sub-batches # GPU 0: sub-batch 1 # GPU 1: sub-batch 2 # Gradients averaged 3. Setup Requirements \u00b6 3.1 Hardware Prerequisites \u00b6 import torch # Check GPU availability print ( f \"GPUs available: { torch . cuda . device_count () } \" ) print ( f \"Current GPU: { torch . cuda . current_device () } \" ) for i in range ( torch . cuda . device_count ()): gpu = torch . cuda . get_device_properties ( i ) print ( f \"GPU { i } : { gpu . name } ( { gpu . total_memory / 1e9 : .1f } GB)\" ) # Recommended print ( f \" \\n Availability check:\" ) print ( f \"CUDA available: { torch . cuda . is_available () } \" ) print ( f \"NCCL available: { torch . distributed . is_nccl_available () } \" ) 3.2 Software Stack \u00b6 # Install required packages pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121 pip install torch-distributed-rpc pip install horovod # Optional: for advanced distributed training pip install pytorch-lightning # Optional: simplified multi-GPU setup 4. Data Parallel Training \u00b6 4.1 DataParallel (Simpler, Single-Machine) \u00b6 import torch import torch.nn as nn class DataParallelWrapper : \"\"\"Wrap TabTune pipeline with DataParallel.\"\"\" def __init__ ( self , model , device_ids = None ): self . model = nn . DataParallel ( model , device_ids = device_ids ) self . device_ids = device_ids or list ( range ( torch . cuda . device_count ())) def fit ( self , X_train , y_train , ** kwargs ): \"\"\"Training with DataParallel.\"\"\" # Batch automatically split across GPUs return self . model . module . fit ( X_train , y_train , ** kwargs ) def predict ( self , X_test ): \"\"\"Inference on primary GPU.\"\"\" return self . model . module . predict ( X_test ) # Usage if torch . cuda . device_count () > 1 : model = DataParallelWrapper ( TabularPipeline ( model_name = 'TabICL' ), device_ids = [ 0 , 1 , 2 , 3 ] # Use GPUs 0-3 ) else : model = TabularPipeline ( model_name = 'TabICL' ) model . fit ( X_train , y_train ) 4.2 DistributedDataParallel (Recommended) \u00b6 More efficient than DataParallel: import torch import torch.distributed as dist from torch.nn.parallel import DistributedDataParallel def setup_distributed (): \"\"\"Setup distributed training environment.\"\"\" dist . init_process_group ( backend = 'nccl' , # NVIDIA Collective Communications Library init_method = 'env://' ) def cleanup (): \"\"\"Cleanup distributed environment.\"\"\" dist . destroy_process_group () def train_distributed (): \"\"\"Distributed training.\"\"\" setup_distributed () # Get process rank and world size rank = dist . get_rank () world_size = dist . get_world_size () # Create model on current device device = torch . device ( f 'cuda: { rank } ' ) pipeline = TabularPipeline ( model_name = 'TabICL' , tuning_strategy = 'base-ft' , tuning_params = { 'device' : device } ) # Wrap with DistributedDataParallel model = DistributedDataParallel ( pipeline . model , device_ids = [ rank ]) # Training code model . fit ( X_train , y_train ) cleanup () # Launch with: python -m torch.distributed.launch --nproc_per_node=4 script.py 5. Launch Methods \u00b6 5.1 torchrun (PyTorch 1.10+) \u00b6 Recommended method : # Single-machine, 4 GPUs torchrun --nproc_per_node = 4 train_script.py # Multi-machine (8 GPUs total) torchrun \\ --nproc_per_node = 4 \\ --nnodes = 2 \\ --node_rank = 0 \\ --master_addr = 192 .168.1.100 \\ --master_port = 29500 \\ train_script.py 5.2 torch.distributed.launch (Legacy) \u00b6 # Single-machine, 4 GPUs python -m torch.distributed.launch \\ --nproc_per_node = 4 \\ train_script.py # With additional args python -m torch.distributed.launch \\ --nproc_per_node = 4 \\ train_script.py \\ --learning_rate 2e-5 \\ --epochs 5 5.3 Manual Launch \u00b6 # Terminal 1: GPU 0 CUDA_VISIBLE_DEVICES = 0 python train_script.py # Terminal 2: GPU 1 CUDA_VISIBLE_DEVICES = 1 python train_script.py # Terminal 3: GPU 2 CUDA_VISIBLE_DEVICES = 2 python train_script.py # Terminal 4: GPU 3 CUDA_VISIBLE_DEVICES = 3 python train_script.py 6. Training Scripts \u00b6 6.1 Complete Distributed Training Script \u00b6 import os import torch import torch.distributed as dist from torch.nn.parallel import DistributedDataParallel from torch.utils.data import DataLoader , DistributedSampler from tabtune import TabularPipeline def main (): \"\"\"Main training function.\"\"\" # Initialize distributed training dist . init_process_group ( backend = 'nccl' ) rank = dist . get_rank () world_size = dist . get_world_size () # Set device device = torch . device ( f 'cuda: { rank } ' ) torch . cuda . set_device ( device ) # Print rank info if rank == 0 : print ( f \"Training on { world_size } GPUs\" ) # Load data X_train , y_train = load_data () # Create distributed sampler train_sampler = DistributedSampler ( dataset = range ( len ( X_train )), num_replicas = world_size , rank = rank , shuffle = True ) # Create dataloader train_loader = DataLoader ( train_sampler , batch_size = 32 , num_workers = 2 ) # Create model pipeline = TabularPipeline ( model_name = 'TabICL' , tuning_strategy = 'base-ft' , tuning_params = { 'device' : device , 'epochs' : 5 , 'learning_rate' : 2e-5 } ) # Wrap with DistributedDataParallel pipeline . model = DistributedDataParallel ( pipeline . model , device_ids = [ rank ] ) # Training if rank == 0 : print ( \"Starting training...\" ) pipeline . fit ( X_train [ train_sampler . indices ], y_train [ train_sampler . indices ]) # Evaluation on rank 0 only if rank == 0 : metrics = pipeline . evaluate ( X_test , y_test ) print ( f \"Validation Accuracy: { metrics [ 'accuracy' ] : .4f } \" ) # Cleanup dist . destroy_process_group () if __name__ == '__main__' : main () 6.2 Synchronization Across Ranks \u00b6 def train_with_sync (): \"\"\"Training with synchronization points.\"\"\" dist . init_process_group ( backend = 'nccl' ) rank = dist . get_rank () # Training for epoch in range ( num_epochs ): # ... training code ... # Synchronize all ranks dist . barrier () # Evaluation (on rank 0 only) if rank == 0 : metrics = evaluate () print ( f \"Epoch { epoch } : { metrics } \" ) # Broadcast best checkpoint from rank 0 if rank == 0 : best_state = pipeline . model . state_dict () else : best_state = None # Distribute to all ranks dist . broadcast_object_list ([ best_state ], src = 0 ) if rank != 0 : pipeline . model . load_state_dict ( best_state [ 0 ]) dist . destroy_process_group () 7. Performance Optimization \u00b6 7.1 Gradient Accumulation \u00b6 Increase effective batch size without memory increase: def train_with_accumulation ( num_accumulation_steps = 4 ): \"\"\"Training with gradient accumulation.\"\"\" optimizer = torch . optim . AdamW ( model . parameters ()) for epoch in range ( num_epochs ): for i , batch in enumerate ( dataloader ): # Forward pass loss = model ( batch ) # Backward (accumulate gradients) ( loss / num_accumulation_steps ) . backward () # Update weights if ( i + 1 ) % num_accumulation_steps == 0 : optimizer . step () optimizer . zero_grad () 7.2 Overlapping Computation & Communication \u00b6 def train_with_overlap (): \"\"\"Training with async communication.\"\"\" for epoch in range ( num_epochs ): for batch in dataloader : # Forward & backward (computation) loss = model ( batch ) loss . backward () # Start async gradient reduction reduction_future = dist . all_reduce ( model . grad , async_op = True # Non-blocking ) # Do other work while reducing # ... # Wait for reduction to complete reduction_future . wait () # Update weights optimizer . step () 7.3 Mixed Precision with Distributed Training \u00b6 from torch.cuda.amp import autocast , GradScaler def train_mixed_precision (): \"\"\"Multi-GPU training with mixed precision.\"\"\" scaler = GradScaler () for epoch in range ( num_epochs ): for batch in dataloader : # Forward with autocast with autocast ( dtype = torch . float16 ): loss = model ( batch ) # Backward with scaling scaler . scale ( loss ) . backward () # All-reduce scaled gradients for param in model . parameters (): if param . grad is not None : dist . all_reduce ( param . grad ) # Update weights scaler . step ( optimizer ) scaler . update () 8. Scaling Efficiency \u00b6 8.1 Linear Scaling Rule \u00b6 # Optimal learning rate for N GPUs base_learning_rate = 2e-5 num_gpus = 4 optimal_learning_rate = base_learning_rate * num_gpus # Why: Batch size \u00d7 num_gpus, so learning rate should scale # Or more conservatively: optimal_learning_rate = base_learning_rate * ( num_gpus ** 0.5 ) 8.2 Speedup Analysis \u00b6 def analyze_scaling ( times_single_gpu , times_multi_gpu ): \"\"\"Analyze multi-GPU speedup.\"\"\" num_gpus = len ( times_multi_gpu ) speedup = times_single_gpu / times_multi_gpu efficiency = speedup / num_gpus # Ideally 1.0 (100%) print ( f \"GPU Count: { num_gpus } \" ) print ( f \"Time (1 GPU): { times_single_gpu : .2f } s\" ) print ( f \"Time ( { num_gpus } GPUs): { times_multi_gpu : .2f } s\" ) print ( f \"Speedup: { speedup : .2f } x ( { efficiency * 100 : .1f } % efficiency)\" ) return speedup , efficiency # Typical results # 2 GPUs: 1.8x speedup (90% efficiency) # 4 GPUs: 3.5x speedup (87.5% efficiency) # 8 GPUs: 6.5x speedup (81% efficiency) 9. Distributed Challenges & Solutions \u00b6 9.1 Communication Overhead \u00b6 # Problem: Communication becomes bottleneck # Solution 1: Larger batch size tuning_params = { 'batch_size' : 128 , # Instead of 32 } # Solution 2: Gradient accumulation tuning_params = { 'batch_size' : 32 , 'gradient_accumulation_steps' : 4 # Effective: 128 } # Solution 3: Reduce communication frequency # Communicate every N steps instead of every step 9.2 Load Imbalance \u00b6 # Problem: Some GPUs finish before others # Solution: Dynamic load balancing def balanced_sampler ( dataset , num_replicas , rank ): \"\"\"Create balanced sampler across ranks.\"\"\" # Ensure each rank gets similar amount of work samples_per_rank = len ( dataset ) // num_replicas remainder = len ( dataset ) % num_replicas start = rank * samples_per_rank + min ( rank , remainder ) end = start + samples_per_rank + ( 1 if rank < remainder else 0 ) indices = list ( range ( start , end )) return DistributedSampler ( indices , num_replicas = num_replicas , rank = rank ) 9.3 Gradient Divergence \u00b6 # Problem: Different GPUs compute different gradients # Solution: Proper synchronization def synchronized_training (): \"\"\"Ensure all ranks have synchronized gradients.\"\"\" for batch in dataloader : # Compute gradients loss = model ( batch ) loss . backward () # Synchronize gradients across all ranks for param in model . parameters (): dist . all_reduce ( param . grad ) param . grad /= world_size # Average # Update optimizer . step () 10. Complete Multi-GPU Example \u00b6 #!/usr/bin/env python \"\"\"Multi-GPU training example.\"\"\" import os import torch import torch.distributed as dist from torch.nn.parallel import DistributedDataParallel from tabtune import TabularPipeline import argparse def main (): parser = argparse . ArgumentParser () parser . add_argument ( '--epochs' , type = int , default = 5 ) parser . add_argument ( '--batch_size' , type = int , default = 32 ) parser . add_argument ( '--learning_rate' , type = float , default = 2e-5 ) args = parser . parse_args () # Initialize distributed training dist . init_process_group ( backend = 'nccl' ) rank = dist . get_rank () world_size = dist . get_world_size () device = torch . device ( f 'cuda: { rank } ' ) if rank == 0 : print ( f \"Starting training on { world_size } GPUs\" ) # Load data X_train , y_train , X_test , y_test = load_dataset () # Scale learning rate with batch size scaled_lr = args . learning_rate * world_size # Create pipeline pipeline = TabularPipeline ( model_name = 'TabICL' , tuning_strategy = 'base-ft' , tuning_params = { 'device' : device , 'epochs' : args . epochs , 'learning_rate' : scaled_lr , 'batch_size' : args . batch_size } ) # Wrap with DDP pipeline . model = DistributedDataParallel ( pipeline . model , device_ids = [ rank ] ) # Training if rank == 0 : print ( f \"Training with LR= { scaled_lr : .2e } , batch_size= { args . batch_size } \" ) pipeline . fit ( X_train , y_train ) # Synchronize before evaluation dist . barrier () # Evaluation on primary GPU only if rank == 0 : metrics = pipeline . evaluate ( X_test , y_test ) print ( f \"Final accuracy: { metrics [ 'accuracy' ] : .4f } \" ) dist . destroy_process_group () if __name__ == '__main__' : main () Launch: torchrun --nproc_per_node = 4 train_distributed.py --epochs 5 --batch_size 32 11. Debugging Multi-GPU Issues \u00b6 11.1 Common Problems \u00b6 # Problem: Hanging/deadlock # Solution: Use timeout and debug flags os . environ [ 'NCCL_DEBUG' ] = 'INFO' os . environ [ 'TORCH_DISTRIBUTED_DEBUG' ] = 'DETAIL' dist . init_process_group ( backend = 'nccl' , timeout = timedelta ( minutes = 30 ) ) # Problem: GPU memory imbalance # Solution: Check and balance if rank == 0 : for i in range ( world_size ): print ( f \"GPU { i } : { torch . cuda . get_device_properties ( i ) . total_memory / 1e9 : .1f } GB\" ) # Problem: Rank synchronization issues # Solution: Add explicit barriers dist . barrier () # Wait for all ranks 12. Best Practices \u00b6 \u2705 Do's \u00b6 \u2705 Use DistributedDataParallel over DataParallel \u2705 Scale learning rate with batch size \u2705 Use proper samplers (DistributedSampler) \u2705 Synchronize at checkpoints \u2705 Evaluate on rank 0 only \u2705 Monitor all GPU memory usage \u2705 Test on fewer GPUs first \u274c Don'ts \u00b6 \u274c Don't use DataParallel for multi-machine \u274c Don't forget to set environment variables \u274c Don't run inference on all ranks \u274c Don't ignore communication overhead \u274c Don't over-subscribe GPU memory \u274c Don't change data on different ranks 13. Performance Benchmarks \u00b6 Dataset: 500K samples, TabICL model 1 GPU: Time: 120 minutes Memory: 12 GB 2 GPUs (DDP): Time: 65 minutes (1.85x speedup) Memory: 6 GB per GPU Efficiency: 92.5% 4 GPUs (DDP): Time: 35 minutes (3.43x speedup) Memory: 3 GB per GPU Efficiency: 85.7% 8 GPUs (DDP): Time: 20 minutes (6.0x speedup) Memory: 1.5 GB per GPU Efficiency: 75% 14. Quick Reference \u00b6 Aspect Single GPU Multi-GPU DDP Setup Simple Moderate Communication None NCCL Speedup 1x ~(GPUs-0.3) Memory/GPU Full Full/GPUs Best Use Development Production 15. Next Steps \u00b6 Memory Optimization - Memory management with DDP Hyperparameter Tuning - Scaling learning rates Tuning Strategies - PEFT with DDP Examples - Multi-GPU benchmarks Scale TabTune efficiently across multiple GPUs for production-grade training!","title":"Multi-GPU Training"},{"location":"advanced/multi-gpu/#multi-gpu-training-scaling-tabtune-across-multiple-gpus","text":"This document provides comprehensive guidance on leveraging multiple GPUs for distributed training with TabTune, enabling faster training and handling of larger models and datasets.","title":"Multi-GPU Training: Scaling TabTune Across Multiple GPUs"},{"location":"advanced/multi-gpu/#1-introduction","text":"Multi-GPU training accelerates TabTune workflows through: Data Parallelism : Distribute data across GPUs Model Parallelism : Distribute model across GPUs Distributed Optimization : Synchronized gradient updates Scaling : Near-linear speedup with multiple GPUs This guide covers setup, strategies, and best practices.","title":"1. Introduction"},{"location":"advanced/multi-gpu/#2-multi-gpu-fundamentals","text":"","title":"2. Multi-GPU Fundamentals"},{"location":"advanced/multi-gpu/#21-parallelism-strategies","text":"flowchart TD A[Multi-GPU Training] --> B[Data Parallelism] A --> C[Model Parallelism] A --> D[Pipeline Parallelism] B --> B1[\"Each GPU gets data batch\"] B --> B2[\"Model replicated on each GPU\"] B --> B3[\"Gradients averaged across GPUs\"] C --> C1[\"Model layers split across GPUs\"] C --> C2[\"Each GPU processes different layers\"] C --> C3[\"Communication between GPUs\"] D --> D1[\"Stages of model pipeline\"] D --> D2[\"Different stages on different GPUs\"] D --> D3[\"Micro-batching for efficiency\"]","title":"2.1 Parallelism Strategies"},{"location":"advanced/multi-gpu/#22-data-parallelism-most-common","text":"Recommended for TabTune - simplest and most effective: # Single GPU training model = TabularPipeline ( model_name = 'TabICL' ) # Loss: 100% on GPU 0 # Data parallel (2 GPUs) # Batch split into 2 sub-batches # GPU 0: sub-batch 1 # GPU 1: sub-batch 2 # Gradients averaged","title":"2.2 Data Parallelism (Most Common)"},{"location":"advanced/multi-gpu/#3-setup-requirements","text":"","title":"3. Setup Requirements"},{"location":"advanced/multi-gpu/#31-hardware-prerequisites","text":"import torch # Check GPU availability print ( f \"GPUs available: { torch . cuda . device_count () } \" ) print ( f \"Current GPU: { torch . cuda . current_device () } \" ) for i in range ( torch . cuda . device_count ()): gpu = torch . cuda . get_device_properties ( i ) print ( f \"GPU { i } : { gpu . name } ( { gpu . total_memory / 1e9 : .1f } GB)\" ) # Recommended print ( f \" \\n Availability check:\" ) print ( f \"CUDA available: { torch . cuda . is_available () } \" ) print ( f \"NCCL available: { torch . distributed . is_nccl_available () } \" )","title":"3.1 Hardware Prerequisites"},{"location":"advanced/multi-gpu/#32-software-stack","text":"# Install required packages pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121 pip install torch-distributed-rpc pip install horovod # Optional: for advanced distributed training pip install pytorch-lightning # Optional: simplified multi-GPU setup","title":"3.2 Software Stack"},{"location":"advanced/multi-gpu/#4-data-parallel-training","text":"","title":"4. Data Parallel Training"},{"location":"advanced/multi-gpu/#41-dataparallel-simpler-single-machine","text":"import torch import torch.nn as nn class DataParallelWrapper : \"\"\"Wrap TabTune pipeline with DataParallel.\"\"\" def __init__ ( self , model , device_ids = None ): self . model = nn . DataParallel ( model , device_ids = device_ids ) self . device_ids = device_ids or list ( range ( torch . cuda . device_count ())) def fit ( self , X_train , y_train , ** kwargs ): \"\"\"Training with DataParallel.\"\"\" # Batch automatically split across GPUs return self . model . module . fit ( X_train , y_train , ** kwargs ) def predict ( self , X_test ): \"\"\"Inference on primary GPU.\"\"\" return self . model . module . predict ( X_test ) # Usage if torch . cuda . device_count () > 1 : model = DataParallelWrapper ( TabularPipeline ( model_name = 'TabICL' ), device_ids = [ 0 , 1 , 2 , 3 ] # Use GPUs 0-3 ) else : model = TabularPipeline ( model_name = 'TabICL' ) model . fit ( X_train , y_train )","title":"4.1 DataParallel (Simpler, Single-Machine)"},{"location":"advanced/multi-gpu/#42-distributeddataparallel-recommended","text":"More efficient than DataParallel: import torch import torch.distributed as dist from torch.nn.parallel import DistributedDataParallel def setup_distributed (): \"\"\"Setup distributed training environment.\"\"\" dist . init_process_group ( backend = 'nccl' , # NVIDIA Collective Communications Library init_method = 'env://' ) def cleanup (): \"\"\"Cleanup distributed environment.\"\"\" dist . destroy_process_group () def train_distributed (): \"\"\"Distributed training.\"\"\" setup_distributed () # Get process rank and world size rank = dist . get_rank () world_size = dist . get_world_size () # Create model on current device device = torch . device ( f 'cuda: { rank } ' ) pipeline = TabularPipeline ( model_name = 'TabICL' , tuning_strategy = 'base-ft' , tuning_params = { 'device' : device } ) # Wrap with DistributedDataParallel model = DistributedDataParallel ( pipeline . model , device_ids = [ rank ]) # Training code model . fit ( X_train , y_train ) cleanup () # Launch with: python -m torch.distributed.launch --nproc_per_node=4 script.py","title":"4.2 DistributedDataParallel (Recommended)"},{"location":"advanced/multi-gpu/#5-launch-methods","text":"","title":"5. Launch Methods"},{"location":"advanced/multi-gpu/#51-torchrun-pytorch-110","text":"Recommended method : # Single-machine, 4 GPUs torchrun --nproc_per_node = 4 train_script.py # Multi-machine (8 GPUs total) torchrun \\ --nproc_per_node = 4 \\ --nnodes = 2 \\ --node_rank = 0 \\ --master_addr = 192 .168.1.100 \\ --master_port = 29500 \\ train_script.py","title":"5.1 torchrun (PyTorch 1.10+)"},{"location":"advanced/multi-gpu/#52-torchdistributedlaunch-legacy","text":"# Single-machine, 4 GPUs python -m torch.distributed.launch \\ --nproc_per_node = 4 \\ train_script.py # With additional args python -m torch.distributed.launch \\ --nproc_per_node = 4 \\ train_script.py \\ --learning_rate 2e-5 \\ --epochs 5","title":"5.2 torch.distributed.launch (Legacy)"},{"location":"advanced/multi-gpu/#53-manual-launch","text":"# Terminal 1: GPU 0 CUDA_VISIBLE_DEVICES = 0 python train_script.py # Terminal 2: GPU 1 CUDA_VISIBLE_DEVICES = 1 python train_script.py # Terminal 3: GPU 2 CUDA_VISIBLE_DEVICES = 2 python train_script.py # Terminal 4: GPU 3 CUDA_VISIBLE_DEVICES = 3 python train_script.py","title":"5.3 Manual Launch"},{"location":"advanced/multi-gpu/#6-training-scripts","text":"","title":"6. Training Scripts"},{"location":"advanced/multi-gpu/#61-complete-distributed-training-script","text":"import os import torch import torch.distributed as dist from torch.nn.parallel import DistributedDataParallel from torch.utils.data import DataLoader , DistributedSampler from tabtune import TabularPipeline def main (): \"\"\"Main training function.\"\"\" # Initialize distributed training dist . init_process_group ( backend = 'nccl' ) rank = dist . get_rank () world_size = dist . get_world_size () # Set device device = torch . device ( f 'cuda: { rank } ' ) torch . cuda . set_device ( device ) # Print rank info if rank == 0 : print ( f \"Training on { world_size } GPUs\" ) # Load data X_train , y_train = load_data () # Create distributed sampler train_sampler = DistributedSampler ( dataset = range ( len ( X_train )), num_replicas = world_size , rank = rank , shuffle = True ) # Create dataloader train_loader = DataLoader ( train_sampler , batch_size = 32 , num_workers = 2 ) # Create model pipeline = TabularPipeline ( model_name = 'TabICL' , tuning_strategy = 'base-ft' , tuning_params = { 'device' : device , 'epochs' : 5 , 'learning_rate' : 2e-5 } ) # Wrap with DistributedDataParallel pipeline . model = DistributedDataParallel ( pipeline . model , device_ids = [ rank ] ) # Training if rank == 0 : print ( \"Starting training...\" ) pipeline . fit ( X_train [ train_sampler . indices ], y_train [ train_sampler . indices ]) # Evaluation on rank 0 only if rank == 0 : metrics = pipeline . evaluate ( X_test , y_test ) print ( f \"Validation Accuracy: { metrics [ 'accuracy' ] : .4f } \" ) # Cleanup dist . destroy_process_group () if __name__ == '__main__' : main ()","title":"6.1 Complete Distributed Training Script"},{"location":"advanced/multi-gpu/#62-synchronization-across-ranks","text":"def train_with_sync (): \"\"\"Training with synchronization points.\"\"\" dist . init_process_group ( backend = 'nccl' ) rank = dist . get_rank () # Training for epoch in range ( num_epochs ): # ... training code ... # Synchronize all ranks dist . barrier () # Evaluation (on rank 0 only) if rank == 0 : metrics = evaluate () print ( f \"Epoch { epoch } : { metrics } \" ) # Broadcast best checkpoint from rank 0 if rank == 0 : best_state = pipeline . model . state_dict () else : best_state = None # Distribute to all ranks dist . broadcast_object_list ([ best_state ], src = 0 ) if rank != 0 : pipeline . model . load_state_dict ( best_state [ 0 ]) dist . destroy_process_group ()","title":"6.2 Synchronization Across Ranks"},{"location":"advanced/multi-gpu/#7-performance-optimization","text":"","title":"7. Performance Optimization"},{"location":"advanced/multi-gpu/#71-gradient-accumulation","text":"Increase effective batch size without memory increase: def train_with_accumulation ( num_accumulation_steps = 4 ): \"\"\"Training with gradient accumulation.\"\"\" optimizer = torch . optim . AdamW ( model . parameters ()) for epoch in range ( num_epochs ): for i , batch in enumerate ( dataloader ): # Forward pass loss = model ( batch ) # Backward (accumulate gradients) ( loss / num_accumulation_steps ) . backward () # Update weights if ( i + 1 ) % num_accumulation_steps == 0 : optimizer . step () optimizer . zero_grad ()","title":"7.1 Gradient Accumulation"},{"location":"advanced/multi-gpu/#72-overlapping-computation-communication","text":"def train_with_overlap (): \"\"\"Training with async communication.\"\"\" for epoch in range ( num_epochs ): for batch in dataloader : # Forward & backward (computation) loss = model ( batch ) loss . backward () # Start async gradient reduction reduction_future = dist . all_reduce ( model . grad , async_op = True # Non-blocking ) # Do other work while reducing # ... # Wait for reduction to complete reduction_future . wait () # Update weights optimizer . step ()","title":"7.2 Overlapping Computation &amp; Communication"},{"location":"advanced/multi-gpu/#73-mixed-precision-with-distributed-training","text":"from torch.cuda.amp import autocast , GradScaler def train_mixed_precision (): \"\"\"Multi-GPU training with mixed precision.\"\"\" scaler = GradScaler () for epoch in range ( num_epochs ): for batch in dataloader : # Forward with autocast with autocast ( dtype = torch . float16 ): loss = model ( batch ) # Backward with scaling scaler . scale ( loss ) . backward () # All-reduce scaled gradients for param in model . parameters (): if param . grad is not None : dist . all_reduce ( param . grad ) # Update weights scaler . step ( optimizer ) scaler . update ()","title":"7.3 Mixed Precision with Distributed Training"},{"location":"advanced/multi-gpu/#8-scaling-efficiency","text":"","title":"8. Scaling Efficiency"},{"location":"advanced/multi-gpu/#81-linear-scaling-rule","text":"# Optimal learning rate for N GPUs base_learning_rate = 2e-5 num_gpus = 4 optimal_learning_rate = base_learning_rate * num_gpus # Why: Batch size \u00d7 num_gpus, so learning rate should scale # Or more conservatively: optimal_learning_rate = base_learning_rate * ( num_gpus ** 0.5 )","title":"8.1 Linear Scaling Rule"},{"location":"advanced/multi-gpu/#82-speedup-analysis","text":"def analyze_scaling ( times_single_gpu , times_multi_gpu ): \"\"\"Analyze multi-GPU speedup.\"\"\" num_gpus = len ( times_multi_gpu ) speedup = times_single_gpu / times_multi_gpu efficiency = speedup / num_gpus # Ideally 1.0 (100%) print ( f \"GPU Count: { num_gpus } \" ) print ( f \"Time (1 GPU): { times_single_gpu : .2f } s\" ) print ( f \"Time ( { num_gpus } GPUs): { times_multi_gpu : .2f } s\" ) print ( f \"Speedup: { speedup : .2f } x ( { efficiency * 100 : .1f } % efficiency)\" ) return speedup , efficiency # Typical results # 2 GPUs: 1.8x speedup (90% efficiency) # 4 GPUs: 3.5x speedup (87.5% efficiency) # 8 GPUs: 6.5x speedup (81% efficiency)","title":"8.2 Speedup Analysis"},{"location":"advanced/multi-gpu/#9-distributed-challenges-solutions","text":"","title":"9. Distributed Challenges &amp; Solutions"},{"location":"advanced/multi-gpu/#91-communication-overhead","text":"# Problem: Communication becomes bottleneck # Solution 1: Larger batch size tuning_params = { 'batch_size' : 128 , # Instead of 32 } # Solution 2: Gradient accumulation tuning_params = { 'batch_size' : 32 , 'gradient_accumulation_steps' : 4 # Effective: 128 } # Solution 3: Reduce communication frequency # Communicate every N steps instead of every step","title":"9.1 Communication Overhead"},{"location":"advanced/multi-gpu/#92-load-imbalance","text":"# Problem: Some GPUs finish before others # Solution: Dynamic load balancing def balanced_sampler ( dataset , num_replicas , rank ): \"\"\"Create balanced sampler across ranks.\"\"\" # Ensure each rank gets similar amount of work samples_per_rank = len ( dataset ) // num_replicas remainder = len ( dataset ) % num_replicas start = rank * samples_per_rank + min ( rank , remainder ) end = start + samples_per_rank + ( 1 if rank < remainder else 0 ) indices = list ( range ( start , end )) return DistributedSampler ( indices , num_replicas = num_replicas , rank = rank )","title":"9.2 Load Imbalance"},{"location":"advanced/multi-gpu/#93-gradient-divergence","text":"# Problem: Different GPUs compute different gradients # Solution: Proper synchronization def synchronized_training (): \"\"\"Ensure all ranks have synchronized gradients.\"\"\" for batch in dataloader : # Compute gradients loss = model ( batch ) loss . backward () # Synchronize gradients across all ranks for param in model . parameters (): dist . all_reduce ( param . grad ) param . grad /= world_size # Average # Update optimizer . step ()","title":"9.3 Gradient Divergence"},{"location":"advanced/multi-gpu/#10-complete-multi-gpu-example","text":"#!/usr/bin/env python \"\"\"Multi-GPU training example.\"\"\" import os import torch import torch.distributed as dist from torch.nn.parallel import DistributedDataParallel from tabtune import TabularPipeline import argparse def main (): parser = argparse . ArgumentParser () parser . add_argument ( '--epochs' , type = int , default = 5 ) parser . add_argument ( '--batch_size' , type = int , default = 32 ) parser . add_argument ( '--learning_rate' , type = float , default = 2e-5 ) args = parser . parse_args () # Initialize distributed training dist . init_process_group ( backend = 'nccl' ) rank = dist . get_rank () world_size = dist . get_world_size () device = torch . device ( f 'cuda: { rank } ' ) if rank == 0 : print ( f \"Starting training on { world_size } GPUs\" ) # Load data X_train , y_train , X_test , y_test = load_dataset () # Scale learning rate with batch size scaled_lr = args . learning_rate * world_size # Create pipeline pipeline = TabularPipeline ( model_name = 'TabICL' , tuning_strategy = 'base-ft' , tuning_params = { 'device' : device , 'epochs' : args . epochs , 'learning_rate' : scaled_lr , 'batch_size' : args . batch_size } ) # Wrap with DDP pipeline . model = DistributedDataParallel ( pipeline . model , device_ids = [ rank ] ) # Training if rank == 0 : print ( f \"Training with LR= { scaled_lr : .2e } , batch_size= { args . batch_size } \" ) pipeline . fit ( X_train , y_train ) # Synchronize before evaluation dist . barrier () # Evaluation on primary GPU only if rank == 0 : metrics = pipeline . evaluate ( X_test , y_test ) print ( f \"Final accuracy: { metrics [ 'accuracy' ] : .4f } \" ) dist . destroy_process_group () if __name__ == '__main__' : main () Launch: torchrun --nproc_per_node = 4 train_distributed.py --epochs 5 --batch_size 32","title":"10. Complete Multi-GPU Example"},{"location":"advanced/multi-gpu/#11-debugging-multi-gpu-issues","text":"","title":"11. Debugging Multi-GPU Issues"},{"location":"advanced/multi-gpu/#111-common-problems","text":"# Problem: Hanging/deadlock # Solution: Use timeout and debug flags os . environ [ 'NCCL_DEBUG' ] = 'INFO' os . environ [ 'TORCH_DISTRIBUTED_DEBUG' ] = 'DETAIL' dist . init_process_group ( backend = 'nccl' , timeout = timedelta ( minutes = 30 ) ) # Problem: GPU memory imbalance # Solution: Check and balance if rank == 0 : for i in range ( world_size ): print ( f \"GPU { i } : { torch . cuda . get_device_properties ( i ) . total_memory / 1e9 : .1f } GB\" ) # Problem: Rank synchronization issues # Solution: Add explicit barriers dist . barrier () # Wait for all ranks","title":"11.1 Common Problems"},{"location":"advanced/multi-gpu/#12-best-practices","text":"","title":"12. Best Practices"},{"location":"advanced/multi-gpu/#dos","text":"\u2705 Use DistributedDataParallel over DataParallel \u2705 Scale learning rate with batch size \u2705 Use proper samplers (DistributedSampler) \u2705 Synchronize at checkpoints \u2705 Evaluate on rank 0 only \u2705 Monitor all GPU memory usage \u2705 Test on fewer GPUs first","title":"\u2705 Do's"},{"location":"advanced/multi-gpu/#donts","text":"\u274c Don't use DataParallel for multi-machine \u274c Don't forget to set environment variables \u274c Don't run inference on all ranks \u274c Don't ignore communication overhead \u274c Don't over-subscribe GPU memory \u274c Don't change data on different ranks","title":"\u274c Don'ts"},{"location":"advanced/multi-gpu/#13-performance-benchmarks","text":"Dataset: 500K samples, TabICL model 1 GPU: Time: 120 minutes Memory: 12 GB 2 GPUs (DDP): Time: 65 minutes (1.85x speedup) Memory: 6 GB per GPU Efficiency: 92.5% 4 GPUs (DDP): Time: 35 minutes (3.43x speedup) Memory: 3 GB per GPU Efficiency: 85.7% 8 GPUs (DDP): Time: 20 minutes (6.0x speedup) Memory: 1.5 GB per GPU Efficiency: 75%","title":"13. Performance Benchmarks"},{"location":"advanced/multi-gpu/#14-quick-reference","text":"Aspect Single GPU Multi-GPU DDP Setup Simple Moderate Communication None NCCL Speedup 1x ~(GPUs-0.3) Memory/GPU Full Full/GPUs Best Use Development Production","title":"14. Quick Reference"},{"location":"advanced/multi-gpu/#15-next-steps","text":"Memory Optimization - Memory management with DDP Hyperparameter Tuning - Scaling learning rates Tuning Strategies - PEFT with DDP Examples - Multi-GPU benchmarks Scale TabTune efficiently across multiple GPUs for production-grade training!","title":"15. Next Steps"},{"location":"advanced/peft-lora/","text":"PEFT & LoRA: Parameter-Efficient Fine-Tuning for Tabular Models \u00b6 This document provides an in-depth guide to Parameter-Efficient Fine-Tuning (PEFT) using Low-Rank Adaptation (LoRA) for TabTune models. Learn the theory, implementation, and best practices for memory-efficient model adaptation. 1. Introduction to PEFT and LoRA \u00b6 1.1 What is PEFT? \u00b6 Parameter-Efficient Fine-Tuning (PEFT) is a set of techniques to adapt large pre-trained models using only a small fraction of the total parameters, dramatically reducing: Memory consumption (90% reduction) Training time (2-3x speedup) Storage requirements (only store small adapters) 1.2 What is LoRA? \u00b6 Low-Rank Adaptation (LoRA) is a specific PEFT technique that: Freezes pre-trained model weights Adds small trainable \"adapter\" layers Uses low-rank decomposition for efficiency Trains only 1-10% of parameters 1.3 Key Innovation \u00b6 Instead of updating all weights, LoRA learns a low-rank approximation of weight updates: [ W' = W_0 + \\Delta W = W_0 + BA ] Where: - (W_0): Original frozen weights (large) - (\\Delta W = BA): Low-rank decomposition - (A): Input projection (small) - (B): Output projection (small) - (r): Rank (typically 4-16, much smaller than weight dimensions) 2. Mathematical Foundation \u00b6 2.1 Low-Rank Decomposition \u00b6 For a weight matrix (W \\in \\mathbb{R}^{d_{out} \\times d_{in}}), LoRA represents updates as: [ \\Delta W = BA, \\quad B \\in \\mathbb{R}^{d_{out} \\times r}, A \\in \\mathbb{R}^{r \\times d_{in}} ] Complexity Reduction : - Full weights: (d_{out} \\times d_{in}) parameters - LoRA: (r(d_{out} + d_{in})) parameters - Compression ratio: (\\frac{r(d_{out} + d_{in})}{d_{out} \\times d_{in}}) Example : For a 768\u00d7768 weight matrix: - Full: 589,824 parameters - LoRA (r=8): 12,288 parameters - Compression: 98% reduction 2.2 Scaling \u00b6 To balance adaptation magnitude, LoRA scales the output: [ h = W_0 x + \\alpha \\frac{1}{r} B(Ax) ] Where (\\alpha) (lora_alpha) controls the scaling factor (\\frac{\\alpha}{r}). Effect of Alpha : - Higher alpha: Larger adaptation magnitude - Default: (\\alpha = 2r) (empirically optimal) 2.3 Dropout \u00b6 Dropout is applied to the input before LoRA projection for regularization: [ h = W_0 x + \\alpha \\frac{1}{r} B(\\text{dropout}(Ax)) ] 3. LoRA in TabTune \u00b6 3.1 LoRA Configuration \u00b6 peft_config = { 'r' : 8 , # Rank (main hyperparameter) 'lora_alpha' : 16 , # Scaling factor 'lora_dropout' : 0.05 , # Dropout probability 'target_modules' : None , # Modules to adapt (model default) 'bias' : 'none' # Bias handling } 3.2 LoRA Linear Layer \u00b6 TabTune implements LoRALinear that wraps standard PyTorch linear layers: class LoRALinear ( nn . Module ): def __init__ ( self , base_linear , r = 8 , alpha = 16 , dropout = 0.05 ): super () . __init__ () self . base = base_linear # Frozen base layer self . lora_A = nn . Linear ( ... , r ) # Adapter A self . lora_B = nn . Linear ( r , ... ) # Adapter B self . dropout = nn . Dropout ( dropout ) self . scaling = alpha / r def forward ( self , x ): # Base forward (no gradients) base_out = self . base ( x ) # LoRA forward lora_out = self . lora_B ( self . lora_A ( self . dropout ( x ))) * self . scaling return base_out + lora_out 3.3 Weight Freezing \u00b6 Base model weights: requires_grad=False LoRA adapters: requires_grad=True Enables gradient computation only on adapters 4. LoRA Hyperparameter Tuning \u00b6 4.1 Rank Selection \u00b6 The rank r is the most critical hyperparameter: Rank Parameters Memory Accuracy Speed r=2 Minimal Very Low Lower \u2b50\u2b50\u2b50\u2b50\u2b50 r=4 Low Low Good \u2b50\u2b50\u2b50\u2b50\u2b50 r=8 Moderate Moderate Better \u2b50\u2b50\u2b50\u2b50 r=16 High High Best \u2b50\u2b50\u2b50 r=32 Very High Very High Optimal \u2b50\u2b50 Guidelines : - Small data (10K) : r=4 (enough for adaptation) - Medium data (100K) : r=8 (balanced) - Large data (1M) : r=16 (more expressive) - Very constrained : r=2 (minimum viable) Rule of Thumb : [ r = \\max(4, \\frac{\\text{dataset_size}}{50000}) ] 4.2 Alpha Selection \u00b6 Alpha controls the magnitude of LoRA contribution: # Recommended: alpha = 2 * rank peft_config = { 'r' : 8 , 'lora_alpha' : 16 , # = 2 * 8 'lora_dropout' : 0.05 } Effects : - Alpha too low : Adaptation weak, training slow - Alpha = 2r : Empirically optimal - Alpha too high : Training unstable, may diverge 4.3 Dropout Probability \u00b6 LoRA dropout acts as regularization: lora_dropout_values = { 0.0 : 'No regularization (may overfit)' , 0.05 : 'Light regularization (default)' , 0.1 : 'Moderate regularization' , 0.2 : 'Strong regularization (for small data)' } Selection : - Large data : 0.05 (default) - Small data : 0.1-0.2 (prevent overfitting) - Already regularized model : 0.0-0.05 5. Target Module Selection \u00b6 5.1 Module Hierarchy \u00b6 TabTune identifies target modules via pattern matching: # LoRA targets linear transformation layers target_modules = { 'column_embeddings' : 'Column feature processing' , 'row_attention' : 'Row-wise interactions' , 'prediction_head' : 'Final prediction' , 'decoder' : 'Feature reconstruction' } 5.2 Model-Specific Defaults \u00b6 Each model has pre-configured target modules optimized for LoRA: TabICL : target_modules = [ 'col_embedder.tf_col' , 'row_interactor' , 'icl_predictor.tf_icl' , 'icl_predictor.decoder' ] TabDPT : target_modules = [ 'transformer_encoder' , 'encoder' , 'y_encoder' , 'head' ] Mitra : target_modules = [ 'x_embedding' , 'layers' , 'final_layer' ] 5.3 Custom Target Selection \u00b6 Override defaults for specific needs: pipeline = TabularPipeline ( model_name = 'TabICL' , tuning_strategy = 'peft' , tuning_params = { 'peft_config' : { 'r' : 8 , 'lora_alpha' : 16 , 'target_modules' : [ 'col_embedder.tf_col' , # Only column embedder 'icl_predictor.decoder' # Plus decoder ] } } ) 6. LoRA Training \u00b6 6.1 Typical Training Loop \u00b6 from tabtune import TabularPipeline # Create pipeline with PEFT pipeline = TabularPipeline ( model_name = 'TabICL' , tuning_strategy = 'peft' , tuning_params = { 'device' : 'cuda' , 'epochs' : 5 , 'learning_rate' : 2e-4 , # Typically higher than base-ft 'peft_config' : { 'r' : 8 , 'lora_alpha' : 16 } } ) # Training (only adapters are updated) pipeline . fit ( X_train , y_train ) # Inference predictions = pipeline . predict ( X_test ) 6.2 Learning Rate Strategy \u00b6 LoRA uses different learning rates than base fine-tuning: # Base fine-tuning (all parameters) learning_rate_base_ft = 2e-5 # LoRA fine-tuning (small parameters) learning_rate_peft = 2e-4 # 10x higher typical # Rationale: Smaller parameter updates need larger learning rates 6.3 Optimizer Configuration \u00b6 # LoRA-specific optimizer settings optimizer_config = { 'optimizer' : 'adamw' , 'learning_rate' : 2e-4 , 'weight_decay' : 0.01 , 'eps' : 1e-8 , 'betas' : ( 0.9 , 0.999 ) } 7. Memory Analysis \u00b6 7.1 Memory Breakdown \u00b6 Base Fine-Tuning (full model): Model weights: 500 MB Optimizer states: 1 GB (Adam: 2x weights) Gradients: 500 MB Activations: 200 MB \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Total: ~2.2 GB per forward/backward LoRA Fine-Tuning (adapters only): Model weights: 500 MB (frozen, no gradients) LoRA adapters: 5 MB Optimizer states: 10 MB (only adapters) Gradients: 5 MB Activations: 200 MB \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Total: ~700 MB per forward/backward ~70% reduction 7.2 Practical Memory Savings \u00b6 Model Base-FT LoRA Savings TabICL 12 GB 4 GB 66% TabDPT 24 GB 8 GB 66% Mitra 20 GB 6 GB 70% 9. Complete Example \u00b6 9.1 Memory-Constrained Scenario \u00b6 from tabtune import TabularPipeline import torch # Check available GPU memory print ( f \"Available GPU memory: { torch . cuda . get_device_properties ( 0 ) . total_memory / 1e9 : .1f } GB\" ) # LoRA for 4GB GPU pipeline = TabularPipeline ( model_name = 'TabICL' , tuning_strategy = 'peft' , tuning_params = { 'device' : 'cuda' , 'epochs' : 5 , 'learning_rate' : 2e-4 , 'batch_size' : 16 , 'peft_config' : { 'r' : 4 , # Lower rank for less memory 'lora_alpha' : 8 , 'lora_dropout' : 0.1 } } ) pipeline . fit ( X_train , y_train ) metrics = pipeline . evaluate ( X_test , y_test ) print ( f \"LoRA Accuracy: { metrics [ 'accuracy' ] : .4f } \" ) 9.2 Rank Exploration \u00b6 from tabtune import TabularLeaderboard # Compare different LoRA ranks lb = TabularLeaderboard ( X_train , X_test , y_train , y_test ) for r in [ 2 , 4 , 8 , 16 ]: lb . add_model ( 'TabICL' , 'peft' , name = f 'LoRA-r { r } ' , tuning_params = { 'epochs' : 5 , 'peft_config' : { 'r' : r , 'lora_alpha' : 2 * r } } ) results = lb . run ( rank_by = 'accuracy' ) print ( lb . get_ranking ()) 9.3 Memory-Speed Trade-off \u00b6 # Explore memory-speed-accuracy trade-off configs = [ { 'r' : 2 , 'name' : 'Ultra-Light' }, { 'r' : 4 , 'name' : 'Light' }, { 'r' : 8 , 'name' : 'Medium' }, { 'r' : 16 , 'name' : 'Heavy' } ] for config in configs : pipeline = TabularPipeline ( model_name = 'TabICL' , tuning_strategy = 'peft' , tuning_params = { 'device' : 'cuda' , 'epochs' : 3 , 'peft_config' : { 'r' : config [ 'r' ]} } ) # Time training import time start = time . time () pipeline . fit ( X_train , y_train ) elapsed = time . time () - start # Get memory usage mem = torch . cuda . max_memory_allocated () / 1e9 # Evaluate metrics = pipeline . evaluate ( X_test , y_test ) print ( f \" { config [ 'name' ] : 12 } | Rank: { config [ 'r' ] : 2 } | \" f \"Time: { elapsed : 6.1f } s | Memory: { mem : 5.1f } GB | \" f \"Accuracy: { metrics [ 'accuracy' ] : .4f } \" ) 10. Saving and Loading LoRA Models \u00b6 10.1 Save LoRA Adapters Only \u00b6 import torch # Save only LoRA adapter weights (minimal storage) lora_state = { 'rank' : 8 , 'alpha' : 16 , 'lora_a' : pipeline . model . lora_A . state_dict (), 'lora_b' : pipeline . model . lora_B . state_dict () } torch . save ( lora_state , 'lora_adapters.pt' ) # ~1-5 MB 10.2 Load and Merge \u00b6 # Load adapters and merge with base model lora_state = torch . load ( 'lora_adapters.pt' ) # Merge LoRA into base weights (optional, for inference optimization) merged_weights = base_weights + ( lora_B @ lora_A ) * alpha / r 10.3 Full Pipeline Serialization \u00b6 # Save complete pipeline with LoRA adapters pipeline . save ( 'pipeline_with_lora.joblib' ) # Load and use loaded = TabularPipeline . load ( 'pipeline_with_lora.joblib' ) predictions = loaded . predict ( X_test ) 11. Troubleshooting \u00b6 Issue: \"LoRA accuracy much lower than base-FT\" \u00b6 Solution : Increase rank peft_config = { 'r' : 16 , # Instead of 8 'lora_alpha' : 32 } Issue: \"Training diverging with LoRA\" \u00b6 Solution : Reduce learning rate tuning_params = { 'learning_rate' : 1e-4 , # Instead of 2e-4 'warmup_steps' : 500 } Issue: \"Still out of memory with LoRA\" \u00b6 Solution : Further reduce parameters peft_config = { 'r' : 2 , # Minimum viable 'lora_dropout' : 0.2 # Stronger regularization } tuning_params = { 'batch_size' : 4 # Smaller batches } Issue: \"LoRA inference slow\" \u00b6 Solution : Use merged weights # Merge LoRA weights into base after training merged_model = merge_lora_weights ( pipeline . model ) 13. Best Practices \u00b6 \u2705 Do's \u00b6 \u2705 Start with r=8 (good default) \u2705 Use 2x learning rate for LoRA vs base-FT \u2705 Include warmup phase (prevent instability) \u2705 Monitor gradient norms \u2705 Use gradient clipping \u2705 Save adapter weights separately \u2705 Test rank selection with leaderboard \u274c Don'ts \u00b6 \u274c Don't use same learning rate as base-FT \u274c Don't train very low ranks (r<2) without good reason \u274c Don't skip regularization on small data \u274c Don't forget to freeze base model \u274c Don't use LoRA on tiny models (overhead not worth it) 14. Comparison: Base-FT vs LoRA \u00b6 Aspect Base-FT LoRA Winner Accuracy High Medium-High Base-FT (~1% better) Memory Very High Low LoRA (70% savings) Speed Slow Fast LoRA (2-3x faster) Storage Huge Tiny LoRA (100x smaller) Scalability Limited Excellent LoRA Production Complex Simple LoRA Learning Curve Medium Low LoRA 15. Quick Reference \u00b6 Task r Alpha Dropout LR Small data (10K) 4 8 0.1 2e-4 Medium data (100K) 8 16 0.05 2e-4 Large data (1M) 16 32 0.02 1e-4 Memory limited 2 4 0.2 1e-4 Max accuracy 16 32 0.05 5e-5 16. Next Steps \u00b6 Tuning Strategies - Compare strategies Hyperparameter Tuning - Full optimization guide Models Overview - PEFT support per model TabularLeaderboard - Compare configurations LoRA enables efficient fine-tuning of large tabular models. Use it for memory-constrained environments while maintaining strong performance!","title":"PEFT & LoRA"},{"location":"advanced/peft-lora/#peft-lora-parameter-efficient-fine-tuning-for-tabular-models","text":"This document provides an in-depth guide to Parameter-Efficient Fine-Tuning (PEFT) using Low-Rank Adaptation (LoRA) for TabTune models. Learn the theory, implementation, and best practices for memory-efficient model adaptation.","title":"PEFT &amp; LoRA: Parameter-Efficient Fine-Tuning for Tabular Models"},{"location":"advanced/peft-lora/#1-introduction-to-peft-and-lora","text":"","title":"1. Introduction to PEFT and LoRA"},{"location":"advanced/peft-lora/#11-what-is-peft","text":"Parameter-Efficient Fine-Tuning (PEFT) is a set of techniques to adapt large pre-trained models using only a small fraction of the total parameters, dramatically reducing: Memory consumption (90% reduction) Training time (2-3x speedup) Storage requirements (only store small adapters)","title":"1.1 What is PEFT?"},{"location":"advanced/peft-lora/#12-what-is-lora","text":"Low-Rank Adaptation (LoRA) is a specific PEFT technique that: Freezes pre-trained model weights Adds small trainable \"adapter\" layers Uses low-rank decomposition for efficiency Trains only 1-10% of parameters","title":"1.2 What is LoRA?"},{"location":"advanced/peft-lora/#13-key-innovation","text":"Instead of updating all weights, LoRA learns a low-rank approximation of weight updates: [ W' = W_0 + \\Delta W = W_0 + BA ] Where: - (W_0): Original frozen weights (large) - (\\Delta W = BA): Low-rank decomposition - (A): Input projection (small) - (B): Output projection (small) - (r): Rank (typically 4-16, much smaller than weight dimensions)","title":"1.3 Key Innovation"},{"location":"advanced/peft-lora/#2-mathematical-foundation","text":"","title":"2. Mathematical Foundation"},{"location":"advanced/peft-lora/#21-low-rank-decomposition","text":"For a weight matrix (W \\in \\mathbb{R}^{d_{out} \\times d_{in}}), LoRA represents updates as: [ \\Delta W = BA, \\quad B \\in \\mathbb{R}^{d_{out} \\times r}, A \\in \\mathbb{R}^{r \\times d_{in}} ] Complexity Reduction : - Full weights: (d_{out} \\times d_{in}) parameters - LoRA: (r(d_{out} + d_{in})) parameters - Compression ratio: (\\frac{r(d_{out} + d_{in})}{d_{out} \\times d_{in}}) Example : For a 768\u00d7768 weight matrix: - Full: 589,824 parameters - LoRA (r=8): 12,288 parameters - Compression: 98% reduction","title":"2.1 Low-Rank Decomposition"},{"location":"advanced/peft-lora/#22-scaling","text":"To balance adaptation magnitude, LoRA scales the output: [ h = W_0 x + \\alpha \\frac{1}{r} B(Ax) ] Where (\\alpha) (lora_alpha) controls the scaling factor (\\frac{\\alpha}{r}). Effect of Alpha : - Higher alpha: Larger adaptation magnitude - Default: (\\alpha = 2r) (empirically optimal)","title":"2.2 Scaling"},{"location":"advanced/peft-lora/#23-dropout","text":"Dropout is applied to the input before LoRA projection for regularization: [ h = W_0 x + \\alpha \\frac{1}{r} B(\\text{dropout}(Ax)) ]","title":"2.3 Dropout"},{"location":"advanced/peft-lora/#3-lora-in-tabtune","text":"","title":"3. LoRA in TabTune"},{"location":"advanced/peft-lora/#31-lora-configuration","text":"peft_config = { 'r' : 8 , # Rank (main hyperparameter) 'lora_alpha' : 16 , # Scaling factor 'lora_dropout' : 0.05 , # Dropout probability 'target_modules' : None , # Modules to adapt (model default) 'bias' : 'none' # Bias handling }","title":"3.1 LoRA Configuration"},{"location":"advanced/peft-lora/#32-lora-linear-layer","text":"TabTune implements LoRALinear that wraps standard PyTorch linear layers: class LoRALinear ( nn . Module ): def __init__ ( self , base_linear , r = 8 , alpha = 16 , dropout = 0.05 ): super () . __init__ () self . base = base_linear # Frozen base layer self . lora_A = nn . Linear ( ... , r ) # Adapter A self . lora_B = nn . Linear ( r , ... ) # Adapter B self . dropout = nn . Dropout ( dropout ) self . scaling = alpha / r def forward ( self , x ): # Base forward (no gradients) base_out = self . base ( x ) # LoRA forward lora_out = self . lora_B ( self . lora_A ( self . dropout ( x ))) * self . scaling return base_out + lora_out","title":"3.2 LoRA Linear Layer"},{"location":"advanced/peft-lora/#33-weight-freezing","text":"Base model weights: requires_grad=False LoRA adapters: requires_grad=True Enables gradient computation only on adapters","title":"3.3 Weight Freezing"},{"location":"advanced/peft-lora/#4-lora-hyperparameter-tuning","text":"","title":"4. LoRA Hyperparameter Tuning"},{"location":"advanced/peft-lora/#41-rank-selection","text":"The rank r is the most critical hyperparameter: Rank Parameters Memory Accuracy Speed r=2 Minimal Very Low Lower \u2b50\u2b50\u2b50\u2b50\u2b50 r=4 Low Low Good \u2b50\u2b50\u2b50\u2b50\u2b50 r=8 Moderate Moderate Better \u2b50\u2b50\u2b50\u2b50 r=16 High High Best \u2b50\u2b50\u2b50 r=32 Very High Very High Optimal \u2b50\u2b50 Guidelines : - Small data (10K) : r=4 (enough for adaptation) - Medium data (100K) : r=8 (balanced) - Large data (1M) : r=16 (more expressive) - Very constrained : r=2 (minimum viable) Rule of Thumb : [ r = \\max(4, \\frac{\\text{dataset_size}}{50000}) ]","title":"4.1 Rank Selection"},{"location":"advanced/peft-lora/#42-alpha-selection","text":"Alpha controls the magnitude of LoRA contribution: # Recommended: alpha = 2 * rank peft_config = { 'r' : 8 , 'lora_alpha' : 16 , # = 2 * 8 'lora_dropout' : 0.05 } Effects : - Alpha too low : Adaptation weak, training slow - Alpha = 2r : Empirically optimal - Alpha too high : Training unstable, may diverge","title":"4.2 Alpha Selection"},{"location":"advanced/peft-lora/#43-dropout-probability","text":"LoRA dropout acts as regularization: lora_dropout_values = { 0.0 : 'No regularization (may overfit)' , 0.05 : 'Light regularization (default)' , 0.1 : 'Moderate regularization' , 0.2 : 'Strong regularization (for small data)' } Selection : - Large data : 0.05 (default) - Small data : 0.1-0.2 (prevent overfitting) - Already regularized model : 0.0-0.05","title":"4.3 Dropout Probability"},{"location":"advanced/peft-lora/#5-target-module-selection","text":"","title":"5. Target Module Selection"},{"location":"advanced/peft-lora/#51-module-hierarchy","text":"TabTune identifies target modules via pattern matching: # LoRA targets linear transformation layers target_modules = { 'column_embeddings' : 'Column feature processing' , 'row_attention' : 'Row-wise interactions' , 'prediction_head' : 'Final prediction' , 'decoder' : 'Feature reconstruction' }","title":"5.1 Module Hierarchy"},{"location":"advanced/peft-lora/#52-model-specific-defaults","text":"Each model has pre-configured target modules optimized for LoRA: TabICL : target_modules = [ 'col_embedder.tf_col' , 'row_interactor' , 'icl_predictor.tf_icl' , 'icl_predictor.decoder' ] TabDPT : target_modules = [ 'transformer_encoder' , 'encoder' , 'y_encoder' , 'head' ] Mitra : target_modules = [ 'x_embedding' , 'layers' , 'final_layer' ]","title":"5.2 Model-Specific Defaults"},{"location":"advanced/peft-lora/#53-custom-target-selection","text":"Override defaults for specific needs: pipeline = TabularPipeline ( model_name = 'TabICL' , tuning_strategy = 'peft' , tuning_params = { 'peft_config' : { 'r' : 8 , 'lora_alpha' : 16 , 'target_modules' : [ 'col_embedder.tf_col' , # Only column embedder 'icl_predictor.decoder' # Plus decoder ] } } )","title":"5.3 Custom Target Selection"},{"location":"advanced/peft-lora/#6-lora-training","text":"","title":"6. LoRA Training"},{"location":"advanced/peft-lora/#61-typical-training-loop","text":"from tabtune import TabularPipeline # Create pipeline with PEFT pipeline = TabularPipeline ( model_name = 'TabICL' , tuning_strategy = 'peft' , tuning_params = { 'device' : 'cuda' , 'epochs' : 5 , 'learning_rate' : 2e-4 , # Typically higher than base-ft 'peft_config' : { 'r' : 8 , 'lora_alpha' : 16 } } ) # Training (only adapters are updated) pipeline . fit ( X_train , y_train ) # Inference predictions = pipeline . predict ( X_test )","title":"6.1 Typical Training Loop"},{"location":"advanced/peft-lora/#62-learning-rate-strategy","text":"LoRA uses different learning rates than base fine-tuning: # Base fine-tuning (all parameters) learning_rate_base_ft = 2e-5 # LoRA fine-tuning (small parameters) learning_rate_peft = 2e-4 # 10x higher typical # Rationale: Smaller parameter updates need larger learning rates","title":"6.2 Learning Rate Strategy"},{"location":"advanced/peft-lora/#63-optimizer-configuration","text":"# LoRA-specific optimizer settings optimizer_config = { 'optimizer' : 'adamw' , 'learning_rate' : 2e-4 , 'weight_decay' : 0.01 , 'eps' : 1e-8 , 'betas' : ( 0.9 , 0.999 ) }","title":"6.3 Optimizer Configuration"},{"location":"advanced/peft-lora/#7-memory-analysis","text":"","title":"7. Memory Analysis"},{"location":"advanced/peft-lora/#71-memory-breakdown","text":"Base Fine-Tuning (full model): Model weights: 500 MB Optimizer states: 1 GB (Adam: 2x weights) Gradients: 500 MB Activations: 200 MB \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Total: ~2.2 GB per forward/backward LoRA Fine-Tuning (adapters only): Model weights: 500 MB (frozen, no gradients) LoRA adapters: 5 MB Optimizer states: 10 MB (only adapters) Gradients: 5 MB Activations: 200 MB \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Total: ~700 MB per forward/backward ~70% reduction","title":"7.1 Memory Breakdown"},{"location":"advanced/peft-lora/#72-practical-memory-savings","text":"Model Base-FT LoRA Savings TabICL 12 GB 4 GB 66% TabDPT 24 GB 8 GB 66% Mitra 20 GB 6 GB 70%","title":"7.2 Practical Memory Savings"},{"location":"advanced/peft-lora/#9-complete-example","text":"","title":"9. Complete Example"},{"location":"advanced/peft-lora/#91-memory-constrained-scenario","text":"from tabtune import TabularPipeline import torch # Check available GPU memory print ( f \"Available GPU memory: { torch . cuda . get_device_properties ( 0 ) . total_memory / 1e9 : .1f } GB\" ) # LoRA for 4GB GPU pipeline = TabularPipeline ( model_name = 'TabICL' , tuning_strategy = 'peft' , tuning_params = { 'device' : 'cuda' , 'epochs' : 5 , 'learning_rate' : 2e-4 , 'batch_size' : 16 , 'peft_config' : { 'r' : 4 , # Lower rank for less memory 'lora_alpha' : 8 , 'lora_dropout' : 0.1 } } ) pipeline . fit ( X_train , y_train ) metrics = pipeline . evaluate ( X_test , y_test ) print ( f \"LoRA Accuracy: { metrics [ 'accuracy' ] : .4f } \" )","title":"9.1 Memory-Constrained Scenario"},{"location":"advanced/peft-lora/#92-rank-exploration","text":"from tabtune import TabularLeaderboard # Compare different LoRA ranks lb = TabularLeaderboard ( X_train , X_test , y_train , y_test ) for r in [ 2 , 4 , 8 , 16 ]: lb . add_model ( 'TabICL' , 'peft' , name = f 'LoRA-r { r } ' , tuning_params = { 'epochs' : 5 , 'peft_config' : { 'r' : r , 'lora_alpha' : 2 * r } } ) results = lb . run ( rank_by = 'accuracy' ) print ( lb . get_ranking ())","title":"9.2 Rank Exploration"},{"location":"advanced/peft-lora/#93-memory-speed-trade-off","text":"# Explore memory-speed-accuracy trade-off configs = [ { 'r' : 2 , 'name' : 'Ultra-Light' }, { 'r' : 4 , 'name' : 'Light' }, { 'r' : 8 , 'name' : 'Medium' }, { 'r' : 16 , 'name' : 'Heavy' } ] for config in configs : pipeline = TabularPipeline ( model_name = 'TabICL' , tuning_strategy = 'peft' , tuning_params = { 'device' : 'cuda' , 'epochs' : 3 , 'peft_config' : { 'r' : config [ 'r' ]} } ) # Time training import time start = time . time () pipeline . fit ( X_train , y_train ) elapsed = time . time () - start # Get memory usage mem = torch . cuda . max_memory_allocated () / 1e9 # Evaluate metrics = pipeline . evaluate ( X_test , y_test ) print ( f \" { config [ 'name' ] : 12 } | Rank: { config [ 'r' ] : 2 } | \" f \"Time: { elapsed : 6.1f } s | Memory: { mem : 5.1f } GB | \" f \"Accuracy: { metrics [ 'accuracy' ] : .4f } \" )","title":"9.3 Memory-Speed Trade-off"},{"location":"advanced/peft-lora/#10-saving-and-loading-lora-models","text":"","title":"10. Saving and Loading LoRA Models"},{"location":"advanced/peft-lora/#101-save-lora-adapters-only","text":"import torch # Save only LoRA adapter weights (minimal storage) lora_state = { 'rank' : 8 , 'alpha' : 16 , 'lora_a' : pipeline . model . lora_A . state_dict (), 'lora_b' : pipeline . model . lora_B . state_dict () } torch . save ( lora_state , 'lora_adapters.pt' ) # ~1-5 MB","title":"10.1 Save LoRA Adapters Only"},{"location":"advanced/peft-lora/#102-load-and-merge","text":"# Load adapters and merge with base model lora_state = torch . load ( 'lora_adapters.pt' ) # Merge LoRA into base weights (optional, for inference optimization) merged_weights = base_weights + ( lora_B @ lora_A ) * alpha / r","title":"10.2 Load and Merge"},{"location":"advanced/peft-lora/#103-full-pipeline-serialization","text":"# Save complete pipeline with LoRA adapters pipeline . save ( 'pipeline_with_lora.joblib' ) # Load and use loaded = TabularPipeline . load ( 'pipeline_with_lora.joblib' ) predictions = loaded . predict ( X_test )","title":"10.3 Full Pipeline Serialization"},{"location":"advanced/peft-lora/#11-troubleshooting","text":"","title":"11. Troubleshooting"},{"location":"advanced/peft-lora/#issue-lora-accuracy-much-lower-than-base-ft","text":"Solution : Increase rank peft_config = { 'r' : 16 , # Instead of 8 'lora_alpha' : 32 }","title":"Issue: \"LoRA accuracy much lower than base-FT\""},{"location":"advanced/peft-lora/#issue-training-diverging-with-lora","text":"Solution : Reduce learning rate tuning_params = { 'learning_rate' : 1e-4 , # Instead of 2e-4 'warmup_steps' : 500 }","title":"Issue: \"Training diverging with LoRA\""},{"location":"advanced/peft-lora/#issue-still-out-of-memory-with-lora","text":"Solution : Further reduce parameters peft_config = { 'r' : 2 , # Minimum viable 'lora_dropout' : 0.2 # Stronger regularization } tuning_params = { 'batch_size' : 4 # Smaller batches }","title":"Issue: \"Still out of memory with LoRA\""},{"location":"advanced/peft-lora/#issue-lora-inference-slow","text":"Solution : Use merged weights # Merge LoRA weights into base after training merged_model = merge_lora_weights ( pipeline . model )","title":"Issue: \"LoRA inference slow\""},{"location":"advanced/peft-lora/#13-best-practices","text":"","title":"13. Best Practices"},{"location":"advanced/peft-lora/#dos","text":"\u2705 Start with r=8 (good default) \u2705 Use 2x learning rate for LoRA vs base-FT \u2705 Include warmup phase (prevent instability) \u2705 Monitor gradient norms \u2705 Use gradient clipping \u2705 Save adapter weights separately \u2705 Test rank selection with leaderboard","title":"\u2705 Do's"},{"location":"advanced/peft-lora/#donts","text":"\u274c Don't use same learning rate as base-FT \u274c Don't train very low ranks (r<2) without good reason \u274c Don't skip regularization on small data \u274c Don't forget to freeze base model \u274c Don't use LoRA on tiny models (overhead not worth it)","title":"\u274c Don'ts"},{"location":"advanced/peft-lora/#14-comparison-base-ft-vs-lora","text":"Aspect Base-FT LoRA Winner Accuracy High Medium-High Base-FT (~1% better) Memory Very High Low LoRA (70% savings) Speed Slow Fast LoRA (2-3x faster) Storage Huge Tiny LoRA (100x smaller) Scalability Limited Excellent LoRA Production Complex Simple LoRA Learning Curve Medium Low LoRA","title":"14. Comparison: Base-FT vs LoRA"},{"location":"advanced/peft-lora/#15-quick-reference","text":"Task r Alpha Dropout LR Small data (10K) 4 8 0.1 2e-4 Medium data (100K) 8 16 0.05 2e-4 Large data (1M) 16 32 0.02 1e-4 Memory limited 2 4 0.2 1e-4 Max accuracy 16 32 0.05 5e-5","title":"15. Quick Reference"},{"location":"advanced/peft-lora/#16-next-steps","text":"Tuning Strategies - Compare strategies Hyperparameter Tuning - Full optimization guide Models Overview - PEFT support per model TabularLeaderboard - Compare configurations LoRA enables efficient fine-tuning of large tabular models. Use it for memory-constrained environments while maintaining strong performance!","title":"16. Next Steps"},{"location":"api/data-processor/","text":"API: DataProcessor \u00b6 Bases: BaseEstimator , TransformerMixin The complete Data Preparation Engine for the TabTune library. Integrates a full suite of standard preprocessing tools with custom, model-specific logic. Source code in tabtune/Dataprocess/data_processor.py 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 class DataProcessor ( BaseEstimator , TransformerMixin ): \"\"\" The complete Data Preparation Engine for the TabTune library. Integrates a full suite of standard preprocessing tools with custom, model-specific logic. \"\"\" def __init__ ( self , model_name = None , override_types = None , imputation_strategy = 'mean' , categorical_encoding = 'onehot' , scaling_strategy = 'standard' , resampling_strategy = None , feature_selection_strategy = None , feature_selection_k = 10 , model_params = None ): self . model_name = model_name self . override_types = override_types self . imputation_strategy = imputation_strategy self . categorical_encoding = categorical_encoding self . scaling_strategy = scaling_strategy self . resampling_strategy = resampling_strategy self . feature_selection_strategy = feature_selection_strategy self . feature_selection_k = feature_selection_k self . model_params = model_params or {} self . _set_model_aware_defaults () # --- Internal State Attributes --- self . column_types_ = {} self . _is_fitted = False self . custom_preprocessor_ = None self . imputer_ = None self . scaler_ = None self . encoder_ = None self . resampler_ = None self . selector_ = None self . label_encoder_ = None self . _correlation_cols_to_drop = [] self . original_cols_ = None self . processing_summary_ = {} def _set_model_aware_defaults ( self ): \"\"\"Sets default strategies based on the model_name to ensure compatibility.\"\"\" if self . model_name : model_defaults = { 'TabPFN' : { 'categorical_encoding' : 'tabpfn_special' }, 'TabICL' : { 'categorical_encoding' : 'tabicl_special' }, 'OrionMSP' : { 'categorical_encoding' : 'orion_msp_special' }, 'ContextTab' : { 'categorical_encoding' : 'contexttab_special' }, 'Mitra' : { 'categorical_encoding' : 'mitra_special' }, 'OrionBix' : { 'categorical_encoding' : 'orion_bix_special' }, 'TabDPT' : { 'categorical_encoding' : 'tabdpt_special' }, } config = model_defaults . get ( self . model_name ) if config : self . imputation_strategy = config . get ( 'imputation_strategy' , self . imputation_strategy ) self . categorical_encoding = config . get ( 'categorical_encoding' , self . categorical_encoding ) self . scaling_strategy = config . get ( 'scaling_strategy' , self . scaling_strategy ) def _get_custom_preprocessor ( self ): \"\"\"Factory method to return the correct custom preprocessor instance.\"\"\" special_encoders = { 'tabpfn_special' : TabPFNPreprocessor , 'tabicl_special' : TabICLPreprocessor , 'orion_msp_special' : OrionMSPPreprocessor , 'contexttab_special' : ContextTabPreprocessor , 'mitra_special' : MitraPreprocessor , 'orion_bix_special' : OrionBixPreprocessor , 'tabdpt_special' : TabDPTPreprocessor , } if self . categorical_encoding in special_encoders : logger . info ( f \"[DataProcessor] Using special preprocessor for: { self . model_name } \" ) PreprocessorClass = special_encoders [ self . categorical_encoding ] if self . categorical_encoding == 'contexttab_special' : # Extract regression parameters from model_params regression_type = self . model_params . get ( 'regression_type' , 'l2' ) num_regression_bins = self . model_params . get ( 'num_regression_bins' , 16 ) return PreprocessorClass ( regression_type = regression_type , num_regression_bins = num_regression_bins ) return PreprocessorClass () return None def fit ( self , X , y = None ): X_fit = X . copy () self . original_cols_ = X_fit . columns . tolist () y_fit = y . copy () if y is not None else None self . custom_preprocessor_ = self . _get_custom_preprocessor () if self . custom_preprocessor_ : self . custom_preprocessor_ . fit ( X , y ) self . processing_summary_ [ 'strategy' ] = 'custom' if hasattr ( self . custom_preprocessor_ , 'get_summary' ): self . processing_summary_ [ 'steps' ] = self . custom_preprocessor_ . get_summary () else : # Log summary for standard path self . processing_summary_ [ 'strategy' ] = 'standard' self . processing_summary_ [ 'steps' ] = {} self . _infer_column_types ( X_fit ) if y_fit is not None : self . label_encoder_ = LabelEncoder () . fit ( y_fit ) self . processing_summary_ [ 'target_encoding' ] = 'LabelEncoder' self . _fit_standard_components ( X_fit , y_fit ) self . _is_fitted = True logger . info ( \"[DataProcessor] All components for pipeline have been fitted.\" ) return self def transform ( self , X , y = None ): if not self . _is_fitted : raise RuntimeError ( \"Must call fit() before calling transform().\" ) X_transformed = X . copy () if self . custom_preprocessor_ : if self . model_name == 'TabPFN' : return self . custom_preprocessor_ . transform ( X_transformed ) else : return self . custom_preprocessor_ . transform ( X_transformed , y ) X_transformed = self . _apply_standard_transforms ( X_transformed ) if y is not None and self . label_encoder_ : y_transformed = self . label_encoder_ . transform ( y ) return X_transformed , y_transformed return X_transformed def fit_transform ( self , X , y = None ): self . fit ( X , y ) if self . custom_preprocessor_ : return self . transform ( X , y ) X_transformed , y_transformed = self . transform ( X , y ) if self . resampling_strategy and y is not None : self . _fit_resampler ( y_transformed ) if self . resampler_ : logger . info ( f \"[DataProcessor] Resampling data with ' { self . resampling_strategy } '...\" ) X_transformed , y_transformed = self . resampler_ . fit_resample ( X_transformed , y_transformed ) self . processing_summary_ [ 'resampling' ] = self . resampling_strategy return X_transformed , y_transformed def get_processing_summary ( self ): \"\"\" Returns a formatted string summarizing the data processing steps. \"\"\" if not self . _is_fitted : logger . warning ( \"[DataProcessor] DataProcessor has not been fitted yet.\" ) raise RuntimeError ( \"DataProcessor has not been fitted yet.\" ) summary_lines = [ \"--- Data Processing Summary ---\" ] if self . processing_summary_ . get ( 'strategy' ) == 'custom' : summary_lines . append ( f \" \\n [Custom Preprocessing for ' { self . model_name } ']\" ) steps = self . processing_summary_ . get ( 'steps' , {}) if not steps : summary_lines . append ( \" - No detailed summary available for this preprocessor.\" ) else : summary_lines . append ( \" \\n Applied Steps:\" ) # --- NEW: Detailed loop for rich summary --- for i , ( step_name , step_info ) in enumerate ( steps . items ()): summary_lines . append ( f \" { i + 1 } . { step_name } :\" ) summary_lines . append ( f \" - { step_info [ 'description' ] } \" ) for detail_line in step_info . get ( 'details' , []): summary_lines . append ( f \" - { detail_line } \" ) elif self . processing_summary_ . get ( 'strategy' ) == 'standard' : summary_lines . append ( \" \\n [Standard Preprocessing Pipeline]\" ) steps = self . processing_summary_ . get ( 'steps' , {}) processed_cols = set () if 'imputation' in steps : step_info = steps [ 'imputation' ] summary_lines . append ( f \" \\n 1. Imputation (Strategy: ' { step_info [ 'strategy' ] } ')\" ) summary_lines . append ( f \" - Applied to { len ( step_info [ 'columns' ]) } numerical features: { ', ' . join ( f '` { c } `' for c in step_info [ 'columns' ]) } \" ) processed_cols . update ( step_info [ 'columns' ]) if 'categorical_encoding' in steps : step_info = steps [ 'categorical_encoding' ] summary_lines . append ( f \" \\n 2. Categorical Encoding (Strategy: ' { step_info [ 'strategy' ] } ')\" ) summary_lines . append ( f \" - Applied to { len ( step_info [ 'columns' ]) } categorical features: { ', ' . join ( f '` { c } `' for c in step_info [ 'columns' ]) } \" ) processed_cols . update ( step_info [ 'columns' ]) if 'scaling' in steps : step_info = steps [ 'scaling' ] summary_lines . append ( f \" \\n 3. Scaling (Strategy: ' { step_info [ 'strategy' ] } ')\" ) summary_lines . append ( f \" - Applied to { len ( step_info [ 'columns' ]) } features (original numerical + encoded categorical).\" ) processed_cols . update ( step_info [ 'columns' ]) if 'feature_selection' in steps : step_info = steps [ 'feature_selection' ] summary_lines . append ( f \" \\n 4. Feature Selection (Strategy: ' { step_info [ 'strategy' ] } ')\" ) if 'dropped_columns' in step_info and step_info [ 'dropped_columns' ]: summary_lines . append ( f \" - Removed { len ( step_info [ 'dropped_columns' ]) } features: { ', ' . join ( f '` { c } `' for c in step_info [ 'dropped_columns' ]) } \" ) else : summary_lines . append ( \" - No features were removed by this step.\" ) untouched_features = [ col for col in self . original_cols_ if col not in processed_cols and col not in ( steps . get ( 'feature_selection' , {}) . get ( 'dropped_columns' , []))] summary_lines . append ( f \" \\n [Untouched Features]\" ) if untouched_features : summary_lines . append ( f \" - { len ( untouched_features ) } features were not modified: { ', ' . join ( f '` { c } `' for c in untouched_features ) } \" ) else : summary_lines . append ( \" - All features were processed by at least one step.\" ) else : summary_lines . append ( \"No processing steps were recorded.\" ) if 'resampling' in self . processing_summary_ : summary_lines . append ( f \" \\n [Resampling]\" ) summary_lines . append ( f \" - Strategy: ' { self . processing_summary_ [ 'resampling' ] } ' applied to the training data.\" ) return \" \\n \" . join ( summary_lines ) def _infer_column_types ( self , X ): self . numerical_cols_ = X . select_dtypes ( include = np . number ) . columns . tolist () self . categorical_cols_ = X . select_dtypes ( exclude = np . number ) . columns . tolist () def _fit_standard_components ( self , X , y ): if self . imputation_strategy != 'none' and self . numerical_cols_ : imputer_map = { 'mean' : SimpleImputer ( strategy = 'mean' ), 'median' : SimpleImputer ( strategy = 'median' ), 'iterative' : IterativeImputer ( random_state = 42 ), 'knn' : KNNImputer ()} self . imputer_ = imputer_map . get ( self . imputation_strategy , SimpleImputer ( strategy = 'mean' )) X [ self . numerical_cols_ ] = self . imputer_ . fit_transform ( X [ self . numerical_cols_ ]) self . processing_summary_ [ 'steps' ][ 'imputation' ] = { 'strategy' : self . imputation_strategy , 'columns' : self . numerical_cols_ } if self . categorical_encoding != 'none' and self . categorical_cols_ : encoder_map = { 'onehot' : OneHotEncoder ( handle_unknown = 'ignore' , sparse_output = False ), 'ordinal' : OrdinalEncoder ( handle_unknown = 'use_encoded_value' , unknown_value =- 1 ), 'target' : TargetEncoder (), 'hashing' : HashingEncoder (), 'binary' : BinaryEncoder ()} self . encoder_ = encoder_map . get ( self . categorical_encoding , OneHotEncoder ( handle_unknown = 'ignore' , sparse_output = False )) self . encoder_ . fit ( X [ self . categorical_cols_ ], y ) self . processing_summary_ [ 'steps' ][ 'categorical_encoding' ] = { 'strategy' : self . categorical_encoding , 'columns' : self . categorical_cols_ } X_encoded = self . _apply_encoding ( X ) numeric_for_scaling = X_encoded . select_dtypes ( include = np . number ) . columns . tolist () if self . scaling_strategy != 'none' and numeric_for_scaling : scaler_map = { 'standard' : StandardScaler (), 'minmax' : MinMaxScaler (), 'robust' : RobustScaler (), 'power_transform' : PowerTransformer ()} self . scaler_ = scaler_map . get ( self . scaling_strategy , StandardScaler ()) self . scaler_ . fit ( X_encoded [ numeric_for_scaling ]) self . processing_summary_ [ 'steps' ][ 'scaling' ] = { 'strategy' : self . scaling_strategy , 'columns' : numeric_for_scaling } X_scaled = self . _apply_scaling ( X_encoded , numeric_for_scaling ) if self . feature_selection_strategy : self . _fit_feature_selector ( X_scaled , y ) def _apply_standard_transforms ( self , X ): X_transformed = X . copy () if self . imputer_ and self . numerical_cols_ : X_transformed [ self . numerical_cols_ ] = self . imputer_ . transform ( X_transformed [ self . numerical_cols_ ]) X_transformed = self . _apply_encoding ( X_transformed ) numeric_for_scaling = X_transformed . select_dtypes ( include = np . number ) . columns . tolist () X_transformed = self . _apply_scaling ( X_transformed , numeric_for_scaling ) if self . selector_ or self . _correlation_cols_to_drop : X_transformed = self . _apply_feature_selection ( X_transformed ) return X_transformed def _apply_encoding ( self , X ): if not self . encoder_ or not self . categorical_cols_ : return X encoded_data = self . encoder_ . transform ( X [ self . categorical_cols_ ]) try : encoded_cols = self . encoder_ . get_feature_names_out ( self . categorical_cols_ ) except : encoded_cols = [ f \"cat_ { i } \" for i in range ( encoded_data . shape [ 1 ])] encoded_df = pd . DataFrame ( encoded_data , index = X . index , columns = encoded_cols ) X_transformed = X . drop ( columns = self . categorical_cols_ ) return pd . concat ([ X_transformed , encoded_df ], axis = 1 ) def _apply_scaling ( self , X , cols_to_scale ): if not self . scaler_ or not cols_to_scale : return X X_scaled = X . copy () X_scaled [ cols_to_scale ] = self . scaler_ . transform ( X_scaled [ cols_to_scale ]) return X_scaled def _fit_feature_selector ( self , X , y ): logger . debug ( f \"[DataProcessor] Fitting feature selector: ' { self . feature_selection_strategy } '...\" ) selector_map = { 'variance' : VarianceThreshold ( threshold = 0.0 ), 'select_k_best_anova' : SelectKBest ( f_classif , k = self . feature_selection_k ), 'select_k_best_chi2' : SelectKBest ( chi2 , k = self . feature_selection_k )} self . selector_ = selector_map . get ( self . feature_selection_strategy ) if self . selector_ : X_to_fit = X . copy () if self . feature_selection_strategy == 'select_k_best_chi2' : X_to_fit = MinMaxScaler () . fit_transform ( X_to_fit ) self . selector_ . fit ( X_to_fit , y ) dropped_cols = X . columns [ ~ self . selector_ . get_support ()] . tolist () self . processing_summary_ [ 'steps' ][ 'feature_selection' ] = { 'strategy' : self . feature_selection_strategy , 'k' : self . feature_selection_k , 'dropped_columns' : dropped_cols } if self . feature_selection_strategy == 'correlation' : self . _fit_correlation_selector ( X ) def _apply_feature_selection ( self , X ): X_selected = X if self . selector_ and self . feature_selection_strategy != 'correlation' : selected_cols = X . columns [ self . selector_ . get_support ()] X_selected = pd . DataFrame ( self . selector_ . transform ( X ), index = X . index , columns = selected_cols ) if self . _correlation_cols_to_drop : X_selected = X_selected . drop ( columns = self . _correlation_cols_to_drop , errors = 'ignore' ) return X_selected def _fit_correlation_selector ( self , X , threshold = 0.9 ): corr_matrix = X . corr () . abs () upper = corr_matrix . where ( np . triu ( np . ones ( corr_matrix . shape ), k = 1 ) . astype ( bool )) self . _correlation_cols_to_drop = [ column for column in upper . columns if any ( upper [ column ] > threshold )] logger . debug ( f \"[DataProcessor] Correlation filter identified { len ( self . _correlation_cols_to_drop ) } columns to drop.\" ) self . processing_summary_ [ 'steps' ][ 'feature_selection' ] = { 'strategy' : 'correlation' , 'threshold' : threshold , 'dropped_columns' : self . _correlation_cols_to_drop } def _fit_resampler ( self , y ): if self . resampling_strategy : k_neighbors = 5 if self . resampling_strategy == 'smote' : min_class_count = pd . Series ( y ) . value_counts () . min () k_neighbors = max ( 1 , min_class_count - 1 ) resampler_map = { 'smote' : SMOTE ( random_state = 42 , k_neighbors = k_neighbors ), 'random_over' : RandomOverSampler ( random_state = 42 ), 'random_under' : RandomUnderSampler ( random_state = 42 ), 'tomek' : TomekLinks (), 'kmeans' : ClusterCentroids ( random_state = 42 ), 'knn' : NeighbourhoodCleaningRule () } self . resampler_ = resampler_map . get ( self . resampling_strategy ) get_processing_summary () \u00b6 Returns a formatted string summarizing the data processing steps. Source code in tabtune/Dataprocess/data_processor.py 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 def get_processing_summary ( self ): \"\"\" Returns a formatted string summarizing the data processing steps. \"\"\" if not self . _is_fitted : logger . warning ( \"[DataProcessor] DataProcessor has not been fitted yet.\" ) raise RuntimeError ( \"DataProcessor has not been fitted yet.\" ) summary_lines = [ \"--- Data Processing Summary ---\" ] if self . processing_summary_ . get ( 'strategy' ) == 'custom' : summary_lines . append ( f \" \\n [Custom Preprocessing for ' { self . model_name } ']\" ) steps = self . processing_summary_ . get ( 'steps' , {}) if not steps : summary_lines . append ( \" - No detailed summary available for this preprocessor.\" ) else : summary_lines . append ( \" \\n Applied Steps:\" ) # --- NEW: Detailed loop for rich summary --- for i , ( step_name , step_info ) in enumerate ( steps . items ()): summary_lines . append ( f \" { i + 1 } . { step_name } :\" ) summary_lines . append ( f \" - { step_info [ 'description' ] } \" ) for detail_line in step_info . get ( 'details' , []): summary_lines . append ( f \" - { detail_line } \" ) elif self . processing_summary_ . get ( 'strategy' ) == 'standard' : summary_lines . append ( \" \\n [Standard Preprocessing Pipeline]\" ) steps = self . processing_summary_ . get ( 'steps' , {}) processed_cols = set () if 'imputation' in steps : step_info = steps [ 'imputation' ] summary_lines . append ( f \" \\n 1. Imputation (Strategy: ' { step_info [ 'strategy' ] } ')\" ) summary_lines . append ( f \" - Applied to { len ( step_info [ 'columns' ]) } numerical features: { ', ' . join ( f '` { c } `' for c in step_info [ 'columns' ]) } \" ) processed_cols . update ( step_info [ 'columns' ]) if 'categorical_encoding' in steps : step_info = steps [ 'categorical_encoding' ] summary_lines . append ( f \" \\n 2. Categorical Encoding (Strategy: ' { step_info [ 'strategy' ] } ')\" ) summary_lines . append ( f \" - Applied to { len ( step_info [ 'columns' ]) } categorical features: { ', ' . join ( f '` { c } `' for c in step_info [ 'columns' ]) } \" ) processed_cols . update ( step_info [ 'columns' ]) if 'scaling' in steps : step_info = steps [ 'scaling' ] summary_lines . append ( f \" \\n 3. Scaling (Strategy: ' { step_info [ 'strategy' ] } ')\" ) summary_lines . append ( f \" - Applied to { len ( step_info [ 'columns' ]) } features (original numerical + encoded categorical).\" ) processed_cols . update ( step_info [ 'columns' ]) if 'feature_selection' in steps : step_info = steps [ 'feature_selection' ] summary_lines . append ( f \" \\n 4. Feature Selection (Strategy: ' { step_info [ 'strategy' ] } ')\" ) if 'dropped_columns' in step_info and step_info [ 'dropped_columns' ]: summary_lines . append ( f \" - Removed { len ( step_info [ 'dropped_columns' ]) } features: { ', ' . join ( f '` { c } `' for c in step_info [ 'dropped_columns' ]) } \" ) else : summary_lines . append ( \" - No features were removed by this step.\" ) untouched_features = [ col for col in self . original_cols_ if col not in processed_cols and col not in ( steps . get ( 'feature_selection' , {}) . get ( 'dropped_columns' , []))] summary_lines . append ( f \" \\n [Untouched Features]\" ) if untouched_features : summary_lines . append ( f \" - { len ( untouched_features ) } features were not modified: { ', ' . join ( f '` { c } `' for c in untouched_features ) } \" ) else : summary_lines . append ( \" - All features were processed by at least one step.\" ) else : summary_lines . append ( \"No processing steps were recorded.\" ) if 'resampling' in self . processing_summary_ : summary_lines . append ( f \" \\n [Resampling]\" ) summary_lines . append ( f \" - Strategy: ' { self . processing_summary_ [ 'resampling' ] } ' applied to the training data.\" ) return \" \\n \" . join ( summary_lines )","title":"DataProcessor"},{"location":"api/data-processor/#api-dataprocessor","text":"Bases: BaseEstimator , TransformerMixin The complete Data Preparation Engine for the TabTune library. Integrates a full suite of standard preprocessing tools with custom, model-specific logic. Source code in tabtune/Dataprocess/data_processor.py 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 class DataProcessor ( BaseEstimator , TransformerMixin ): \"\"\" The complete Data Preparation Engine for the TabTune library. Integrates a full suite of standard preprocessing tools with custom, model-specific logic. \"\"\" def __init__ ( self , model_name = None , override_types = None , imputation_strategy = 'mean' , categorical_encoding = 'onehot' , scaling_strategy = 'standard' , resampling_strategy = None , feature_selection_strategy = None , feature_selection_k = 10 , model_params = None ): self . model_name = model_name self . override_types = override_types self . imputation_strategy = imputation_strategy self . categorical_encoding = categorical_encoding self . scaling_strategy = scaling_strategy self . resampling_strategy = resampling_strategy self . feature_selection_strategy = feature_selection_strategy self . feature_selection_k = feature_selection_k self . model_params = model_params or {} self . _set_model_aware_defaults () # --- Internal State Attributes --- self . column_types_ = {} self . _is_fitted = False self . custom_preprocessor_ = None self . imputer_ = None self . scaler_ = None self . encoder_ = None self . resampler_ = None self . selector_ = None self . label_encoder_ = None self . _correlation_cols_to_drop = [] self . original_cols_ = None self . processing_summary_ = {} def _set_model_aware_defaults ( self ): \"\"\"Sets default strategies based on the model_name to ensure compatibility.\"\"\" if self . model_name : model_defaults = { 'TabPFN' : { 'categorical_encoding' : 'tabpfn_special' }, 'TabICL' : { 'categorical_encoding' : 'tabicl_special' }, 'OrionMSP' : { 'categorical_encoding' : 'orion_msp_special' }, 'ContextTab' : { 'categorical_encoding' : 'contexttab_special' }, 'Mitra' : { 'categorical_encoding' : 'mitra_special' }, 'OrionBix' : { 'categorical_encoding' : 'orion_bix_special' }, 'TabDPT' : { 'categorical_encoding' : 'tabdpt_special' }, } config = model_defaults . get ( self . model_name ) if config : self . imputation_strategy = config . get ( 'imputation_strategy' , self . imputation_strategy ) self . categorical_encoding = config . get ( 'categorical_encoding' , self . categorical_encoding ) self . scaling_strategy = config . get ( 'scaling_strategy' , self . scaling_strategy ) def _get_custom_preprocessor ( self ): \"\"\"Factory method to return the correct custom preprocessor instance.\"\"\" special_encoders = { 'tabpfn_special' : TabPFNPreprocessor , 'tabicl_special' : TabICLPreprocessor , 'orion_msp_special' : OrionMSPPreprocessor , 'contexttab_special' : ContextTabPreprocessor , 'mitra_special' : MitraPreprocessor , 'orion_bix_special' : OrionBixPreprocessor , 'tabdpt_special' : TabDPTPreprocessor , } if self . categorical_encoding in special_encoders : logger . info ( f \"[DataProcessor] Using special preprocessor for: { self . model_name } \" ) PreprocessorClass = special_encoders [ self . categorical_encoding ] if self . categorical_encoding == 'contexttab_special' : # Extract regression parameters from model_params regression_type = self . model_params . get ( 'regression_type' , 'l2' ) num_regression_bins = self . model_params . get ( 'num_regression_bins' , 16 ) return PreprocessorClass ( regression_type = regression_type , num_regression_bins = num_regression_bins ) return PreprocessorClass () return None def fit ( self , X , y = None ): X_fit = X . copy () self . original_cols_ = X_fit . columns . tolist () y_fit = y . copy () if y is not None else None self . custom_preprocessor_ = self . _get_custom_preprocessor () if self . custom_preprocessor_ : self . custom_preprocessor_ . fit ( X , y ) self . processing_summary_ [ 'strategy' ] = 'custom' if hasattr ( self . custom_preprocessor_ , 'get_summary' ): self . processing_summary_ [ 'steps' ] = self . custom_preprocessor_ . get_summary () else : # Log summary for standard path self . processing_summary_ [ 'strategy' ] = 'standard' self . processing_summary_ [ 'steps' ] = {} self . _infer_column_types ( X_fit ) if y_fit is not None : self . label_encoder_ = LabelEncoder () . fit ( y_fit ) self . processing_summary_ [ 'target_encoding' ] = 'LabelEncoder' self . _fit_standard_components ( X_fit , y_fit ) self . _is_fitted = True logger . info ( \"[DataProcessor] All components for pipeline have been fitted.\" ) return self def transform ( self , X , y = None ): if not self . _is_fitted : raise RuntimeError ( \"Must call fit() before calling transform().\" ) X_transformed = X . copy () if self . custom_preprocessor_ : if self . model_name == 'TabPFN' : return self . custom_preprocessor_ . transform ( X_transformed ) else : return self . custom_preprocessor_ . transform ( X_transformed , y ) X_transformed = self . _apply_standard_transforms ( X_transformed ) if y is not None and self . label_encoder_ : y_transformed = self . label_encoder_ . transform ( y ) return X_transformed , y_transformed return X_transformed def fit_transform ( self , X , y = None ): self . fit ( X , y ) if self . custom_preprocessor_ : return self . transform ( X , y ) X_transformed , y_transformed = self . transform ( X , y ) if self . resampling_strategy and y is not None : self . _fit_resampler ( y_transformed ) if self . resampler_ : logger . info ( f \"[DataProcessor] Resampling data with ' { self . resampling_strategy } '...\" ) X_transformed , y_transformed = self . resampler_ . fit_resample ( X_transformed , y_transformed ) self . processing_summary_ [ 'resampling' ] = self . resampling_strategy return X_transformed , y_transformed def get_processing_summary ( self ): \"\"\" Returns a formatted string summarizing the data processing steps. \"\"\" if not self . _is_fitted : logger . warning ( \"[DataProcessor] DataProcessor has not been fitted yet.\" ) raise RuntimeError ( \"DataProcessor has not been fitted yet.\" ) summary_lines = [ \"--- Data Processing Summary ---\" ] if self . processing_summary_ . get ( 'strategy' ) == 'custom' : summary_lines . append ( f \" \\n [Custom Preprocessing for ' { self . model_name } ']\" ) steps = self . processing_summary_ . get ( 'steps' , {}) if not steps : summary_lines . append ( \" - No detailed summary available for this preprocessor.\" ) else : summary_lines . append ( \" \\n Applied Steps:\" ) # --- NEW: Detailed loop for rich summary --- for i , ( step_name , step_info ) in enumerate ( steps . items ()): summary_lines . append ( f \" { i + 1 } . { step_name } :\" ) summary_lines . append ( f \" - { step_info [ 'description' ] } \" ) for detail_line in step_info . get ( 'details' , []): summary_lines . append ( f \" - { detail_line } \" ) elif self . processing_summary_ . get ( 'strategy' ) == 'standard' : summary_lines . append ( \" \\n [Standard Preprocessing Pipeline]\" ) steps = self . processing_summary_ . get ( 'steps' , {}) processed_cols = set () if 'imputation' in steps : step_info = steps [ 'imputation' ] summary_lines . append ( f \" \\n 1. Imputation (Strategy: ' { step_info [ 'strategy' ] } ')\" ) summary_lines . append ( f \" - Applied to { len ( step_info [ 'columns' ]) } numerical features: { ', ' . join ( f '` { c } `' for c in step_info [ 'columns' ]) } \" ) processed_cols . update ( step_info [ 'columns' ]) if 'categorical_encoding' in steps : step_info = steps [ 'categorical_encoding' ] summary_lines . append ( f \" \\n 2. Categorical Encoding (Strategy: ' { step_info [ 'strategy' ] } ')\" ) summary_lines . append ( f \" - Applied to { len ( step_info [ 'columns' ]) } categorical features: { ', ' . join ( f '` { c } `' for c in step_info [ 'columns' ]) } \" ) processed_cols . update ( step_info [ 'columns' ]) if 'scaling' in steps : step_info = steps [ 'scaling' ] summary_lines . append ( f \" \\n 3. Scaling (Strategy: ' { step_info [ 'strategy' ] } ')\" ) summary_lines . append ( f \" - Applied to { len ( step_info [ 'columns' ]) } features (original numerical + encoded categorical).\" ) processed_cols . update ( step_info [ 'columns' ]) if 'feature_selection' in steps : step_info = steps [ 'feature_selection' ] summary_lines . append ( f \" \\n 4. Feature Selection (Strategy: ' { step_info [ 'strategy' ] } ')\" ) if 'dropped_columns' in step_info and step_info [ 'dropped_columns' ]: summary_lines . append ( f \" - Removed { len ( step_info [ 'dropped_columns' ]) } features: { ', ' . join ( f '` { c } `' for c in step_info [ 'dropped_columns' ]) } \" ) else : summary_lines . append ( \" - No features were removed by this step.\" ) untouched_features = [ col for col in self . original_cols_ if col not in processed_cols and col not in ( steps . get ( 'feature_selection' , {}) . get ( 'dropped_columns' , []))] summary_lines . append ( f \" \\n [Untouched Features]\" ) if untouched_features : summary_lines . append ( f \" - { len ( untouched_features ) } features were not modified: { ', ' . join ( f '` { c } `' for c in untouched_features ) } \" ) else : summary_lines . append ( \" - All features were processed by at least one step.\" ) else : summary_lines . append ( \"No processing steps were recorded.\" ) if 'resampling' in self . processing_summary_ : summary_lines . append ( f \" \\n [Resampling]\" ) summary_lines . append ( f \" - Strategy: ' { self . processing_summary_ [ 'resampling' ] } ' applied to the training data.\" ) return \" \\n \" . join ( summary_lines ) def _infer_column_types ( self , X ): self . numerical_cols_ = X . select_dtypes ( include = np . number ) . columns . tolist () self . categorical_cols_ = X . select_dtypes ( exclude = np . number ) . columns . tolist () def _fit_standard_components ( self , X , y ): if self . imputation_strategy != 'none' and self . numerical_cols_ : imputer_map = { 'mean' : SimpleImputer ( strategy = 'mean' ), 'median' : SimpleImputer ( strategy = 'median' ), 'iterative' : IterativeImputer ( random_state = 42 ), 'knn' : KNNImputer ()} self . imputer_ = imputer_map . get ( self . imputation_strategy , SimpleImputer ( strategy = 'mean' )) X [ self . numerical_cols_ ] = self . imputer_ . fit_transform ( X [ self . numerical_cols_ ]) self . processing_summary_ [ 'steps' ][ 'imputation' ] = { 'strategy' : self . imputation_strategy , 'columns' : self . numerical_cols_ } if self . categorical_encoding != 'none' and self . categorical_cols_ : encoder_map = { 'onehot' : OneHotEncoder ( handle_unknown = 'ignore' , sparse_output = False ), 'ordinal' : OrdinalEncoder ( handle_unknown = 'use_encoded_value' , unknown_value =- 1 ), 'target' : TargetEncoder (), 'hashing' : HashingEncoder (), 'binary' : BinaryEncoder ()} self . encoder_ = encoder_map . get ( self . categorical_encoding , OneHotEncoder ( handle_unknown = 'ignore' , sparse_output = False )) self . encoder_ . fit ( X [ self . categorical_cols_ ], y ) self . processing_summary_ [ 'steps' ][ 'categorical_encoding' ] = { 'strategy' : self . categorical_encoding , 'columns' : self . categorical_cols_ } X_encoded = self . _apply_encoding ( X ) numeric_for_scaling = X_encoded . select_dtypes ( include = np . number ) . columns . tolist () if self . scaling_strategy != 'none' and numeric_for_scaling : scaler_map = { 'standard' : StandardScaler (), 'minmax' : MinMaxScaler (), 'robust' : RobustScaler (), 'power_transform' : PowerTransformer ()} self . scaler_ = scaler_map . get ( self . scaling_strategy , StandardScaler ()) self . scaler_ . fit ( X_encoded [ numeric_for_scaling ]) self . processing_summary_ [ 'steps' ][ 'scaling' ] = { 'strategy' : self . scaling_strategy , 'columns' : numeric_for_scaling } X_scaled = self . _apply_scaling ( X_encoded , numeric_for_scaling ) if self . feature_selection_strategy : self . _fit_feature_selector ( X_scaled , y ) def _apply_standard_transforms ( self , X ): X_transformed = X . copy () if self . imputer_ and self . numerical_cols_ : X_transformed [ self . numerical_cols_ ] = self . imputer_ . transform ( X_transformed [ self . numerical_cols_ ]) X_transformed = self . _apply_encoding ( X_transformed ) numeric_for_scaling = X_transformed . select_dtypes ( include = np . number ) . columns . tolist () X_transformed = self . _apply_scaling ( X_transformed , numeric_for_scaling ) if self . selector_ or self . _correlation_cols_to_drop : X_transformed = self . _apply_feature_selection ( X_transformed ) return X_transformed def _apply_encoding ( self , X ): if not self . encoder_ or not self . categorical_cols_ : return X encoded_data = self . encoder_ . transform ( X [ self . categorical_cols_ ]) try : encoded_cols = self . encoder_ . get_feature_names_out ( self . categorical_cols_ ) except : encoded_cols = [ f \"cat_ { i } \" for i in range ( encoded_data . shape [ 1 ])] encoded_df = pd . DataFrame ( encoded_data , index = X . index , columns = encoded_cols ) X_transformed = X . drop ( columns = self . categorical_cols_ ) return pd . concat ([ X_transformed , encoded_df ], axis = 1 ) def _apply_scaling ( self , X , cols_to_scale ): if not self . scaler_ or not cols_to_scale : return X X_scaled = X . copy () X_scaled [ cols_to_scale ] = self . scaler_ . transform ( X_scaled [ cols_to_scale ]) return X_scaled def _fit_feature_selector ( self , X , y ): logger . debug ( f \"[DataProcessor] Fitting feature selector: ' { self . feature_selection_strategy } '...\" ) selector_map = { 'variance' : VarianceThreshold ( threshold = 0.0 ), 'select_k_best_anova' : SelectKBest ( f_classif , k = self . feature_selection_k ), 'select_k_best_chi2' : SelectKBest ( chi2 , k = self . feature_selection_k )} self . selector_ = selector_map . get ( self . feature_selection_strategy ) if self . selector_ : X_to_fit = X . copy () if self . feature_selection_strategy == 'select_k_best_chi2' : X_to_fit = MinMaxScaler () . fit_transform ( X_to_fit ) self . selector_ . fit ( X_to_fit , y ) dropped_cols = X . columns [ ~ self . selector_ . get_support ()] . tolist () self . processing_summary_ [ 'steps' ][ 'feature_selection' ] = { 'strategy' : self . feature_selection_strategy , 'k' : self . feature_selection_k , 'dropped_columns' : dropped_cols } if self . feature_selection_strategy == 'correlation' : self . _fit_correlation_selector ( X ) def _apply_feature_selection ( self , X ): X_selected = X if self . selector_ and self . feature_selection_strategy != 'correlation' : selected_cols = X . columns [ self . selector_ . get_support ()] X_selected = pd . DataFrame ( self . selector_ . transform ( X ), index = X . index , columns = selected_cols ) if self . _correlation_cols_to_drop : X_selected = X_selected . drop ( columns = self . _correlation_cols_to_drop , errors = 'ignore' ) return X_selected def _fit_correlation_selector ( self , X , threshold = 0.9 ): corr_matrix = X . corr () . abs () upper = corr_matrix . where ( np . triu ( np . ones ( corr_matrix . shape ), k = 1 ) . astype ( bool )) self . _correlation_cols_to_drop = [ column for column in upper . columns if any ( upper [ column ] > threshold )] logger . debug ( f \"[DataProcessor] Correlation filter identified { len ( self . _correlation_cols_to_drop ) } columns to drop.\" ) self . processing_summary_ [ 'steps' ][ 'feature_selection' ] = { 'strategy' : 'correlation' , 'threshold' : threshold , 'dropped_columns' : self . _correlation_cols_to_drop } def _fit_resampler ( self , y ): if self . resampling_strategy : k_neighbors = 5 if self . resampling_strategy == 'smote' : min_class_count = pd . Series ( y ) . value_counts () . min () k_neighbors = max ( 1 , min_class_count - 1 ) resampler_map = { 'smote' : SMOTE ( random_state = 42 , k_neighbors = k_neighbors ), 'random_over' : RandomOverSampler ( random_state = 42 ), 'random_under' : RandomUnderSampler ( random_state = 42 ), 'tomek' : TomekLinks (), 'kmeans' : ClusterCentroids ( random_state = 42 ), 'knn' : NeighbourhoodCleaningRule () } self . resampler_ = resampler_map . get ( self . resampling_strategy )","title":"API: DataProcessor"},{"location":"api/data-processor/#tabtune.Dataprocess.data_processor.DataProcessor.get_processing_summary","text":"Returns a formatted string summarizing the data processing steps. Source code in tabtune/Dataprocess/data_processor.py 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 def get_processing_summary ( self ): \"\"\" Returns a formatted string summarizing the data processing steps. \"\"\" if not self . _is_fitted : logger . warning ( \"[DataProcessor] DataProcessor has not been fitted yet.\" ) raise RuntimeError ( \"DataProcessor has not been fitted yet.\" ) summary_lines = [ \"--- Data Processing Summary ---\" ] if self . processing_summary_ . get ( 'strategy' ) == 'custom' : summary_lines . append ( f \" \\n [Custom Preprocessing for ' { self . model_name } ']\" ) steps = self . processing_summary_ . get ( 'steps' , {}) if not steps : summary_lines . append ( \" - No detailed summary available for this preprocessor.\" ) else : summary_lines . append ( \" \\n Applied Steps:\" ) # --- NEW: Detailed loop for rich summary --- for i , ( step_name , step_info ) in enumerate ( steps . items ()): summary_lines . append ( f \" { i + 1 } . { step_name } :\" ) summary_lines . append ( f \" - { step_info [ 'description' ] } \" ) for detail_line in step_info . get ( 'details' , []): summary_lines . append ( f \" - { detail_line } \" ) elif self . processing_summary_ . get ( 'strategy' ) == 'standard' : summary_lines . append ( \" \\n [Standard Preprocessing Pipeline]\" ) steps = self . processing_summary_ . get ( 'steps' , {}) processed_cols = set () if 'imputation' in steps : step_info = steps [ 'imputation' ] summary_lines . append ( f \" \\n 1. Imputation (Strategy: ' { step_info [ 'strategy' ] } ')\" ) summary_lines . append ( f \" - Applied to { len ( step_info [ 'columns' ]) } numerical features: { ', ' . join ( f '` { c } `' for c in step_info [ 'columns' ]) } \" ) processed_cols . update ( step_info [ 'columns' ]) if 'categorical_encoding' in steps : step_info = steps [ 'categorical_encoding' ] summary_lines . append ( f \" \\n 2. Categorical Encoding (Strategy: ' { step_info [ 'strategy' ] } ')\" ) summary_lines . append ( f \" - Applied to { len ( step_info [ 'columns' ]) } categorical features: { ', ' . join ( f '` { c } `' for c in step_info [ 'columns' ]) } \" ) processed_cols . update ( step_info [ 'columns' ]) if 'scaling' in steps : step_info = steps [ 'scaling' ] summary_lines . append ( f \" \\n 3. Scaling (Strategy: ' { step_info [ 'strategy' ] } ')\" ) summary_lines . append ( f \" - Applied to { len ( step_info [ 'columns' ]) } features (original numerical + encoded categorical).\" ) processed_cols . update ( step_info [ 'columns' ]) if 'feature_selection' in steps : step_info = steps [ 'feature_selection' ] summary_lines . append ( f \" \\n 4. Feature Selection (Strategy: ' { step_info [ 'strategy' ] } ')\" ) if 'dropped_columns' in step_info and step_info [ 'dropped_columns' ]: summary_lines . append ( f \" - Removed { len ( step_info [ 'dropped_columns' ]) } features: { ', ' . join ( f '` { c } `' for c in step_info [ 'dropped_columns' ]) } \" ) else : summary_lines . append ( \" - No features were removed by this step.\" ) untouched_features = [ col for col in self . original_cols_ if col not in processed_cols and col not in ( steps . get ( 'feature_selection' , {}) . get ( 'dropped_columns' , []))] summary_lines . append ( f \" \\n [Untouched Features]\" ) if untouched_features : summary_lines . append ( f \" - { len ( untouched_features ) } features were not modified: { ', ' . join ( f '` { c } `' for c in untouched_features ) } \" ) else : summary_lines . append ( \" - All features were processed by at least one step.\" ) else : summary_lines . append ( \"No processing steps were recorded.\" ) if 'resampling' in self . processing_summary_ : summary_lines . append ( f \" \\n [Resampling]\" ) summary_lines . append ( f \" - Strategy: ' { self . processing_summary_ [ 'resampling' ] } ' applied to the training data.\" ) return \" \\n \" . join ( summary_lines )","title":"get_processing_summary"},{"location":"api/leaderboard/","text":"API: TabularLeaderboard \u00b6 A leaderboard utility to benchmark multiple TabTune pipeline configurations on a single, pre-split dataset. Source code in tabtune/TabularLeaderboard/leaderboard.py 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 class TabularLeaderboard : \"\"\" A leaderboard utility to benchmark multiple TabTune pipeline configurations on a single, pre-split dataset. \"\"\" def __init__ ( self , X_train : pd . DataFrame , X_test : pd . DataFrame , y_train : pd . Series , y_test : pd . Series ): \"\"\" Initializes the leaderboard with a user-provided, pre-split dataset. Args: X_train (pd.DataFrame): Training features. X_test (pd.DataFrame): Testing features. y_train (pd.Series): Training target. y_test (pd.Series): Testing target. \"\"\" self . X_train = X_train self . X_test = X_test self . y_train = y_train self . y_test = y_test self . models_to_run = [] self . results = [] logger . info ( \"[Leaderboard] Leaderboard initialized with custom dataset\" ) logger . info ( f \"[Leaderboard] Data prepared with { self . X_train . shape [ 0 ] } training samples and { self . X_test . shape [ 0 ] } test samples\" ) def add_model ( self , model_name : str , tuning_strategy : str = 'inference' , model_params : dict = None , tuning_params : dict = None ): \"\"\" Adds a model configuration to the list of contestants for the leaderboard. \"\"\" config = { \"model_name\" : model_name , \"tuning_strategy\" : tuning_strategy , \"model_params\" : model_params or {}, \"tuning_params\" : tuning_params or {} } self . models_to_run . append ( config ) logger . info ( f \"[Leaderboard] Added to leaderboard: { model_name } (Strategy: { tuning_strategy } )\" ) def run ( self , rank_by : str = 'roc_auc_score' ): \"\"\" Runs all added model configurations, evaluates them, and displays a sorted leaderboard. \"\"\" logger . info ( \" \\n \" + \"=\" * 60 ) logger . info ( \"[Leaderboard] Starting Leaderboard Run\" ) for i , config in enumerate ( self . models_to_run ): logger . info ( \" \\n \" + \"=\" * 40 ) logger . info ( f \"[Leaderboard] [ { i + 1 } / { len ( self . models_to_run ) } ] Running: { config [ 'model_name' ] } (Strategy: { config [ 'tuning_strategy' ] } )\" ) logger . info ( \"=\" * 40 ) try : pipeline = TabularPipeline ( model_name = config [ 'model_name' ], tuning_strategy = config [ 'tuning_strategy' ], model_params = config [ 'model_params' ], tuning_params = config [ 'tuning_params' ] ) pipeline . fit ( self . X_train , self . y_train ) metrics = pipeline . evaluate ( self . X_test , self . y_test ) result_row = { 'Model' : config [ 'model_name' ], 'Strategy' : config [ 'tuning_strategy' ], 'Accuracy' : metrics . get ( 'accuracy' , 0 ), 'F1 Score' : metrics . get ( 'f1_score' , 0 ), 'ROC AUC' : metrics . get ( 'roc_auc_score' , 0 ) } self . results . append ( result_row ) except Exception as e : logger . error ( f \"[Leaderboard] Error running { config [ 'model_name' ] } : { e } \" ) result_row = { 'Model' : config [ 'model_name' ], 'Strategy' : config [ 'tuning_strategy' ], 'Accuracy' : 'Failed' , 'F1 Score' : 'Failed' , 'ROC AUC' : 'Failed' } self . results . append ( result_row ) logger . info ( \" \\n \" + \"=\" * 60 ) logger . info ( \"[Leaderboard] Leaderboard Complete\" ) leaderboard_df = pd . DataFrame ( self . results ) sort_map = { 'accuracy' : 'Accuracy' , 'f1_score' : 'F1 Score' , 'roc_auc_score' : 'ROC AUC' } sort_column = sort_map . get ( rank_by , 'ROC AUC' ) if sort_column in leaderboard_df . columns : leaderboard_df [ sort_column ] = pd . to_numeric ( leaderboard_df [ sort_column ], errors = 'coerce' ) leaderboard_df = leaderboard_df . sort_values ( by = sort_column , ascending = False ) . reset_index ( drop = True ) leaderboard_df . index = leaderboard_df . index + 1 leaderboard_df . index . name = 'Rank' display ( Markdown ( \"Leaderboard Results\" )) display ( leaderboard_df ) return leaderboard_df __init__ ( X_train , X_test , y_train , y_test ) \u00b6 Initializes the leaderboard with a user-provided, pre-split dataset. Parameters: Name Type Description Default X_train DataFrame Training features. required X_test DataFrame Testing features. required y_train Series Training target. required y_test Series Testing target. required Source code in tabtune/TabularLeaderboard/leaderboard.py 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 def __init__ ( self , X_train : pd . DataFrame , X_test : pd . DataFrame , y_train : pd . Series , y_test : pd . Series ): \"\"\" Initializes the leaderboard with a user-provided, pre-split dataset. Args: X_train (pd.DataFrame): Training features. X_test (pd.DataFrame): Testing features. y_train (pd.Series): Training target. y_test (pd.Series): Testing target. \"\"\" self . X_train = X_train self . X_test = X_test self . y_train = y_train self . y_test = y_test self . models_to_run = [] self . results = [] logger . info ( \"[Leaderboard] Leaderboard initialized with custom dataset\" ) logger . info ( f \"[Leaderboard] Data prepared with { self . X_train . shape [ 0 ] } training samples and { self . X_test . shape [ 0 ] } test samples\" ) add_model ( model_name , tuning_strategy = 'inference' , model_params = None , tuning_params = None ) \u00b6 Adds a model configuration to the list of contestants for the leaderboard. Source code in tabtune/TabularLeaderboard/leaderboard.py 38 39 40 41 42 43 44 45 46 47 48 49 50 def add_model ( self , model_name : str , tuning_strategy : str = 'inference' , model_params : dict = None , tuning_params : dict = None ): \"\"\" Adds a model configuration to the list of contestants for the leaderboard. \"\"\" config = { \"model_name\" : model_name , \"tuning_strategy\" : tuning_strategy , \"model_params\" : model_params or {}, \"tuning_params\" : tuning_params or {} } self . models_to_run . append ( config ) logger . info ( f \"[Leaderboard] Added to leaderboard: { model_name } (Strategy: { tuning_strategy } )\" ) run ( rank_by = 'roc_auc_score' ) \u00b6 Runs all added model configurations, evaluates them, and displays a sorted leaderboard. Source code in tabtune/TabularLeaderboard/leaderboard.py 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 def run ( self , rank_by : str = 'roc_auc_score' ): \"\"\" Runs all added model configurations, evaluates them, and displays a sorted leaderboard. \"\"\" logger . info ( \" \\n \" + \"=\" * 60 ) logger . info ( \"[Leaderboard] Starting Leaderboard Run\" ) for i , config in enumerate ( self . models_to_run ): logger . info ( \" \\n \" + \"=\" * 40 ) logger . info ( f \"[Leaderboard] [ { i + 1 } / { len ( self . models_to_run ) } ] Running: { config [ 'model_name' ] } (Strategy: { config [ 'tuning_strategy' ] } )\" ) logger . info ( \"=\" * 40 ) try : pipeline = TabularPipeline ( model_name = config [ 'model_name' ], tuning_strategy = config [ 'tuning_strategy' ], model_params = config [ 'model_params' ], tuning_params = config [ 'tuning_params' ] ) pipeline . fit ( self . X_train , self . y_train ) metrics = pipeline . evaluate ( self . X_test , self . y_test ) result_row = { 'Model' : config [ 'model_name' ], 'Strategy' : config [ 'tuning_strategy' ], 'Accuracy' : metrics . get ( 'accuracy' , 0 ), 'F1 Score' : metrics . get ( 'f1_score' , 0 ), 'ROC AUC' : metrics . get ( 'roc_auc_score' , 0 ) } self . results . append ( result_row ) except Exception as e : logger . error ( f \"[Leaderboard] Error running { config [ 'model_name' ] } : { e } \" ) result_row = { 'Model' : config [ 'model_name' ], 'Strategy' : config [ 'tuning_strategy' ], 'Accuracy' : 'Failed' , 'F1 Score' : 'Failed' , 'ROC AUC' : 'Failed' } self . results . append ( result_row ) logger . info ( \" \\n \" + \"=\" * 60 ) logger . info ( \"[Leaderboard] Leaderboard Complete\" ) leaderboard_df = pd . DataFrame ( self . results ) sort_map = { 'accuracy' : 'Accuracy' , 'f1_score' : 'F1 Score' , 'roc_auc_score' : 'ROC AUC' } sort_column = sort_map . get ( rank_by , 'ROC AUC' ) if sort_column in leaderboard_df . columns : leaderboard_df [ sort_column ] = pd . to_numeric ( leaderboard_df [ sort_column ], errors = 'coerce' ) leaderboard_df = leaderboard_df . sort_values ( by = sort_column , ascending = False ) . reset_index ( drop = True ) leaderboard_df . index = leaderboard_df . index + 1 leaderboard_df . index . name = 'Rank' display ( Markdown ( \"Leaderboard Results\" )) display ( leaderboard_df ) return leaderboard_df","title":"TabularLeaderboard"},{"location":"api/leaderboard/#api-tabularleaderboard","text":"A leaderboard utility to benchmark multiple TabTune pipeline configurations on a single, pre-split dataset. Source code in tabtune/TabularLeaderboard/leaderboard.py 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 class TabularLeaderboard : \"\"\" A leaderboard utility to benchmark multiple TabTune pipeline configurations on a single, pre-split dataset. \"\"\" def __init__ ( self , X_train : pd . DataFrame , X_test : pd . DataFrame , y_train : pd . Series , y_test : pd . Series ): \"\"\" Initializes the leaderboard with a user-provided, pre-split dataset. Args: X_train (pd.DataFrame): Training features. X_test (pd.DataFrame): Testing features. y_train (pd.Series): Training target. y_test (pd.Series): Testing target. \"\"\" self . X_train = X_train self . X_test = X_test self . y_train = y_train self . y_test = y_test self . models_to_run = [] self . results = [] logger . info ( \"[Leaderboard] Leaderboard initialized with custom dataset\" ) logger . info ( f \"[Leaderboard] Data prepared with { self . X_train . shape [ 0 ] } training samples and { self . X_test . shape [ 0 ] } test samples\" ) def add_model ( self , model_name : str , tuning_strategy : str = 'inference' , model_params : dict = None , tuning_params : dict = None ): \"\"\" Adds a model configuration to the list of contestants for the leaderboard. \"\"\" config = { \"model_name\" : model_name , \"tuning_strategy\" : tuning_strategy , \"model_params\" : model_params or {}, \"tuning_params\" : tuning_params or {} } self . models_to_run . append ( config ) logger . info ( f \"[Leaderboard] Added to leaderboard: { model_name } (Strategy: { tuning_strategy } )\" ) def run ( self , rank_by : str = 'roc_auc_score' ): \"\"\" Runs all added model configurations, evaluates them, and displays a sorted leaderboard. \"\"\" logger . info ( \" \\n \" + \"=\" * 60 ) logger . info ( \"[Leaderboard] Starting Leaderboard Run\" ) for i , config in enumerate ( self . models_to_run ): logger . info ( \" \\n \" + \"=\" * 40 ) logger . info ( f \"[Leaderboard] [ { i + 1 } / { len ( self . models_to_run ) } ] Running: { config [ 'model_name' ] } (Strategy: { config [ 'tuning_strategy' ] } )\" ) logger . info ( \"=\" * 40 ) try : pipeline = TabularPipeline ( model_name = config [ 'model_name' ], tuning_strategy = config [ 'tuning_strategy' ], model_params = config [ 'model_params' ], tuning_params = config [ 'tuning_params' ] ) pipeline . fit ( self . X_train , self . y_train ) metrics = pipeline . evaluate ( self . X_test , self . y_test ) result_row = { 'Model' : config [ 'model_name' ], 'Strategy' : config [ 'tuning_strategy' ], 'Accuracy' : metrics . get ( 'accuracy' , 0 ), 'F1 Score' : metrics . get ( 'f1_score' , 0 ), 'ROC AUC' : metrics . get ( 'roc_auc_score' , 0 ) } self . results . append ( result_row ) except Exception as e : logger . error ( f \"[Leaderboard] Error running { config [ 'model_name' ] } : { e } \" ) result_row = { 'Model' : config [ 'model_name' ], 'Strategy' : config [ 'tuning_strategy' ], 'Accuracy' : 'Failed' , 'F1 Score' : 'Failed' , 'ROC AUC' : 'Failed' } self . results . append ( result_row ) logger . info ( \" \\n \" + \"=\" * 60 ) logger . info ( \"[Leaderboard] Leaderboard Complete\" ) leaderboard_df = pd . DataFrame ( self . results ) sort_map = { 'accuracy' : 'Accuracy' , 'f1_score' : 'F1 Score' , 'roc_auc_score' : 'ROC AUC' } sort_column = sort_map . get ( rank_by , 'ROC AUC' ) if sort_column in leaderboard_df . columns : leaderboard_df [ sort_column ] = pd . to_numeric ( leaderboard_df [ sort_column ], errors = 'coerce' ) leaderboard_df = leaderboard_df . sort_values ( by = sort_column , ascending = False ) . reset_index ( drop = True ) leaderboard_df . index = leaderboard_df . index + 1 leaderboard_df . index . name = 'Rank' display ( Markdown ( \"Leaderboard Results\" )) display ( leaderboard_df ) return leaderboard_df","title":"API: TabularLeaderboard"},{"location":"api/leaderboard/#tabtune.TabularLeaderboard.leaderboard.TabularLeaderboard.__init__","text":"Initializes the leaderboard with a user-provided, pre-split dataset. Parameters: Name Type Description Default X_train DataFrame Training features. required X_test DataFrame Testing features. required y_train Series Training target. required y_test Series Testing target. required Source code in tabtune/TabularLeaderboard/leaderboard.py 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 def __init__ ( self , X_train : pd . DataFrame , X_test : pd . DataFrame , y_train : pd . Series , y_test : pd . Series ): \"\"\" Initializes the leaderboard with a user-provided, pre-split dataset. Args: X_train (pd.DataFrame): Training features. X_test (pd.DataFrame): Testing features. y_train (pd.Series): Training target. y_test (pd.Series): Testing target. \"\"\" self . X_train = X_train self . X_test = X_test self . y_train = y_train self . y_test = y_test self . models_to_run = [] self . results = [] logger . info ( \"[Leaderboard] Leaderboard initialized with custom dataset\" ) logger . info ( f \"[Leaderboard] Data prepared with { self . X_train . shape [ 0 ] } training samples and { self . X_test . shape [ 0 ] } test samples\" )","title":"__init__"},{"location":"api/leaderboard/#tabtune.TabularLeaderboard.leaderboard.TabularLeaderboard.add_model","text":"Adds a model configuration to the list of contestants for the leaderboard. Source code in tabtune/TabularLeaderboard/leaderboard.py 38 39 40 41 42 43 44 45 46 47 48 49 50 def add_model ( self , model_name : str , tuning_strategy : str = 'inference' , model_params : dict = None , tuning_params : dict = None ): \"\"\" Adds a model configuration to the list of contestants for the leaderboard. \"\"\" config = { \"model_name\" : model_name , \"tuning_strategy\" : tuning_strategy , \"model_params\" : model_params or {}, \"tuning_params\" : tuning_params or {} } self . models_to_run . append ( config ) logger . info ( f \"[Leaderboard] Added to leaderboard: { model_name } (Strategy: { tuning_strategy } )\" )","title":"add_model"},{"location":"api/leaderboard/#tabtune.TabularLeaderboard.leaderboard.TabularLeaderboard.run","text":"Runs all added model configurations, evaluates them, and displays a sorted leaderboard. Source code in tabtune/TabularLeaderboard/leaderboard.py 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 def run ( self , rank_by : str = 'roc_auc_score' ): \"\"\" Runs all added model configurations, evaluates them, and displays a sorted leaderboard. \"\"\" logger . info ( \" \\n \" + \"=\" * 60 ) logger . info ( \"[Leaderboard] Starting Leaderboard Run\" ) for i , config in enumerate ( self . models_to_run ): logger . info ( \" \\n \" + \"=\" * 40 ) logger . info ( f \"[Leaderboard] [ { i + 1 } / { len ( self . models_to_run ) } ] Running: { config [ 'model_name' ] } (Strategy: { config [ 'tuning_strategy' ] } )\" ) logger . info ( \"=\" * 40 ) try : pipeline = TabularPipeline ( model_name = config [ 'model_name' ], tuning_strategy = config [ 'tuning_strategy' ], model_params = config [ 'model_params' ], tuning_params = config [ 'tuning_params' ] ) pipeline . fit ( self . X_train , self . y_train ) metrics = pipeline . evaluate ( self . X_test , self . y_test ) result_row = { 'Model' : config [ 'model_name' ], 'Strategy' : config [ 'tuning_strategy' ], 'Accuracy' : metrics . get ( 'accuracy' , 0 ), 'F1 Score' : metrics . get ( 'f1_score' , 0 ), 'ROC AUC' : metrics . get ( 'roc_auc_score' , 0 ) } self . results . append ( result_row ) except Exception as e : logger . error ( f \"[Leaderboard] Error running { config [ 'model_name' ] } : { e } \" ) result_row = { 'Model' : config [ 'model_name' ], 'Strategy' : config [ 'tuning_strategy' ], 'Accuracy' : 'Failed' , 'F1 Score' : 'Failed' , 'ROC AUC' : 'Failed' } self . results . append ( result_row ) logger . info ( \" \\n \" + \"=\" * 60 ) logger . info ( \"[Leaderboard] Leaderboard Complete\" ) leaderboard_df = pd . DataFrame ( self . results ) sort_map = { 'accuracy' : 'Accuracy' , 'f1_score' : 'F1 Score' , 'roc_auc_score' : 'ROC AUC' } sort_column = sort_map . get ( rank_by , 'ROC AUC' ) if sort_column in leaderboard_df . columns : leaderboard_df [ sort_column ] = pd . to_numeric ( leaderboard_df [ sort_column ], errors = 'coerce' ) leaderboard_df = leaderboard_df . sort_values ( by = sort_column , ascending = False ) . reset_index ( drop = True ) leaderboard_df . index = leaderboard_df . index + 1 leaderboard_df . index . name = 'Rank' display ( Markdown ( \"Leaderboard Results\" )) display ( leaderboard_df ) return leaderboard_df","title":"run"},{"location":"api/peft-utils/","text":"API: PEFT Utilities \u00b6 inject_custom_lora_into_linear_layers ( model , target_names = None , r = 8 , alpha = 16 , dropout = 0.0 , exclude_patterns = None ) \u00b6 Inject LoRA adapters into linear layers, optionally excluding certain patterns. Source code in tabtune/TuningManager/peft_utils.py 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 def inject_custom_lora_into_linear_layers ( model : nn . Module , target_names : Optional [ Sequence [ str ]] = None , r : int = 8 , alpha : int = 16 , dropout : float = 0.0 , exclude_patterns : Optional [ Sequence [ str ]] = None , ) -> nn . Module : \"\"\"Inject LoRA adapters into linear layers, optionally excluding certain patterns.\"\"\" items = _collect_linear_items ( model ) tokens = [ t . lower () for t in target_names or ()] exclude_tokens = [ e . lower () for e in exclude_patterns or ()] wrapped_count = 0 for parent , attr , dotted in items : # Skip if doesn't match target patterns if tokens and not _should_wrap ( dotted , tokens ): continue # Skip if matches exclude patterns if exclude_tokens and _should_wrap ( dotted , exclude_tokens ): continue base_linear = getattr ( parent , attr ) setattr ( parent , attr , LoRALinear ( base_linear , r = r , alpha = alpha , dropout = dropout )) wrapped_count += 1 return model","title":"PEFT Utils"},{"location":"api/peft-utils/#api-peft-utilities","text":"","title":"API: PEFT Utilities"},{"location":"api/peft-utils/#tabtune.TuningManager.peft_utils.inject_custom_lora_into_linear_layers","text":"Inject LoRA adapters into linear layers, optionally excluding certain patterns. Source code in tabtune/TuningManager/peft_utils.py 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 def inject_custom_lora_into_linear_layers ( model : nn . Module , target_names : Optional [ Sequence [ str ]] = None , r : int = 8 , alpha : int = 16 , dropout : float = 0.0 , exclude_patterns : Optional [ Sequence [ str ]] = None , ) -> nn . Module : \"\"\"Inject LoRA adapters into linear layers, optionally excluding certain patterns.\"\"\" items = _collect_linear_items ( model ) tokens = [ t . lower () for t in target_names or ()] exclude_tokens = [ e . lower () for e in exclude_patterns or ()] wrapped_count = 0 for parent , attr , dotted in items : # Skip if doesn't match target patterns if tokens and not _should_wrap ( dotted , tokens ): continue # Skip if matches exclude patterns if exclude_tokens and _should_wrap ( dotted , exclude_tokens ): continue base_linear = getattr ( parent , attr ) setattr ( parent , attr , LoRALinear ( base_linear , r = r , alpha = alpha , dropout = dropout )) wrapped_count += 1 return model","title":"inject_custom_lora_into_linear_layers"},{"location":"api/pipeline/","text":"API: TabularPipeline \u00b6 Complete API reference for the TabularPipeline class\u2014the main entry point for TabTune. The complete TabularPipeline with a robust constructor that explicitly handles parameters for each component and uses late initialization for complex models like ContextTab and Mitra. Source code in tabtune/TabularPipeline/pipeline.py 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755 756 757 758 759 760 761 762 763 764 765 766 767 768 769 770 771 772 773 774 775 776 777 778 779 780 781 782 783 784 785 786 787 788 789 790 791 792 793 794 795 796 797 798 799 800 801 802 803 804 805 806 807 808 809 810 811 812 813 814 815 816 817 818 819 820 821 822 823 824 825 826 827 828 829 830 831 832 833 834 835 836 837 838 839 840 841 842 843 844 845 846 847 848 849 850 851 852 853 854 855 856 857 858 859 860 861 862 863 864 865 866 867 868 869 870 871 872 873 874 875 876 877 878 879 880 881 882 883 884 885 886 887 888 889 890 891 892 893 894 895 896 897 898 899 900 901 902 903 904 905 906 907 908 909 910 911 912 913 914 915 916 917 918 919 920 921 922 923 924 925 926 927 928 929 930 931 932 933 934 935 936 937 938 939 940 941 942 943 944 945 946 947 948 949 950 951 952 953 954 955 956 957 958 959 960 961 962 963 964 965 966 967 968 969 970 971 972 973 974 975 976 977 978 979 980 981 982 983 984 985 986 987 988 989 990 991 992 993 994 995 996 997 998 999 1000 1001 1002 1003 1004 1005 1006 1007 1008 1009 1010 1011 1012 1013 1014 1015 1016 1017 1018 1019 1020 1021 1022 1023 1024 1025 1026 1027 1028 1029 1030 1031 1032 1033 1034 1035 1036 1037 1038 1039 1040 1041 1042 1043 1044 1045 1046 1047 1048 1049 1050 1051 1052 1053 1054 1055 1056 1057 1058 1059 1060 1061 1062 1063 1064 1065 1066 1067 1068 1069 1070 1071 1072 1073 1074 1075 1076 1077 1078 1079 1080 1081 1082 1083 1084 1085 1086 1087 1088 1089 1090 1091 1092 1093 1094 1095 1096 1097 1098 1099 1100 1101 1102 1103 1104 1105 1106 1107 1108 1109 1110 1111 1112 1113 1114 1115 1116 1117 1118 1119 1120 1121 1122 1123 1124 1125 1126 1127 1128 1129 1130 1131 1132 1133 1134 class TabularPipeline : \"\"\" The complete TabularPipeline with a robust constructor that explicitly handles parameters for each component and uses late initialization for complex models like ContextTab and Mitra. \"\"\" def __init__ ( self , model_name : str , task_type : str = 'classification' , tuning_strategy : str = 'inference' , tuning_params : dict | None = None , processor_params : dict | None = None , model_params : dict | None = None , model_checkpoint_path : str | None = None , finetune_mode : str = 'meta-learning' ): print ( \" \\n \" + \"=\" * 80 ) print ( r \"\"\" \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2557 \u2588\u2588\u2588\u2588\u2588\u2557 \u2588\u2588\u2588\u2588\u2588\u2588\u2557 \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2557\u2588\u2588\u2557 \u2588\u2588\u2557\u2588\u2588\u2588\u2557 \u2588\u2588\u2557\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2557 \u255a\u2550\u2550\u2588\u2588\u2554\u2550\u2550\u255d\u2588\u2588\u2554\u2550\u2550\u2588\u2588\u2557\u2588\u2588\u2554\u2550\u2550\u2588\u2588\u2557 \u255a\u2550\u2550\u2588\u2588\u2554\u2550\u2550\u255d\u2588\u2588\u2551 \u2588\u2588\u2551\u2588\u2588\u2588\u2588\u2557 \u2588\u2588\u2551\u2588\u2588\u2554\u2550\u2550\u2550\u2550\u255d \u2588\u2588\u2551 \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2551\u2588\u2588\u2588\u2588\u2588\u2588\u2554\u255d \u2588\u2588\u2551 \u2588\u2588\u2551 \u2588\u2588\u2551\u2588\u2588\u2554\u2588\u2588\u2557 \u2588\u2588\u2551\u2588\u2588\u2588\u2588\u2588\u2557 \u2588\u2588\u2551 \u2588\u2588\u2554\u2550\u2550\u2588\u2588\u2551\u2588\u2588\u2554\u2550\u2550\u2588\u2588\u2557 \u2588\u2588\u2551 \u2588\u2588\u2551 \u2588\u2588\u2551\u2588\u2588\u2551\u255a\u2588\u2588\u2557\u2588\u2588\u2551\u2588\u2588\u2554\u2550\u2550\u255d \u2588\u2588\u2551 \u2588\u2588\u2551 \u2588\u2588\u2551\u2588\u2588\u2588\u2588\u2588\u2588\u2554\u255d \u2588\u2588\u2551 \u255a\u2588\u2588\u2588\u2588\u2588\u2588\u2554\u255d\u2588\u2588\u2551 \u255a\u2588\u2588\u2588\u2588\u2551\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2557 \u255a\u2550\u255d \u255a\u2550\u255d \u255a\u2550\u255d\u255a\u2550\u2550\u2550\u2550\u2550\u255d \u255a\u2550\u255d \u255a\u2550\u2550\u2550\u2550\u2550\u255d \u255a\u2550\u255d \u255a\u2550\u2550\u2550\u255d\u255a\u2550\u2550\u2550\u2550\u2550\u2550\u255d \"\"\" ) print ( \"Unified Library for Fine-Tuning and Inference of Foundational Tabular Models\" ) print ( \"=\" * 80 + \" \\n \" ) self . model_name = model_name self . task_type = task_type self . tuning_strategy = tuning_strategy self . tuning_params = tuning_params or {} self . model_params = model_params or {} self . processor = DataProcessor ( model_name = self . model_name , ** ( processor_params or {})) self . tuner = TuningManager () self . model = None self . model_checkpoint_path = model_checkpoint_path self . finetune_mode = finetune_mode if self . tuning_strategy in ( 'finetune' , 'base-ft' , 'peft' ): self . tuning_params [ 'finetune_mode' ] = self . finetune_mode if self . model_name in [ 'TabPFN' ]: device = 'cuda' if torch . cuda . is_available () else 'cpu' config = { 'device' : device , 'ignore_pretraining_limits' : True } config . update ( self . model_params ) logger . info ( f \"[Pipeline] Config: { config } \" ) self . model = TabPFNClassifier ( ** config ) if self . tuning_strategy in [ 'finetune' , 'base-ft' , 'peft' ] and hasattr ( self . model , '_initialize_model_variables' ): self . model . _initialize_model_variables () elif self . model_name == 'ContextTab' : self . model = ConTextTabClassifier ( ** self . model_params ) elif self . model_name in [ 'TabICL' , 'OrionBix' , 'OrionMSP' ]: device = 'cuda' if torch . cuda . is_available () else 'cpu' config = { 'n_jobs' : 1 , 'device' : device } config . update ( self . model_params ) if self . model_name == 'TabICL' : self . model = TabICLClassifier ( ** config ) if self . tuning_strategy == 'finetune' : self . model . _load_model () elif self . model_name == 'OrionMSP' : self . model = OrionMSPClassifier ( ** config ) if self . tuning_strategy == 'finetune' : self . model . _load_model () else : self . model = OrionBixClassifier ( ** config ) if self . tuning_strategy == 'finetune' : self . model . _load_model () elif self . model_name == 'TabDPT' : # Use GPU if available, otherwise fall back to CPU device = 'cuda' if torch . cuda . is_available () else 'cpu' config = { 'device' : device , 'compile' : True , # Disable compilation to avoid GPU issues 'use_flash' : True , # Disable flash attention to avoid kernel issues 'normalizer' : 'standard' , 'missing_indicators' : False , 'clip_sigma' : 4.0 , 'feature_reduction' : 'pca' , 'faiss_metric' : 'l2' , # Inference parameters with GPU-friendly defaults 'n_ensembles' : 8 , 'temperature' : 0.8 , 'context_size' : 512 , 'permute_classes' : True , 'seed' : None , } config . update ( self . model_params ) # All parameters now valid self . model = TabDPTClassifier ( ** config ) # Handle models that require late initialization (processor needs to be fit first) elif self . model_name not in [ 'Mitra' , 'APT' ]: raise ValueError ( f \"Model ' { self . model_name } ' not supported.\" ) if self . model is not None and self . model_checkpoint_path : logger . info ( f \"[Pipeline] Attempting to load model state from checkpoint: { self . model_checkpoint_path } \" ) try : # Determine the underlying torch model attribute torch_model = None if hasattr ( self . model , 'model_' ): # For TabPFN, TabICL, OrionMSP, OrionBix torch_model = self . model . model_ elif hasattr ( self . model , 'model' ): # For ContextTab, TabDPT torch_model = self . model . model elif isinstance ( self . model , torch . nn . Module ): # For Mitra (Tab2D) torch_model = self . model if torch_model : torch_model . load_state_dict ( torch . load ( self . model_checkpoint_path , map_location = torch . device ( 'cpu' ))) logger . info ( f \"[Pipeline] Successfully loaded checkpoint for { type ( self . model ) . _name_ } .\" ) else : logger . warning ( f \"[Pipeline] Could not determine the underlying torch model for { type ( self . model ) . _name_ } to load checkpoint.\" ) except Exception as e : logger . error ( f \"[Pipeline] Failed to load checkpoint: { e } \" ) self . _is_fitted = False self . X_train_processed_ = None self . y_train_processed_ = None logger . info ( f \"[Pipeline] TabularPipeline initialized for model ' { self . model_name } ', task ' { self . task_type } ', with strategy ' { self . tuning_strategy } '\" ) ( \"TabTune - Unified Library for fine-tuning and inference of Foundational Tabular Models\" ) def __del__ ( self ): \"\"\"Cleanup method to properly shut down resources when pipeline is destroyed.\"\"\" # ContextTab ZMQ server cleanup is handled automatically by atexit.register() # in the start_embedding_server function, so no manual cleanup needed pass def fit ( self , X : pd . DataFrame , y : pd . Series ): self . X_raw_train = X . copy () self . y_raw_train = y . copy () logger . info ( \"[Pipeline] Starting fit process\" ) # Special handling for models that are TRULY self-contained and do not need the pipeline's processor for inference if self . tuning_strategy == 'inference' and isinstance ( self . model , ( TabICLClassifier , OrionMSPClassifier , OrionBixClassifier )): logger . info ( \"[Pipeline] Handing off to TuningManager for inference setup.\" ) self . processor . fit ( X , y ) self . model = self . tuner . tune ( self . model , X , y , strategy = self . tuning_strategy ) self . _is_fitted = True logger . info ( \"[Pipeline] Fit process complete\" ) return self #For ALL other models and strategies (including ConTextTab), we must fit the DataProcessor first. logger . info ( \"[Pipeline] Fitting data processor...\" ) self . processor . fit ( X , y ) # Handle ConTextTab inference AFTER the processor has been fitted if self . tuning_strategy == 'inference' and isinstance ( self . model , ConTextTabClassifier ): logger . info ( f \"[Pipeline] Handing off to TuningManager for inference setup for { self . model_name } \" ) # The tuner calls the model's native .fit() method with the raw data self . model = self . tuner . tune ( self . model , X , y , strategy = self . tuning_strategy ) self . _is_fitted = True logger . info ( \"[Pipeline] Fit process complete\" ) return self # Late initialization for models that need info from the fitted processor if self . model is None : logger . info ( \"[Pipeline] Performing late initialization of the model...\" ) if self . model_name == 'Mitra' : n_classes = len ( self . processor . custom_preprocessor_ . label_encoder_ . classes_ ) device = 'cuda' if torch . cuda . is_available () else 'cpu' config = { 'dim' : 256 , 'n_layers' : 6 , 'n_heads' : 8 , 'task' : 'CLASSIFICATION' , 'dim_output' : n_classes , 'use_pretrained_weights' : False , 'path_to_weights' : '' , 'device' : device } config . update ( self . model_params ) self . model = Tab2D ( ** config ) if self . model_checkpoint_path : logger . info ( f \"[Pipeline] Attempting to load model state from checkpoint for late-initialized model: { self . model_checkpoint_path } \" ) try : self . model . load_state_dict ( torch . load ( self . model_checkpoint_path , map_location = torch . device ())) logger . info ( f \"[Pipeline] Successfully loaded checkpoint for { type ( self . model ) . _name_ } .\" ) except Exception as e : logger . error ( f \"[Pipeline] Failed to load checkpoint: { e } \" ) if hasattr ( self . model , 'to' ): device_str = self . tuning_params . get ( 'device' , 'cuda' if torch . cuda . is_available () else 'cpu' ) device = torch . device ( device_str ) self . model . to ( device ) if self . model_name == 'Mitra' : # Set device type on model or wrapper try : setattr ( self . model , 'device_type' , device_str ) except Exception : pass if isinstance ( self . model , ( TabICLClassifier , OrionMSPClassifier , OrionBixClassifier )): self . model . device = device if isinstance ( self . model , ConTextTabClassifier ) and self . tuning_strategy in [ 'finetune' , 'base-ft' ]: logger . info ( \"[Pipeline] Preparing raw data for ConTextTab fine-tuning\" ) if not isinstance ( X , pd . DataFrame ): X_to_tune = pd . DataFrame ( X ) else : X_to_tune = X . copy () if not isinstance ( y , pd . Series ): y_to_tune = pd . Series ( y ) else : y_to_tune = y . copy () else : logger . info ( \"[Pipeline] Transforming data for model tuning...\" ) processed_data = self . processor . transform ( X , y ) if isinstance ( processed_data , tuple ): self . X_train_processed_ , self . y_train_processed_ = processed_data else : self . X_train_processed_ = processed_data if hasattr ( self . processor , 'custom_preprocessor_' ) and hasattr ( self . processor . custom_preprocessor_ , 'label_encoder_' ) and self . processor . custom_preprocessor_ . label_encoder_ is not None : self . y_train_processed_ = self . processor . custom_preprocessor_ . label_encoder_ . transform ( y ) else : # Fallback for models without a main processor label encoder self . y_train_processed_ = y X_to_tune , y_to_tune = self . X_train_processed_ , self . y_train_processed_ logger . info ( \"[Pipeline] Handing off to Tuning Manager\" ) if self . tuning_strategy == \"peft\" : logger . info ( \"[Pipeline] PEFT MODE: Attempting Parameter-Efficient Fine-Tuning\" ) logger . info ( \"[Pipeline] NOTE: PEFT may have compatibility limitations with tabular models\" ) logger . info ( \"[Pipeline] FALLBACK: Base fine-tuning will be used if PEFT fails\" ) self . model = self . tuner . tune ( self . model , X_to_tune , y_to_tune , strategy = self . tuning_strategy , params = self . tuning_params , processor = self . processor ) if isinstance ( self . model , TabDPTClassifier ) and self . tuning_strategy in [ 'finetune' , 'base-ft' , 'peft' ]: logger . info ( \"[Pipeline] Finalizing TabDPT setup after fine-tuning\" ) self . model . num_classes = len ( np . unique ( y_to_tune )) # Fit the model for inference after fine-tuning self . model . fit ( X_to_tune , y_to_tune ) self . _is_fitted = True logger . info ( \"[Pipeline] Fit process complete\" ) if self . tuning_strategy == \"peft\" : logger . info ( \"[Pipeline] PEFT STATUS SUMMARY\" ) logger . info ( \"[Pipeline] LoRA adapters were applied to the model\" ) logger . warning ( \"[Pipeline] Note: PEFT compatibility with tabular models is experimental\" ) logger . info ( \"[Pipeline] If you encounter issues, try 'base-ft' strategy for full compatibility\" ) logger . info ( \"[Pipeline] See documentation for more details on PEFT limitations\" ) return self def predict ( self , X : pd . DataFrame ) -> np . ndarray : if not self . _is_fitted : raise RuntimeError ( \"You must call fit() on the pipeline before calling predict().\" ) logger . info ( \"[Pipeline] Starting prediction\" ) if hasattr ( self . model , 'model' ) and isinstance ( self . model . model , torch . nn . Module ): self . model . model . eval () elif hasattr ( self . model , 'model_' ) and isinstance ( self . model . model_ , torch . nn . Module ): self . model . model_ . eval () if isinstance ( self . model , TabPFNClassifier ): if self . tuning_strategy in [ 'finetune' , 'base-ft' , 'peft' ]: logger . debug ( \"[Pipeline] Setting TabPFN inference context (without refitting weights)...\" ) # Store current model weights saved_weights = self . model . model_ . state_dict () self . model . model_ . load_state_dict ( saved_weights ) # Call fit to set up inference context self . model . fit ( self . X_train_processed_ , self . y_train_processed_ ) # Restore fine-tuned weights immediately #self.model.model_.load_state_dict(saved_weights) logger . debug ( \"[Pipeline] Restored fine-tuned weights after context setup\" ) X_processed = self . processor . transform ( X ) return self . model . predict ( X_processed ) if isinstance ( self . model , TabDPTClassifier ): # Apply the same preprocessing as during fit() X_processed = self . processor . transform ( X ) # Get integer predictions from model predictions_raw = self . model . predict ( X_processed ) # Convert integer predictions back to original string labels (same as TabICL/OrionMSP/OrionBix) predictions = self . processor . custom_preprocessor_ . label_encoder_ . inverse_transform ( predictions_raw ) return predictions if isinstance ( self . model , ( ConTextTabClassifier )): logger . debug ( f \"[Pipeline] Using model's native in-context prediction for { type ( self . model ) . __name__ } \" ) predictions = self . model . predict ( X ) elif isinstance ( self . model , ( TabICLClassifier , OrionMSPClassifier , OrionBixClassifier )): logger . debug ( f \"[Pipeline] Using model's native in-context prediction for { type ( self . model ) . __name__ } \" ) X_processed = self . processor . transform ( X ) #predictions = self.model.predict(X) if self . tuning_strategy == 'inference' : # For inference mode, pass raw data directly to the model # The model's internal encoders will handle the preprocessing predictions = self . model . predict ( X ) else : # For fine-tuning mode, use preprocessed data to match training label_encoder = self . processor . custom_preprocessor_ . label_encoder_ known_class = label_encoder . classes_ [ 0 ] y_dummy = pd . Series ([ known_class ] * len ( X )) X_query , _ = self . processor . transform ( X , y_dummy ) # Convert to DataFrame to maintain feature names for sklearn compatibility if not isinstance ( X_query , pd . DataFrame ): # Prefer processor feature names if available; else fall back to input X cols = None if hasattr ( self . processor , \"feature_names_\" ) and self . processor . feature_names_ is not None : cols = list ( self . processor . feature_names_ ) elif hasattr ( X , \"columns\" ): cols = list ( X . columns ) # Avoid shape/columns mismatch if cols is not None and hasattr ( X_query , \"shape\" ) and X_query . shape [ 1 ] != len ( cols ): cols = None X_query = pd . DataFrame ( X_query , columns = cols ) predictions = self . model . predict ( X_query ) # Convert numerical predictions back to string format for evaluation if self . tuning_strategy in [ 'finetune' , 'base-ft' , 'peft' ] and hasattr ( self . processor , 'custom_preprocessor_' ) and hasattr ( self . processor . custom_preprocessor_ , 'label_encoder_' ): predictions = self . processor . custom_preprocessor_ . label_encoder_ . inverse_transform ( predictions ) elif self . model_name == 'Mitra' : logger . debug ( \"[Pipeline] Using in-context prediction for Mitra (Tab2D)\" ) label_encoder = self . processor . custom_preprocessor_ . label_encoder_ known_class = label_encoder . classes_ [ 0 ] y_dummy = pd . Series ([ known_class ] * len ( X )) X_query , _ = self . processor . transform ( X , y_dummy ) X_support , y_support = self . X_train_processed_ , self . y_train_processed_ device_str = self . tuning_params . get ( 'device' , 'cuda' if torch . cuda . is_available () else 'cpu' ) device = device_str X_support_t = torch . tensor ( X_support , dtype = torch . float32 ) . unsqueeze ( 0 ) . to ( device ) y_support_t = torch . tensor ( y_support , dtype = torch . long ) . unsqueeze ( 0 ) . to ( device ) X_query_t = torch . tensor ( X_query , dtype = torch . float32 ) . unsqueeze ( 0 ) . to ( device ) b , f = X_support_t . shape [ 0 ], X_support_t . shape [ 2 ] padding_features = torch . zeros ( b , f , dtype = torch . bool , device = device ) padding_obs_support = torch . zeros_like ( y_support_t , dtype = torch . bool , device = device ) padding_obs_query = torch . zeros ( b , X_query_t . shape [ 1 ], dtype = torch . bool , device = device ) self . model . eval () with torch . no_grad (): logits = self . model ( x_support = X_support_t , y_support = y_support_t , x_query = X_query_t , padding_features = padding_features , padding_obs_support = padding_obs_support , padding_obs_query__ = padding_obs_query ) predictions_raw = logits . squeeze ( 0 ) . cpu () . numpy () . argmax ( axis =- 1 ) predictions = self . processor . custom_preprocessor_ . label_encoder_ . inverse_transform ( predictions_raw ) else : # TabPFN and other standard models do not need y_dummy for prediction transforms logger . debug ( \"[Pipeline] Applying learned transformations to new data\" ) X_processed = self . processor . transform ( X ) # Pass only X logger . debug ( \"[Pipeline] Getting predictions from the model\" ) predictions = self . model . predict ( X_processed ) return predictions def predict_proba ( self , X : pd . DataFrame ) -> np . ndarray : \"\"\" Predicts class probabilities for the input data. Required for calculating AUC score. \"\"\" if not self . _is_fitted : raise RuntimeError ( \"You must call fit() on the pipeline before calling predict_proba().\" ) logger . info ( \"[Pipeline] Starting probability prediction\" ) if hasattr ( self . model , 'model' ) and isinstance ( self . model . model , torch . nn . Module ): self . model . model . eval () elif hasattr ( self . model , 'model_' ) and isinstance ( self . model . model_ , torch . nn . Module ): self . model . model_ . eval () if isinstance ( self . model , TabDPTClassifier ): logger . debug ( \"[Pipeline] Using TabDPT's internal predict_proba\" ) # Apply the same preprocessing as during fit() X_processed = self . processor . transform ( X ) # Use stored defaults from model initialization return self . model . ensemble_predict_proba ( X_processed ) elif isinstance ( self . model , TabPFNClassifier ): # Special handling for fine-tuned TabPFN to set inference context if self . tuning_strategy in [ 'finetune' , 'base-ft' , 'peft' ]: logger . debug ( \"[Pipeline] Setting TabPFN inference context for proba...\" ) self . model . fit ( self . X_train_processed_ , self . y_train_processed_ ) X_processed = self . processor . transform ( X ) return self . model . predict_proba ( X_processed ) if isinstance ( self . model , ( TabICLClassifier , OrionMSPClassifier , OrionBixClassifier , ConTextTabClassifier )): logger . debug ( \"[Pipeline] Using model's native predict_proba method\" ) X_processed = self . processor . transform ( X ) if isinstance ( self . model , ( ConTextTabClassifier )): return self . model . predict_proba ( X ) if isinstance ( self . model , ( TabICLClassifier , OrionMSPClassifier , OrionBixClassifier )): if self . tuning_strategy == 'inference' : return self . model . predict_proba ( X ) else : label_encoder = self . processor . custom_preprocessor_ . label_encoder_ known_class = label_encoder . classes_ [ 0 ] y_dummy = pd . Series ([ known_class ] * len ( X )) X_query , _ = self . processor . transform ( X , y_dummy ) # Convert to DataFrame to maintain feature names for sklearn compatibility if not isinstance ( X_query , pd . DataFrame ): # Prefer processor feature names if available; else fall back to input X cols = None if hasattr ( self . processor , \"feature_names_\" ) and self . processor . feature_names_ is not None : cols = list ( self . processor . feature_names_ ) elif hasattr ( X , \"columns\" ): cols = list ( X . columns ) # Avoid shape/columns mismatch if cols is not None and hasattr ( X_query , \"shape\" ) and X_query . shape [ 1 ] != len ( cols ): cols = None X_query = pd . DataFrame ( X_query , columns = cols ) return self . model . predict_proba ( X_query ) return self . model . predict_proba ( X_processed ) label_encoder = self . processor . custom_preprocessor_ . label_encoder_ known_class = label_encoder . classes_ [ 0 ] y_dummy = pd . Series ([ known_class ] * len ( X )) X_query , _ = self . processor . transform ( X , y_dummy ) X_support = self . X_train_processed_ y_support = self . y_train_processed_ device = next ( self . model . parameters ()) . device X_support_t = torch . tensor ( X_support , dtype = torch . float32 ) . unsqueeze ( 0 ) . to ( device ) y_support_t = torch . tensor ( y_support , dtype = torch . long ) . unsqueeze ( 0 ) . to ( device ) X_query_t = torch . tensor ( X_query , dtype = torch . float32 ) . unsqueeze ( 0 ) . to ( device ) self . model . eval () with torch . no_grad (): if isinstance ( self . model , Tab2D ): logger . debug ( \"[Pipeline] Generating probabilities for Mitra (Tab2D)\" ) b , f = X_support_t . shape [ 0 ], X_support_t . shape [ 2 ] padding_features = torch . zeros ( b , f , dtype = torch . bool , device = device ) padding_obs_support = torch . zeros_like ( y_support_t , dtype = torch . bool , device = device ) padding_obs_query = torch . zeros ( b , X_query_t . shape [ 1 ], dtype = torch . bool , device = device ) logits = self . model ( x_support = X_support_t , y_support = y_support_t , x_query = X_query_t , padding_features = padding_features , padding_obs_support = padding_obs_support , padding_obs_query__ = padding_obs_query ) probabilities = torch . softmax ( logits . squeeze ( 0 ), dim =- 1 ) . cpu () . numpy () else : if self . model_name == 'Mitra' : # Not implemented for Mitra raise NotImplementedError ( \"predict_proba is not implemented for Mitra (Tab2D)\" ) raise NotImplementedError ( f \"predict_proba is not implemented for model type { type ( self . model ) . __name__ } \" ) logger . info ( \"[Pipeline] Probability prediction complete\" ) return probabilities ############### Helpers ############################# def _get_model_class_labels ( self ): \"\"\" Best-effort to recover the class label order that predict_proba columns use. \"\"\" # sklearn-style estimators if hasattr ( self . model , \"classes_\" ): return list ( self . model . classes_ ) if hasattr ( self . model , \"y_encoder_\" ) and hasattr ( self . model . y_encoder_ , \"classes_\" ): return list ( self . model . y_encoder_ . classes_ ) if hasattr ( self . model , \"classes_\" ): return list ( self . model . classes_ ) return None def _align_proba_to_encoder ( self , probabilities , label_encoder ): \"\"\" Ensure the columns of `probabilities` line up with label_encoder.classes_. Returns a 2D array with shape (n_samples, K) where K==len(label_encoder.classes_). If the model returns only the positive-class column for binary, we upcast it to two columns [P(class0), P(class1)] assuming classes_ are [0,1] after encoding. \"\"\" import numpy as np # Force 2D and validate input if probabilities is None : logger . warning ( \"[Pipeline] Probabilities are None in _align_proba_to_encoder\" ) return None if probabilities . ndim == 1 : probabilities = probabilities . reshape ( - 1 , 1 ) # Check for empty probabilities if probabilities . size == 0 : logger . warning ( \"[Pipeline] Empty probabilities array in _align_proba_to_encoder\" ) return None encoder_classes = list ( label_encoder . classes_ ) K = len ( encoder_classes ) # Binary convenience cases if K == 2 : if probabilities . shape [ 1 ] == 1 : # Validate that single column probabilities are in [0, 1] p_pos = probabilities [:, 0 ] if np . any ( p_pos < 0 ) or np . any ( p_pos > 1 ): logger . warning ( f \"[Pipeline] Single-column probabilities outside [0,1] range (min: { p_pos . min () : .6f } , max: { p_pos . max () : .6f } )\" ) # assume encoder maps positives to label 1 (LabelEncoder does 0..K-1) p_neg = 1.0 - p_pos return np . column_stack ([ p_neg , p_pos ]) # or two columns already \u2014 validate and return elif probabilities . shape [ 1 ] == 2 : # Validate that probabilities are in [0, 1] if np . any ( probabilities < 0 ) or np . any ( probabilities > 1 ): logger . warning ( f \"[Pipeline] Two-column probabilities outside [0,1] range (min: { probabilities . min () : .6f } , max: { probabilities . max () : .6f } )\" ) return probabilities else : logger . warning ( f \"[Pipeline] Unexpected number of probability columns ( { probabilities . shape [ 1 ] } ) for binary classification\" ) return None # Multiclass: align by class labels model_labels = self . _get_model_class_labels () # If we can't recover model labels, assume current order already matches encoder if not model_labels or probabilities . shape [ 1 ] == K and set ( model_labels ) == set ( encoder_classes ): # Still ensure shape matches if probabilities . shape [ 1 ] == K : # Validate that probabilities are in [0, 1] if np . any ( probabilities < 0 ) or np . any ( probabilities > 1 ): logger . warning ( f \"[Pipeline] Multiclass probabilities outside [0,1] range (min: { probabilities . min () : .6f } , max: { probabilities . max () : .6f } )\" ) return probabilities else : logger . warning ( f \"[Pipeline] Shape mismatch: expected { K } columns, got { probabilities . shape [ 1 ] } \" ) return None # Build aligned matrix (zeros for any missing classes) aligned = np . zeros (( probabilities . shape [ 0 ], K ), dtype = float ) # Map model label -> encoder index try : model_to_encoder_idx = { lbl : int ( label_encoder . transform ([ lbl ])[ 0 ]) for lbl in model_labels } except Exception : # If transform fails (types differ), fall back to identity numeric mapping model_to_encoder_idx = {} for j , lbl in enumerate ( model_labels ): try : enc_idx = int ( lbl ) # numeric labels already 0..K-1 except Exception : enc_idx = j model_to_encoder_idx [ lbl ] = enc_idx for j_model , lbl in enumerate ( model_labels ): if j_model >= probabilities . shape [ 1 ]: break enc_j = model_to_encoder_idx . get ( lbl , None ) if enc_j is not None and 0 <= enc_j < K : aligned [:, enc_j ] = probabilities [:, j_model ] # Final validation of aligned probabilities if np . any ( aligned < 0 ) or np . any ( aligned > 1 ): logger . warning ( f \"[Pipeline] Aligned probabilities outside [0,1] range (min: { aligned . min () : .6f } , max: { aligned . max () : .6f } )\" ) # Check if any samples have all-zero probabilities (missing class predictions) zero_rows = np . all ( aligned == 0 , axis = 1 ) if np . any ( zero_rows ): logger . warning ( f \"[Pipeline] { np . sum ( zero_rows ) } samples have all-zero probabilities (missing class predictions)\" ) return aligned def evaluate ( self , X_test : pd . DataFrame , y_test : pd . Series , output_format : str = 'rich' ): \"\"\" Makes predictions on the test set and prints a report with Accuracy, F1 Score, and ROC AUC Score. \"\"\" if not self . _is_fitted : raise RuntimeError ( \"You must call fit() on the pipeline before evaluating.\" ) logger . info ( \" \\n \" + \"=\" * 60 ) logger . info ( \"[Pipeline] Running Evaluation\" ) predictions = self . predict ( X_test ) if self . task_type == 'classification' : probabilities = self . predict_proba ( X_test ) y_test_encoded = None # Case 1: Custom preprocessor has a label encoder (Mitra, TabICL, APT, OrionMSP, OrionBix) if hasattr ( self . processor , 'custom_preprocessor_' ) and hasattr ( self . processor . custom_preprocessor_ , 'label_encoder_' ): y_test_encoded = self . processor . custom_preprocessor_ . label_encoder_ . transform ( y_test ) elif isinstance ( self . model , ( TabICLClassifier , OrionMSPClassifier , OrionBixClassifier )): y_test_encoded = self . model . y_encoder_ . transform ( y_test ) elif isinstance ( self . model , TabPFNClassifier ): le = LabelEncoder () le . classes_ = self . model . classes_ # Use the classes the model learned during .fit() y_test_encoded = le . transform ( y_test ) # Case 3: Standard pipeline with a main label encoder elif isinstance ( self . model , ConTextTabClassifier ): if hasattr ( self . processor_ , 'label_encoder_' ): if y_test . dtype == object or y_test . dtype . kind in { 'U' , 'S' }: y_test = self . processor_ . label_encoder_ . transform ( y_test ) elif hasattr ( self . processor , 'label_encoder_' ) and self . processor . label_encoder_ is not None : y_test_encoded = self . processor . label_encoder_ . transform ( y_test ) if y_test_encoded is None : raise RuntimeError ( \"Could not find a fitted label encoder to evaluate metrics.\" ) accuracy = accuracy_score ( y_test , predictions ) f1 = f1_score ( y_test , predictions , average = 'weighted' ) mcc = matthews_corrcoef ( y_test , predictions ) precision = precision_score ( y_test , predictions , average = 'weighted' ) recall = recall_score ( y_test , predictions , average = 'weighted' ) # Guard: AUC is undefined if the test fold has < 2 classes unique_test = np . unique ( y_test_encoded ) if len ( unique_test ) < 2 : auc = float ( \"nan\" ) else : # Align probability columns to the SAME label order used by y_test_encoded # Choose the same encoder you used above when computing y_test_encoded if hasattr ( self . processor , 'custom_preprocessor_' ) and hasattr ( self . processor . custom_preprocessor_ , 'label_encoder_' ): le = self . processor . custom_preprocessor_ . label_encoder_ elif isinstance ( self . model , ( TabICLClassifier , OrionBixClassifier , OrionMSPClassifier )): le = self . model . y_encoder_ elif isinstance ( self . model , TabPFNClassifier ): le = LabelEncoder (); le . classes_ = self . model . classes_ elif hasattr ( self . processor , 'label_encoder_' ) and self . processor . label_encoder_ is not None : le = self . processor . label_encoder_ else : raise RuntimeError ( \"Could not find a fitted label encoder to align probabilities.\" ) probs_aligned = self . _align_proba_to_encoder ( probabilities , le ) # Binary vs multiclass handling with explicit labels to match encoded y K = len ( le . classes_ ) if K == 2 : # probs_aligned has 2 columns by construction: [:, 1] is positive class auc = roc_auc_score ( y_test_encoded , probs_aligned [:, 1 ]) else : auc = roc_auc_score ( y_test_encoded , probs_aligned , labels = list ( range ( K )), # encoded labels are 0..K-1 multi_class = \"ovr\" , average = \"weighted\" , ) results = { \"accuracy\" : accuracy , \"roc_auc_score\" : auc , \"f1_score\" : f1 , \"precision\" : precision , \"recall\" : recall , \"mcc\" : mcc } if output_format == 'json' : print ( json . dumps ( results , indent = 4 )) elif output_format == 'rich' : logger . info ( \" \\n \" + \"=\" * 60 ) logger . info ( \"[Pipeline] Running Evaluation\" ) logger . info ( \" \\n [Pipeline] Evaluation Report\" ) logger . info ( f \"[Pipeline] Accuracy: { accuracy : .4f } \" ) logger . info ( f \"[Pipeline] Weighted F1-Score: { f1 : .4f } \" ) logger . info ( f \"[Pipeline] Weighted Precision: { precision : .4f } \" ) logger . info ( f \"[Pipeline] Weighted Recall: { recall : .4f } \" ) logger . info ( f \"[Pipeline] MCC: { mcc : .4f } \" ) logger . info ( f \"[Pipeline] ROC AUC Score: { auc : .4f } \" ) logger . info ( \" \\n [Pipeline] Classification Report\" ) logger . info ( classification_report ( y_test , predictions , zero_division = 0 )) logger . info ( \"=\" * 60 ) else : logger . warning ( f \"[Pipeline] Unknown output_format: ' { output_format } '. No output printed.\" ) return results def save ( self , file_path : str ): if not self . _is_fitted : raise RuntimeError ( \"You can only save a pipeline after it has been fitted.\" ) logger . info ( f \"[Pipeline] Saving pipeline to { file_path } \" ) joblib . dump ( self , file_path ) logger . info ( \"[Pipeline] Pipeline saved successfully\" ) @classmethod def load ( cls , file_path : str ): logger . info ( f \"[Pipeline] Loading pipeline from { file_path } \" ) pipeline = joblib . load ( file_path ) logger . info ( \"[Pipeline] Pipeline loaded successfully\" ) return pipeline def show_processing_summary ( self ): \"\"\" Retrieves and logs the data processing summary from the DataProcessor. \"\"\" logger . info ( \" \\n \" + \"=\" * 60 ) summary = self . processor . get_processing_summary () # Log the multi-line summary as a single message summary_lines = summary . split ( ' \\n ' ) for line in summary_lines : logger . info ( line ) def _calculate_calibration_errors ( self , y_true , y_prob , n_bins = 10 ): \"\"\"Helper to calculate ECE and MCE.\"\"\" confidences = np . max ( y_prob , axis = 1 ) predictions = np . argmax ( y_prob , axis = 1 ) accuracies = ( predictions == y_true ) ece = 0.0 mce = 0.0 bin_boundaries = np . linspace ( 0 , 1 , n_bins + 1 ) for i in range ( n_bins ): in_bin = ( confidences > bin_boundaries [ i ]) & ( confidences <= bin_boundaries [ i + 1 ]) prop_in_bin = np . mean ( in_bin ) if prop_in_bin > 0 : accuracy_in_bin = np . mean ( accuracies [ in_bin ]) avg_confidence_in_bin = np . mean ( confidences [ in_bin ]) bin_abs_err = np . abs ( accuracy_in_bin - avg_confidence_in_bin ) ece += prop_in_bin * bin_abs_err mce = max ( mce , bin_abs_err ) return ece , mce def evaluate_calibration ( self , X_test : pd . DataFrame , y_test : pd . Series , n_bins : int = 15 , output_format : str = 'rich' ): \"\"\" Calculates and provides a detailed report on model calibration metrics. This version supports both binary and multiclass classification. \"\"\" if not self . _is_fitted : raise RuntimeError ( \"You must call fit() on the pipeline before evaluating calibration.\" ) # --- Metric Calculation (common for all formats) --- probabilities = self . predict_proba ( X_test ) # 1. Find the correct label encoder (same logic as in evaluate()) le = None if hasattr ( self . processor , 'custom_preprocessor_' ) and hasattr ( self . processor . custom_preprocessor_ , 'label_encoder_' ): le = self . processor . custom_preprocessor_ . label_encoder_ elif isinstance ( self . model , ( TabICLClassifier , OrionBixClassifier , OrionMSPClassifier )): # Use model's internal encoder if in inference mode if hasattr ( self . model , 'y_encoder_' ): le = self . model . y_encoder_ # Use processor's encoder if in finetune mode elif hasattr ( self . processor , 'custom_preprocessor_' ) and hasattr ( self . processor . custom_preprocessor_ , 'label_encoder_' ): le = self . processor . custom_preprocessor_ . label_encoder_ elif isinstance ( self . model , TabPFNClassifier ): if hasattr ( self . model , 'classes_' ): le = LabelEncoder () le . classes_ = self . model . classes_ elif hasattr ( self . processor , 'label_encoder_' ) and self . processor . label_encoder_ is not None : le = self . processor . label_encoder_ if le is None : raise RuntimeError ( \"Could not find a fitted label encoder to evaluate calibration.\" ) # 2. Encode y_test using the found encoder y_test_encoded = le . transform ( y_test ) # 3. Align probability columns to match the encoder's class order probs_aligned = self . _align_proba_to_encoder ( probabilities , le ) # 4. Calculate metrics using the aligned probabilities # brier_score_loss handles (n_samples, n_classes) for multiclass # when y_true is (n_samples,) with integer labels [0, K-1]. # Validate inputs before calculating Brier score if probs_aligned is None : logger . warning ( \"[Pipeline] Probabilities are None, skipping Brier score calculation\" ) brier_score = float ( 'nan' ) else : # Check for NaN or infinite values if np . any ( np . isnan ( probs_aligned )) or np . any ( np . isinf ( probs_aligned )): logger . warning ( \"[Pipeline] Probabilities contain NaN or infinite values, skipping Brier score calculation\" ) brier_score = float ( 'nan' ) else : # Validate that probabilities sum to 1.0 (within tolerance) prob_sums = np . sum ( probs_aligned , axis = 1 ) if not np . allclose ( prob_sums , 1.0 , rtol = 1e-6 ): logger . warning ( f \"[Pipeline] Probabilities don't sum to 1.0 (range: { prob_sums . min () : .6f } to { prob_sums . max () : .6f } )\" ) logger . warning ( \"[Pipeline] This may indicate model calibration issues\" ) # Validate that y_test_encoded contains valid class indices max_class_idx = len ( le . classes_ ) - 1 if np . any ( y_test_encoded < 0 ) or np . any ( y_test_encoded > max_class_idx ): logger . warning ( f \"[Pipeline] Invalid class indices in y_test_encoded (range: { y_test_encoded . min () } to { y_test_encoded . max () } )\" ) logger . warning ( f \"[Pipeline] Expected range: 0 to { max_class_idx } \" ) brier_score = float ( 'nan' ) else : try : brier_score = brier_score_loss ( y_test_encoded , probs_aligned ) except Exception as e : logger . error ( f \"[Pipeline] Error calculating Brier score: { e } \" ) brier_score = float ( 'nan' ) # _calculate_calibration_errors also works with (n, K) probability matrix if probs_aligned is None : logger . warning ( \"[Pipeline] Probabilities are None, skipping ECE and MCE calculation\" ) ece , mce = float ( 'nan' ), float ( 'nan' ) else : ece , mce = self . _calculate_calibration_errors ( y_test_encoded , probs_aligned , n_bins = n_bins ) results = { \"brier_score_loss\" : brier_score , \"expected_calibration_error\" : ece , \"maximum_calibration_error\" : mce } if output_format == 'rich' : logger . info ( \" \\n \" + \"=\" * 80 ) logger . info ( \"[Pipeline] Running Detailed Calibration Evaluation\" ) logger . info ( \"=\" * 80 ) logger . info ( \"[Pipeline] Calibration measures how well a model's predicted probabilities match the true likelihood of outcomes.\" ) logger . info ( \"[Pipeline] A well-calibrated model is trustworthy: if it predicts a 70% probability, it should be correct 70 % o f the time. \\n \" ) logger . info ( \"[Pipeline] Brier Score Loss\" ) logger . info ( \"[Pipeline] Measures the mean squared difference between predicted probabilities and actual outcomes.\" ) if np . isnan ( brier_score ): logger . info ( f \"[Pipeline] Your Score: NaN (calculation skipped due to validation issues)\" ) logger . info ( \"[Pipeline] Interpretation: Check warnings above for details on why Brier score could not be calculated.\" ) else : logger . info ( f \"[Pipeline] Your Score: { brier_score : .4f } \" ) logger . info ( \"[Pipeline] Interpretation: Scores range from 0.0 to 1.0, where lower is better. A score near 0.0 indicates excellent calibration.\" ) logger . info ( \"[Pipeline] Note: For multiclass problems, this is the average Brier score across all classes.\" ) logger . info ( \"[Pipeline] Note: For imbalanced datasets, consider class-specific Brier scores for better insights.\" ) logger . info ( \"\" ) logger . info ( \"[Pipeline] Expected & Maximum Calibration Error (ECE / MCE)\" ) logger . info ( \"[Pipeline] These metrics group predictions into bins by confidence (e.g., 80-90%) and measure the gap between the average confidence and the actual accuracy in each bin.\" ) if np . isnan ( ece ) or np . isnan ( mce ): logger . info ( f \"[Pipeline] Expected Calibration Error (ECE): NaN (calculation skipped due to validation issues)\" ) logger . info ( f \"[Pipeline] Maximum Calibration Error (MCE): NaN (calculation skipped due to validation issues)\" ) logger . info ( \"[Pipeline] Interpretation: Check warnings above for details on why ECE/MCE could not be calculated.\" ) else : logger . info ( f \"[Pipeline] Expected Calibration Error (ECE): { ece : .4f } \" ) logger . info ( f \"[Pipeline] Interpretation: ECE represents the average gap between confidence and accuracy across all bins. Your score indicates the model's confidence is off by an average of { ece * 100 : .2f } %. An ECE below 0.05 (5%) is generally considered good.\" ) logger . info ( f \"[Pipeline] Maximum Calibration Error (MCE): { mce : .4f } \" ) logger . info ( \"[Pipeline] Interpretation: MCE identifies the single worst-performing bin, representing the 'worst-case scenario' for your model's calibration. A high MCE reveals specific confidence ranges where the model is particularly unreliable.\" ) logger . info ( \"\" ) logger . info ( \"=\" * 80 ) elif output_format == 'json' : print ( json . dumps ( results , indent = 4 )) else : logger . warning ( f \"[Pipeline] Unknown output_format: ' { output_format } '. No console output printed.\" ) # The method still returns the dictionary for programmatic use return results def evaluate_fairness ( self , X_test : pd . DataFrame , y_test : pd . Series , sensitive_features : pd . Series , output_format : str = 'rich' ): \"\"\" Calculates and provides a detailed report on group fairness metrics. \"\"\" if not self . _is_fitted : raise RuntimeError ( \"You must call fit() on the pipeline before evaluating fairness.\" ) predictions = self . predict ( X_test ) y_test_encoded , predictions_encoded = self . _get_encoded_labels ( y_test , predictions ) spd = demographic_parity_difference ( y_true = y_test_encoded , y_pred = predictions_encoded , sensitive_features = sensitive_features ) eod = equal_opportunity_difference ( y_true = y_test_encoded , y_pred = predictions_encoded , sensitive_features = sensitive_features ) aod = equalized_odds_difference ( y_true = y_test_encoded , y_pred = predictions_encoded , sensitive_features = sensitive_features ) results = { \"statistical_parity_difference\" : spd , \"equal_opportunity_difference\" : eod , \"equalized_odds_difference\" : aod } if output_format == 'rich' : logger . info ( \" \\n \" + \"=\" * 80 ) logger . info ( \"[Pipeline] Running Detailed Fairness Evaluation\" ) logger . info ( \"=\" * 80 ) logger . info ( f \"[Pipeline] Fairness is evaluated with respect to the ' { sensitive_features . name } ' attribute.\" ) logger . info ( \"[Pipeline] These metrics measure disparities in model behavior between different groups. For these difference-based metrics, a value of 0 indicates perfect fairness. \\n \" ) logger . info ( \"[Pipeline] Statistical Parity Difference (Selection Rate)\" ) logger . info ( \"[Pipeline] Measures the difference in the rate of positive predictions (e.g., 'Churn') between groups.\" ) logger . info ( f \"[Pipeline] Your Score: { spd : .4f } \" ) logger . info ( f \"[Pipeline] Interpretation: Your score means there is a { abs ( spd * 100 ) : .2f } % difference in the selection rate between groups. Values close to 0 are ideal. Disparities above 10-20% are often considered significant. \\n \" ) logger . info ( \"[Pipeline] Equal Opportunity Difference (True Positive Rate)\" ) logger . info ( \"[Pipeline] Measures the difference in the true positive rate\u2014the rate at which the model correctly identifies positive outcomes\u2014between groups.\" ) logger . info ( f \"[Pipeline] Your Score: { eod : .4f } \" ) logger . info ( f \"[Pipeline] Interpretation: For cases that are genuinely positive, your score means the model's ability to correctly identify them differs by { abs ( eod * 100 ) : .2f } % between groups. High values indicate the model's benefits are not being applied equally. \\n \" ) logger . info ( \"[Pipeline] Equalized Odds Difference (Overall Error Rate)\" ) logger . info ( \"[Pipeline] Measures the larger of the true positive rate difference and the false positive rate difference between groups.\" ) logger . info ( f \"[Pipeline] Your Score: { aod : .4f } \" ) logger . info ( f \"[Pipeline] Interpretation: This score represents the 'worst-case' error rate disparity. A score of { abs ( aod * 100 ) : .2f } % indicates the largest gap in performance. If this value is close to the Equal Opportunity Difference, the main issue is with true positives. \\n \" ) logger . info ( \"=\" * 80 ) elif output_format == 'json' : print ( json . dumps ( results , indent = 4 )) else : logger . warning ( f \"[Pipeline] Unknown output_format: ' { output_format } '. No console output printed.\" ) return results def _get_encoded_labels ( self , y_true , y_pred ): \"\"\"Helper to consistently encode true and predicted labels.\"\"\" y_true_encoded = None y_pred_encoded = None # Find the correct LabelEncoder le = None if hasattr ( self . processor , 'custom_preprocessor_' ) and hasattr ( self . processor . custom_preprocessor_ , 'label_encoder_' ): le = self . processor . custom_preprocessor_ . label_encoder_ elif isinstance ( self . model , ( TabICLClassifier , OrionMSPClassifier , OrionBixClassifier , TabPFNClassifier )): # Fit a temporary encoder on the training labels seen during .fit() le = LabelEncoder () . fit ( self . y_train_processed_ if self . y_train_processed_ is not None else y_true ) else : raise RuntimeError ( \"Could not find a fitted label encoder to evaluate metrics.\" ) y_true_encoded = le . transform ( y_true ) # Handle cases where y_pred might be different (e.g., raw y_test for fairness) if y_pred is not None : y_pred_encoded = le . transform ( y_pred ) return y_true_encoded , y_pred_encoded def baseline ( self , X_train : pd . DataFrame , y_train : pd . Series , X_test : pd . DataFrame , y_test : pd . Series , models : list | str | None = None , time_limit : int = 60 ): \"\"\" Trains and evaluates baseline models using AutoGluon on the provided train/test split. Now returns per-model F1 scores along with validation scores and training time. \"\"\" try : from autogluon.tabular import TabularPredictor from sklearn.metrics import accuracy_score , f1_score except ImportError : raise ImportError ( \"AutoGluon is not installed. Install it with: pip install autogluon\" ) logger . info ( \"Preparing data for AutoGluon...\" ) # Prepare data with target column X_train_with_label = X_train . copy () X_train_with_label [ '__target__' ] = y_train . values if hasattr ( y_train , 'values' ) else y_train X_test_with_label = X_test . copy () X_test_with_label [ '__target__' ] = y_test . values if hasattr ( y_test , 'values' ) else y_test # Configure model hyperparameters hyperparameters = None if models is not None : models_to_run = [ models ] if isinstance ( models , str ) else models model_map = { 'xgboost' : 'XGB' , 'catboost' : 'CAT' , 'randomforest' : 'RF' , 'lightgbm' : 'GBM' , 'extratrees' : 'XT' , 'knn' : 'KNN' , 'linear' : 'LR' , 'neuralnet' : 'NN_TORCH' } ag_models = [ model_map . get ( m . lower (), m . upper ()) for m in models_to_run ] hyperparameters = { model : {} for model in ag_models } logger . info ( f \"Training AutoGluon predictor with time_limit= { time_limit } s...\" ) start_time = time . time () predictor = TabularPredictor ( label = '__target__' , eval_metric = 'accuracy' , verbosity = 2 ) . fit ( train_data = X_train_with_label , time_limit = time_limit , hyperparameters = hyperparameters , presets = 'medium_quality' ) total_train_time = time . time () - start_time logger . info ( \"Generating test predictions using best model ensemble...\" ) predictions = predictor . predict ( X_test ) overall_accuracy = accuracy_score ( y_test , predictions ) overall_f1 = f1_score ( y_test , predictions , average = 'weighted' ) leaderboard = predictor . leaderboard ( X_test_with_label , silent = True ) baseline_results = [] logger . info ( \"Calculating per-model F1 scores...\" ) for _ , row in leaderboard . iterrows (): model_name = row [ 'model' ] # Individual model predictions model_pred = predictor . predict ( X_test , model = model_name ) # Model-specific F1 score model_f1 = f1_score ( y_test , model_pred , average = 'weighted' ) baseline_results . append ({ \"Model\" : model_name , \"Validation Score\" : row [ 'score_val' ], \"F1 Score\" : model_f1 , \"Training Time\" : row [ 'fit_time' ] }) logger . info ( \" \\n AutoGluon Baseline Evaluation Report\" ) logger . info ( f \"Overall Accuracy: { overall_accuracy : .4f } \" ) logger . info ( f \"Overall Weighted F1-Score: { overall_f1 : .4f } \" ) logger . info ( f \"Total Training Time: { total_train_time : .2f } s \\n \" ) header = f \" { 'Model' : <30 } { 'Val Score' : <15 } { 'F1 Score' : <15 } { 'Train Time (s)' : <15 } \" logger . info ( header ) for result in baseline_results : logger . info ( f \" { result [ 'Model' ] : <30 } { result [ 'Validation Score' ] : <15.4f } \" f \" { result [ 'F1 Score' ] : <15.4f } { result [ 'Training Time' ] : <15.2f } \" ) logger . info ( \"=\" * 80 ) return { \"overall_accuracy\" : overall_accuracy , \"overall_f1\" : overall_f1 , \"total_training_time\" : total_train_time , \"individual_models\" : baseline_results , \"predictor\" : predictor , \"leaderboard\" : leaderboard } def evaluate_checkpoints ( self , X_test , y_test , checkpoint_dir , epochs , map_location : str | None = None ): results = {} for ep in epochs : ckpt_name = f \" { type ( self . model ) . __name__ } _epoch { ep } .pt\" ckpt_path = os . path . join ( checkpoint_dir , ckpt_name ) if not os . path . exists ( ckpt_path ): logger . warning ( f \" - Missing checkpoint for epoch { ep } , skipping\" ) continue logger . info ( f \" \\n \ud83d\udd01 Evaluating checkpoint at epoch { ep } \" ) self . model = self . tuner . load_checkpoint ( self . model , ckpt_path , map_location or 'cpu' ) for name , param in self . model . model . named_parameters (): logger . info ( f \" { name } mean: { torch . mean ( param ) . item () : .6f } \" ) break # then evaluate normally metrics = self . evaluate ( X_test , y_test ) results [ ep ] = metrics return results __del__ () \u00b6 Cleanup method to properly shut down resources when pipeline is destroyed. Source code in tabtune/TabularPipeline/pipeline.py 186 187 188 189 190 def __del__ ( self ): \"\"\"Cleanup method to properly shut down resources when pipeline is destroyed.\"\"\" # ContextTab ZMQ server cleanup is handled automatically by atexit.register() # in the start_embedding_server function, so no manual cleanup needed pass baseline ( X_train , y_train , X_test , y_test , models = None , time_limit = 60 ) \u00b6 Trains and evaluates baseline models using AutoGluon on the provided train/test split. Now returns per-model F1 scores along with validation scores and training time. Source code in tabtune/TabularPipeline/pipeline.py 1010 1011 1012 1013 1014 1015 1016 1017 1018 1019 1020 1021 1022 1023 1024 1025 1026 1027 1028 1029 1030 1031 1032 1033 1034 1035 1036 1037 1038 1039 1040 1041 1042 1043 1044 1045 1046 1047 1048 1049 1050 1051 1052 1053 1054 1055 1056 1057 1058 1059 1060 1061 1062 1063 1064 1065 1066 1067 1068 1069 1070 1071 1072 1073 1074 1075 1076 1077 1078 1079 1080 1081 1082 1083 1084 1085 1086 1087 1088 1089 1090 1091 1092 1093 1094 1095 1096 1097 1098 1099 1100 1101 1102 1103 1104 1105 1106 1107 1108 1109 1110 def baseline ( self , X_train : pd . DataFrame , y_train : pd . Series , X_test : pd . DataFrame , y_test : pd . Series , models : list | str | None = None , time_limit : int = 60 ): \"\"\" Trains and evaluates baseline models using AutoGluon on the provided train/test split. Now returns per-model F1 scores along with validation scores and training time. \"\"\" try : from autogluon.tabular import TabularPredictor from sklearn.metrics import accuracy_score , f1_score except ImportError : raise ImportError ( \"AutoGluon is not installed. Install it with: pip install autogluon\" ) logger . info ( \"Preparing data for AutoGluon...\" ) # Prepare data with target column X_train_with_label = X_train . copy () X_train_with_label [ '__target__' ] = y_train . values if hasattr ( y_train , 'values' ) else y_train X_test_with_label = X_test . copy () X_test_with_label [ '__target__' ] = y_test . values if hasattr ( y_test , 'values' ) else y_test # Configure model hyperparameters hyperparameters = None if models is not None : models_to_run = [ models ] if isinstance ( models , str ) else models model_map = { 'xgboost' : 'XGB' , 'catboost' : 'CAT' , 'randomforest' : 'RF' , 'lightgbm' : 'GBM' , 'extratrees' : 'XT' , 'knn' : 'KNN' , 'linear' : 'LR' , 'neuralnet' : 'NN_TORCH' } ag_models = [ model_map . get ( m . lower (), m . upper ()) for m in models_to_run ] hyperparameters = { model : {} for model in ag_models } logger . info ( f \"Training AutoGluon predictor with time_limit= { time_limit } s...\" ) start_time = time . time () predictor = TabularPredictor ( label = '__target__' , eval_metric = 'accuracy' , verbosity = 2 ) . fit ( train_data = X_train_with_label , time_limit = time_limit , hyperparameters = hyperparameters , presets = 'medium_quality' ) total_train_time = time . time () - start_time logger . info ( \"Generating test predictions using best model ensemble...\" ) predictions = predictor . predict ( X_test ) overall_accuracy = accuracy_score ( y_test , predictions ) overall_f1 = f1_score ( y_test , predictions , average = 'weighted' ) leaderboard = predictor . leaderboard ( X_test_with_label , silent = True ) baseline_results = [] logger . info ( \"Calculating per-model F1 scores...\" ) for _ , row in leaderboard . iterrows (): model_name = row [ 'model' ] # Individual model predictions model_pred = predictor . predict ( X_test , model = model_name ) # Model-specific F1 score model_f1 = f1_score ( y_test , model_pred , average = 'weighted' ) baseline_results . append ({ \"Model\" : model_name , \"Validation Score\" : row [ 'score_val' ], \"F1 Score\" : model_f1 , \"Training Time\" : row [ 'fit_time' ] }) logger . info ( \" \\n AutoGluon Baseline Evaluation Report\" ) logger . info ( f \"Overall Accuracy: { overall_accuracy : .4f } \" ) logger . info ( f \"Overall Weighted F1-Score: { overall_f1 : .4f } \" ) logger . info ( f \"Total Training Time: { total_train_time : .2f } s \\n \" ) header = f \" { 'Model' : <30 } { 'Val Score' : <15 } { 'F1 Score' : <15 } { 'Train Time (s)' : <15 } \" logger . info ( header ) for result in baseline_results : logger . info ( f \" { result [ 'Model' ] : <30 } { result [ 'Validation Score' ] : <15.4f } \" f \" { result [ 'F1 Score' ] : <15.4f } { result [ 'Training Time' ] : <15.2f } \" ) logger . info ( \"=\" * 80 ) return { \"overall_accuracy\" : overall_accuracy , \"overall_f1\" : overall_f1 , \"total_training_time\" : total_train_time , \"individual_models\" : baseline_results , \"predictor\" : predictor , \"leaderboard\" : leaderboard } evaluate ( X_test , y_test , output_format = 'rich' ) \u00b6 Makes predictions on the test set and prints a report with Accuracy, F1 Score, and ROC AUC Score. Source code in tabtune/TabularPipeline/pipeline.py 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749 def evaluate ( self , X_test : pd . DataFrame , y_test : pd . Series , output_format : str = 'rich' ): \"\"\" Makes predictions on the test set and prints a report with Accuracy, F1 Score, and ROC AUC Score. \"\"\" if not self . _is_fitted : raise RuntimeError ( \"You must call fit() on the pipeline before evaluating.\" ) logger . info ( \" \\n \" + \"=\" * 60 ) logger . info ( \"[Pipeline] Running Evaluation\" ) predictions = self . predict ( X_test ) if self . task_type == 'classification' : probabilities = self . predict_proba ( X_test ) y_test_encoded = None # Case 1: Custom preprocessor has a label encoder (Mitra, TabICL, APT, OrionMSP, OrionBix) if hasattr ( self . processor , 'custom_preprocessor_' ) and hasattr ( self . processor . custom_preprocessor_ , 'label_encoder_' ): y_test_encoded = self . processor . custom_preprocessor_ . label_encoder_ . transform ( y_test ) elif isinstance ( self . model , ( TabICLClassifier , OrionMSPClassifier , OrionBixClassifier )): y_test_encoded = self . model . y_encoder_ . transform ( y_test ) elif isinstance ( self . model , TabPFNClassifier ): le = LabelEncoder () le . classes_ = self . model . classes_ # Use the classes the model learned during .fit() y_test_encoded = le . transform ( y_test ) # Case 3: Standard pipeline with a main label encoder elif isinstance ( self . model , ConTextTabClassifier ): if hasattr ( self . processor_ , 'label_encoder_' ): if y_test . dtype == object or y_test . dtype . kind in { 'U' , 'S' }: y_test = self . processor_ . label_encoder_ . transform ( y_test ) elif hasattr ( self . processor , 'label_encoder_' ) and self . processor . label_encoder_ is not None : y_test_encoded = self . processor . label_encoder_ . transform ( y_test ) if y_test_encoded is None : raise RuntimeError ( \"Could not find a fitted label encoder to evaluate metrics.\" ) accuracy = accuracy_score ( y_test , predictions ) f1 = f1_score ( y_test , predictions , average = 'weighted' ) mcc = matthews_corrcoef ( y_test , predictions ) precision = precision_score ( y_test , predictions , average = 'weighted' ) recall = recall_score ( y_test , predictions , average = 'weighted' ) # Guard: AUC is undefined if the test fold has < 2 classes unique_test = np . unique ( y_test_encoded ) if len ( unique_test ) < 2 : auc = float ( \"nan\" ) else : # Align probability columns to the SAME label order used by y_test_encoded # Choose the same encoder you used above when computing y_test_encoded if hasattr ( self . processor , 'custom_preprocessor_' ) and hasattr ( self . processor . custom_preprocessor_ , 'label_encoder_' ): le = self . processor . custom_preprocessor_ . label_encoder_ elif isinstance ( self . model , ( TabICLClassifier , OrionBixClassifier , OrionMSPClassifier )): le = self . model . y_encoder_ elif isinstance ( self . model , TabPFNClassifier ): le = LabelEncoder (); le . classes_ = self . model . classes_ elif hasattr ( self . processor , 'label_encoder_' ) and self . processor . label_encoder_ is not None : le = self . processor . label_encoder_ else : raise RuntimeError ( \"Could not find a fitted label encoder to align probabilities.\" ) probs_aligned = self . _align_proba_to_encoder ( probabilities , le ) # Binary vs multiclass handling with explicit labels to match encoded y K = len ( le . classes_ ) if K == 2 : # probs_aligned has 2 columns by construction: [:, 1] is positive class auc = roc_auc_score ( y_test_encoded , probs_aligned [:, 1 ]) else : auc = roc_auc_score ( y_test_encoded , probs_aligned , labels = list ( range ( K )), # encoded labels are 0..K-1 multi_class = \"ovr\" , average = \"weighted\" , ) results = { \"accuracy\" : accuracy , \"roc_auc_score\" : auc , \"f1_score\" : f1 , \"precision\" : precision , \"recall\" : recall , \"mcc\" : mcc } if output_format == 'json' : print ( json . dumps ( results , indent = 4 )) elif output_format == 'rich' : logger . info ( \" \\n \" + \"=\" * 60 ) logger . info ( \"[Pipeline] Running Evaluation\" ) logger . info ( \" \\n [Pipeline] Evaluation Report\" ) logger . info ( f \"[Pipeline] Accuracy: { accuracy : .4f } \" ) logger . info ( f \"[Pipeline] Weighted F1-Score: { f1 : .4f } \" ) logger . info ( f \"[Pipeline] Weighted Precision: { precision : .4f } \" ) logger . info ( f \"[Pipeline] Weighted Recall: { recall : .4f } \" ) logger . info ( f \"[Pipeline] MCC: { mcc : .4f } \" ) logger . info ( f \"[Pipeline] ROC AUC Score: { auc : .4f } \" ) logger . info ( \" \\n [Pipeline] Classification Report\" ) logger . info ( classification_report ( y_test , predictions , zero_division = 0 )) logger . info ( \"=\" * 60 ) else : logger . warning ( f \"[Pipeline] Unknown output_format: ' { output_format } '. No output printed.\" ) return results evaluate_calibration ( X_test , y_test , n_bins = 15 , output_format = 'rich' ) \u00b6 Calculates and provides a detailed report on model calibration metrics. This version supports both binary and multiclass classification. Source code in tabtune/TabularPipeline/pipeline.py 803 804 805 806 807 808 809 810 811 812 813 814 815 816 817 818 819 820 821 822 823 824 825 826 827 828 829 830 831 832 833 834 835 836 837 838 839 840 841 842 843 844 845 846 847 848 849 850 851 852 853 854 855 856 857 858 859 860 861 862 863 864 865 866 867 868 869 870 871 872 873 874 875 876 877 878 879 880 881 882 883 884 885 886 887 888 889 890 891 892 893 894 895 896 897 898 899 900 901 902 903 904 905 906 907 908 909 910 911 912 913 914 915 916 917 918 919 920 921 922 923 924 925 926 927 928 929 def evaluate_calibration ( self , X_test : pd . DataFrame , y_test : pd . Series , n_bins : int = 15 , output_format : str = 'rich' ): \"\"\" Calculates and provides a detailed report on model calibration metrics. This version supports both binary and multiclass classification. \"\"\" if not self . _is_fitted : raise RuntimeError ( \"You must call fit() on the pipeline before evaluating calibration.\" ) # --- Metric Calculation (common for all formats) --- probabilities = self . predict_proba ( X_test ) # 1. Find the correct label encoder (same logic as in evaluate()) le = None if hasattr ( self . processor , 'custom_preprocessor_' ) and hasattr ( self . processor . custom_preprocessor_ , 'label_encoder_' ): le = self . processor . custom_preprocessor_ . label_encoder_ elif isinstance ( self . model , ( TabICLClassifier , OrionBixClassifier , OrionMSPClassifier )): # Use model's internal encoder if in inference mode if hasattr ( self . model , 'y_encoder_' ): le = self . model . y_encoder_ # Use processor's encoder if in finetune mode elif hasattr ( self . processor , 'custom_preprocessor_' ) and hasattr ( self . processor . custom_preprocessor_ , 'label_encoder_' ): le = self . processor . custom_preprocessor_ . label_encoder_ elif isinstance ( self . model , TabPFNClassifier ): if hasattr ( self . model , 'classes_' ): le = LabelEncoder () le . classes_ = self . model . classes_ elif hasattr ( self . processor , 'label_encoder_' ) and self . processor . label_encoder_ is not None : le = self . processor . label_encoder_ if le is None : raise RuntimeError ( \"Could not find a fitted label encoder to evaluate calibration.\" ) # 2. Encode y_test using the found encoder y_test_encoded = le . transform ( y_test ) # 3. Align probability columns to match the encoder's class order probs_aligned = self . _align_proba_to_encoder ( probabilities , le ) # 4. Calculate metrics using the aligned probabilities # brier_score_loss handles (n_samples, n_classes) for multiclass # when y_true is (n_samples,) with integer labels [0, K-1]. # Validate inputs before calculating Brier score if probs_aligned is None : logger . warning ( \"[Pipeline] Probabilities are None, skipping Brier score calculation\" ) brier_score = float ( 'nan' ) else : # Check for NaN or infinite values if np . any ( np . isnan ( probs_aligned )) or np . any ( np . isinf ( probs_aligned )): logger . warning ( \"[Pipeline] Probabilities contain NaN or infinite values, skipping Brier score calculation\" ) brier_score = float ( 'nan' ) else : # Validate that probabilities sum to 1.0 (within tolerance) prob_sums = np . sum ( probs_aligned , axis = 1 ) if not np . allclose ( prob_sums , 1.0 , rtol = 1e-6 ): logger . warning ( f \"[Pipeline] Probabilities don't sum to 1.0 (range: { prob_sums . min () : .6f } to { prob_sums . max () : .6f } )\" ) logger . warning ( \"[Pipeline] This may indicate model calibration issues\" ) # Validate that y_test_encoded contains valid class indices max_class_idx = len ( le . classes_ ) - 1 if np . any ( y_test_encoded < 0 ) or np . any ( y_test_encoded > max_class_idx ): logger . warning ( f \"[Pipeline] Invalid class indices in y_test_encoded (range: { y_test_encoded . min () } to { y_test_encoded . max () } )\" ) logger . warning ( f \"[Pipeline] Expected range: 0 to { max_class_idx } \" ) brier_score = float ( 'nan' ) else : try : brier_score = brier_score_loss ( y_test_encoded , probs_aligned ) except Exception as e : logger . error ( f \"[Pipeline] Error calculating Brier score: { e } \" ) brier_score = float ( 'nan' ) # _calculate_calibration_errors also works with (n, K) probability matrix if probs_aligned is None : logger . warning ( \"[Pipeline] Probabilities are None, skipping ECE and MCE calculation\" ) ece , mce = float ( 'nan' ), float ( 'nan' ) else : ece , mce = self . _calculate_calibration_errors ( y_test_encoded , probs_aligned , n_bins = n_bins ) results = { \"brier_score_loss\" : brier_score , \"expected_calibration_error\" : ece , \"maximum_calibration_error\" : mce } if output_format == 'rich' : logger . info ( \" \\n \" + \"=\" * 80 ) logger . info ( \"[Pipeline] Running Detailed Calibration Evaluation\" ) logger . info ( \"=\" * 80 ) logger . info ( \"[Pipeline] Calibration measures how well a model's predicted probabilities match the true likelihood of outcomes.\" ) logger . info ( \"[Pipeline] A well-calibrated model is trustworthy: if it predicts a 70% probability, it should be correct 70 % o f the time. \\n \" ) logger . info ( \"[Pipeline] Brier Score Loss\" ) logger . info ( \"[Pipeline] Measures the mean squared difference between predicted probabilities and actual outcomes.\" ) if np . isnan ( brier_score ): logger . info ( f \"[Pipeline] Your Score: NaN (calculation skipped due to validation issues)\" ) logger . info ( \"[Pipeline] Interpretation: Check warnings above for details on why Brier score could not be calculated.\" ) else : logger . info ( f \"[Pipeline] Your Score: { brier_score : .4f } \" ) logger . info ( \"[Pipeline] Interpretation: Scores range from 0.0 to 1.0, where lower is better. A score near 0.0 indicates excellent calibration.\" ) logger . info ( \"[Pipeline] Note: For multiclass problems, this is the average Brier score across all classes.\" ) logger . info ( \"[Pipeline] Note: For imbalanced datasets, consider class-specific Brier scores for better insights.\" ) logger . info ( \"\" ) logger . info ( \"[Pipeline] Expected & Maximum Calibration Error (ECE / MCE)\" ) logger . info ( \"[Pipeline] These metrics group predictions into bins by confidence (e.g., 80-90%) and measure the gap between the average confidence and the actual accuracy in each bin.\" ) if np . isnan ( ece ) or np . isnan ( mce ): logger . info ( f \"[Pipeline] Expected Calibration Error (ECE): NaN (calculation skipped due to validation issues)\" ) logger . info ( f \"[Pipeline] Maximum Calibration Error (MCE): NaN (calculation skipped due to validation issues)\" ) logger . info ( \"[Pipeline] Interpretation: Check warnings above for details on why ECE/MCE could not be calculated.\" ) else : logger . info ( f \"[Pipeline] Expected Calibration Error (ECE): { ece : .4f } \" ) logger . info ( f \"[Pipeline] Interpretation: ECE represents the average gap between confidence and accuracy across all bins. Your score indicates the model's confidence is off by an average of { ece * 100 : .2f } %. An ECE below 0.05 (5%) is generally considered good.\" ) logger . info ( f \"[Pipeline] Maximum Calibration Error (MCE): { mce : .4f } \" ) logger . info ( \"[Pipeline] Interpretation: MCE identifies the single worst-performing bin, representing the 'worst-case scenario' for your model's calibration. A high MCE reveals specific confidence ranges where the model is particularly unreliable.\" ) logger . info ( \"\" ) logger . info ( \"=\" * 80 ) elif output_format == 'json' : print ( json . dumps ( results , indent = 4 )) else : logger . warning ( f \"[Pipeline] Unknown output_format: ' { output_format } '. No console output printed.\" ) # The method still returns the dictionary for programmatic use return results evaluate_fairness ( X_test , y_test , sensitive_features , output_format = 'rich' ) \u00b6 Calculates and provides a detailed report on group fairness metrics. Source code in tabtune/TabularPipeline/pipeline.py 931 932 933 934 935 936 937 938 939 940 941 942 943 944 945 946 947 948 949 950 951 952 953 954 955 956 957 958 959 960 961 962 963 964 965 966 967 968 969 970 971 972 973 974 975 976 977 978 979 980 981 982 983 984 985 986 def evaluate_fairness ( self , X_test : pd . DataFrame , y_test : pd . Series , sensitive_features : pd . Series , output_format : str = 'rich' ): \"\"\" Calculates and provides a detailed report on group fairness metrics. \"\"\" if not self . _is_fitted : raise RuntimeError ( \"You must call fit() on the pipeline before evaluating fairness.\" ) predictions = self . predict ( X_test ) y_test_encoded , predictions_encoded = self . _get_encoded_labels ( y_test , predictions ) spd = demographic_parity_difference ( y_true = y_test_encoded , y_pred = predictions_encoded , sensitive_features = sensitive_features ) eod = equal_opportunity_difference ( y_true = y_test_encoded , y_pred = predictions_encoded , sensitive_features = sensitive_features ) aod = equalized_odds_difference ( y_true = y_test_encoded , y_pred = predictions_encoded , sensitive_features = sensitive_features ) results = { \"statistical_parity_difference\" : spd , \"equal_opportunity_difference\" : eod , \"equalized_odds_difference\" : aod } if output_format == 'rich' : logger . info ( \" \\n \" + \"=\" * 80 ) logger . info ( \"[Pipeline] Running Detailed Fairness Evaluation\" ) logger . info ( \"=\" * 80 ) logger . info ( f \"[Pipeline] Fairness is evaluated with respect to the ' { sensitive_features . name } ' attribute.\" ) logger . info ( \"[Pipeline] These metrics measure disparities in model behavior between different groups. For these difference-based metrics, a value of 0 indicates perfect fairness. \\n \" ) logger . info ( \"[Pipeline] Statistical Parity Difference (Selection Rate)\" ) logger . info ( \"[Pipeline] Measures the difference in the rate of positive predictions (e.g., 'Churn') between groups.\" ) logger . info ( f \"[Pipeline] Your Score: { spd : .4f } \" ) logger . info ( f \"[Pipeline] Interpretation: Your score means there is a { abs ( spd * 100 ) : .2f } % difference in the selection rate between groups. Values close to 0 are ideal. Disparities above 10-20% are often considered significant. \\n \" ) logger . info ( \"[Pipeline] Equal Opportunity Difference (True Positive Rate)\" ) logger . info ( \"[Pipeline] Measures the difference in the true positive rate\u2014the rate at which the model correctly identifies positive outcomes\u2014between groups.\" ) logger . info ( f \"[Pipeline] Your Score: { eod : .4f } \" ) logger . info ( f \"[Pipeline] Interpretation: For cases that are genuinely positive, your score means the model's ability to correctly identify them differs by { abs ( eod * 100 ) : .2f } % between groups. High values indicate the model's benefits are not being applied equally. \\n \" ) logger . info ( \"[Pipeline] Equalized Odds Difference (Overall Error Rate)\" ) logger . info ( \"[Pipeline] Measures the larger of the true positive rate difference and the false positive rate difference between groups.\" ) logger . info ( f \"[Pipeline] Your Score: { aod : .4f } \" ) logger . info ( f \"[Pipeline] Interpretation: This score represents the 'worst-case' error rate disparity. A score of { abs ( aod * 100 ) : .2f } % indicates the largest gap in performance. If this value is close to the Equal Opportunity Difference, the main issue is with true positives. \\n \" ) logger . info ( \"=\" * 80 ) elif output_format == 'json' : print ( json . dumps ( results , indent = 4 )) else : logger . warning ( f \"[Pipeline] Unknown output_format: ' { output_format } '. No console output printed.\" ) return results predict_proba ( X ) \u00b6 Predicts class probabilities for the input data. Required for calculating AUC score. Source code in tabtune/TabularPipeline/pipeline.py 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 def predict_proba ( self , X : pd . DataFrame ) -> np . ndarray : \"\"\" Predicts class probabilities for the input data. Required for calculating AUC score. \"\"\" if not self . _is_fitted : raise RuntimeError ( \"You must call fit() on the pipeline before calling predict_proba().\" ) logger . info ( \"[Pipeline] Starting probability prediction\" ) if hasattr ( self . model , 'model' ) and isinstance ( self . model . model , torch . nn . Module ): self . model . model . eval () elif hasattr ( self . model , 'model_' ) and isinstance ( self . model . model_ , torch . nn . Module ): self . model . model_ . eval () if isinstance ( self . model , TabDPTClassifier ): logger . debug ( \"[Pipeline] Using TabDPT's internal predict_proba\" ) # Apply the same preprocessing as during fit() X_processed = self . processor . transform ( X ) # Use stored defaults from model initialization return self . model . ensemble_predict_proba ( X_processed ) elif isinstance ( self . model , TabPFNClassifier ): # Special handling for fine-tuned TabPFN to set inference context if self . tuning_strategy in [ 'finetune' , 'base-ft' , 'peft' ]: logger . debug ( \"[Pipeline] Setting TabPFN inference context for proba...\" ) self . model . fit ( self . X_train_processed_ , self . y_train_processed_ ) X_processed = self . processor . transform ( X ) return self . model . predict_proba ( X_processed ) if isinstance ( self . model , ( TabICLClassifier , OrionMSPClassifier , OrionBixClassifier , ConTextTabClassifier )): logger . debug ( \"[Pipeline] Using model's native predict_proba method\" ) X_processed = self . processor . transform ( X ) if isinstance ( self . model , ( ConTextTabClassifier )): return self . model . predict_proba ( X ) if isinstance ( self . model , ( TabICLClassifier , OrionMSPClassifier , OrionBixClassifier )): if self . tuning_strategy == 'inference' : return self . model . predict_proba ( X ) else : label_encoder = self . processor . custom_preprocessor_ . label_encoder_ known_class = label_encoder . classes_ [ 0 ] y_dummy = pd . Series ([ known_class ] * len ( X )) X_query , _ = self . processor . transform ( X , y_dummy ) # Convert to DataFrame to maintain feature names for sklearn compatibility if not isinstance ( X_query , pd . DataFrame ): # Prefer processor feature names if available; else fall back to input X cols = None if hasattr ( self . processor , \"feature_names_\" ) and self . processor . feature_names_ is not None : cols = list ( self . processor . feature_names_ ) elif hasattr ( X , \"columns\" ): cols = list ( X . columns ) # Avoid shape/columns mismatch if cols is not None and hasattr ( X_query , \"shape\" ) and X_query . shape [ 1 ] != len ( cols ): cols = None X_query = pd . DataFrame ( X_query , columns = cols ) return self . model . predict_proba ( X_query ) return self . model . predict_proba ( X_processed ) label_encoder = self . processor . custom_preprocessor_ . label_encoder_ known_class = label_encoder . classes_ [ 0 ] y_dummy = pd . Series ([ known_class ] * len ( X )) X_query , _ = self . processor . transform ( X , y_dummy ) X_support = self . X_train_processed_ y_support = self . y_train_processed_ device = next ( self . model . parameters ()) . device X_support_t = torch . tensor ( X_support , dtype = torch . float32 ) . unsqueeze ( 0 ) . to ( device ) y_support_t = torch . tensor ( y_support , dtype = torch . long ) . unsqueeze ( 0 ) . to ( device ) X_query_t = torch . tensor ( X_query , dtype = torch . float32 ) . unsqueeze ( 0 ) . to ( device ) self . model . eval () with torch . no_grad (): if isinstance ( self . model , Tab2D ): logger . debug ( \"[Pipeline] Generating probabilities for Mitra (Tab2D)\" ) b , f = X_support_t . shape [ 0 ], X_support_t . shape [ 2 ] padding_features = torch . zeros ( b , f , dtype = torch . bool , device = device ) padding_obs_support = torch . zeros_like ( y_support_t , dtype = torch . bool , device = device ) padding_obs_query = torch . zeros ( b , X_query_t . shape [ 1 ], dtype = torch . bool , device = device ) logits = self . model ( x_support = X_support_t , y_support = y_support_t , x_query = X_query_t , padding_features = padding_features , padding_obs_support = padding_obs_support , padding_obs_query__ = padding_obs_query ) probabilities = torch . softmax ( logits . squeeze ( 0 ), dim =- 1 ) . cpu () . numpy () else : if self . model_name == 'Mitra' : # Not implemented for Mitra raise NotImplementedError ( \"predict_proba is not implemented for Mitra (Tab2D)\" ) raise NotImplementedError ( f \"predict_proba is not implemented for model type { type ( self . model ) . __name__ } \" ) logger . info ( \"[Pipeline] Probability prediction complete\" ) return probabilities show_processing_summary () \u00b6 Retrieves and logs the data processing summary from the DataProcessor. Source code in tabtune/TabularPipeline/pipeline.py 765 766 767 768 769 770 771 772 773 774 775 def show_processing_summary ( self ): \"\"\" Retrieves and logs the data processing summary from the DataProcessor. \"\"\" logger . info ( \" \\n \" + \"=\" * 60 ) summary = self . processor . get_processing_summary () # Log the multi-line summary as a single message summary_lines = summary . split ( ' \\n ' ) for line in summary_lines : logger . info ( line ) Overview \u00b6 TabularPipeline provides a scikit-learn-compatible interface for training and using tabular foundation models. It coordinates data preprocessing, model initialization, training, and inference. Constructor \u00b6 TabularPipeline.__init__() \u00b6 TabularPipeline ( model_name : str , task_type : str = 'classification' , tuning_strategy : str = 'inference' , tuning_params : dict | None = None , processor_params : dict | None = None , model_params : dict | None = None , model_checkpoint_path : str | None = None , finetune_mode : str = 'meta-learning' ) Parameters \u00b6 model_name (str, required) - Name of the model to use. - Supported values: 'TabPFN' , 'TabICL' , 'OrionMSP' , 'OrionBix' , 'TabDPT' , 'Mitra' , 'ContextTab' - Example: model_name=\"TabICL\" task_type (str, default: 'classification' ) - Type of machine learning task. - Currently supported: 'classification' - Planned: 'regression' - Example: task_type=\"classification\" tuning_strategy (str, default: 'inference' ) - Training/fine-tuning strategy to use. - Options: - 'inference' : Zero-shot predictions (no training) - 'base-ft' : Full fine-tuning of all parameters - 'peft' : Parameter-efficient fine-tuning with LoRA adapters - Example: tuning_strategy=\"peft\" tuning_params (dict, optional) - Hyperparameters for training/inference. - Common parameters: - device (str): 'cuda' or 'cpu' (default: auto-detected) - epochs (int): Number of training epochs - learning_rate (float): Learning rate for optimizer - batch_size (int): Batch size for training - peft_config (dict): LoRA configuration for PEFT strategy - support_size (int): Context size for episodic training - query_size (int): Query size for episodic training - Example: tuning_params = { \"device\" : \"cuda\" , \"epochs\" : 5 , \"learning_rate\" : 2e-5 , \"batch_size\" : 8 } processor_params (dict, optional) - Parameters for data preprocessing. - Common parameters: - imputation_strategy (str): 'mean' , 'median' , 'mode' , 'knn' - scaling_strategy (str): 'standard' , 'minmax' , 'robust' - categorical_encoding (str): Encoding method (auto-selected for model-specific) - resampling_strategy (str): 'smote' , 'random_oversample' , etc. - Example: processor_params = { \"imputation_strategy\" : \"median\" , \"scaling_strategy\" : \"standard\" } model_params (dict, optional) - Direct parameters passed to the model constructor. - Model-specific (see individual model documentation). - Example for TabICL: model_params = { \"n_estimators\" : 16 , \"softmax_temperature\" : 0.9 } model_checkpoint_path (str, optional) - Path to a pre-trained model checkpoint ( .pt file). - If provided, loads weights from checkpoint instead of default pre-trained weights. - Example: model_checkpoint_path=\"./checkpoints/tabicl_epoch5.pt\" finetune_mode (str, default: 'meta-learning' ) - Fine-tuning mode for models that support it. - Options: - 'meta-learning' : Episodic meta-learning (default) - 'sft' : Standard supervised fine-tuning - Example: finetune_mode=\"sft\" Returns \u00b6 Returns a TabularPipeline instance (not yet fitted). Core Methods \u00b6 .fit(X, y) \u00b6 Train the pipeline on training data. pipeline . fit ( X_train : pd . DataFrame , y_train : pd . Series ) -> TabularPipeline Parameters \u00b6 X (pd.DataFrame): Training features y (pd.Series): Training labels Returns \u00b6 Returns self (allows method chaining). What it does \u00b6 Fits the DataProcessor on training data (learns preprocessing transformations) Applies preprocessing to training data Initializes the model (if late initialization required) Trains the model using TuningManager (if strategy != 'inference' ) Example \u00b6 pipeline = TabularPipeline ( model_name = \"TabICL\" , tuning_strategy = \"base-ft\" ) pipeline . fit ( X_train , y_train ) .predict(X) \u00b6 Generate predictions on new data. predictions = pipeline . predict ( X_test : pd . DataFrame ) -> np . ndarray Parameters \u00b6 X (pd.DataFrame): Features for prediction Returns \u00b6 predictions (np.ndarray): Predicted class labels (shape: (n_samples,) ) Notes \u00b6 Automatically applies learned preprocessing Converts class indices back to original label format Must call .fit() before .predict() Example \u00b6 predictions = pipeline . predict ( X_test ) print ( f \"Predictions shape: { predictions . shape } \" ) print ( f \"Unique classes: { np . unique ( predictions ) } \" ) .predict_proba(X) \u00b6 Get probability predictions for classification. probabilities = pipeline . predict_proba ( X_test : pd . DataFrame ) -> np . ndarray Parameters \u00b6 X (pd.DataFrame): Features for prediction Returns \u00b6 probabilities (np.ndarray): Class probabilities (shape: (n_samples, n_classes) ) Notes \u00b6 Each row sums to 1.0 Column order matches label encoder classes Required for ROC AUC calculation Example \u00b6 probabilities = pipeline . predict_proba ( X_test ) print ( f \"Probabilities shape: { probabilities . shape } \" ) print ( f \"Row sums: { probabilities . sum ( axis = 1 ) } \" ) # Should be ~1.0 .evaluate(X, y, output_format='rich') \u00b6 Evaluate model performance on test data. metrics = pipeline . evaluate ( X_test : pd . DataFrame , y_test : pd . Series , output_format : str = 'rich' ) -> dict Parameters \u00b6 X (pd.DataFrame): Test features y (pd.Series): True labels output_format (str): 'rich' (formatted console output) or 'json' (dict only) Returns \u00b6 metrics (dict): Dictionary with evaluation metrics: accuracy (float): Overall accuracy roc_auc_score (float): ROC AUC (binary/multi-class) f1_score (float): Weighted F1 score precision (float): Weighted precision recall (float): Weighted recall mcc (float): Matthews Correlation Coefficient Example \u00b6 metrics = pipeline . evaluate ( X_test , y_test ) print ( f \"Accuracy: { metrics [ 'accuracy' ] : .4f } \" ) print ( f \"ROC AUC: { metrics [ 'roc_auc_score' ] : .4f } \" ) .save(file_path) \u00b6 Save the entire pipeline to disk. pipeline . save ( file_path : str ) -> None Parameters \u00b6 file_path (str): Path to save pipeline (typically .joblib extension) What it saves \u00b6 DataProcessor state (preprocessing transformations) Model weights and state Configuration (model_name, strategy, params) Label encoders Notes \u00b6 Must call .fit() before saving Uses joblib for serialization Large files (includes model weights) Example \u00b6 pipeline . fit ( X_train , y_train ) pipeline . save ( \"my_pipeline.joblib\" ) .load(file_path) (classmethod) \u00b6 Load a saved pipeline from disk. loaded_pipeline = TabularPipeline . load ( file_path : str ) -> TabularPipeline Parameters \u00b6 file_path (str): Path to saved pipeline file Returns \u00b6 TabularPipeline : Loaded pipeline instance (already fitted) Example \u00b6 loaded_pipeline = TabularPipeline . load ( \"my_pipeline.joblib\" ) predictions = loaded_pipeline . predict ( X_new ) Additional Methods \u00b6 .evaluate_calibration(X, y, n_bins=15, output_format='rich') \u00b6 Evaluate model calibration (how well probabilities match actual outcomes). calibration_metrics = pipeline . evaluate_calibration ( X_test : pd . DataFrame , y_test : pd . Series , n_bins : int = 15 , output_format : str = 'rich' ) -> dict Returns \u00b6 dict : Contains: brier_score_loss (float): Mean squared error of probabilities expected_calibration_error (float): Average calibration error maximum_calibration_error (float): Worst-case calibration error .evaluate_fairness(X, y, sensitive_features, output_format='rich') \u00b6 Evaluate group fairness metrics. fairness_metrics = pipeline . evaluate_fairness ( X_test : pd . DataFrame , y_test : pd . Series , sensitive_features : pd . Series , output_format : str = 'rich' ) -> dict Returns \u00b6 dict : Contains: statistical_parity_difference (float): Selection rate disparity equal_opportunity_difference (float): True positive rate disparity equalized_odds_difference (float): Overall error rate disparity .show_processing_summary() \u00b6 Display a summary of data preprocessing steps applied. pipeline . show_processing_summary () -> None Example Output \u00b6 Data Processing Summary: - Imputation: mean (numerical), mode (categorical) - Scaling: standard - Encoding: tabicl_special - Features: 50 numerical, 10 categorical .baseline(X_train, y_train, X_test, y_test, models=None, time_limit=60) \u00b6 Compare TabTune models against AutoGluon baselines. baseline_results = pipeline . baseline ( X_train : pd . DataFrame , y_train : pd . Series , X_test : pd . DataFrame , y_test : pd . Series , models : list | str | None = None , time_limit : int = 60 ) -> dict Returns \u00b6 dict : Contains AutoGluon baseline results and leaderboard Usage Patterns \u00b6 Pattern 1: Quick Inference Baseline \u00b6 from tabtune import TabularPipeline pipeline = TabularPipeline ( model_name = \"TabPFN\" , tuning_strategy = \"inference\" ) pipeline . fit ( X_train , y_train ) metrics = pipeline . evaluate ( X_test , y_test ) Pattern 2: Production Fine-Tuning \u00b6 pipeline = TabularPipeline ( model_name = \"OrionBix\" , tuning_strategy = \"base-ft\" , tuning_params = { \"device\" : \"cuda\" , \"epochs\" : 10 , \"learning_rate\" : 2e-5 , \"save_checkpoint_path\" : \"./checkpoints/model.pt\" } ) pipeline . fit ( X_train , y_train ) pipeline . save ( \"production_model.joblib\" ) Pattern 3: Memory-Efficient PEFT \u00b6 pipeline = TabularPipeline ( model_name = \"TabICL\" , tuning_strategy = \"peft\" , tuning_params = { \"device\" : \"cuda\" , \"epochs\" : 5 , \"learning_rate\" : 2e-4 , \"peft_config\" : { \"r\" : 8 , \"lora_alpha\" : 16 , \"lora_dropout\" : 0.05 } } ) pipeline . fit ( X_train , y_train ) Error Handling \u00b6 Common Exceptions \u00b6 RuntimeError : \"You must call fit() before predict()\" - Cause : Calling predict/evaluate before fitting - Solution : Call .fit() first ValueError : \"Model 'X' not supported\" - Cause : Invalid model name - Solution : Check supported models list RuntimeError : \"CUDA out of memory\" - Cause : Insufficient GPU memory - Solution : Use PEFT, reduce batch size, or use CPU See Also \u00b6 Pipeline Overview : Detailed usage guide Tuning Strategies : Strategy comparisons Model Selection : Choosing the right model Troubleshooting : Common issues and solutions","title":"TabularPipeline"},{"location":"api/pipeline/#api-tabularpipeline","text":"Complete API reference for the TabularPipeline class\u2014the main entry point for TabTune. The complete TabularPipeline with a robust constructor that explicitly handles parameters for each component and uses late initialization for complex models like ContextTab and Mitra. Source code in tabtune/TabularPipeline/pipeline.py 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755 756 757 758 759 760 761 762 763 764 765 766 767 768 769 770 771 772 773 774 775 776 777 778 779 780 781 782 783 784 785 786 787 788 789 790 791 792 793 794 795 796 797 798 799 800 801 802 803 804 805 806 807 808 809 810 811 812 813 814 815 816 817 818 819 820 821 822 823 824 825 826 827 828 829 830 831 832 833 834 835 836 837 838 839 840 841 842 843 844 845 846 847 848 849 850 851 852 853 854 855 856 857 858 859 860 861 862 863 864 865 866 867 868 869 870 871 872 873 874 875 876 877 878 879 880 881 882 883 884 885 886 887 888 889 890 891 892 893 894 895 896 897 898 899 900 901 902 903 904 905 906 907 908 909 910 911 912 913 914 915 916 917 918 919 920 921 922 923 924 925 926 927 928 929 930 931 932 933 934 935 936 937 938 939 940 941 942 943 944 945 946 947 948 949 950 951 952 953 954 955 956 957 958 959 960 961 962 963 964 965 966 967 968 969 970 971 972 973 974 975 976 977 978 979 980 981 982 983 984 985 986 987 988 989 990 991 992 993 994 995 996 997 998 999 1000 1001 1002 1003 1004 1005 1006 1007 1008 1009 1010 1011 1012 1013 1014 1015 1016 1017 1018 1019 1020 1021 1022 1023 1024 1025 1026 1027 1028 1029 1030 1031 1032 1033 1034 1035 1036 1037 1038 1039 1040 1041 1042 1043 1044 1045 1046 1047 1048 1049 1050 1051 1052 1053 1054 1055 1056 1057 1058 1059 1060 1061 1062 1063 1064 1065 1066 1067 1068 1069 1070 1071 1072 1073 1074 1075 1076 1077 1078 1079 1080 1081 1082 1083 1084 1085 1086 1087 1088 1089 1090 1091 1092 1093 1094 1095 1096 1097 1098 1099 1100 1101 1102 1103 1104 1105 1106 1107 1108 1109 1110 1111 1112 1113 1114 1115 1116 1117 1118 1119 1120 1121 1122 1123 1124 1125 1126 1127 1128 1129 1130 1131 1132 1133 1134 class TabularPipeline : \"\"\" The complete TabularPipeline with a robust constructor that explicitly handles parameters for each component and uses late initialization for complex models like ContextTab and Mitra. \"\"\" def __init__ ( self , model_name : str , task_type : str = 'classification' , tuning_strategy : str = 'inference' , tuning_params : dict | None = None , processor_params : dict | None = None , model_params : dict | None = None , model_checkpoint_path : str | None = None , finetune_mode : str = 'meta-learning' ): print ( \" \\n \" + \"=\" * 80 ) print ( r \"\"\" \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2557 \u2588\u2588\u2588\u2588\u2588\u2557 \u2588\u2588\u2588\u2588\u2588\u2588\u2557 \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2557\u2588\u2588\u2557 \u2588\u2588\u2557\u2588\u2588\u2588\u2557 \u2588\u2588\u2557\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2557 \u255a\u2550\u2550\u2588\u2588\u2554\u2550\u2550\u255d\u2588\u2588\u2554\u2550\u2550\u2588\u2588\u2557\u2588\u2588\u2554\u2550\u2550\u2588\u2588\u2557 \u255a\u2550\u2550\u2588\u2588\u2554\u2550\u2550\u255d\u2588\u2588\u2551 \u2588\u2588\u2551\u2588\u2588\u2588\u2588\u2557 \u2588\u2588\u2551\u2588\u2588\u2554\u2550\u2550\u2550\u2550\u255d \u2588\u2588\u2551 \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2551\u2588\u2588\u2588\u2588\u2588\u2588\u2554\u255d \u2588\u2588\u2551 \u2588\u2588\u2551 \u2588\u2588\u2551\u2588\u2588\u2554\u2588\u2588\u2557 \u2588\u2588\u2551\u2588\u2588\u2588\u2588\u2588\u2557 \u2588\u2588\u2551 \u2588\u2588\u2554\u2550\u2550\u2588\u2588\u2551\u2588\u2588\u2554\u2550\u2550\u2588\u2588\u2557 \u2588\u2588\u2551 \u2588\u2588\u2551 \u2588\u2588\u2551\u2588\u2588\u2551\u255a\u2588\u2588\u2557\u2588\u2588\u2551\u2588\u2588\u2554\u2550\u2550\u255d \u2588\u2588\u2551 \u2588\u2588\u2551 \u2588\u2588\u2551\u2588\u2588\u2588\u2588\u2588\u2588\u2554\u255d \u2588\u2588\u2551 \u255a\u2588\u2588\u2588\u2588\u2588\u2588\u2554\u255d\u2588\u2588\u2551 \u255a\u2588\u2588\u2588\u2588\u2551\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2557 \u255a\u2550\u255d \u255a\u2550\u255d \u255a\u2550\u255d\u255a\u2550\u2550\u2550\u2550\u2550\u255d \u255a\u2550\u255d \u255a\u2550\u2550\u2550\u2550\u2550\u255d \u255a\u2550\u255d \u255a\u2550\u2550\u2550\u255d\u255a\u2550\u2550\u2550\u2550\u2550\u2550\u255d \"\"\" ) print ( \"Unified Library for Fine-Tuning and Inference of Foundational Tabular Models\" ) print ( \"=\" * 80 + \" \\n \" ) self . model_name = model_name self . task_type = task_type self . tuning_strategy = tuning_strategy self . tuning_params = tuning_params or {} self . model_params = model_params or {} self . processor = DataProcessor ( model_name = self . model_name , ** ( processor_params or {})) self . tuner = TuningManager () self . model = None self . model_checkpoint_path = model_checkpoint_path self . finetune_mode = finetune_mode if self . tuning_strategy in ( 'finetune' , 'base-ft' , 'peft' ): self . tuning_params [ 'finetune_mode' ] = self . finetune_mode if self . model_name in [ 'TabPFN' ]: device = 'cuda' if torch . cuda . is_available () else 'cpu' config = { 'device' : device , 'ignore_pretraining_limits' : True } config . update ( self . model_params ) logger . info ( f \"[Pipeline] Config: { config } \" ) self . model = TabPFNClassifier ( ** config ) if self . tuning_strategy in [ 'finetune' , 'base-ft' , 'peft' ] and hasattr ( self . model , '_initialize_model_variables' ): self . model . _initialize_model_variables () elif self . model_name == 'ContextTab' : self . model = ConTextTabClassifier ( ** self . model_params ) elif self . model_name in [ 'TabICL' , 'OrionBix' , 'OrionMSP' ]: device = 'cuda' if torch . cuda . is_available () else 'cpu' config = { 'n_jobs' : 1 , 'device' : device } config . update ( self . model_params ) if self . model_name == 'TabICL' : self . model = TabICLClassifier ( ** config ) if self . tuning_strategy == 'finetune' : self . model . _load_model () elif self . model_name == 'OrionMSP' : self . model = OrionMSPClassifier ( ** config ) if self . tuning_strategy == 'finetune' : self . model . _load_model () else : self . model = OrionBixClassifier ( ** config ) if self . tuning_strategy == 'finetune' : self . model . _load_model () elif self . model_name == 'TabDPT' : # Use GPU if available, otherwise fall back to CPU device = 'cuda' if torch . cuda . is_available () else 'cpu' config = { 'device' : device , 'compile' : True , # Disable compilation to avoid GPU issues 'use_flash' : True , # Disable flash attention to avoid kernel issues 'normalizer' : 'standard' , 'missing_indicators' : False , 'clip_sigma' : 4.0 , 'feature_reduction' : 'pca' , 'faiss_metric' : 'l2' , # Inference parameters with GPU-friendly defaults 'n_ensembles' : 8 , 'temperature' : 0.8 , 'context_size' : 512 , 'permute_classes' : True , 'seed' : None , } config . update ( self . model_params ) # All parameters now valid self . model = TabDPTClassifier ( ** config ) # Handle models that require late initialization (processor needs to be fit first) elif self . model_name not in [ 'Mitra' , 'APT' ]: raise ValueError ( f \"Model ' { self . model_name } ' not supported.\" ) if self . model is not None and self . model_checkpoint_path : logger . info ( f \"[Pipeline] Attempting to load model state from checkpoint: { self . model_checkpoint_path } \" ) try : # Determine the underlying torch model attribute torch_model = None if hasattr ( self . model , 'model_' ): # For TabPFN, TabICL, OrionMSP, OrionBix torch_model = self . model . model_ elif hasattr ( self . model , 'model' ): # For ContextTab, TabDPT torch_model = self . model . model elif isinstance ( self . model , torch . nn . Module ): # For Mitra (Tab2D) torch_model = self . model if torch_model : torch_model . load_state_dict ( torch . load ( self . model_checkpoint_path , map_location = torch . device ( 'cpu' ))) logger . info ( f \"[Pipeline] Successfully loaded checkpoint for { type ( self . model ) . _name_ } .\" ) else : logger . warning ( f \"[Pipeline] Could not determine the underlying torch model for { type ( self . model ) . _name_ } to load checkpoint.\" ) except Exception as e : logger . error ( f \"[Pipeline] Failed to load checkpoint: { e } \" ) self . _is_fitted = False self . X_train_processed_ = None self . y_train_processed_ = None logger . info ( f \"[Pipeline] TabularPipeline initialized for model ' { self . model_name } ', task ' { self . task_type } ', with strategy ' { self . tuning_strategy } '\" ) ( \"TabTune - Unified Library for fine-tuning and inference of Foundational Tabular Models\" ) def __del__ ( self ): \"\"\"Cleanup method to properly shut down resources when pipeline is destroyed.\"\"\" # ContextTab ZMQ server cleanup is handled automatically by atexit.register() # in the start_embedding_server function, so no manual cleanup needed pass def fit ( self , X : pd . DataFrame , y : pd . Series ): self . X_raw_train = X . copy () self . y_raw_train = y . copy () logger . info ( \"[Pipeline] Starting fit process\" ) # Special handling for models that are TRULY self-contained and do not need the pipeline's processor for inference if self . tuning_strategy == 'inference' and isinstance ( self . model , ( TabICLClassifier , OrionMSPClassifier , OrionBixClassifier )): logger . info ( \"[Pipeline] Handing off to TuningManager for inference setup.\" ) self . processor . fit ( X , y ) self . model = self . tuner . tune ( self . model , X , y , strategy = self . tuning_strategy ) self . _is_fitted = True logger . info ( \"[Pipeline] Fit process complete\" ) return self #For ALL other models and strategies (including ConTextTab), we must fit the DataProcessor first. logger . info ( \"[Pipeline] Fitting data processor...\" ) self . processor . fit ( X , y ) # Handle ConTextTab inference AFTER the processor has been fitted if self . tuning_strategy == 'inference' and isinstance ( self . model , ConTextTabClassifier ): logger . info ( f \"[Pipeline] Handing off to TuningManager for inference setup for { self . model_name } \" ) # The tuner calls the model's native .fit() method with the raw data self . model = self . tuner . tune ( self . model , X , y , strategy = self . tuning_strategy ) self . _is_fitted = True logger . info ( \"[Pipeline] Fit process complete\" ) return self # Late initialization for models that need info from the fitted processor if self . model is None : logger . info ( \"[Pipeline] Performing late initialization of the model...\" ) if self . model_name == 'Mitra' : n_classes = len ( self . processor . custom_preprocessor_ . label_encoder_ . classes_ ) device = 'cuda' if torch . cuda . is_available () else 'cpu' config = { 'dim' : 256 , 'n_layers' : 6 , 'n_heads' : 8 , 'task' : 'CLASSIFICATION' , 'dim_output' : n_classes , 'use_pretrained_weights' : False , 'path_to_weights' : '' , 'device' : device } config . update ( self . model_params ) self . model = Tab2D ( ** config ) if self . model_checkpoint_path : logger . info ( f \"[Pipeline] Attempting to load model state from checkpoint for late-initialized model: { self . model_checkpoint_path } \" ) try : self . model . load_state_dict ( torch . load ( self . model_checkpoint_path , map_location = torch . device ())) logger . info ( f \"[Pipeline] Successfully loaded checkpoint for { type ( self . model ) . _name_ } .\" ) except Exception as e : logger . error ( f \"[Pipeline] Failed to load checkpoint: { e } \" ) if hasattr ( self . model , 'to' ): device_str = self . tuning_params . get ( 'device' , 'cuda' if torch . cuda . is_available () else 'cpu' ) device = torch . device ( device_str ) self . model . to ( device ) if self . model_name == 'Mitra' : # Set device type on model or wrapper try : setattr ( self . model , 'device_type' , device_str ) except Exception : pass if isinstance ( self . model , ( TabICLClassifier , OrionMSPClassifier , OrionBixClassifier )): self . model . device = device if isinstance ( self . model , ConTextTabClassifier ) and self . tuning_strategy in [ 'finetune' , 'base-ft' ]: logger . info ( \"[Pipeline] Preparing raw data for ConTextTab fine-tuning\" ) if not isinstance ( X , pd . DataFrame ): X_to_tune = pd . DataFrame ( X ) else : X_to_tune = X . copy () if not isinstance ( y , pd . Series ): y_to_tune = pd . Series ( y ) else : y_to_tune = y . copy () else : logger . info ( \"[Pipeline] Transforming data for model tuning...\" ) processed_data = self . processor . transform ( X , y ) if isinstance ( processed_data , tuple ): self . X_train_processed_ , self . y_train_processed_ = processed_data else : self . X_train_processed_ = processed_data if hasattr ( self . processor , 'custom_preprocessor_' ) and hasattr ( self . processor . custom_preprocessor_ , 'label_encoder_' ) and self . processor . custom_preprocessor_ . label_encoder_ is not None : self . y_train_processed_ = self . processor . custom_preprocessor_ . label_encoder_ . transform ( y ) else : # Fallback for models without a main processor label encoder self . y_train_processed_ = y X_to_tune , y_to_tune = self . X_train_processed_ , self . y_train_processed_ logger . info ( \"[Pipeline] Handing off to Tuning Manager\" ) if self . tuning_strategy == \"peft\" : logger . info ( \"[Pipeline] PEFT MODE: Attempting Parameter-Efficient Fine-Tuning\" ) logger . info ( \"[Pipeline] NOTE: PEFT may have compatibility limitations with tabular models\" ) logger . info ( \"[Pipeline] FALLBACK: Base fine-tuning will be used if PEFT fails\" ) self . model = self . tuner . tune ( self . model , X_to_tune , y_to_tune , strategy = self . tuning_strategy , params = self . tuning_params , processor = self . processor ) if isinstance ( self . model , TabDPTClassifier ) and self . tuning_strategy in [ 'finetune' , 'base-ft' , 'peft' ]: logger . info ( \"[Pipeline] Finalizing TabDPT setup after fine-tuning\" ) self . model . num_classes = len ( np . unique ( y_to_tune )) # Fit the model for inference after fine-tuning self . model . fit ( X_to_tune , y_to_tune ) self . _is_fitted = True logger . info ( \"[Pipeline] Fit process complete\" ) if self . tuning_strategy == \"peft\" : logger . info ( \"[Pipeline] PEFT STATUS SUMMARY\" ) logger . info ( \"[Pipeline] LoRA adapters were applied to the model\" ) logger . warning ( \"[Pipeline] Note: PEFT compatibility with tabular models is experimental\" ) logger . info ( \"[Pipeline] If you encounter issues, try 'base-ft' strategy for full compatibility\" ) logger . info ( \"[Pipeline] See documentation for more details on PEFT limitations\" ) return self def predict ( self , X : pd . DataFrame ) -> np . ndarray : if not self . _is_fitted : raise RuntimeError ( \"You must call fit() on the pipeline before calling predict().\" ) logger . info ( \"[Pipeline] Starting prediction\" ) if hasattr ( self . model , 'model' ) and isinstance ( self . model . model , torch . nn . Module ): self . model . model . eval () elif hasattr ( self . model , 'model_' ) and isinstance ( self . model . model_ , torch . nn . Module ): self . model . model_ . eval () if isinstance ( self . model , TabPFNClassifier ): if self . tuning_strategy in [ 'finetune' , 'base-ft' , 'peft' ]: logger . debug ( \"[Pipeline] Setting TabPFN inference context (without refitting weights)...\" ) # Store current model weights saved_weights = self . model . model_ . state_dict () self . model . model_ . load_state_dict ( saved_weights ) # Call fit to set up inference context self . model . fit ( self . X_train_processed_ , self . y_train_processed_ ) # Restore fine-tuned weights immediately #self.model.model_.load_state_dict(saved_weights) logger . debug ( \"[Pipeline] Restored fine-tuned weights after context setup\" ) X_processed = self . processor . transform ( X ) return self . model . predict ( X_processed ) if isinstance ( self . model , TabDPTClassifier ): # Apply the same preprocessing as during fit() X_processed = self . processor . transform ( X ) # Get integer predictions from model predictions_raw = self . model . predict ( X_processed ) # Convert integer predictions back to original string labels (same as TabICL/OrionMSP/OrionBix) predictions = self . processor . custom_preprocessor_ . label_encoder_ . inverse_transform ( predictions_raw ) return predictions if isinstance ( self . model , ( ConTextTabClassifier )): logger . debug ( f \"[Pipeline] Using model's native in-context prediction for { type ( self . model ) . __name__ } \" ) predictions = self . model . predict ( X ) elif isinstance ( self . model , ( TabICLClassifier , OrionMSPClassifier , OrionBixClassifier )): logger . debug ( f \"[Pipeline] Using model's native in-context prediction for { type ( self . model ) . __name__ } \" ) X_processed = self . processor . transform ( X ) #predictions = self.model.predict(X) if self . tuning_strategy == 'inference' : # For inference mode, pass raw data directly to the model # The model's internal encoders will handle the preprocessing predictions = self . model . predict ( X ) else : # For fine-tuning mode, use preprocessed data to match training label_encoder = self . processor . custom_preprocessor_ . label_encoder_ known_class = label_encoder . classes_ [ 0 ] y_dummy = pd . Series ([ known_class ] * len ( X )) X_query , _ = self . processor . transform ( X , y_dummy ) # Convert to DataFrame to maintain feature names for sklearn compatibility if not isinstance ( X_query , pd . DataFrame ): # Prefer processor feature names if available; else fall back to input X cols = None if hasattr ( self . processor , \"feature_names_\" ) and self . processor . feature_names_ is not None : cols = list ( self . processor . feature_names_ ) elif hasattr ( X , \"columns\" ): cols = list ( X . columns ) # Avoid shape/columns mismatch if cols is not None and hasattr ( X_query , \"shape\" ) and X_query . shape [ 1 ] != len ( cols ): cols = None X_query = pd . DataFrame ( X_query , columns = cols ) predictions = self . model . predict ( X_query ) # Convert numerical predictions back to string format for evaluation if self . tuning_strategy in [ 'finetune' , 'base-ft' , 'peft' ] and hasattr ( self . processor , 'custom_preprocessor_' ) and hasattr ( self . processor . custom_preprocessor_ , 'label_encoder_' ): predictions = self . processor . custom_preprocessor_ . label_encoder_ . inverse_transform ( predictions ) elif self . model_name == 'Mitra' : logger . debug ( \"[Pipeline] Using in-context prediction for Mitra (Tab2D)\" ) label_encoder = self . processor . custom_preprocessor_ . label_encoder_ known_class = label_encoder . classes_ [ 0 ] y_dummy = pd . Series ([ known_class ] * len ( X )) X_query , _ = self . processor . transform ( X , y_dummy ) X_support , y_support = self . X_train_processed_ , self . y_train_processed_ device_str = self . tuning_params . get ( 'device' , 'cuda' if torch . cuda . is_available () else 'cpu' ) device = device_str X_support_t = torch . tensor ( X_support , dtype = torch . float32 ) . unsqueeze ( 0 ) . to ( device ) y_support_t = torch . tensor ( y_support , dtype = torch . long ) . unsqueeze ( 0 ) . to ( device ) X_query_t = torch . tensor ( X_query , dtype = torch . float32 ) . unsqueeze ( 0 ) . to ( device ) b , f = X_support_t . shape [ 0 ], X_support_t . shape [ 2 ] padding_features = torch . zeros ( b , f , dtype = torch . bool , device = device ) padding_obs_support = torch . zeros_like ( y_support_t , dtype = torch . bool , device = device ) padding_obs_query = torch . zeros ( b , X_query_t . shape [ 1 ], dtype = torch . bool , device = device ) self . model . eval () with torch . no_grad (): logits = self . model ( x_support = X_support_t , y_support = y_support_t , x_query = X_query_t , padding_features = padding_features , padding_obs_support = padding_obs_support , padding_obs_query__ = padding_obs_query ) predictions_raw = logits . squeeze ( 0 ) . cpu () . numpy () . argmax ( axis =- 1 ) predictions = self . processor . custom_preprocessor_ . label_encoder_ . inverse_transform ( predictions_raw ) else : # TabPFN and other standard models do not need y_dummy for prediction transforms logger . debug ( \"[Pipeline] Applying learned transformations to new data\" ) X_processed = self . processor . transform ( X ) # Pass only X logger . debug ( \"[Pipeline] Getting predictions from the model\" ) predictions = self . model . predict ( X_processed ) return predictions def predict_proba ( self , X : pd . DataFrame ) -> np . ndarray : \"\"\" Predicts class probabilities for the input data. Required for calculating AUC score. \"\"\" if not self . _is_fitted : raise RuntimeError ( \"You must call fit() on the pipeline before calling predict_proba().\" ) logger . info ( \"[Pipeline] Starting probability prediction\" ) if hasattr ( self . model , 'model' ) and isinstance ( self . model . model , torch . nn . Module ): self . model . model . eval () elif hasattr ( self . model , 'model_' ) and isinstance ( self . model . model_ , torch . nn . Module ): self . model . model_ . eval () if isinstance ( self . model , TabDPTClassifier ): logger . debug ( \"[Pipeline] Using TabDPT's internal predict_proba\" ) # Apply the same preprocessing as during fit() X_processed = self . processor . transform ( X ) # Use stored defaults from model initialization return self . model . ensemble_predict_proba ( X_processed ) elif isinstance ( self . model , TabPFNClassifier ): # Special handling for fine-tuned TabPFN to set inference context if self . tuning_strategy in [ 'finetune' , 'base-ft' , 'peft' ]: logger . debug ( \"[Pipeline] Setting TabPFN inference context for proba...\" ) self . model . fit ( self . X_train_processed_ , self . y_train_processed_ ) X_processed = self . processor . transform ( X ) return self . model . predict_proba ( X_processed ) if isinstance ( self . model , ( TabICLClassifier , OrionMSPClassifier , OrionBixClassifier , ConTextTabClassifier )): logger . debug ( \"[Pipeline] Using model's native predict_proba method\" ) X_processed = self . processor . transform ( X ) if isinstance ( self . model , ( ConTextTabClassifier )): return self . model . predict_proba ( X ) if isinstance ( self . model , ( TabICLClassifier , OrionMSPClassifier , OrionBixClassifier )): if self . tuning_strategy == 'inference' : return self . model . predict_proba ( X ) else : label_encoder = self . processor . custom_preprocessor_ . label_encoder_ known_class = label_encoder . classes_ [ 0 ] y_dummy = pd . Series ([ known_class ] * len ( X )) X_query , _ = self . processor . transform ( X , y_dummy ) # Convert to DataFrame to maintain feature names for sklearn compatibility if not isinstance ( X_query , pd . DataFrame ): # Prefer processor feature names if available; else fall back to input X cols = None if hasattr ( self . processor , \"feature_names_\" ) and self . processor . feature_names_ is not None : cols = list ( self . processor . feature_names_ ) elif hasattr ( X , \"columns\" ): cols = list ( X . columns ) # Avoid shape/columns mismatch if cols is not None and hasattr ( X_query , \"shape\" ) and X_query . shape [ 1 ] != len ( cols ): cols = None X_query = pd . DataFrame ( X_query , columns = cols ) return self . model . predict_proba ( X_query ) return self . model . predict_proba ( X_processed ) label_encoder = self . processor . custom_preprocessor_ . label_encoder_ known_class = label_encoder . classes_ [ 0 ] y_dummy = pd . Series ([ known_class ] * len ( X )) X_query , _ = self . processor . transform ( X , y_dummy ) X_support = self . X_train_processed_ y_support = self . y_train_processed_ device = next ( self . model . parameters ()) . device X_support_t = torch . tensor ( X_support , dtype = torch . float32 ) . unsqueeze ( 0 ) . to ( device ) y_support_t = torch . tensor ( y_support , dtype = torch . long ) . unsqueeze ( 0 ) . to ( device ) X_query_t = torch . tensor ( X_query , dtype = torch . float32 ) . unsqueeze ( 0 ) . to ( device ) self . model . eval () with torch . no_grad (): if isinstance ( self . model , Tab2D ): logger . debug ( \"[Pipeline] Generating probabilities for Mitra (Tab2D)\" ) b , f = X_support_t . shape [ 0 ], X_support_t . shape [ 2 ] padding_features = torch . zeros ( b , f , dtype = torch . bool , device = device ) padding_obs_support = torch . zeros_like ( y_support_t , dtype = torch . bool , device = device ) padding_obs_query = torch . zeros ( b , X_query_t . shape [ 1 ], dtype = torch . bool , device = device ) logits = self . model ( x_support = X_support_t , y_support = y_support_t , x_query = X_query_t , padding_features = padding_features , padding_obs_support = padding_obs_support , padding_obs_query__ = padding_obs_query ) probabilities = torch . softmax ( logits . squeeze ( 0 ), dim =- 1 ) . cpu () . numpy () else : if self . model_name == 'Mitra' : # Not implemented for Mitra raise NotImplementedError ( \"predict_proba is not implemented for Mitra (Tab2D)\" ) raise NotImplementedError ( f \"predict_proba is not implemented for model type { type ( self . model ) . __name__ } \" ) logger . info ( \"[Pipeline] Probability prediction complete\" ) return probabilities ############### Helpers ############################# def _get_model_class_labels ( self ): \"\"\" Best-effort to recover the class label order that predict_proba columns use. \"\"\" # sklearn-style estimators if hasattr ( self . model , \"classes_\" ): return list ( self . model . classes_ ) if hasattr ( self . model , \"y_encoder_\" ) and hasattr ( self . model . y_encoder_ , \"classes_\" ): return list ( self . model . y_encoder_ . classes_ ) if hasattr ( self . model , \"classes_\" ): return list ( self . model . classes_ ) return None def _align_proba_to_encoder ( self , probabilities , label_encoder ): \"\"\" Ensure the columns of `probabilities` line up with label_encoder.classes_. Returns a 2D array with shape (n_samples, K) where K==len(label_encoder.classes_). If the model returns only the positive-class column for binary, we upcast it to two columns [P(class0), P(class1)] assuming classes_ are [0,1] after encoding. \"\"\" import numpy as np # Force 2D and validate input if probabilities is None : logger . warning ( \"[Pipeline] Probabilities are None in _align_proba_to_encoder\" ) return None if probabilities . ndim == 1 : probabilities = probabilities . reshape ( - 1 , 1 ) # Check for empty probabilities if probabilities . size == 0 : logger . warning ( \"[Pipeline] Empty probabilities array in _align_proba_to_encoder\" ) return None encoder_classes = list ( label_encoder . classes_ ) K = len ( encoder_classes ) # Binary convenience cases if K == 2 : if probabilities . shape [ 1 ] == 1 : # Validate that single column probabilities are in [0, 1] p_pos = probabilities [:, 0 ] if np . any ( p_pos < 0 ) or np . any ( p_pos > 1 ): logger . warning ( f \"[Pipeline] Single-column probabilities outside [0,1] range (min: { p_pos . min () : .6f } , max: { p_pos . max () : .6f } )\" ) # assume encoder maps positives to label 1 (LabelEncoder does 0..K-1) p_neg = 1.0 - p_pos return np . column_stack ([ p_neg , p_pos ]) # or two columns already \u2014 validate and return elif probabilities . shape [ 1 ] == 2 : # Validate that probabilities are in [0, 1] if np . any ( probabilities < 0 ) or np . any ( probabilities > 1 ): logger . warning ( f \"[Pipeline] Two-column probabilities outside [0,1] range (min: { probabilities . min () : .6f } , max: { probabilities . max () : .6f } )\" ) return probabilities else : logger . warning ( f \"[Pipeline] Unexpected number of probability columns ( { probabilities . shape [ 1 ] } ) for binary classification\" ) return None # Multiclass: align by class labels model_labels = self . _get_model_class_labels () # If we can't recover model labels, assume current order already matches encoder if not model_labels or probabilities . shape [ 1 ] == K and set ( model_labels ) == set ( encoder_classes ): # Still ensure shape matches if probabilities . shape [ 1 ] == K : # Validate that probabilities are in [0, 1] if np . any ( probabilities < 0 ) or np . any ( probabilities > 1 ): logger . warning ( f \"[Pipeline] Multiclass probabilities outside [0,1] range (min: { probabilities . min () : .6f } , max: { probabilities . max () : .6f } )\" ) return probabilities else : logger . warning ( f \"[Pipeline] Shape mismatch: expected { K } columns, got { probabilities . shape [ 1 ] } \" ) return None # Build aligned matrix (zeros for any missing classes) aligned = np . zeros (( probabilities . shape [ 0 ], K ), dtype = float ) # Map model label -> encoder index try : model_to_encoder_idx = { lbl : int ( label_encoder . transform ([ lbl ])[ 0 ]) for lbl in model_labels } except Exception : # If transform fails (types differ), fall back to identity numeric mapping model_to_encoder_idx = {} for j , lbl in enumerate ( model_labels ): try : enc_idx = int ( lbl ) # numeric labels already 0..K-1 except Exception : enc_idx = j model_to_encoder_idx [ lbl ] = enc_idx for j_model , lbl in enumerate ( model_labels ): if j_model >= probabilities . shape [ 1 ]: break enc_j = model_to_encoder_idx . get ( lbl , None ) if enc_j is not None and 0 <= enc_j < K : aligned [:, enc_j ] = probabilities [:, j_model ] # Final validation of aligned probabilities if np . any ( aligned < 0 ) or np . any ( aligned > 1 ): logger . warning ( f \"[Pipeline] Aligned probabilities outside [0,1] range (min: { aligned . min () : .6f } , max: { aligned . max () : .6f } )\" ) # Check if any samples have all-zero probabilities (missing class predictions) zero_rows = np . all ( aligned == 0 , axis = 1 ) if np . any ( zero_rows ): logger . warning ( f \"[Pipeline] { np . sum ( zero_rows ) } samples have all-zero probabilities (missing class predictions)\" ) return aligned def evaluate ( self , X_test : pd . DataFrame , y_test : pd . Series , output_format : str = 'rich' ): \"\"\" Makes predictions on the test set and prints a report with Accuracy, F1 Score, and ROC AUC Score. \"\"\" if not self . _is_fitted : raise RuntimeError ( \"You must call fit() on the pipeline before evaluating.\" ) logger . info ( \" \\n \" + \"=\" * 60 ) logger . info ( \"[Pipeline] Running Evaluation\" ) predictions = self . predict ( X_test ) if self . task_type == 'classification' : probabilities = self . predict_proba ( X_test ) y_test_encoded = None # Case 1: Custom preprocessor has a label encoder (Mitra, TabICL, APT, OrionMSP, OrionBix) if hasattr ( self . processor , 'custom_preprocessor_' ) and hasattr ( self . processor . custom_preprocessor_ , 'label_encoder_' ): y_test_encoded = self . processor . custom_preprocessor_ . label_encoder_ . transform ( y_test ) elif isinstance ( self . model , ( TabICLClassifier , OrionMSPClassifier , OrionBixClassifier )): y_test_encoded = self . model . y_encoder_ . transform ( y_test ) elif isinstance ( self . model , TabPFNClassifier ): le = LabelEncoder () le . classes_ = self . model . classes_ # Use the classes the model learned during .fit() y_test_encoded = le . transform ( y_test ) # Case 3: Standard pipeline with a main label encoder elif isinstance ( self . model , ConTextTabClassifier ): if hasattr ( self . processor_ , 'label_encoder_' ): if y_test . dtype == object or y_test . dtype . kind in { 'U' , 'S' }: y_test = self . processor_ . label_encoder_ . transform ( y_test ) elif hasattr ( self . processor , 'label_encoder_' ) and self . processor . label_encoder_ is not None : y_test_encoded = self . processor . label_encoder_ . transform ( y_test ) if y_test_encoded is None : raise RuntimeError ( \"Could not find a fitted label encoder to evaluate metrics.\" ) accuracy = accuracy_score ( y_test , predictions ) f1 = f1_score ( y_test , predictions , average = 'weighted' ) mcc = matthews_corrcoef ( y_test , predictions ) precision = precision_score ( y_test , predictions , average = 'weighted' ) recall = recall_score ( y_test , predictions , average = 'weighted' ) # Guard: AUC is undefined if the test fold has < 2 classes unique_test = np . unique ( y_test_encoded ) if len ( unique_test ) < 2 : auc = float ( \"nan\" ) else : # Align probability columns to the SAME label order used by y_test_encoded # Choose the same encoder you used above when computing y_test_encoded if hasattr ( self . processor , 'custom_preprocessor_' ) and hasattr ( self . processor . custom_preprocessor_ , 'label_encoder_' ): le = self . processor . custom_preprocessor_ . label_encoder_ elif isinstance ( self . model , ( TabICLClassifier , OrionBixClassifier , OrionMSPClassifier )): le = self . model . y_encoder_ elif isinstance ( self . model , TabPFNClassifier ): le = LabelEncoder (); le . classes_ = self . model . classes_ elif hasattr ( self . processor , 'label_encoder_' ) and self . processor . label_encoder_ is not None : le = self . processor . label_encoder_ else : raise RuntimeError ( \"Could not find a fitted label encoder to align probabilities.\" ) probs_aligned = self . _align_proba_to_encoder ( probabilities , le ) # Binary vs multiclass handling with explicit labels to match encoded y K = len ( le . classes_ ) if K == 2 : # probs_aligned has 2 columns by construction: [:, 1] is positive class auc = roc_auc_score ( y_test_encoded , probs_aligned [:, 1 ]) else : auc = roc_auc_score ( y_test_encoded , probs_aligned , labels = list ( range ( K )), # encoded labels are 0..K-1 multi_class = \"ovr\" , average = \"weighted\" , ) results = { \"accuracy\" : accuracy , \"roc_auc_score\" : auc , \"f1_score\" : f1 , \"precision\" : precision , \"recall\" : recall , \"mcc\" : mcc } if output_format == 'json' : print ( json . dumps ( results , indent = 4 )) elif output_format == 'rich' : logger . info ( \" \\n \" + \"=\" * 60 ) logger . info ( \"[Pipeline] Running Evaluation\" ) logger . info ( \" \\n [Pipeline] Evaluation Report\" ) logger . info ( f \"[Pipeline] Accuracy: { accuracy : .4f } \" ) logger . info ( f \"[Pipeline] Weighted F1-Score: { f1 : .4f } \" ) logger . info ( f \"[Pipeline] Weighted Precision: { precision : .4f } \" ) logger . info ( f \"[Pipeline] Weighted Recall: { recall : .4f } \" ) logger . info ( f \"[Pipeline] MCC: { mcc : .4f } \" ) logger . info ( f \"[Pipeline] ROC AUC Score: { auc : .4f } \" ) logger . info ( \" \\n [Pipeline] Classification Report\" ) logger . info ( classification_report ( y_test , predictions , zero_division = 0 )) logger . info ( \"=\" * 60 ) else : logger . warning ( f \"[Pipeline] Unknown output_format: ' { output_format } '. No output printed.\" ) return results def save ( self , file_path : str ): if not self . _is_fitted : raise RuntimeError ( \"You can only save a pipeline after it has been fitted.\" ) logger . info ( f \"[Pipeline] Saving pipeline to { file_path } \" ) joblib . dump ( self , file_path ) logger . info ( \"[Pipeline] Pipeline saved successfully\" ) @classmethod def load ( cls , file_path : str ): logger . info ( f \"[Pipeline] Loading pipeline from { file_path } \" ) pipeline = joblib . load ( file_path ) logger . info ( \"[Pipeline] Pipeline loaded successfully\" ) return pipeline def show_processing_summary ( self ): \"\"\" Retrieves and logs the data processing summary from the DataProcessor. \"\"\" logger . info ( \" \\n \" + \"=\" * 60 ) summary = self . processor . get_processing_summary () # Log the multi-line summary as a single message summary_lines = summary . split ( ' \\n ' ) for line in summary_lines : logger . info ( line ) def _calculate_calibration_errors ( self , y_true , y_prob , n_bins = 10 ): \"\"\"Helper to calculate ECE and MCE.\"\"\" confidences = np . max ( y_prob , axis = 1 ) predictions = np . argmax ( y_prob , axis = 1 ) accuracies = ( predictions == y_true ) ece = 0.0 mce = 0.0 bin_boundaries = np . linspace ( 0 , 1 , n_bins + 1 ) for i in range ( n_bins ): in_bin = ( confidences > bin_boundaries [ i ]) & ( confidences <= bin_boundaries [ i + 1 ]) prop_in_bin = np . mean ( in_bin ) if prop_in_bin > 0 : accuracy_in_bin = np . mean ( accuracies [ in_bin ]) avg_confidence_in_bin = np . mean ( confidences [ in_bin ]) bin_abs_err = np . abs ( accuracy_in_bin - avg_confidence_in_bin ) ece += prop_in_bin * bin_abs_err mce = max ( mce , bin_abs_err ) return ece , mce def evaluate_calibration ( self , X_test : pd . DataFrame , y_test : pd . Series , n_bins : int = 15 , output_format : str = 'rich' ): \"\"\" Calculates and provides a detailed report on model calibration metrics. This version supports both binary and multiclass classification. \"\"\" if not self . _is_fitted : raise RuntimeError ( \"You must call fit() on the pipeline before evaluating calibration.\" ) # --- Metric Calculation (common for all formats) --- probabilities = self . predict_proba ( X_test ) # 1. Find the correct label encoder (same logic as in evaluate()) le = None if hasattr ( self . processor , 'custom_preprocessor_' ) and hasattr ( self . processor . custom_preprocessor_ , 'label_encoder_' ): le = self . processor . custom_preprocessor_ . label_encoder_ elif isinstance ( self . model , ( TabICLClassifier , OrionBixClassifier , OrionMSPClassifier )): # Use model's internal encoder if in inference mode if hasattr ( self . model , 'y_encoder_' ): le = self . model . y_encoder_ # Use processor's encoder if in finetune mode elif hasattr ( self . processor , 'custom_preprocessor_' ) and hasattr ( self . processor . custom_preprocessor_ , 'label_encoder_' ): le = self . processor . custom_preprocessor_ . label_encoder_ elif isinstance ( self . model , TabPFNClassifier ): if hasattr ( self . model , 'classes_' ): le = LabelEncoder () le . classes_ = self . model . classes_ elif hasattr ( self . processor , 'label_encoder_' ) and self . processor . label_encoder_ is not None : le = self . processor . label_encoder_ if le is None : raise RuntimeError ( \"Could not find a fitted label encoder to evaluate calibration.\" ) # 2. Encode y_test using the found encoder y_test_encoded = le . transform ( y_test ) # 3. Align probability columns to match the encoder's class order probs_aligned = self . _align_proba_to_encoder ( probabilities , le ) # 4. Calculate metrics using the aligned probabilities # brier_score_loss handles (n_samples, n_classes) for multiclass # when y_true is (n_samples,) with integer labels [0, K-1]. # Validate inputs before calculating Brier score if probs_aligned is None : logger . warning ( \"[Pipeline] Probabilities are None, skipping Brier score calculation\" ) brier_score = float ( 'nan' ) else : # Check for NaN or infinite values if np . any ( np . isnan ( probs_aligned )) or np . any ( np . isinf ( probs_aligned )): logger . warning ( \"[Pipeline] Probabilities contain NaN or infinite values, skipping Brier score calculation\" ) brier_score = float ( 'nan' ) else : # Validate that probabilities sum to 1.0 (within tolerance) prob_sums = np . sum ( probs_aligned , axis = 1 ) if not np . allclose ( prob_sums , 1.0 , rtol = 1e-6 ): logger . warning ( f \"[Pipeline] Probabilities don't sum to 1.0 (range: { prob_sums . min () : .6f } to { prob_sums . max () : .6f } )\" ) logger . warning ( \"[Pipeline] This may indicate model calibration issues\" ) # Validate that y_test_encoded contains valid class indices max_class_idx = len ( le . classes_ ) - 1 if np . any ( y_test_encoded < 0 ) or np . any ( y_test_encoded > max_class_idx ): logger . warning ( f \"[Pipeline] Invalid class indices in y_test_encoded (range: { y_test_encoded . min () } to { y_test_encoded . max () } )\" ) logger . warning ( f \"[Pipeline] Expected range: 0 to { max_class_idx } \" ) brier_score = float ( 'nan' ) else : try : brier_score = brier_score_loss ( y_test_encoded , probs_aligned ) except Exception as e : logger . error ( f \"[Pipeline] Error calculating Brier score: { e } \" ) brier_score = float ( 'nan' ) # _calculate_calibration_errors also works with (n, K) probability matrix if probs_aligned is None : logger . warning ( \"[Pipeline] Probabilities are None, skipping ECE and MCE calculation\" ) ece , mce = float ( 'nan' ), float ( 'nan' ) else : ece , mce = self . _calculate_calibration_errors ( y_test_encoded , probs_aligned , n_bins = n_bins ) results = { \"brier_score_loss\" : brier_score , \"expected_calibration_error\" : ece , \"maximum_calibration_error\" : mce } if output_format == 'rich' : logger . info ( \" \\n \" + \"=\" * 80 ) logger . info ( \"[Pipeline] Running Detailed Calibration Evaluation\" ) logger . info ( \"=\" * 80 ) logger . info ( \"[Pipeline] Calibration measures how well a model's predicted probabilities match the true likelihood of outcomes.\" ) logger . info ( \"[Pipeline] A well-calibrated model is trustworthy: if it predicts a 70% probability, it should be correct 70 % o f the time. \\n \" ) logger . info ( \"[Pipeline] Brier Score Loss\" ) logger . info ( \"[Pipeline] Measures the mean squared difference between predicted probabilities and actual outcomes.\" ) if np . isnan ( brier_score ): logger . info ( f \"[Pipeline] Your Score: NaN (calculation skipped due to validation issues)\" ) logger . info ( \"[Pipeline] Interpretation: Check warnings above for details on why Brier score could not be calculated.\" ) else : logger . info ( f \"[Pipeline] Your Score: { brier_score : .4f } \" ) logger . info ( \"[Pipeline] Interpretation: Scores range from 0.0 to 1.0, where lower is better. A score near 0.0 indicates excellent calibration.\" ) logger . info ( \"[Pipeline] Note: For multiclass problems, this is the average Brier score across all classes.\" ) logger . info ( \"[Pipeline] Note: For imbalanced datasets, consider class-specific Brier scores for better insights.\" ) logger . info ( \"\" ) logger . info ( \"[Pipeline] Expected & Maximum Calibration Error (ECE / MCE)\" ) logger . info ( \"[Pipeline] These metrics group predictions into bins by confidence (e.g., 80-90%) and measure the gap between the average confidence and the actual accuracy in each bin.\" ) if np . isnan ( ece ) or np . isnan ( mce ): logger . info ( f \"[Pipeline] Expected Calibration Error (ECE): NaN (calculation skipped due to validation issues)\" ) logger . info ( f \"[Pipeline] Maximum Calibration Error (MCE): NaN (calculation skipped due to validation issues)\" ) logger . info ( \"[Pipeline] Interpretation: Check warnings above for details on why ECE/MCE could not be calculated.\" ) else : logger . info ( f \"[Pipeline] Expected Calibration Error (ECE): { ece : .4f } \" ) logger . info ( f \"[Pipeline] Interpretation: ECE represents the average gap between confidence and accuracy across all bins. Your score indicates the model's confidence is off by an average of { ece * 100 : .2f } %. An ECE below 0.05 (5%) is generally considered good.\" ) logger . info ( f \"[Pipeline] Maximum Calibration Error (MCE): { mce : .4f } \" ) logger . info ( \"[Pipeline] Interpretation: MCE identifies the single worst-performing bin, representing the 'worst-case scenario' for your model's calibration. A high MCE reveals specific confidence ranges where the model is particularly unreliable.\" ) logger . info ( \"\" ) logger . info ( \"=\" * 80 ) elif output_format == 'json' : print ( json . dumps ( results , indent = 4 )) else : logger . warning ( f \"[Pipeline] Unknown output_format: ' { output_format } '. No console output printed.\" ) # The method still returns the dictionary for programmatic use return results def evaluate_fairness ( self , X_test : pd . DataFrame , y_test : pd . Series , sensitive_features : pd . Series , output_format : str = 'rich' ): \"\"\" Calculates and provides a detailed report on group fairness metrics. \"\"\" if not self . _is_fitted : raise RuntimeError ( \"You must call fit() on the pipeline before evaluating fairness.\" ) predictions = self . predict ( X_test ) y_test_encoded , predictions_encoded = self . _get_encoded_labels ( y_test , predictions ) spd = demographic_parity_difference ( y_true = y_test_encoded , y_pred = predictions_encoded , sensitive_features = sensitive_features ) eod = equal_opportunity_difference ( y_true = y_test_encoded , y_pred = predictions_encoded , sensitive_features = sensitive_features ) aod = equalized_odds_difference ( y_true = y_test_encoded , y_pred = predictions_encoded , sensitive_features = sensitive_features ) results = { \"statistical_parity_difference\" : spd , \"equal_opportunity_difference\" : eod , \"equalized_odds_difference\" : aod } if output_format == 'rich' : logger . info ( \" \\n \" + \"=\" * 80 ) logger . info ( \"[Pipeline] Running Detailed Fairness Evaluation\" ) logger . info ( \"=\" * 80 ) logger . info ( f \"[Pipeline] Fairness is evaluated with respect to the ' { sensitive_features . name } ' attribute.\" ) logger . info ( \"[Pipeline] These metrics measure disparities in model behavior between different groups. For these difference-based metrics, a value of 0 indicates perfect fairness. \\n \" ) logger . info ( \"[Pipeline] Statistical Parity Difference (Selection Rate)\" ) logger . info ( \"[Pipeline] Measures the difference in the rate of positive predictions (e.g., 'Churn') between groups.\" ) logger . info ( f \"[Pipeline] Your Score: { spd : .4f } \" ) logger . info ( f \"[Pipeline] Interpretation: Your score means there is a { abs ( spd * 100 ) : .2f } % difference in the selection rate between groups. Values close to 0 are ideal. Disparities above 10-20% are often considered significant. \\n \" ) logger . info ( \"[Pipeline] Equal Opportunity Difference (True Positive Rate)\" ) logger . info ( \"[Pipeline] Measures the difference in the true positive rate\u2014the rate at which the model correctly identifies positive outcomes\u2014between groups.\" ) logger . info ( f \"[Pipeline] Your Score: { eod : .4f } \" ) logger . info ( f \"[Pipeline] Interpretation: For cases that are genuinely positive, your score means the model's ability to correctly identify them differs by { abs ( eod * 100 ) : .2f } % between groups. High values indicate the model's benefits are not being applied equally. \\n \" ) logger . info ( \"[Pipeline] Equalized Odds Difference (Overall Error Rate)\" ) logger . info ( \"[Pipeline] Measures the larger of the true positive rate difference and the false positive rate difference between groups.\" ) logger . info ( f \"[Pipeline] Your Score: { aod : .4f } \" ) logger . info ( f \"[Pipeline] Interpretation: This score represents the 'worst-case' error rate disparity. A score of { abs ( aod * 100 ) : .2f } % indicates the largest gap in performance. If this value is close to the Equal Opportunity Difference, the main issue is with true positives. \\n \" ) logger . info ( \"=\" * 80 ) elif output_format == 'json' : print ( json . dumps ( results , indent = 4 )) else : logger . warning ( f \"[Pipeline] Unknown output_format: ' { output_format } '. No console output printed.\" ) return results def _get_encoded_labels ( self , y_true , y_pred ): \"\"\"Helper to consistently encode true and predicted labels.\"\"\" y_true_encoded = None y_pred_encoded = None # Find the correct LabelEncoder le = None if hasattr ( self . processor , 'custom_preprocessor_' ) and hasattr ( self . processor . custom_preprocessor_ , 'label_encoder_' ): le = self . processor . custom_preprocessor_ . label_encoder_ elif isinstance ( self . model , ( TabICLClassifier , OrionMSPClassifier , OrionBixClassifier , TabPFNClassifier )): # Fit a temporary encoder on the training labels seen during .fit() le = LabelEncoder () . fit ( self . y_train_processed_ if self . y_train_processed_ is not None else y_true ) else : raise RuntimeError ( \"Could not find a fitted label encoder to evaluate metrics.\" ) y_true_encoded = le . transform ( y_true ) # Handle cases where y_pred might be different (e.g., raw y_test for fairness) if y_pred is not None : y_pred_encoded = le . transform ( y_pred ) return y_true_encoded , y_pred_encoded def baseline ( self , X_train : pd . DataFrame , y_train : pd . Series , X_test : pd . DataFrame , y_test : pd . Series , models : list | str | None = None , time_limit : int = 60 ): \"\"\" Trains and evaluates baseline models using AutoGluon on the provided train/test split. Now returns per-model F1 scores along with validation scores and training time. \"\"\" try : from autogluon.tabular import TabularPredictor from sklearn.metrics import accuracy_score , f1_score except ImportError : raise ImportError ( \"AutoGluon is not installed. Install it with: pip install autogluon\" ) logger . info ( \"Preparing data for AutoGluon...\" ) # Prepare data with target column X_train_with_label = X_train . copy () X_train_with_label [ '__target__' ] = y_train . values if hasattr ( y_train , 'values' ) else y_train X_test_with_label = X_test . copy () X_test_with_label [ '__target__' ] = y_test . values if hasattr ( y_test , 'values' ) else y_test # Configure model hyperparameters hyperparameters = None if models is not None : models_to_run = [ models ] if isinstance ( models , str ) else models model_map = { 'xgboost' : 'XGB' , 'catboost' : 'CAT' , 'randomforest' : 'RF' , 'lightgbm' : 'GBM' , 'extratrees' : 'XT' , 'knn' : 'KNN' , 'linear' : 'LR' , 'neuralnet' : 'NN_TORCH' } ag_models = [ model_map . get ( m . lower (), m . upper ()) for m in models_to_run ] hyperparameters = { model : {} for model in ag_models } logger . info ( f \"Training AutoGluon predictor with time_limit= { time_limit } s...\" ) start_time = time . time () predictor = TabularPredictor ( label = '__target__' , eval_metric = 'accuracy' , verbosity = 2 ) . fit ( train_data = X_train_with_label , time_limit = time_limit , hyperparameters = hyperparameters , presets = 'medium_quality' ) total_train_time = time . time () - start_time logger . info ( \"Generating test predictions using best model ensemble...\" ) predictions = predictor . predict ( X_test ) overall_accuracy = accuracy_score ( y_test , predictions ) overall_f1 = f1_score ( y_test , predictions , average = 'weighted' ) leaderboard = predictor . leaderboard ( X_test_with_label , silent = True ) baseline_results = [] logger . info ( \"Calculating per-model F1 scores...\" ) for _ , row in leaderboard . iterrows (): model_name = row [ 'model' ] # Individual model predictions model_pred = predictor . predict ( X_test , model = model_name ) # Model-specific F1 score model_f1 = f1_score ( y_test , model_pred , average = 'weighted' ) baseline_results . append ({ \"Model\" : model_name , \"Validation Score\" : row [ 'score_val' ], \"F1 Score\" : model_f1 , \"Training Time\" : row [ 'fit_time' ] }) logger . info ( \" \\n AutoGluon Baseline Evaluation Report\" ) logger . info ( f \"Overall Accuracy: { overall_accuracy : .4f } \" ) logger . info ( f \"Overall Weighted F1-Score: { overall_f1 : .4f } \" ) logger . info ( f \"Total Training Time: { total_train_time : .2f } s \\n \" ) header = f \" { 'Model' : <30 } { 'Val Score' : <15 } { 'F1 Score' : <15 } { 'Train Time (s)' : <15 } \" logger . info ( header ) for result in baseline_results : logger . info ( f \" { result [ 'Model' ] : <30 } { result [ 'Validation Score' ] : <15.4f } \" f \" { result [ 'F1 Score' ] : <15.4f } { result [ 'Training Time' ] : <15.2f } \" ) logger . info ( \"=\" * 80 ) return { \"overall_accuracy\" : overall_accuracy , \"overall_f1\" : overall_f1 , \"total_training_time\" : total_train_time , \"individual_models\" : baseline_results , \"predictor\" : predictor , \"leaderboard\" : leaderboard } def evaluate_checkpoints ( self , X_test , y_test , checkpoint_dir , epochs , map_location : str | None = None ): results = {} for ep in epochs : ckpt_name = f \" { type ( self . model ) . __name__ } _epoch { ep } .pt\" ckpt_path = os . path . join ( checkpoint_dir , ckpt_name ) if not os . path . exists ( ckpt_path ): logger . warning ( f \" - Missing checkpoint for epoch { ep } , skipping\" ) continue logger . info ( f \" \\n \ud83d\udd01 Evaluating checkpoint at epoch { ep } \" ) self . model = self . tuner . load_checkpoint ( self . model , ckpt_path , map_location or 'cpu' ) for name , param in self . model . model . named_parameters (): logger . info ( f \" { name } mean: { torch . mean ( param ) . item () : .6f } \" ) break # then evaluate normally metrics = self . evaluate ( X_test , y_test ) results [ ep ] = metrics return results","title":"API: TabularPipeline"},{"location":"api/pipeline/#tabtune.TabularPipeline.pipeline.TabularPipeline.__del__","text":"Cleanup method to properly shut down resources when pipeline is destroyed. Source code in tabtune/TabularPipeline/pipeline.py 186 187 188 189 190 def __del__ ( self ): \"\"\"Cleanup method to properly shut down resources when pipeline is destroyed.\"\"\" # ContextTab ZMQ server cleanup is handled automatically by atexit.register() # in the start_embedding_server function, so no manual cleanup needed pass","title":"__del__"},{"location":"api/pipeline/#tabtune.TabularPipeline.pipeline.TabularPipeline.baseline","text":"Trains and evaluates baseline models using AutoGluon on the provided train/test split. Now returns per-model F1 scores along with validation scores and training time. Source code in tabtune/TabularPipeline/pipeline.py 1010 1011 1012 1013 1014 1015 1016 1017 1018 1019 1020 1021 1022 1023 1024 1025 1026 1027 1028 1029 1030 1031 1032 1033 1034 1035 1036 1037 1038 1039 1040 1041 1042 1043 1044 1045 1046 1047 1048 1049 1050 1051 1052 1053 1054 1055 1056 1057 1058 1059 1060 1061 1062 1063 1064 1065 1066 1067 1068 1069 1070 1071 1072 1073 1074 1075 1076 1077 1078 1079 1080 1081 1082 1083 1084 1085 1086 1087 1088 1089 1090 1091 1092 1093 1094 1095 1096 1097 1098 1099 1100 1101 1102 1103 1104 1105 1106 1107 1108 1109 1110 def baseline ( self , X_train : pd . DataFrame , y_train : pd . Series , X_test : pd . DataFrame , y_test : pd . Series , models : list | str | None = None , time_limit : int = 60 ): \"\"\" Trains and evaluates baseline models using AutoGluon on the provided train/test split. Now returns per-model F1 scores along with validation scores and training time. \"\"\" try : from autogluon.tabular import TabularPredictor from sklearn.metrics import accuracy_score , f1_score except ImportError : raise ImportError ( \"AutoGluon is not installed. Install it with: pip install autogluon\" ) logger . info ( \"Preparing data for AutoGluon...\" ) # Prepare data with target column X_train_with_label = X_train . copy () X_train_with_label [ '__target__' ] = y_train . values if hasattr ( y_train , 'values' ) else y_train X_test_with_label = X_test . copy () X_test_with_label [ '__target__' ] = y_test . values if hasattr ( y_test , 'values' ) else y_test # Configure model hyperparameters hyperparameters = None if models is not None : models_to_run = [ models ] if isinstance ( models , str ) else models model_map = { 'xgboost' : 'XGB' , 'catboost' : 'CAT' , 'randomforest' : 'RF' , 'lightgbm' : 'GBM' , 'extratrees' : 'XT' , 'knn' : 'KNN' , 'linear' : 'LR' , 'neuralnet' : 'NN_TORCH' } ag_models = [ model_map . get ( m . lower (), m . upper ()) for m in models_to_run ] hyperparameters = { model : {} for model in ag_models } logger . info ( f \"Training AutoGluon predictor with time_limit= { time_limit } s...\" ) start_time = time . time () predictor = TabularPredictor ( label = '__target__' , eval_metric = 'accuracy' , verbosity = 2 ) . fit ( train_data = X_train_with_label , time_limit = time_limit , hyperparameters = hyperparameters , presets = 'medium_quality' ) total_train_time = time . time () - start_time logger . info ( \"Generating test predictions using best model ensemble...\" ) predictions = predictor . predict ( X_test ) overall_accuracy = accuracy_score ( y_test , predictions ) overall_f1 = f1_score ( y_test , predictions , average = 'weighted' ) leaderboard = predictor . leaderboard ( X_test_with_label , silent = True ) baseline_results = [] logger . info ( \"Calculating per-model F1 scores...\" ) for _ , row in leaderboard . iterrows (): model_name = row [ 'model' ] # Individual model predictions model_pred = predictor . predict ( X_test , model = model_name ) # Model-specific F1 score model_f1 = f1_score ( y_test , model_pred , average = 'weighted' ) baseline_results . append ({ \"Model\" : model_name , \"Validation Score\" : row [ 'score_val' ], \"F1 Score\" : model_f1 , \"Training Time\" : row [ 'fit_time' ] }) logger . info ( \" \\n AutoGluon Baseline Evaluation Report\" ) logger . info ( f \"Overall Accuracy: { overall_accuracy : .4f } \" ) logger . info ( f \"Overall Weighted F1-Score: { overall_f1 : .4f } \" ) logger . info ( f \"Total Training Time: { total_train_time : .2f } s \\n \" ) header = f \" { 'Model' : <30 } { 'Val Score' : <15 } { 'F1 Score' : <15 } { 'Train Time (s)' : <15 } \" logger . info ( header ) for result in baseline_results : logger . info ( f \" { result [ 'Model' ] : <30 } { result [ 'Validation Score' ] : <15.4f } \" f \" { result [ 'F1 Score' ] : <15.4f } { result [ 'Training Time' ] : <15.2f } \" ) logger . info ( \"=\" * 80 ) return { \"overall_accuracy\" : overall_accuracy , \"overall_f1\" : overall_f1 , \"total_training_time\" : total_train_time , \"individual_models\" : baseline_results , \"predictor\" : predictor , \"leaderboard\" : leaderboard }","title":"baseline"},{"location":"api/pipeline/#tabtune.TabularPipeline.pipeline.TabularPipeline.evaluate","text":"Makes predictions on the test set and prints a report with Accuracy, F1 Score, and ROC AUC Score. Source code in tabtune/TabularPipeline/pipeline.py 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749 def evaluate ( self , X_test : pd . DataFrame , y_test : pd . Series , output_format : str = 'rich' ): \"\"\" Makes predictions on the test set and prints a report with Accuracy, F1 Score, and ROC AUC Score. \"\"\" if not self . _is_fitted : raise RuntimeError ( \"You must call fit() on the pipeline before evaluating.\" ) logger . info ( \" \\n \" + \"=\" * 60 ) logger . info ( \"[Pipeline] Running Evaluation\" ) predictions = self . predict ( X_test ) if self . task_type == 'classification' : probabilities = self . predict_proba ( X_test ) y_test_encoded = None # Case 1: Custom preprocessor has a label encoder (Mitra, TabICL, APT, OrionMSP, OrionBix) if hasattr ( self . processor , 'custom_preprocessor_' ) and hasattr ( self . processor . custom_preprocessor_ , 'label_encoder_' ): y_test_encoded = self . processor . custom_preprocessor_ . label_encoder_ . transform ( y_test ) elif isinstance ( self . model , ( TabICLClassifier , OrionMSPClassifier , OrionBixClassifier )): y_test_encoded = self . model . y_encoder_ . transform ( y_test ) elif isinstance ( self . model , TabPFNClassifier ): le = LabelEncoder () le . classes_ = self . model . classes_ # Use the classes the model learned during .fit() y_test_encoded = le . transform ( y_test ) # Case 3: Standard pipeline with a main label encoder elif isinstance ( self . model , ConTextTabClassifier ): if hasattr ( self . processor_ , 'label_encoder_' ): if y_test . dtype == object or y_test . dtype . kind in { 'U' , 'S' }: y_test = self . processor_ . label_encoder_ . transform ( y_test ) elif hasattr ( self . processor , 'label_encoder_' ) and self . processor . label_encoder_ is not None : y_test_encoded = self . processor . label_encoder_ . transform ( y_test ) if y_test_encoded is None : raise RuntimeError ( \"Could not find a fitted label encoder to evaluate metrics.\" ) accuracy = accuracy_score ( y_test , predictions ) f1 = f1_score ( y_test , predictions , average = 'weighted' ) mcc = matthews_corrcoef ( y_test , predictions ) precision = precision_score ( y_test , predictions , average = 'weighted' ) recall = recall_score ( y_test , predictions , average = 'weighted' ) # Guard: AUC is undefined if the test fold has < 2 classes unique_test = np . unique ( y_test_encoded ) if len ( unique_test ) < 2 : auc = float ( \"nan\" ) else : # Align probability columns to the SAME label order used by y_test_encoded # Choose the same encoder you used above when computing y_test_encoded if hasattr ( self . processor , 'custom_preprocessor_' ) and hasattr ( self . processor . custom_preprocessor_ , 'label_encoder_' ): le = self . processor . custom_preprocessor_ . label_encoder_ elif isinstance ( self . model , ( TabICLClassifier , OrionBixClassifier , OrionMSPClassifier )): le = self . model . y_encoder_ elif isinstance ( self . model , TabPFNClassifier ): le = LabelEncoder (); le . classes_ = self . model . classes_ elif hasattr ( self . processor , 'label_encoder_' ) and self . processor . label_encoder_ is not None : le = self . processor . label_encoder_ else : raise RuntimeError ( \"Could not find a fitted label encoder to align probabilities.\" ) probs_aligned = self . _align_proba_to_encoder ( probabilities , le ) # Binary vs multiclass handling with explicit labels to match encoded y K = len ( le . classes_ ) if K == 2 : # probs_aligned has 2 columns by construction: [:, 1] is positive class auc = roc_auc_score ( y_test_encoded , probs_aligned [:, 1 ]) else : auc = roc_auc_score ( y_test_encoded , probs_aligned , labels = list ( range ( K )), # encoded labels are 0..K-1 multi_class = \"ovr\" , average = \"weighted\" , ) results = { \"accuracy\" : accuracy , \"roc_auc_score\" : auc , \"f1_score\" : f1 , \"precision\" : precision , \"recall\" : recall , \"mcc\" : mcc } if output_format == 'json' : print ( json . dumps ( results , indent = 4 )) elif output_format == 'rich' : logger . info ( \" \\n \" + \"=\" * 60 ) logger . info ( \"[Pipeline] Running Evaluation\" ) logger . info ( \" \\n [Pipeline] Evaluation Report\" ) logger . info ( f \"[Pipeline] Accuracy: { accuracy : .4f } \" ) logger . info ( f \"[Pipeline] Weighted F1-Score: { f1 : .4f } \" ) logger . info ( f \"[Pipeline] Weighted Precision: { precision : .4f } \" ) logger . info ( f \"[Pipeline] Weighted Recall: { recall : .4f } \" ) logger . info ( f \"[Pipeline] MCC: { mcc : .4f } \" ) logger . info ( f \"[Pipeline] ROC AUC Score: { auc : .4f } \" ) logger . info ( \" \\n [Pipeline] Classification Report\" ) logger . info ( classification_report ( y_test , predictions , zero_division = 0 )) logger . info ( \"=\" * 60 ) else : logger . warning ( f \"[Pipeline] Unknown output_format: ' { output_format } '. No output printed.\" ) return results","title":"evaluate"},{"location":"api/pipeline/#tabtune.TabularPipeline.pipeline.TabularPipeline.evaluate_calibration","text":"Calculates and provides a detailed report on model calibration metrics. This version supports both binary and multiclass classification. Source code in tabtune/TabularPipeline/pipeline.py 803 804 805 806 807 808 809 810 811 812 813 814 815 816 817 818 819 820 821 822 823 824 825 826 827 828 829 830 831 832 833 834 835 836 837 838 839 840 841 842 843 844 845 846 847 848 849 850 851 852 853 854 855 856 857 858 859 860 861 862 863 864 865 866 867 868 869 870 871 872 873 874 875 876 877 878 879 880 881 882 883 884 885 886 887 888 889 890 891 892 893 894 895 896 897 898 899 900 901 902 903 904 905 906 907 908 909 910 911 912 913 914 915 916 917 918 919 920 921 922 923 924 925 926 927 928 929 def evaluate_calibration ( self , X_test : pd . DataFrame , y_test : pd . Series , n_bins : int = 15 , output_format : str = 'rich' ): \"\"\" Calculates and provides a detailed report on model calibration metrics. This version supports both binary and multiclass classification. \"\"\" if not self . _is_fitted : raise RuntimeError ( \"You must call fit() on the pipeline before evaluating calibration.\" ) # --- Metric Calculation (common for all formats) --- probabilities = self . predict_proba ( X_test ) # 1. Find the correct label encoder (same logic as in evaluate()) le = None if hasattr ( self . processor , 'custom_preprocessor_' ) and hasattr ( self . processor . custom_preprocessor_ , 'label_encoder_' ): le = self . processor . custom_preprocessor_ . label_encoder_ elif isinstance ( self . model , ( TabICLClassifier , OrionBixClassifier , OrionMSPClassifier )): # Use model's internal encoder if in inference mode if hasattr ( self . model , 'y_encoder_' ): le = self . model . y_encoder_ # Use processor's encoder if in finetune mode elif hasattr ( self . processor , 'custom_preprocessor_' ) and hasattr ( self . processor . custom_preprocessor_ , 'label_encoder_' ): le = self . processor . custom_preprocessor_ . label_encoder_ elif isinstance ( self . model , TabPFNClassifier ): if hasattr ( self . model , 'classes_' ): le = LabelEncoder () le . classes_ = self . model . classes_ elif hasattr ( self . processor , 'label_encoder_' ) and self . processor . label_encoder_ is not None : le = self . processor . label_encoder_ if le is None : raise RuntimeError ( \"Could not find a fitted label encoder to evaluate calibration.\" ) # 2. Encode y_test using the found encoder y_test_encoded = le . transform ( y_test ) # 3. Align probability columns to match the encoder's class order probs_aligned = self . _align_proba_to_encoder ( probabilities , le ) # 4. Calculate metrics using the aligned probabilities # brier_score_loss handles (n_samples, n_classes) for multiclass # when y_true is (n_samples,) with integer labels [0, K-1]. # Validate inputs before calculating Brier score if probs_aligned is None : logger . warning ( \"[Pipeline] Probabilities are None, skipping Brier score calculation\" ) brier_score = float ( 'nan' ) else : # Check for NaN or infinite values if np . any ( np . isnan ( probs_aligned )) or np . any ( np . isinf ( probs_aligned )): logger . warning ( \"[Pipeline] Probabilities contain NaN or infinite values, skipping Brier score calculation\" ) brier_score = float ( 'nan' ) else : # Validate that probabilities sum to 1.0 (within tolerance) prob_sums = np . sum ( probs_aligned , axis = 1 ) if not np . allclose ( prob_sums , 1.0 , rtol = 1e-6 ): logger . warning ( f \"[Pipeline] Probabilities don't sum to 1.0 (range: { prob_sums . min () : .6f } to { prob_sums . max () : .6f } )\" ) logger . warning ( \"[Pipeline] This may indicate model calibration issues\" ) # Validate that y_test_encoded contains valid class indices max_class_idx = len ( le . classes_ ) - 1 if np . any ( y_test_encoded < 0 ) or np . any ( y_test_encoded > max_class_idx ): logger . warning ( f \"[Pipeline] Invalid class indices in y_test_encoded (range: { y_test_encoded . min () } to { y_test_encoded . max () } )\" ) logger . warning ( f \"[Pipeline] Expected range: 0 to { max_class_idx } \" ) brier_score = float ( 'nan' ) else : try : brier_score = brier_score_loss ( y_test_encoded , probs_aligned ) except Exception as e : logger . error ( f \"[Pipeline] Error calculating Brier score: { e } \" ) brier_score = float ( 'nan' ) # _calculate_calibration_errors also works with (n, K) probability matrix if probs_aligned is None : logger . warning ( \"[Pipeline] Probabilities are None, skipping ECE and MCE calculation\" ) ece , mce = float ( 'nan' ), float ( 'nan' ) else : ece , mce = self . _calculate_calibration_errors ( y_test_encoded , probs_aligned , n_bins = n_bins ) results = { \"brier_score_loss\" : brier_score , \"expected_calibration_error\" : ece , \"maximum_calibration_error\" : mce } if output_format == 'rich' : logger . info ( \" \\n \" + \"=\" * 80 ) logger . info ( \"[Pipeline] Running Detailed Calibration Evaluation\" ) logger . info ( \"=\" * 80 ) logger . info ( \"[Pipeline] Calibration measures how well a model's predicted probabilities match the true likelihood of outcomes.\" ) logger . info ( \"[Pipeline] A well-calibrated model is trustworthy: if it predicts a 70% probability, it should be correct 70 % o f the time. \\n \" ) logger . info ( \"[Pipeline] Brier Score Loss\" ) logger . info ( \"[Pipeline] Measures the mean squared difference between predicted probabilities and actual outcomes.\" ) if np . isnan ( brier_score ): logger . info ( f \"[Pipeline] Your Score: NaN (calculation skipped due to validation issues)\" ) logger . info ( \"[Pipeline] Interpretation: Check warnings above for details on why Brier score could not be calculated.\" ) else : logger . info ( f \"[Pipeline] Your Score: { brier_score : .4f } \" ) logger . info ( \"[Pipeline] Interpretation: Scores range from 0.0 to 1.0, where lower is better. A score near 0.0 indicates excellent calibration.\" ) logger . info ( \"[Pipeline] Note: For multiclass problems, this is the average Brier score across all classes.\" ) logger . info ( \"[Pipeline] Note: For imbalanced datasets, consider class-specific Brier scores for better insights.\" ) logger . info ( \"\" ) logger . info ( \"[Pipeline] Expected & Maximum Calibration Error (ECE / MCE)\" ) logger . info ( \"[Pipeline] These metrics group predictions into bins by confidence (e.g., 80-90%) and measure the gap between the average confidence and the actual accuracy in each bin.\" ) if np . isnan ( ece ) or np . isnan ( mce ): logger . info ( f \"[Pipeline] Expected Calibration Error (ECE): NaN (calculation skipped due to validation issues)\" ) logger . info ( f \"[Pipeline] Maximum Calibration Error (MCE): NaN (calculation skipped due to validation issues)\" ) logger . info ( \"[Pipeline] Interpretation: Check warnings above for details on why ECE/MCE could not be calculated.\" ) else : logger . info ( f \"[Pipeline] Expected Calibration Error (ECE): { ece : .4f } \" ) logger . info ( f \"[Pipeline] Interpretation: ECE represents the average gap between confidence and accuracy across all bins. Your score indicates the model's confidence is off by an average of { ece * 100 : .2f } %. An ECE below 0.05 (5%) is generally considered good.\" ) logger . info ( f \"[Pipeline] Maximum Calibration Error (MCE): { mce : .4f } \" ) logger . info ( \"[Pipeline] Interpretation: MCE identifies the single worst-performing bin, representing the 'worst-case scenario' for your model's calibration. A high MCE reveals specific confidence ranges where the model is particularly unreliable.\" ) logger . info ( \"\" ) logger . info ( \"=\" * 80 ) elif output_format == 'json' : print ( json . dumps ( results , indent = 4 )) else : logger . warning ( f \"[Pipeline] Unknown output_format: ' { output_format } '. No console output printed.\" ) # The method still returns the dictionary for programmatic use return results","title":"evaluate_calibration"},{"location":"api/pipeline/#tabtune.TabularPipeline.pipeline.TabularPipeline.evaluate_fairness","text":"Calculates and provides a detailed report on group fairness metrics. Source code in tabtune/TabularPipeline/pipeline.py 931 932 933 934 935 936 937 938 939 940 941 942 943 944 945 946 947 948 949 950 951 952 953 954 955 956 957 958 959 960 961 962 963 964 965 966 967 968 969 970 971 972 973 974 975 976 977 978 979 980 981 982 983 984 985 986 def evaluate_fairness ( self , X_test : pd . DataFrame , y_test : pd . Series , sensitive_features : pd . Series , output_format : str = 'rich' ): \"\"\" Calculates and provides a detailed report on group fairness metrics. \"\"\" if not self . _is_fitted : raise RuntimeError ( \"You must call fit() on the pipeline before evaluating fairness.\" ) predictions = self . predict ( X_test ) y_test_encoded , predictions_encoded = self . _get_encoded_labels ( y_test , predictions ) spd = demographic_parity_difference ( y_true = y_test_encoded , y_pred = predictions_encoded , sensitive_features = sensitive_features ) eod = equal_opportunity_difference ( y_true = y_test_encoded , y_pred = predictions_encoded , sensitive_features = sensitive_features ) aod = equalized_odds_difference ( y_true = y_test_encoded , y_pred = predictions_encoded , sensitive_features = sensitive_features ) results = { \"statistical_parity_difference\" : spd , \"equal_opportunity_difference\" : eod , \"equalized_odds_difference\" : aod } if output_format == 'rich' : logger . info ( \" \\n \" + \"=\" * 80 ) logger . info ( \"[Pipeline] Running Detailed Fairness Evaluation\" ) logger . info ( \"=\" * 80 ) logger . info ( f \"[Pipeline] Fairness is evaluated with respect to the ' { sensitive_features . name } ' attribute.\" ) logger . info ( \"[Pipeline] These metrics measure disparities in model behavior between different groups. For these difference-based metrics, a value of 0 indicates perfect fairness. \\n \" ) logger . info ( \"[Pipeline] Statistical Parity Difference (Selection Rate)\" ) logger . info ( \"[Pipeline] Measures the difference in the rate of positive predictions (e.g., 'Churn') between groups.\" ) logger . info ( f \"[Pipeline] Your Score: { spd : .4f } \" ) logger . info ( f \"[Pipeline] Interpretation: Your score means there is a { abs ( spd * 100 ) : .2f } % difference in the selection rate between groups. Values close to 0 are ideal. Disparities above 10-20% are often considered significant. \\n \" ) logger . info ( \"[Pipeline] Equal Opportunity Difference (True Positive Rate)\" ) logger . info ( \"[Pipeline] Measures the difference in the true positive rate\u2014the rate at which the model correctly identifies positive outcomes\u2014between groups.\" ) logger . info ( f \"[Pipeline] Your Score: { eod : .4f } \" ) logger . info ( f \"[Pipeline] Interpretation: For cases that are genuinely positive, your score means the model's ability to correctly identify them differs by { abs ( eod * 100 ) : .2f } % between groups. High values indicate the model's benefits are not being applied equally. \\n \" ) logger . info ( \"[Pipeline] Equalized Odds Difference (Overall Error Rate)\" ) logger . info ( \"[Pipeline] Measures the larger of the true positive rate difference and the false positive rate difference between groups.\" ) logger . info ( f \"[Pipeline] Your Score: { aod : .4f } \" ) logger . info ( f \"[Pipeline] Interpretation: This score represents the 'worst-case' error rate disparity. A score of { abs ( aod * 100 ) : .2f } % indicates the largest gap in performance. If this value is close to the Equal Opportunity Difference, the main issue is with true positives. \\n \" ) logger . info ( \"=\" * 80 ) elif output_format == 'json' : print ( json . dumps ( results , indent = 4 )) else : logger . warning ( f \"[Pipeline] Unknown output_format: ' { output_format } '. No console output printed.\" ) return results","title":"evaluate_fairness"},{"location":"api/pipeline/#tabtune.TabularPipeline.pipeline.TabularPipeline.predict_proba","text":"Predicts class probabilities for the input data. Required for calculating AUC score. Source code in tabtune/TabularPipeline/pipeline.py 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 def predict_proba ( self , X : pd . DataFrame ) -> np . ndarray : \"\"\" Predicts class probabilities for the input data. Required for calculating AUC score. \"\"\" if not self . _is_fitted : raise RuntimeError ( \"You must call fit() on the pipeline before calling predict_proba().\" ) logger . info ( \"[Pipeline] Starting probability prediction\" ) if hasattr ( self . model , 'model' ) and isinstance ( self . model . model , torch . nn . Module ): self . model . model . eval () elif hasattr ( self . model , 'model_' ) and isinstance ( self . model . model_ , torch . nn . Module ): self . model . model_ . eval () if isinstance ( self . model , TabDPTClassifier ): logger . debug ( \"[Pipeline] Using TabDPT's internal predict_proba\" ) # Apply the same preprocessing as during fit() X_processed = self . processor . transform ( X ) # Use stored defaults from model initialization return self . model . ensemble_predict_proba ( X_processed ) elif isinstance ( self . model , TabPFNClassifier ): # Special handling for fine-tuned TabPFN to set inference context if self . tuning_strategy in [ 'finetune' , 'base-ft' , 'peft' ]: logger . debug ( \"[Pipeline] Setting TabPFN inference context for proba...\" ) self . model . fit ( self . X_train_processed_ , self . y_train_processed_ ) X_processed = self . processor . transform ( X ) return self . model . predict_proba ( X_processed ) if isinstance ( self . model , ( TabICLClassifier , OrionMSPClassifier , OrionBixClassifier , ConTextTabClassifier )): logger . debug ( \"[Pipeline] Using model's native predict_proba method\" ) X_processed = self . processor . transform ( X ) if isinstance ( self . model , ( ConTextTabClassifier )): return self . model . predict_proba ( X ) if isinstance ( self . model , ( TabICLClassifier , OrionMSPClassifier , OrionBixClassifier )): if self . tuning_strategy == 'inference' : return self . model . predict_proba ( X ) else : label_encoder = self . processor . custom_preprocessor_ . label_encoder_ known_class = label_encoder . classes_ [ 0 ] y_dummy = pd . Series ([ known_class ] * len ( X )) X_query , _ = self . processor . transform ( X , y_dummy ) # Convert to DataFrame to maintain feature names for sklearn compatibility if not isinstance ( X_query , pd . DataFrame ): # Prefer processor feature names if available; else fall back to input X cols = None if hasattr ( self . processor , \"feature_names_\" ) and self . processor . feature_names_ is not None : cols = list ( self . processor . feature_names_ ) elif hasattr ( X , \"columns\" ): cols = list ( X . columns ) # Avoid shape/columns mismatch if cols is not None and hasattr ( X_query , \"shape\" ) and X_query . shape [ 1 ] != len ( cols ): cols = None X_query = pd . DataFrame ( X_query , columns = cols ) return self . model . predict_proba ( X_query ) return self . model . predict_proba ( X_processed ) label_encoder = self . processor . custom_preprocessor_ . label_encoder_ known_class = label_encoder . classes_ [ 0 ] y_dummy = pd . Series ([ known_class ] * len ( X )) X_query , _ = self . processor . transform ( X , y_dummy ) X_support = self . X_train_processed_ y_support = self . y_train_processed_ device = next ( self . model . parameters ()) . device X_support_t = torch . tensor ( X_support , dtype = torch . float32 ) . unsqueeze ( 0 ) . to ( device ) y_support_t = torch . tensor ( y_support , dtype = torch . long ) . unsqueeze ( 0 ) . to ( device ) X_query_t = torch . tensor ( X_query , dtype = torch . float32 ) . unsqueeze ( 0 ) . to ( device ) self . model . eval () with torch . no_grad (): if isinstance ( self . model , Tab2D ): logger . debug ( \"[Pipeline] Generating probabilities for Mitra (Tab2D)\" ) b , f = X_support_t . shape [ 0 ], X_support_t . shape [ 2 ] padding_features = torch . zeros ( b , f , dtype = torch . bool , device = device ) padding_obs_support = torch . zeros_like ( y_support_t , dtype = torch . bool , device = device ) padding_obs_query = torch . zeros ( b , X_query_t . shape [ 1 ], dtype = torch . bool , device = device ) logits = self . model ( x_support = X_support_t , y_support = y_support_t , x_query = X_query_t , padding_features = padding_features , padding_obs_support = padding_obs_support , padding_obs_query__ = padding_obs_query ) probabilities = torch . softmax ( logits . squeeze ( 0 ), dim =- 1 ) . cpu () . numpy () else : if self . model_name == 'Mitra' : # Not implemented for Mitra raise NotImplementedError ( \"predict_proba is not implemented for Mitra (Tab2D)\" ) raise NotImplementedError ( f \"predict_proba is not implemented for model type { type ( self . model ) . __name__ } \" ) logger . info ( \"[Pipeline] Probability prediction complete\" ) return probabilities","title":"predict_proba"},{"location":"api/pipeline/#tabtune.TabularPipeline.pipeline.TabularPipeline.show_processing_summary","text":"Retrieves and logs the data processing summary from the DataProcessor. Source code in tabtune/TabularPipeline/pipeline.py 765 766 767 768 769 770 771 772 773 774 775 def show_processing_summary ( self ): \"\"\" Retrieves and logs the data processing summary from the DataProcessor. \"\"\" logger . info ( \" \\n \" + \"=\" * 60 ) summary = self . processor . get_processing_summary () # Log the multi-line summary as a single message summary_lines = summary . split ( ' \\n ' ) for line in summary_lines : logger . info ( line )","title":"show_processing_summary"},{"location":"api/pipeline/#overview","text":"TabularPipeline provides a scikit-learn-compatible interface for training and using tabular foundation models. It coordinates data preprocessing, model initialization, training, and inference.","title":"Overview"},{"location":"api/pipeline/#constructor","text":"","title":"Constructor"},{"location":"api/pipeline/#tabularpipeline__init__","text":"TabularPipeline ( model_name : str , task_type : str = 'classification' , tuning_strategy : str = 'inference' , tuning_params : dict | None = None , processor_params : dict | None = None , model_params : dict | None = None , model_checkpoint_path : str | None = None , finetune_mode : str = 'meta-learning' )","title":"TabularPipeline.__init__()"},{"location":"api/pipeline/#parameters","text":"model_name (str, required) - Name of the model to use. - Supported values: 'TabPFN' , 'TabICL' , 'OrionMSP' , 'OrionBix' , 'TabDPT' , 'Mitra' , 'ContextTab' - Example: model_name=\"TabICL\" task_type (str, default: 'classification' ) - Type of machine learning task. - Currently supported: 'classification' - Planned: 'regression' - Example: task_type=\"classification\" tuning_strategy (str, default: 'inference' ) - Training/fine-tuning strategy to use. - Options: - 'inference' : Zero-shot predictions (no training) - 'base-ft' : Full fine-tuning of all parameters - 'peft' : Parameter-efficient fine-tuning with LoRA adapters - Example: tuning_strategy=\"peft\" tuning_params (dict, optional) - Hyperparameters for training/inference. - Common parameters: - device (str): 'cuda' or 'cpu' (default: auto-detected) - epochs (int): Number of training epochs - learning_rate (float): Learning rate for optimizer - batch_size (int): Batch size for training - peft_config (dict): LoRA configuration for PEFT strategy - support_size (int): Context size for episodic training - query_size (int): Query size for episodic training - Example: tuning_params = { \"device\" : \"cuda\" , \"epochs\" : 5 , \"learning_rate\" : 2e-5 , \"batch_size\" : 8 } processor_params (dict, optional) - Parameters for data preprocessing. - Common parameters: - imputation_strategy (str): 'mean' , 'median' , 'mode' , 'knn' - scaling_strategy (str): 'standard' , 'minmax' , 'robust' - categorical_encoding (str): Encoding method (auto-selected for model-specific) - resampling_strategy (str): 'smote' , 'random_oversample' , etc. - Example: processor_params = { \"imputation_strategy\" : \"median\" , \"scaling_strategy\" : \"standard\" } model_params (dict, optional) - Direct parameters passed to the model constructor. - Model-specific (see individual model documentation). - Example for TabICL: model_params = { \"n_estimators\" : 16 , \"softmax_temperature\" : 0.9 } model_checkpoint_path (str, optional) - Path to a pre-trained model checkpoint ( .pt file). - If provided, loads weights from checkpoint instead of default pre-trained weights. - Example: model_checkpoint_path=\"./checkpoints/tabicl_epoch5.pt\" finetune_mode (str, default: 'meta-learning' ) - Fine-tuning mode for models that support it. - Options: - 'meta-learning' : Episodic meta-learning (default) - 'sft' : Standard supervised fine-tuning - Example: finetune_mode=\"sft\"","title":"Parameters"},{"location":"api/pipeline/#returns","text":"Returns a TabularPipeline instance (not yet fitted).","title":"Returns"},{"location":"api/pipeline/#core-methods","text":"","title":"Core Methods"},{"location":"api/pipeline/#fitx-y","text":"Train the pipeline on training data. pipeline . fit ( X_train : pd . DataFrame , y_train : pd . Series ) -> TabularPipeline","title":".fit(X, y)"},{"location":"api/pipeline/#parameters_1","text":"X (pd.DataFrame): Training features y (pd.Series): Training labels","title":"Parameters"},{"location":"api/pipeline/#returns_1","text":"Returns self (allows method chaining).","title":"Returns"},{"location":"api/pipeline/#what-it-does","text":"Fits the DataProcessor on training data (learns preprocessing transformations) Applies preprocessing to training data Initializes the model (if late initialization required) Trains the model using TuningManager (if strategy != 'inference' )","title":"What it does"},{"location":"api/pipeline/#example","text":"pipeline = TabularPipeline ( model_name = \"TabICL\" , tuning_strategy = \"base-ft\" ) pipeline . fit ( X_train , y_train )","title":"Example"},{"location":"api/pipeline/#predictx","text":"Generate predictions on new data. predictions = pipeline . predict ( X_test : pd . DataFrame ) -> np . ndarray","title":".predict(X)"},{"location":"api/pipeline/#parameters_2","text":"X (pd.DataFrame): Features for prediction","title":"Parameters"},{"location":"api/pipeline/#returns_2","text":"predictions (np.ndarray): Predicted class labels (shape: (n_samples,) )","title":"Returns"},{"location":"api/pipeline/#notes","text":"Automatically applies learned preprocessing Converts class indices back to original label format Must call .fit() before .predict()","title":"Notes"},{"location":"api/pipeline/#example_1","text":"predictions = pipeline . predict ( X_test ) print ( f \"Predictions shape: { predictions . shape } \" ) print ( f \"Unique classes: { np . unique ( predictions ) } \" )","title":"Example"},{"location":"api/pipeline/#predict_probax","text":"Get probability predictions for classification. probabilities = pipeline . predict_proba ( X_test : pd . DataFrame ) -> np . ndarray","title":".predict_proba(X)"},{"location":"api/pipeline/#parameters_3","text":"X (pd.DataFrame): Features for prediction","title":"Parameters"},{"location":"api/pipeline/#returns_3","text":"probabilities (np.ndarray): Class probabilities (shape: (n_samples, n_classes) )","title":"Returns"},{"location":"api/pipeline/#notes_1","text":"Each row sums to 1.0 Column order matches label encoder classes Required for ROC AUC calculation","title":"Notes"},{"location":"api/pipeline/#example_2","text":"probabilities = pipeline . predict_proba ( X_test ) print ( f \"Probabilities shape: { probabilities . shape } \" ) print ( f \"Row sums: { probabilities . sum ( axis = 1 ) } \" ) # Should be ~1.0","title":"Example"},{"location":"api/pipeline/#evaluatex-y-output_formatrich","text":"Evaluate model performance on test data. metrics = pipeline . evaluate ( X_test : pd . DataFrame , y_test : pd . Series , output_format : str = 'rich' ) -> dict","title":".evaluate(X, y, output_format='rich')"},{"location":"api/pipeline/#parameters_4","text":"X (pd.DataFrame): Test features y (pd.Series): True labels output_format (str): 'rich' (formatted console output) or 'json' (dict only)","title":"Parameters"},{"location":"api/pipeline/#returns_4","text":"metrics (dict): Dictionary with evaluation metrics: accuracy (float): Overall accuracy roc_auc_score (float): ROC AUC (binary/multi-class) f1_score (float): Weighted F1 score precision (float): Weighted precision recall (float): Weighted recall mcc (float): Matthews Correlation Coefficient","title":"Returns"},{"location":"api/pipeline/#example_3","text":"metrics = pipeline . evaluate ( X_test , y_test ) print ( f \"Accuracy: { metrics [ 'accuracy' ] : .4f } \" ) print ( f \"ROC AUC: { metrics [ 'roc_auc_score' ] : .4f } \" )","title":"Example"},{"location":"api/pipeline/#savefile_path","text":"Save the entire pipeline to disk. pipeline . save ( file_path : str ) -> None","title":".save(file_path)"},{"location":"api/pipeline/#parameters_5","text":"file_path (str): Path to save pipeline (typically .joblib extension)","title":"Parameters"},{"location":"api/pipeline/#what-it-saves","text":"DataProcessor state (preprocessing transformations) Model weights and state Configuration (model_name, strategy, params) Label encoders","title":"What it saves"},{"location":"api/pipeline/#notes_2","text":"Must call .fit() before saving Uses joblib for serialization Large files (includes model weights)","title":"Notes"},{"location":"api/pipeline/#example_4","text":"pipeline . fit ( X_train , y_train ) pipeline . save ( \"my_pipeline.joblib\" )","title":"Example"},{"location":"api/pipeline/#loadfile_path-classmethod","text":"Load a saved pipeline from disk. loaded_pipeline = TabularPipeline . load ( file_path : str ) -> TabularPipeline","title":".load(file_path) (classmethod)"},{"location":"api/pipeline/#parameters_6","text":"file_path (str): Path to saved pipeline file","title":"Parameters"},{"location":"api/pipeline/#returns_5","text":"TabularPipeline : Loaded pipeline instance (already fitted)","title":"Returns"},{"location":"api/pipeline/#example_5","text":"loaded_pipeline = TabularPipeline . load ( \"my_pipeline.joblib\" ) predictions = loaded_pipeline . predict ( X_new )","title":"Example"},{"location":"api/pipeline/#additional-methods","text":"","title":"Additional Methods"},{"location":"api/pipeline/#evaluate_calibrationx-y-n_bins15-output_formatrich","text":"Evaluate model calibration (how well probabilities match actual outcomes). calibration_metrics = pipeline . evaluate_calibration ( X_test : pd . DataFrame , y_test : pd . Series , n_bins : int = 15 , output_format : str = 'rich' ) -> dict","title":".evaluate_calibration(X, y, n_bins=15, output_format='rich')"},{"location":"api/pipeline/#returns_6","text":"dict : Contains: brier_score_loss (float): Mean squared error of probabilities expected_calibration_error (float): Average calibration error maximum_calibration_error (float): Worst-case calibration error","title":"Returns"},{"location":"api/pipeline/#evaluate_fairnessx-y-sensitive_features-output_formatrich","text":"Evaluate group fairness metrics. fairness_metrics = pipeline . evaluate_fairness ( X_test : pd . DataFrame , y_test : pd . Series , sensitive_features : pd . Series , output_format : str = 'rich' ) -> dict","title":".evaluate_fairness(X, y, sensitive_features, output_format='rich')"},{"location":"api/pipeline/#returns_7","text":"dict : Contains: statistical_parity_difference (float): Selection rate disparity equal_opportunity_difference (float): True positive rate disparity equalized_odds_difference (float): Overall error rate disparity","title":"Returns"},{"location":"api/pipeline/#show_processing_summary","text":"Display a summary of data preprocessing steps applied. pipeline . show_processing_summary () -> None","title":".show_processing_summary()"},{"location":"api/pipeline/#example-output","text":"Data Processing Summary: - Imputation: mean (numerical), mode (categorical) - Scaling: standard - Encoding: tabicl_special - Features: 50 numerical, 10 categorical","title":"Example Output"},{"location":"api/pipeline/#baselinex_train-y_train-x_test-y_test-modelsnone-time_limit60","text":"Compare TabTune models against AutoGluon baselines. baseline_results = pipeline . baseline ( X_train : pd . DataFrame , y_train : pd . Series , X_test : pd . DataFrame , y_test : pd . Series , models : list | str | None = None , time_limit : int = 60 ) -> dict","title":".baseline(X_train, y_train, X_test, y_test, models=None, time_limit=60)"},{"location":"api/pipeline/#returns_8","text":"dict : Contains AutoGluon baseline results and leaderboard","title":"Returns"},{"location":"api/pipeline/#usage-patterns","text":"","title":"Usage Patterns"},{"location":"api/pipeline/#pattern-1-quick-inference-baseline","text":"from tabtune import TabularPipeline pipeline = TabularPipeline ( model_name = \"TabPFN\" , tuning_strategy = \"inference\" ) pipeline . fit ( X_train , y_train ) metrics = pipeline . evaluate ( X_test , y_test )","title":"Pattern 1: Quick Inference Baseline"},{"location":"api/pipeline/#pattern-2-production-fine-tuning","text":"pipeline = TabularPipeline ( model_name = \"OrionBix\" , tuning_strategy = \"base-ft\" , tuning_params = { \"device\" : \"cuda\" , \"epochs\" : 10 , \"learning_rate\" : 2e-5 , \"save_checkpoint_path\" : \"./checkpoints/model.pt\" } ) pipeline . fit ( X_train , y_train ) pipeline . save ( \"production_model.joblib\" )","title":"Pattern 2: Production Fine-Tuning"},{"location":"api/pipeline/#pattern-3-memory-efficient-peft","text":"pipeline = TabularPipeline ( model_name = \"TabICL\" , tuning_strategy = \"peft\" , tuning_params = { \"device\" : \"cuda\" , \"epochs\" : 5 , \"learning_rate\" : 2e-4 , \"peft_config\" : { \"r\" : 8 , \"lora_alpha\" : 16 , \"lora_dropout\" : 0.05 } } ) pipeline . fit ( X_train , y_train )","title":"Pattern 3: Memory-Efficient PEFT"},{"location":"api/pipeline/#error-handling","text":"","title":"Error Handling"},{"location":"api/pipeline/#common-exceptions","text":"RuntimeError : \"You must call fit() before predict()\" - Cause : Calling predict/evaluate before fitting - Solution : Call .fit() first ValueError : \"Model 'X' not supported\" - Cause : Invalid model name - Solution : Check supported models list RuntimeError : \"CUDA out of memory\" - Cause : Insufficient GPU memory - Solution : Use PEFT, reduce batch size, or use CPU","title":"Common Exceptions"},{"location":"api/pipeline/#see-also","text":"Pipeline Overview : Detailed usage guide Tuning Strategies : Strategy comparisons Model Selection : Choosing the right model Troubleshooting : Common issues and solutions","title":"See Also"},{"location":"api/tuning-manager/","text":"API: TuningManager \u00b6 Handles the model adaptation process Source code in tabtune/TuningManager/tuning.py 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755 756 757 758 759 760 761 762 763 764 765 766 767 768 769 770 771 772 773 774 775 776 777 778 779 780 781 782 783 784 785 786 787 788 789 790 791 792 793 794 795 796 797 798 799 800 801 802 803 804 805 806 807 808 809 810 811 812 813 814 815 816 817 818 819 820 821 822 823 824 825 826 827 828 829 830 831 832 833 834 835 836 837 838 839 840 841 842 843 844 845 846 847 848 849 850 851 852 853 854 855 856 857 858 859 860 861 862 863 864 865 866 867 868 869 870 871 872 873 874 875 876 877 878 879 880 881 882 883 884 885 886 887 888 889 890 891 892 893 894 895 896 897 898 899 900 901 902 903 904 905 906 907 908 909 910 911 912 913 914 915 916 917 918 919 920 921 922 923 924 925 926 927 928 929 930 931 932 933 934 935 936 937 938 939 940 941 942 943 944 945 946 947 948 949 950 951 952 953 954 955 956 957 958 959 960 961 962 963 964 965 966 967 968 969 970 971 972 973 974 975 976 977 978 979 980 981 982 983 984 985 986 987 988 989 990 991 992 993 994 995 996 997 998 999 1000 1001 1002 1003 1004 1005 1006 1007 1008 1009 1010 1011 1012 1013 1014 1015 1016 1017 1018 1019 1020 1021 1022 1023 1024 1025 1026 1027 1028 1029 1030 1031 1032 1033 1034 1035 1036 1037 1038 1039 1040 1041 1042 1043 1044 1045 1046 1047 1048 1049 1050 1051 1052 1053 1054 1055 1056 1057 1058 1059 1060 1061 1062 1063 1064 1065 1066 1067 1068 1069 1070 1071 1072 1073 1074 1075 1076 1077 1078 1079 1080 1081 1082 1083 1084 1085 1086 1087 1088 1089 1090 1091 1092 1093 1094 1095 1096 1097 1098 1099 1100 1101 1102 1103 1104 1105 1106 1107 1108 1109 1110 1111 1112 1113 1114 1115 1116 1117 1118 1119 1120 1121 1122 1123 1124 1125 1126 1127 1128 1129 1130 1131 1132 1133 1134 1135 1136 1137 1138 1139 1140 1141 1142 1143 1144 1145 1146 1147 1148 1149 1150 1151 1152 1153 1154 1155 1156 1157 1158 1159 1160 1161 1162 1163 1164 1165 1166 1167 1168 1169 1170 1171 1172 1173 1174 1175 1176 1177 1178 1179 1180 1181 1182 1183 1184 1185 1186 1187 1188 1189 1190 1191 1192 1193 1194 1195 1196 1197 1198 1199 1200 1201 1202 1203 1204 1205 1206 1207 1208 1209 1210 1211 1212 1213 1214 1215 1216 1217 1218 1219 1220 1221 1222 1223 1224 1225 1226 1227 1228 1229 1230 1231 1232 1233 1234 1235 1236 1237 1238 1239 1240 1241 1242 1243 1244 1245 1246 1247 1248 1249 1250 1251 1252 1253 1254 1255 1256 1257 1258 1259 1260 1261 1262 1263 1264 1265 1266 1267 1268 1269 1270 1271 1272 1273 1274 1275 1276 1277 1278 1279 1280 1281 1282 1283 1284 1285 1286 1287 1288 1289 1290 1291 1292 1293 1294 1295 1296 1297 1298 1299 1300 1301 1302 1303 1304 1305 1306 1307 1308 1309 1310 1311 1312 1313 1314 1315 1316 1317 1318 1319 1320 1321 1322 1323 1324 1325 1326 1327 1328 1329 1330 1331 1332 1333 1334 1335 1336 1337 1338 1339 1340 1341 1342 1343 1344 1345 1346 1347 1348 1349 1350 1351 1352 1353 1354 1355 1356 1357 1358 1359 1360 1361 1362 1363 1364 1365 1366 1367 1368 1369 1370 1371 1372 1373 1374 1375 1376 1377 1378 1379 1380 1381 1382 1383 1384 1385 1386 1387 1388 1389 1390 1391 1392 1393 1394 1395 1396 1397 1398 1399 1400 1401 1402 1403 1404 1405 1406 1407 1408 1409 1410 1411 1412 1413 1414 1415 1416 1417 1418 1419 1420 1421 1422 1423 1424 1425 1426 1427 1428 1429 1430 1431 1432 1433 1434 1435 1436 1437 1438 1439 1440 1441 1442 1443 1444 1445 1446 1447 1448 1449 1450 1451 1452 1453 1454 1455 1456 1457 1458 1459 1460 1461 1462 1463 1464 1465 1466 1467 1468 1469 1470 1471 1472 1473 1474 1475 1476 1477 1478 1479 1480 1481 1482 1483 1484 1485 1486 1487 1488 1489 1490 1491 1492 1493 1494 1495 1496 1497 1498 1499 1500 1501 1502 1503 1504 1505 1506 1507 1508 1509 1510 1511 1512 1513 1514 1515 1516 1517 1518 1519 1520 1521 1522 1523 1524 1525 1526 1527 1528 1529 1530 1531 1532 1533 1534 1535 1536 1537 1538 1539 1540 1541 1542 1543 1544 1545 1546 1547 1548 1549 1550 1551 1552 1553 1554 1555 1556 1557 1558 1559 1560 1561 1562 1563 1564 class TuningManager : \"\"\" Handles the model adaptation process \"\"\" def tune ( self , model , X_train , y_train , strategy = 'inference' , params = None , processor = None ): params_copy = dict ( params ) if isinstance ( params , dict ) else {} finetune_mode = params_copy . pop ( 'finetune_mode' , 'meta-learning' ) save_checkpoint_path = params_copy . pop ( 'save_checkpoint_path' , None ) if save_checkpoint_path is None : default_dir = params_copy . get ( \"checkpoint_dir\" , \"./checkpoints\" ) if not os . path . exists ( default_dir ): os . makedirs ( default_dir ) save_checkpoint_path = os . path . join ( default_dir , f \" { type ( model ) . __name__ } _latest.pt\" ) # Strategy selection: accept either explicit 'peft' strategy or finetune_method='peft' finetune_method = params_copy . pop ( 'finetune_method' , None ) peft_config = params_copy . pop ( 'peft_config' , None ) selected_strategy = strategy if strategy == 'finetune' and finetune_method == 'peft' : selected_strategy = 'peft' elif strategy == 'finetune' : selected_strategy = 'base-ft' is_finetuned = False original_is_tab2d = isinstance ( model , Tab2D ) if ( isinstance ( model , Tab2D ) or original_is_tab2d ) and selected_strategy in ( 'finetune' , 'base-ft' , 'peft' ): if finetune_mode == 'sft' : logger . info ( \"[TuningManager] Using Pure SFT for Mitra (task-optimized)\" ) self . _finetune_mitra_pure_sft ( model , X_train , y_train , params = params_copy , peft_config = peft_config ) else : # default: 'meta-learning' logger . info ( \"[TuningManager] Using Episodic Meta-Learning for Mitra (default)\" ) self . _finetune_mitra ( model , X_train , y_train , params = params_copy , peft_config = peft_config ) is_finetuned = True elif isinstance ( model , TabPFNClassifier ) and selected_strategy in ( 'finetune' , 'base-ft' , 'peft' ): if finetune_mode == 'sft' : logger . info ( \"[TuningManager] Using Pure SFT for TabPFN (task-optimized)\" ) self . _finetune_tabpfn_pure_sft ( model , X_train , y_train , params = params_copy , peft_config = peft_config ) else : # default: 'meta-learning' logger . info ( \"[TuningManager] Using Episodic Meta-Learning for TabPFN (default)\" ) self . _finetune_tabpfn ( model , X_train , y_train , params = params_copy , peft_config = peft_config ) is_finetuned = True elif isinstance ( model , ( TabICLClassifier , OrionMSPClassifier , OrionBixClassifier )) and selected_strategy in ( 'finetune' , 'base-ft' , 'peft' ): if finetune_mode == 'meta-learning' : logger . info ( \"[TuningManager] Meta Learning based FT\" ) self . _finetune_tabicl ( model , X_train , y_train , params = params_copy , peft_config = peft_config ) else : logger . info ( \"[TuningManager] Performing SFT\" ) self . _finetune_tabicl_simple_sft ( model , X_train , y_train , params = params_copy , peft_config = peft_config ) is_finetuned = True elif isinstance ( model , ConTextTabClassifier ) and selected_strategy in ( 'finetune' , 'base-ft' , 'peft' ): self . _full_finetune_model ( model , X_train , y_train , params = params_copy , processor = processor , peft_config = peft_config ) is_finetuned = True elif isinstance ( model , TabDPTClassifier ) and selected_strategy in ( 'finetune' , 'base-ft' , 'peft' ): if finetune_mode == 'sft' : logger . info ( \"[TuningManager] Using Pure SFT for TabDPT (task-optimized)\" ) self . _finetune_tabdpt_pure_sft ( model , X_train , y_train , params = params_copy , processor = processor , peft_config = peft_config ) else : # default: 'meta-learning' logger . info ( \"[TuningManager] Using Episodic Meta-Learning for TabDPT (default)\" ) self . _finetune_tabdpt ( model , X_train , y_train , params = params_copy , processor = processor , peft_config = peft_config ) is_finetuned = True elif isinstance ( model , ( Tab2D )) and selected_strategy == 'inference' : logger . info ( \"[TuningManager] In-context learning model in inference mode. No training needed.\" ) pass elif isinstance ( model , ( TabICLClassifier , OrionMSPClassifier , OrionBixClassifier )) and selected_strategy == 'inference' : logger . info ( \"[TuningManager] Applying standard .fit() for TabICL setup (inference mode)\" ) model . fit ( X_train , y_train ) else : logger . info ( \"[TuningManager] Applying standard model fitting (.fit)\" ) model . fit ( X_train , y_train ) if is_finetuned and save_checkpoint_path : self . _save_checkpoint ( model , save_checkpoint_path ) logger . info ( f \"[TuningManager] Saved fine-tuned checkpoint to { save_checkpoint_path } \" ) model = self . load_checkpoint ( model , save_checkpoint_path , map_location = \"cuda\" if torch . cuda . is_available () else \"cpu\" ) logger . info ( \"[TuningManager] Reloaded fine-tuned weights into model for inference\" ) if isinstance ( model , torch . nn . Module ): model . eval () elif hasattr ( model , 'model' ): model . model . eval () elif hasattr ( model , 'model_' ): model . model_ . eval () logger . info ( \"[TuningManager] Reloaded fine-tuned weights and set model to eval mode\" ) return model def _maybe_save_epoch_ckpt ( self , model , ckpt_dir , ckpt_epochs , epoch , prefix ): if ckpt_dir and ( epoch in ckpt_epochs ): fname = f \" { prefix } _epoch { epoch } .pt\" path = os . path . join ( ckpt_dir , fname ) self . _save_checkpoint ( model , path ) def _save_checkpoint ( self , model , path : str ): logger . info ( f \"[TuningManager] Saving model checkpoint to { path } \" ) torch_model = None if hasattr ( model , 'model_' ): # For TabPFN, TabICL, OrionMSP, OrionBix torch_model = model . model_ elif hasattr ( model , 'model' ): # For ContextTab, TabDPT torch_model = model . model elif isinstance ( model , torch . nn . Module ): # For Mitra torch_model = model if torch_model : try : # Ensure path is a string here! if not isinstance ( path , str ): raise ValueError ( \"Checkpoint path must be a string\" ) torch . save ( torch_model . state_dict (), path ) logger . info ( f \"[TuningManager] Checkpoint saved successfully to { path } \" ) except Exception as e : logger . error ( f \"[TuningManager] Failed to save checkpoint: { e } \" ) else : logger . warning ( f \"[TuningManager] No compatible torch model found to save checkpoint\" ) def load_checkpoint ( self , model , ckpt_path : str , map_location = 'cpu' ): \"\"\"Loads a checkpoint automatically to correct submodule.\"\"\" if not os . path . exists ( ckpt_path ): logger . warning ( f \"[TuningManager] Checkpoint path { ckpt_path } not found\" ) return model state = torch . load ( ckpt_path , map_location = map_location ) state_dict = state . get ( 'model_state_dict' , state ) candidates = [ getattr ( model , 'model_' , None ), getattr ( model , 'model' , None ), model ] for candidate in candidates : if isinstance ( candidate , torch . nn . Module ): try : candidate . load_state_dict ( state_dict , strict = False ) logger . info ( f \"[TuningManager] Loaded checkpoint weights into { type ( candidate ) . __name__ } \" ) return model except Exception as e : logger . warning ( f \"[TuningManager] Could not load into { type ( candidate ) . __name__ } : { e } \" ) logger . error ( \"[TuningManager] Failed to load weights into model\" ) return model def _full_finetune_model ( self , model , X_train , y_train , params = None , processor = None , peft_config = None ): \"\"\" Performs a standard full fine-tuning loop. This has been refactored to use the model's own tokenizer for batch preparation, ensuring correctness. \"\"\" logger . info ( f \"[TuningManager] Starting full fine-tuning for { type ( model ) . __name__ } \" ) config = { \"device\" : \"cuda\" if torch . cuda . is_available () else \"cpu\" , \"epochs\" : 5 , \"learning_rate\" : 1e-4 , \"batch_size\" : 128 , \"show_progress\" : True } if params : config . update ( params ) logger . debug ( f \"[TuningManager] Using fine-tuning config: { config } \" ) is_contexttab = isinstance ( model , ConTextTabClassifier ) torch_model = model . model device = torch . device ( config [ \"device\" ]) torch_model . to ( device ) torch_model . train () for param in torch_model . parameters (): param . data = param . data . to ( device ) if is_contexttab : logger . info ( \"[TuningManager] Fitting the ConTextTab wrapper to set its data context\" ) model . fit ( X_train , y_train ) if peft_config : logger . warning ( \"[TuningManager] WARNING: ConTextTab PEFT support is currently experimental and may cause prediction issues\" ) logger . warning ( \"[TuningManager] ConTextTab's complex embedding pipeline may conflict with LoRA adapters\" ) logger . info ( \"[TuningManager] RECOMMENDATION: Use 'base-ft' strategy for ConTextTab instead of 'peft'\" ) logger . info ( \"[TuningManager] FALLBACK: Proceeding with standard base fine-tuning\" ) peft_config = None # Disable PEFT for ConTextTab optimizer = Adam ( torch_model . parameters (), lr = config [ \"learning_rate\" ]) loss_fn = torch . nn . CrossEntropyLoss () # Create a simple dataset of indices dataset = TensorDataset ( torch . arange ( len ( X_train ))) dataloader = DataLoader ( dataset , batch_size = config [ \"batch_size\" ], shuffle = True ) for epoch in range ( 1 , config [ \"epochs\" ] + 1 ): iterable = dataloader if config [ \"show_progress\" ]: iterable = tqdm ( dataloader , desc = f \"Finetuning Epoch { epoch } \" ) for batch_indices in iterable : # Get the raw data for the current batch if hasattr ( X_train , 'iloc' ): # DataFrame X_batch_raw = X_train . iloc [ batch_indices [ 0 ] . numpy ()] y_batch_raw = y_train . iloc [ batch_indices [ 0 ] . numpy ()] else : # numpy array X_batch_raw = X_train [ batch_indices [ 0 ] . numpy ()] y_batch_raw = y_train [ batch_indices [ 0 ] . numpy ()] optimizer . zero_grad () if is_contexttab : # Use the model's own tokenizer to prepare the batch # This guarantees the correct format. data_batch = model . get_tokenized_data ( X_batch_raw , bagging_index = epoch ) # Move tensors to the correct device for k , v in data_batch . items (): if isinstance ( v , torch . Tensor ): data_batch [ k ] = v . to ( device ) elif isinstance ( v , dict ): # Handle nested dicts like \u2060\u202fdata['data']\u202f\u2060 for k_inner , v_inner in v . items (): if isinstance ( v_inner , torch . Tensor ): v [ k_inner ] = v_inner . to ( device ) y_batch = data_batch [ 'data' ][ 'target' ] # Ensure y_batch is Long type for cross-entropy loss (ContextTab may return Float) if y_batch . dtype != torch . long : y_batch = y_batch . long () logits = torch_model ( ** data_batch ) else : # Fallback for other potential models X_batch_processed , y_batch_processed = processor . transform ( X_batch_raw , y_batch_raw ) X_batch = torch . tensor ( X_batch_processed , dtype = torch . float32 ) . to ( device ) y_batch = torch . tensor ( y_batch_processed , dtype = torch . long ) . to ( device ) logits = torch_model ( X_batch ) loss = loss_fn ( logits , y_batch ) loss . backward () optimizer . step () if config [ \"show_progress\" ]: iterable . set_postfix ( loss = f \" { loss . item () : .4f } \" ) logger . info ( \"[TuningManager] Full fine-tuning complete\" ) def _finetune_tabpfn ( self , model : TabPFNClassifier , X_train_processed : pd . DataFrame , y_train_processed : pd . Series , params : dict | None = None , peft_config = None ): logger . info ( \"[TuningManager] Starting advanced TabPFN fine-tuning\" ) config = { \"device\" : \"cuda\" if torch . cuda . is_available () else \"cpu\" , \"epochs\" : 3 , \"learning_rate\" : 1e-5 , \"batch_size\" : 256 , \"show_progress\" : True } if params : config . update ( params ) logger . debug ( f \"[TuningManager] Using fine-tuning config: { config } \" ) device = torch . device ( config [ \"device\" ]) model . model_ . to ( device ) for param in model . model_ . parameters (): param . data = param . data . to ( device ) if peft_config : logger . warning ( \"[TuningManager] WARNING: TabPFN PEFT support is currently experimental and unstable\" ) logger . warning ( \"[TuningManager] TabPFN's batched inference engine conflicts with LoRA adapter state\" ) logger . info ( \"[TuningManager] RECOMMENDATION: Use 'base-ft' strategy for TabPFN instead of 'peft'\" ) logger . info ( \"[TuningManager] FALLBACK: Proceeding with standard base fine-tuning\" ) peft_config = None # Disable PEFT for TabPFN optimizer = Adam ( model . model_ . parameters (), lr = config [ \"learning_rate\" ]) loss_function = torch . nn . CrossEntropyLoss () def stratified_splitter ( X , y ): \"\"\" A robust splitter that attempts to stratify and falls back gracefully. \"\"\" # Check if the target is multiclass and has at least 2 samples per class y_series = pd . Series ( y ) if y_series . nunique () > 1 and y_series . value_counts () . min () > 1 : # If stratification is possible, use it. return train_test_split ( X , y , test_size = 0.3 , stratify = y , random_state = 42 ) else : # Otherwise, use a standard random split. return train_test_split ( X , y , test_size = 0.3 , random_state = 42 ) # Use our new, robust splitter function directly. splitter = stratified_splitter #splitter = partial(train_test_split, test_size=0.3, stratify=None) training_datasets = model . get_preprocessed_datasets ( X_train_processed , y_train_processed , splitter , config [ \"batch_size\" ] ) finetuning_dataloader = DataLoader ( training_datasets , batch_size = 1 , collate_fn = meta_dataset_collator ) for epoch in range ( 1 , config [ \"epochs\" ] + 1 ): iterable = finetuning_dataloader if config [ \"show_progress\" ]: iterable = tqdm ( finetuning_dataloader , desc = f \"Finetuning Epoch { epoch } \" ) def _move_to_device ( item , target_device : torch . device ): if isinstance ( item , torch . Tensor ): return item . to ( target_device ) if isinstance ( item , list ): return [ _move_to_device ( x , target_device ) for x in item ] if isinstance ( item , tuple ): return tuple ( _move_to_device ( x , target_device ) for x in item ) if isinstance ( item , dict ): return { k : _move_to_device ( v , target_device ) for k , v in item . items ()} return item for ( X_train_batch , X_test_batch , y_train_batch , y_test_batch , cat_ixs , confs ) in iterable : if len ( np . unique ( y_train_batch )) != len ( np . unique ( y_test_batch )): logger . debug ( \"[TuningManager] Skipping batch with inconsistent number of classes between train and test splits\" ) continue X_train_batch = _move_to_device ( X_train_batch , device ) y_train_batch = _move_to_device ( y_train_batch , device ) X_test_batch = _move_to_device ( X_test_batch , device ) y_test_batch = _move_to_device ( y_test_batch , device ) optimizer . zero_grad () model . fit_from_preprocessed ( X_train_batch , y_train_batch , cat_ixs , confs ) predictions = model . forward ( X_test_batch , return_logits = True ) if isinstance ( predictions , torch . Tensor ) and predictions . device != device : predictions = predictions . to ( device ) # y_test_batch has already been moved above; in rare cases where it is a list # choose the first element (batch_size == 1 in our collator) if isinstance ( y_test_batch , list ) and len ( y_test_batch ) > 0 and isinstance ( y_test_batch [ 0 ], torch . Tensor ): target = y_test_batch [ 0 ] else : target = y_test_batch loss = loss_function ( predictions , target ) loss . backward () optimizer . step () if config [ \"show_progress\" ]: iterable . set_postfix ( loss = f \" { loss . item () : .4f } \" ) model . batched = False logger . info ( \"[TuningManager] Fine-tuning complete\" ) logger . debug ( \"[TuningManager] Setting fine-tuned model context for inference...\" ) #model.fit(X_train_processed, y_train_processed) def _finetune_tabpfn_pure_sft ( self , model : TabPFNClassifier , X_train_processed : pd . DataFrame , y_train_processed : pd . Series , params : dict | None = None , peft_config = None ): \"\"\" Performs SFT-style finetuning. This is different from the meta-learning loop by: 1. Using the *entire* dataset to create ONE single, large (Support, Query) episode. 2. Training repeatedly over this single episode for multiple epochs. This forces the model to specialize on the single task derived from the full dataset, giving the \"SFT sense\". \"\"\" import torch import numpy as np import pandas as pd from torch.optim import Adam from torch.utils.data import DataLoader from tqdm import tqdm from sklearn.model_selection import train_test_split from sklearn.preprocessing import LabelEncoder # This collator is required by the TabPFN API try : from ..models.tabpfn.utils import meta_dataset_collator except ImportError : logger . error ( \"[TuningManager] FATAL: meta_dataset_collator not found. Please fix the import path\" ) # Define a minimal fallback if import fails def meta_dataset_collator ( batch ): return batch [ 0 ] logger . warning ( \"[TuningManager] Using a placeholder meta_dataset_collator. This may fail\" ) # Helper to move tensors def _move_to_device ( item , target_device : torch . device ): if isinstance ( item , torch . Tensor ): return item . to ( target_device ) if isinstance ( item , list ): return [ _move_to_device ( x , target_device ) for x in item ] if isinstance ( item , tuple ): return tuple ( _move_to_device ( x , target_device ) for x in item ) if isinstance ( item , dict ): return { k : _move_to_device ( v , target_device ) for k , v in item . items ()} return item logger . info ( \"[TuningManager] Starting TabPFN SFT fine-tuning\" ) config = { \"device\" : \"cuda\" if torch . cuda . is_available () else \"cpu\" , \"epochs\" : 25 , # More epochs needed as we only have one \"batch\" \"learning_rate\" : 1e-5 , \"show_progress\" : True , \"max_episode_size\" : len ( X_train_processed ), \"query_set_ratio\" : 0.3 , \"weight_decay\" : 1e-4 } if params : # Allow user to override SFT defaults config . update ( params ) # Ensure max_episode_size isn't accidentally overridden by 'batch_size' if 'batch_size' in params : logger . warning ( \"[TuningManager] Ignoring 'batch_size' param, using 'max_episode_size' for SFT\" ) config . pop ( 'batch_size' , None ) logger . debug ( f \"[TuningManager] Using SFT-style config: { config } \" ) device = torch . device ( config [ \"device\" ]) model . model_ . to ( device ) model . model_ . train () # Set to train mode for param in model . model_ . parameters (): param . data = param . data . to ( device ) if peft_config : logger . warning ( \"[TuningManager] TabPFN PEFT not supported, falling back to base fine-tuning\" ) peft_config = None optimizer = Adam ( model . model_ . parameters (), lr = config [ \"learning_rate\" ], weight_decay = config [ \"weight_decay\" ]) loss_function = torch . nn . CrossEntropyLoss () # --- Data & Label Preprocessing --- # (This section is the same as the meta-learning function) if isinstance ( X_train_processed , pd . DataFrame ): X_train_processed_np = X_train_processed . to_numpy () else : X_train_processed_np = X_train_processed if isinstance ( y_train_processed , ( pd . Series , pd . DataFrame )): y_train_processed_np = y_train_processed . to_numpy () else : y_train_processed_np = y_train_processed if y_train_processed_np . dtype == object or not np . issubdtype ( y_train_processed_np . dtype , np . number ): logger . info ( \"[TuningManager] Converting non-numeric labels...\" ) le = LabelEncoder () y_train_processed_np = le . fit_transform ( y_train_processed_np ) if not hasattr ( model , 'label_encoder_' ): model . label_encoder_ = le def sft_episode_splitter ( X , y ): y_series = pd . Series ( y ) test_size = config [ \"query_set_ratio\" ] if y_series . nunique () > 1 and y_series . value_counts () . min () > 1 : return train_test_split ( X , y , test_size = test_size , stratify = y , random_state = 42 ) else : return train_test_split ( X , y , test_size = test_size , random_state = 42 ) logger . info ( f \"[TuningManager] Creating a single SFT task from { len ( X_train_processed_np ) } samples...\" ) training_datasets = model . get_preprocessed_datasets ( X_train_processed_np , y_train_processed_np , sft_episode_splitter , config [ \"max_episode_size\" ] # <-- This makes it ONE episode ) episode_dataloader = DataLoader ( training_datasets , batch_size = 1 , collate_fn = meta_dataset_collator , shuffle = False ) for epoch in range ( 1 , config [ \"epochs\" ] + 1 ): iterable = tqdm ( episode_dataloader , desc = f \"SFT Epoch { epoch } \" , leave = False ) epoch_losses = [] for ( X_support , X_query , y_support , y_query , cat_ixs , confs ) in iterable : if len ( np . unique ( y_support )) != len ( np . unique ( y_query )): logger . warning ( \"[TuningManager] Skipping epoch: Inconsistent classes in SFT split\" ) continue X_support = _move_to_device ( X_support , device ) y_support = _move_to_device ( y_support , device ) X_query = _move_to_device ( X_query , device ) y_query = _move_to_device ( y_query , device ) optimizer . zero_grad () # 1. Set the (large) Support Set as the prompt model . fit_from_preprocessed ( X_support , y_support , cat_ixs , confs ) # 2. Predict on the (large) Query Set predictions = model . forward ( X_query , return_logits = True ) if isinstance ( predictions , torch . Tensor ) and predictions . device != device : predictions = predictions . to ( device ) target = y_query [ 0 ] if isinstance ( y_query , list ) else y_query # 3. Calculate loss and backpropagate loss = loss_function ( predictions , target ) loss . backward () # SFT HINT 4: Add gradient clipping for stability torch . nn . utils . clip_grad_norm_ ( model . model_ . parameters (), max_norm = 1.0 ) optimizer . step () epoch_losses . append ( loss . item ()) iterable . set_postfix ( loss = f \" { loss . item () : .4f } \" ) avg_loss = np . mean ( epoch_losses ) if epoch_losses else float ( 'nan' ) logger . info ( f \"[TuningManager] Epoch [ { epoch } / { config [ 'epochs' ] } ]: Task Loss = { avg_loss : .4f } \" ) model . batched = False model . model_ . eval () logger . info ( \"[TuningManager] SFT-style finetuning complete\" ) return model def _finetune_tabicl ( self , model : ( TabICLClassifier , OrionMSPClassifier , OrionBixClassifier ), X_train_processed : np . ndarray , y_train_processed : np . ndarray , params : dict | None = None , peft_config = None ): logger . info ( \"[TuningManager] Starting advanced TabICL/OrionMSP/OrionBix fine-tuning\" ) config = { \"device\" : \"cuda\" if torch . cuda . is_available () else \"cpu\" , \"epochs\" : 5 , \"learning_rate\" : 2e-6 , \"show_progress\" : True , \"support_size\" : 48 , \"query_size\" : 32 , \"n_episodes\" : 1000 } if params : config . update ( params ) logger . debug ( f \"[TuningManager] Using fine-tuning config: { config } \" ) model . fit ( X_train_processed , y_train_processed ) model . _load_model () model . fit ( X_train_processed , y_train_processed ) device = torch . device ( config [ \"device\" ]) if peft_config : try : if isinstance ( model , OrionBixClassifier ): model_key = \"OrionBix\" elif isinstance ( model , OrionMSPClassifier ): model_key = \"OrionMSP\" else : model_key = \"TabICL\" model . model_ = apply_tabular_lora ( model_key , model . model_ , peft_config ) logger . info ( f \"[TuningManager] PEFT SUCCESS: Applied LoRA adapters to { model_key } model\" ) except Exception as e : logger . warning ( f \"[TuningManager] PEFT FAILED: TabICL/OrionMSP/OrionBix incompatible with PEFT: { e } \" ) logger . info ( \"[TuningManager] FALLBACK: Proceeding with base fine-tuning (fully supported)\" ) model . model_ . to ( device ) model . model_ . train () # --- discover the true logits width from a safe 1-class probe --- C_out = None with torch . no_grad (): # make one tiny 1-class episode from the first few rows X_np = X_train_processed if isinstance ( X_train_processed , np . ndarray ) else X_train_processed . to_numpy () y_np = y_train_processed if isinstance ( y_train_processed , np . ndarray ) else y_train_processed . to_numpy () # pick a class that has >= (support_size + query_size) examples; fall back to any class s_sz = int ( config . get ( \"support_size\" , 48 )) q_sz = int ( config . get ( \"query_size\" , 32 )) need = s_sz + q_sz cls , idx = None , None for c in np . unique ( y_np ): cand = np . nonzero ( y_np == c )[ 0 ] if cand . size >= need : idx = cand [: need ] cls = c break if idx is None : idx = np . arange ( min ( need , len ( y_np ))) cls = y_np [ idx [ 0 ]] X_ep = torch . from_numpy ( X_np [ idx ]) . float () . unsqueeze ( 0 ) . to ( device ) # [1, S+Q, F] ys = torch . full (( s_sz ,), 0 , dtype = torch . long , device = device ) # all support -> class 0 # pack as your forward expects: first S as support, rest as query logits_probe = model . model_ ( X_ep , ys . unsqueeze ( 0 )) # [1, Q, C_eff] typically C_out = int ( logits_probe . squeeze ( 0 ) . size ( - 1 )) # safety if C_out <= 0 : raise RuntimeError ( \"Could not infer logits width (C_out).\" ) for param in model . model_ . parameters (): param . data = param . data . to ( device ) optimizer = Adam ( model . model_ . parameters (), lr = config [ \"learning_rate\" ]) loss_fn = torch . nn . CrossEntropyLoss () meta_dataset = TabICLMetaDataset ( X_train_processed , y_train_processed , support_size = int ( config . get ( \"support_size\" , 48 )), query_size = int ( config . get ( \"query_size\" , 32 )), n_episodes = int ( config . get ( \"n_episodes\" , 1000 )) ) dataloader = DataLoader ( meta_dataset , batch_size = 1 , shuffle = True ) for epoch in range ( 1 , config [ \"epochs\" ] + 1 ): iterable = dataloader if config [ \"show_progress\" ]: iterable = tqdm ( dataloader , desc = f \"Finetuning Epoch { epoch } \" ) for X_episode , y_support , y_query in iterable : X_episode , y_support , y_query = X_episode . to ( device ), y_support . to ( device ), y_query . to ( device ) optimizer . zero_grad () ys = y_support . squeeze ( 0 ) . long () yq = y_query . squeeze ( 0 ) . long () supp = torch . unique ( ys ) # keep at most C_out classes so the head can represent them keep = supp [: C_out ] # build map only for kept classes; others -> -1 (excluded) yq_m = torch . full_like ( yq , - 1 ) ys_m = torch . full_like ( ys , - 1 ) for i , c in enumerate ( keep ): ys_m [ ys == c ] = i yq_m [ yq == c ] = i # prune support rows that were dropped keep_mask = ( ys_m >= 0 ) if not keep_mask . any (): continue ys_m = ys_m [ keep_mask ] X_support_kept = X_episode [:, : ys . shape [ 0 ], :][:, keep_mask , :] X_query_part = X_episode [:, ys . shape [ 0 ]:, :] X_episode = torch . cat ([ X_support_kept , X_query_part ], dim = 1 ) # if any query label was excluded, skip this episode (avoids OOB gathers) if ( yq_m < 0 ) . any (): continue # forward with episodic labels (contiguous, \u2264 C_out) logits = model . model_ ( X_episode , ys_m . unsqueeze ( 0 )) # [1, Q, <=C_out] logits = logits . squeeze ( 0 ) # [Q, <=C_out] # ensure mapping fits the actual head width (in case adapters changed it mid-run) if logits . size ( - 1 ) < yq_m . max () . item () + 1 : continue # skip this episode if it exceeds head capacity loss = loss_fn ( logits , yq_m ) loss . backward () optimizer . step () if config [ \"show_progress\" ]: iterable . set_postfix ( loss = f \" { loss . item () : .4f } \" ) logger . info ( \"[TuningManager] Fine-tuning complete\" ) def _finetune_tabicl_pure_sft ( self , model : ( TabICLClassifier , OrionMSPClassifier , OrionBixClassifier ) , X_train_processed , y_train_processed , params = None , peft_config = None ): \"\"\" PURE SFT FINE-TUNING (Not Recommended for TabICL) Standard supervised fine-tuning on full batches WITHOUT episodic structure. WARNING: This ignores TabICL's meta-learning design and may: - Reduce generalization to new tasks - Increase catastrophic forgetting - Overfit to the specific target task Use ONLY for: - Benchmarking against traditional fine-tuning - Comparison studies - Tasks where you explicitly want to sacrifice generalization for accuracy \"\"\" logger . warning ( \"[TuningManager] WARNING: Pure SFT on TabICL breaks its meta-learning design\" ) logger . warning ( \"[TuningManager] This approach may reduce generalization to new tasks\" ) logger . info ( \"[TuningManager] RECOMMENDATION: Use episodic or SFT-hybrid instead\" ) logger . info ( \"[TuningManager] PROCEED: Using pure SFT (use only for comparisons)\" ) config = { \"device\" : \"cuda\" if torch . cuda . is_available () else \"cpu\" , \"epochs\" : 10 , \"learning_rate\" : 1e-5 , \"batch_size\" : 32 , \"show_progress\" : True , \"weight_decay\" : 1e-4 , \"warmup_epochs\" : 1 } if params : config . update ( params ) logger . debug ( f \"[TuningManager] Using config: { config } \" ) device = torch . device ( config [ \"device\" ]) model . fit ( X_train_processed , y_train_processed ) model . _load_model () model . model_ . to ( device ) model . model_ . train () C_out = None with torch . no_grad (): # make one tiny 1-class episode from the first few rows X_np = X_train_processed if isinstance ( X_train_processed , np . ndarray ) else X_train_processed . to_numpy () y_np = y_train_processed if isinstance ( y_train_processed , np . ndarray ) else y_train_processed . to_numpy () # pick a class that has >= (support_size + query_size) examples; fall back to any class s_sz = int ( config . get ( \"support_size\" , 48 )) q_sz = int ( config . get ( \"query_size\" , 32 )) need = s_sz + q_sz cls , idx = None , None for c in np . unique ( y_np ): cand = np . nonzero ( y_np == c )[ 0 ] if cand . size >= need : idx = cand [: need ] cls = c break if idx is None : idx = np . arange ( min ( need , len ( y_np ))) cls = y_np [ idx [ 0 ]] X_ep = torch . from_numpy ( X_np [ idx ]) . float () . unsqueeze ( 0 ) . to ( device ) # [1, S+Q, F] ys = torch . full (( s_sz ,), 0 , dtype = torch . long , device = device ) # all support -> class 0 # pack as your forward expects: first S as support, rest as query logits_probe = model . model_ ( X_ep , ys . unsqueeze ( 0 )) # [1, Q, C_eff] typically C_out = int ( logits_probe . squeeze ( 0 ) . size ( - 1 )) # safety if C_out <= 0 : raise RuntimeError ( \"Could not infer logits width (C_out).\" ) for param in model . model_ . parameters (): param . data = param . data . to ( device ) if peft_config : try : model . model_ = apply_tabular_lora ( \"TabICL\" , model . model_ , peft_config ) logger . info ( \"[TuningManager] Applied LoRA adapters to TabICL (pure SFT)\" ) except Exception as e : logger . warning ( f \"[TuningManager] LoRA failed: { e } . Proceeding with base pure SFT fine-tuning\" ) # Create standard supervised dataset dataset = TensorDataset ( torch . from_numpy ( X_train_processed ) . float (), torch . from_numpy ( y_train_processed ) . long () ) dataloader = DataLoader ( dataset , batch_size = config [ \"batch_size\" ], shuffle = True ) optimizer = torch . optim . Adam ( model . model_ . parameters (), lr = config [ \"learning_rate\" ], weight_decay = config [ \"weight_decay\" ]) loss_fn = torch . nn . CrossEntropyLoss () # Optional: Learning rate scheduler total_steps = len ( dataloader ) * config [ \"epochs\" ] warmup_steps = len ( dataloader ) * config [ \"warmup_epochs\" ] def lr_lambda ( current_step ): if current_step < warmup_steps : return float ( current_step ) / float ( max ( 1 , warmup_steps )) return max ( 0.0 , float ( total_steps - current_step ) / float ( max ( 1 , total_steps - warmup_steps ))) scheduler = torch . optim . lr_scheduler . LambdaLR ( optimizer , lr_lambda ) step = 0 for epoch in range ( 1 , config [ \"epochs\" ] + 1 ): iterable = dataloader if config [ \"show_progress\" ]: iterable = tqdm ( dataloader , desc = f \"Pure SFT Epoch { epoch } \" , leave = False ) epoch_loss = 0 for X_batch , y_batch in iterable : X_batch = X_batch . to ( device ) y_batch = y_batch . to ( device ) # split batch into a small pseudo-episode: half support, half query mid = X_batch . size ( 0 ) // 2 X_support , y_support = X_batch [: mid ], y_batch [: mid ] X_query , y_query = X_batch [ mid :], y_batch [ mid :] X_episode = torch . cat ([ X_support , X_query ], dim = 0 ) . unsqueeze ( 0 ) ys = y_support . squeeze ( 0 ) . long () yq = y_query . squeeze ( 0 ) . long () supp = torch . unique ( ys ) # keep at most C_out classes so the head can represent them keep = supp [: C_out ] # build map only for kept classes; others -> -1 (excluded) yq_m = torch . full_like ( yq , - 1 ) ys_m = torch . full_like ( ys , - 1 ) for i , c in enumerate ( keep ): ys_m [ ys == c ] = i yq_m [ yq == c ] = i # prune support rows that were dropped keep_mask = ( ys_m >= 0 ) if not keep_mask . any (): continue ys_m = ys_m [ keep_mask ] X_support_kept = X_episode [:, : ys . shape [ 0 ], :][:, keep_mask , :] X_query_part = X_episode [:, ys . shape [ 0 ]:, :] X_episode = torch . cat ([ X_support_kept , X_query_part ], dim = 1 ) # if any query label was excluded, skip this episode (avoids OOB gathers) if ( yq_m < 0 ) . any (): continue # forward with episodic labels (contiguous, \u2264 C_out) logits = model . model_ ( X_episode , ys_m . unsqueeze ( 0 )) # [1, Q, <=C_out] logits = logits . squeeze ( 0 ) # [Q, <=C_out] # ensure mapping fits the actual head width (in case adapters changed it mid-run) if logits . size ( - 1 ) < yq_m . max () . item () + 1 : continue # skip this episode if it exceeds head capacity loss = loss_fn ( logits , yq_m ) loss . backward () optimizer . step () scheduler . step () epoch_loss += loss . item () step += 1 if config [ \"show_progress\" ]: iterable . set_postfix ( loss = f \" { loss . item () : .4f } \" , lr = f \" { scheduler . get_last_lr () : .2e } \" ) logger . info ( f \"[TuningManager] Epoch { epoch } : Avg Loss = { epoch_loss / len ( dataloader ) : .4f } , \" f \"LR = { scheduler . get_last_lr () : .2e } \" ) logger . warning ( \"[TuningManager] Pure SFT training complete (remember: not recommended for TabICL)\" ) def _finetune_mitra ( self , model , X_train_processed , y_train_processed , params = None , peft_config = None ): \"\"\" Performs episodic fine-tuning for in-context models like Mitra (Tab2D). \"\"\" logger . info ( f \"[TuningManager] Starting episodic fine-tuning for { type ( model ) . __name__ } \" ) config = { \"device\" : \"cuda\" if torch . cuda . is_available () else \"cpu\" , \"epochs\" : 3 , \"learning_rate\" : 1e-5 , \"batch_size\" : 4 , \"support_size\" : 128 , \"query_size\" : 128 , \"steps_per_epoch\" : 50 , \"show_progress\" : True } if params : config . update ( params ) logger . debug ( f \"[TuningManager] Using fine-tuning config: { config } \" ) device = torch . device ( config [ \"device\" ]) if peft_config : try : model = apply_tabular_lora ( \"Mitra\" , model , peft_config ) logger . info ( \"[TuningManager] PEFT SUCCESS: Applied LoRA adapters to Mitra (Tab2D) model\" ) except Exception as e : logger . warning ( f \"[TuningManager] PEFT FAILED: Mitra (Tab2D) incompatible with PEFT: { e } \" ) logger . info ( \"[TuningManager] FALLBACK: Proceeding with base fine-tuning (fully supported)\" ) model . to ( device ) model . train () for param in model . parameters (): param . data = param . data . to ( device ) optimizer = Adam ( model . parameters (), lr = config [ \"learning_rate\" ]) loss_fn = torch . nn . CrossEntropyLoss () n_samples = X_train_processed . shape [ 0 ] episode_size = config [ 'support_size' ] + config [ 'query_size' ] for epoch in range ( 1 , config [ \"epochs\" ] + 1 ): iterable = range ( config [ 'steps_per_epoch' ]) if config [ \"show_progress\" ]: iterable = tqdm ( iterable , desc = f \"Finetuning Epoch { epoch } \" ) for step in iterable : optimizer . zero_grad () X_episodes , y_episodes = [], [] for _ in range ( config [ 'batch_size' ]): # episode size does not exceed available samples if episode_size > n_samples : logger . warning ( f \"[TuningManager] Warning: Episode size ( { episode_size } ) is larger than the dataset size ( { n_samples } ). Using all samples\" ) indices = np . arange ( n_samples ) np . random . shuffle ( indices ) else : indices = np . random . choice ( n_samples , episode_size , replace = False ) X_episodes . append ( X_train_processed [ indices ]) y_episodes . append ( y_train_processed [ indices ]) X_batch = torch . from_numpy ( np . stack ( X_episodes )) . to ( device ) y_batch = torch . from_numpy ( np . stack ( y_episodes )) . long () . to ( device ) s_size = config [ 'support_size' ] X_support , X_query = X_batch [:, : s_size , :], X_batch [:, s_size :, :] y_support , y_query = y_batch [:, : s_size ], y_batch [:, s_size :] b , f = X_support . shape [ 0 ], X_support . shape [ 2 ] padding_features = torch . zeros ( b , f , dtype = torch . bool , device = device ) padding_obs_support = torch . zeros_like ( y_support , dtype = torch . bool , device = device ) padding_obs_query = torch . zeros ( b , X_query . shape [ 1 ], dtype = torch . bool , device = device ) logits = model ( x_support = X_support , y_support = y_support , x_query = X_query , padding_features = padding_features , padding_obs_support = padding_obs_support , padding_obs_query__ = padding_obs_query ) loss = loss_fn ( logits . reshape ( - 1 , logits . size ( - 1 )), y_query . reshape ( - 1 )) loss . backward () optimizer . step () if config [ \"show_progress\" ]: iterable . set_postfix ( loss = f \" { loss . item () : .4f } \" ) logger . info ( \"[TuningManager] Episodic fine-tuning complete\" ) def _finetune_tabdpt ( self , model : TabDPTClassifier , X_train_processed : np . ndarray , y_train_processed : np . ndarray , params : dict | None = None , processor = None , peft_config = None ): \"\"\" Performs episodic fine-tuning for the TabDPT model. \"\"\" logger . info ( f \"[TuningManager] Starting episodic fine-tuning for { type ( model ) . __name__ } \" ) # Determine number of classes from training data num_classes = len ( np . unique ( y_train_processed )) logger . info ( f \"[TuningManager] Detected { num_classes } classes in training data\" ) config = { \"device\" : \"cuda\" if torch . cuda . is_available () else \"cpu\" , \"epochs\" : 5 , \"learning_rate\" : 1e-5 , \"batch_size\" : 8 , \"support_size\" : 512 , \"query_size\" : 256 , \"steps_per_epoch\" : 100 , \"show_progress\" : True } if params : config . update ( params ) logger . debug ( f \"[TuningManager] Using fine-tuning config: { config } \" ) device = torch . device ( config [ \"device\" ]) if peft_config : try : model . model = apply_tabular_lora ( \"TabDPT\" , model . model , peft_config ) logger . info ( \"[TuningManager] PEFT SUCCESS: Applied LoRA to TabDPT model\" ) except Exception as e : logger . warning ( f \"[TuningManager] PEFT not compatible with TabDPT: { e } . Proceeding with base fine-tuning\" ) model . model . to ( device ) model . model . train () for param in model . model . parameters (): param . data = param . data . to ( device ) for buffer in model . model . buffers (): buffer . data = buffer . data . to ( device ) # Also ensure the model's device attribute is updated model . device = str ( device ) # TabDPT now handles projection internally, so only use model parameters trainable_params = list ( model . model . parameters ()) optimizer = torch . optim . Adam ( trainable_params , lr = config [ \"learning_rate\" ]) loss_fn = torch . nn . CrossEntropyLoss () n_samples = X_train_processed . shape [ 0 ] #episode_size = config['support_size'] + config['query_size'] # Compute PCA basis on GPU once, no autograd if getattr ( model , \"feature_reduction\" , \"pca\" ) == \"pca\" and X_train_processed . shape [ 1 ] > model . max_features : with torch . no_grad (): if not hasattr ( model , \"V\" ): x_dev = torch . from_numpy ( X_train_processed ) . to ( device ) . float () q = min ( x_dev . shape [ 0 ], model . max_features ) _ , _ , V = torch . pca_lowrank ( x_dev , q = q ) model . V = V model . V . requires_grad_ ( False ) for epoch in range ( 1 , config [ \"epochs\" ] + 1 ): iterable = range ( config [ 'steps_per_epoch' ]) if config [ \"show_progress\" ]: iterable = tqdm ( iterable , desc = f \"Finetuning Epoch { epoch } \" ) for step in iterable : optimizer . zero_grad () episode_size = config [ 'support_size' ] + config [ 'query_size' ] if episode_size > n_samples : scale = n_samples / float ( episode_size ) s = max ( 1 , int ( config [ 'support_size' ] * scale )) q = max ( 1 , int ( config [ 'query_size' ] * scale )) else : s , q = config [ 'support_size' ], config [ 'query_size' ] indices = np . random . choice ( n_samples , s + q , replace = False ) X_episode = torch . from_numpy ( X_train_processed [ indices ]) . float () . to ( device ) y_episode = torch . from_numpy ( y_train_processed [ indices ]) . long () . to ( device ) # JIT PCA projection on GPU without affecting gradients if getattr ( model , \"feature_reduction\" , \"pca\" ) == \"pca\" and X_episode . shape [ - 1 ] > model . max_features and hasattr ( model , \"V\" ): with torch . no_grad (): X_episode = X_episode @ model . V X_support = X_episode [: s ] . unsqueeze ( 0 ) y_support = y_episode [: s ] . unsqueeze ( 0 ) X_query = X_episode [ s :] . unsqueeze ( 0 ) y_query = y_episode [ s :] # Apply padding to match model's expected feature count X_support = pad_x ( X_support , model . max_features ) X_query = pad_x ( X_query , model . max_features ) x_src = torch . cat ([ X_support , X_query ], dim = 1 ) ys = y_support . squeeze ( 0 ) . long () yq = y_query . long () supp = torch . unique ( ys ) max_id = int ( max ( int ( ys . max ()), int ( yq . max ()))) emap = torch . full (( max_id + 1 ,), - 1 , dtype = torch . long , device = ys . device ) for i , c in enumerate ( supp ): emap [ int ( c )] = i ys_m = emap [ ys ] yq_m = emap [ yq ] # Skip episode if query label isn't in support (avoids OOB inside model/CE) if ( yq_m < 0 ) . any (): continue logits = model . model ( x_src = x_src , y_src = ys_m . unsqueeze ( 0 ) . unsqueeze ( - 1 ) . float (), task = 'cls' ) if logits . dim () == 3 : if logits . size ( 1 ) == 1 : logits = logits [:, 0 , :] elif logits . size ( 0 ) == 1 : logits = logits [ 0 , :, :] else : Q = yq_m . size ( 0 ) logits = logits [ - Q :, 0 , :] elif logits . dim () == 2 : pass elif logits . dim () == 1 : logits = logits . unsqueeze ( 0 ) else : raise ValueError ( f \"Unexpected logits shape { tuple ( logits . shape ) } ; expected 2D or 3D.\" ) # --- Guard CE range and compute loss with EPISODIC targets --- if int ( yq_m . max () . item ()) >= logits . size ( - 1 ): continue loss = loss_fn ( logits , yq_m ) loss . backward () optimizer . step () if config [ \"show_progress\" ]: iterable . set_postfix ( loss = f \" { loss . item () : .4f } \" ) # Clean up: ensure model is in eval mode and on correct device after finetuning model . model . eval () model . model . to ( device ) # Ensure all parameters and buffers are on the correct device for param in model . model . parameters (): param . data = param . data . to ( device ) for buffer in model . model . buffers (): buffer . data = buffer . data . to ( device ) logger . info ( \"[TuningManager] Episodic fine-tuning complete\" ) def _finetune_mitra_pure_sft ( self , model , X_train_processed , y_train_processed , params = None , peft_config = None ): \"\"\" PURE SFT FOR MITRA Unlike TabICL, pure SFT works naturally for Mitra because: 1. Forward method is flexible with sequence dimensions 2. Padding masks handle variable-length sequences 3. Better for task-specific optimization This is suitable when you want to fully optimize for target task accuracy. \"\"\" logger . info ( \"[TuningManager] Starting Mitra Pure SFT Fine-tuning\" ) config = { \"device\" : \"cuda\" if torch . cuda . is_available () else \"cpu\" , \"epochs\" : 5 , \"learning_rate\" : 1e-5 , \"batch_size\" : 128 , \"show_progress\" : True , \"weight_decay\" : 1e-4 , \"warmup_epochs\" : 1 } if params : config . update ( params ) logger . debug ( f \"[TuningManager] Using config: { config } \" ) device = torch . device ( config [ \"device\" ]) model . to ( device ) model . train () for param in model . parameters (): param . data = param . data . to ( device ) if peft_config : try : model = apply_tabular_lora ( \"Mitra\" , model , peft_config ) logger . info ( \"[TuningManager] Applied LoRA adapters to Mitra (pure SFT)\" ) except Exception as e : logger . warning ( f \"[TuningManager] LoRA failed: { e } \" ) # Create dataset dataset = TensorDataset ( torch . from_numpy ( X_train_processed ) . float (), torch . from_numpy ( y_train_processed ) . long () ) dataloader = DataLoader ( dataset , batch_size = config [ \"batch_size\" ], shuffle = True ) optimizer = torch . optim . Adam ( model . parameters (), lr = config [ \"learning_rate\" ], weight_decay = config [ \"weight_decay\" ]) loss_fn = torch . nn . CrossEntropyLoss () # LR scheduler total_steps = len ( dataloader ) * config [ \"epochs\" ] warmup_steps = len ( dataloader ) * config [ \"warmup_epochs\" ] def lr_lambda ( current_step ): if current_step < warmup_steps : return float ( current_step ) / float ( max ( 1 , warmup_steps )) return max ( 0.0 , float ( total_steps - current_step ) / float ( max ( 1 , total_steps - warmup_steps ))) scheduler = torch . optim . lr_scheduler . LambdaLR ( optimizer , lr_lambda ) for epoch in range ( 1 , config [ \"epochs\" ] + 1 ): iterable = dataloader if config [ \"show_progress\" ]: iterable = tqdm ( dataloader , desc = f \"Pure SFT Epoch { epoch } \" , leave = False ) epoch_loss = 0 for X_batch , y_batch in iterable : X_batch = X_batch . to ( device ) y_batch = y_batch . to ( device ) # Convert to episodic format for Mitra # [B, F] -> [B, 1, F] (treat entire batch as query with no support) X_support = X_batch . unsqueeze ( 1 ) y_support = y_batch . unsqueeze ( 1 ) X_query = X_batch . unsqueeze ( 1 ) b , f = X_support . shape [ 0 ], X_support . shape [ 2 ] padding_features = torch . zeros ( b , f , dtype = torch . bool , device = device ) padding_obs_support = torch . zeros_like ( y_support , dtype = torch . bool , device = device ) padding_obs_query = torch . zeros ( b , X_query . shape [ 1 ], dtype = torch . bool , device = device ) optimizer . zero_grad () logits = model ( x_support = X_support , y_support = y_support , x_query = X_query , padding_features = padding_features , padding_obs_support = padding_obs_support , padding_obs_query__ = padding_obs_query ) loss = loss_fn ( logits . reshape ( - 1 , logits . size ( - 1 )), y_batch ) loss . backward () optimizer . step () scheduler . step () epoch_loss += loss . item () if config [ \"show_progress\" ]: iterable . set_postfix ( loss = f \" { loss . item () : .4f } \" ) logger . info ( f \"[TuningManager] Epoch { epoch } : Avg Loss = { epoch_loss / len ( dataloader ) : .4f } \" ) logger . info ( \"[TuningManager] Pure SFT fine-tuning complete\" ) def _finetune_tabdpt_pure_sft ( self , model , X_train_processed , y_train_processed , params = None , processor = None , peft_config = None ): \"\"\" PURE SUPERVISED FINE-TUNING FOR TabDPT Standard batch-wise supervised training without episodic sampling. Works similarly to Mitra's pure SFT approach. Args: model: TabDPTClassifier instance X_train_processed: Preprocessed features (numpy array) y_train_processed: Target labels (numpy array) params: Fine-tuning hyperparameters processor: TabDPT processor with projector peft_config: PEFT configuration (optional) \"\"\" logger . info ( \"[TuningManager] Starting TabDPT Pure Supervised Fine-Tuning\" ) # Normalize labels to contiguous 0..C-1 IDs (prevents CE out-of-range) classes , y_train_processed = np . unique ( y_train_processed , return_inverse = True ) y_train_processed = y_train_processed . astype ( np . int64 ) num_classes = len ( classes ) logger . info ( f \"[TuningManager] Detected { num_classes } classes in training data (contiguous remap)\" ) # (Optional) keep mapping if you need to inverse-transform later model . classes_ = classes config = { \"device\" : \"cuda\" if torch . cuda . is_available () else \"cpu\" , \"epochs\" : 5 , \"learning_rate\" : 2e-5 , \"batch_size\" : 32 , \"show_progress\" : True , \"weight_decay\" : 1e-4 , \"warmup_epochs\" : 1 } if params : config . update ( params ) logger . debug ( f \"[TuningManager] Using config: { config } \" ) device = torch . device ( config [ \"device\" ]) if peft_config : try : model . model = apply_tabular_lora ( \"TabDPT\" , model . model , peft_config ) logger . info ( \"[TuningManager] Applied LoRA adapters to TabDPT (Pure SFT)\" ) except Exception as e : logger . warning ( f \"[TuningManager] PEFT failed: { e } . Proceeding with base fine-tuning\" ) model . model . to ( device ) model . model . train () for param in model . model . parameters (): param . data = param . data . to ( device ) for buffer in model . model . buffers (): buffer . data = buffer . data . to ( device ) model . device = str ( device ) # Compute PCA basis on GPU once, no autograd (only if needed) if getattr ( model , \"feature_reduction\" , \"pca\" ) == \"pca\" and X_train_processed . shape [ 1 ] > model . max_features : with torch . no_grad (): if not hasattr ( model , \"V\" ): x_dev = torch . from_numpy ( X_train_processed ) . to ( device ) . float () q = min ( x_dev . shape [ 0 ], model . max_features ) _ , _ , V = torch . pca_lowrank ( x_dev , q = q ) model . V = V model . V . requires_grad_ ( False ) trainable_params = list ( model . model . parameters ()) if processor and hasattr ( processor , 'custom_preprocessor_' ) and hasattr ( processor . custom_preprocessor_ , 'projector_' ): trainable_params += list ( processor . custom_preprocessor_ . projector_ . parameters ()) logger . info ( \"[TuningManager] Including projector parameters in optimizer\" ) optimizer = torch . optim . Adam ( trainable_params , lr = config [ \"learning_rate\" ], weight_decay = config [ \"weight_decay\" ] ) loss_fn = torch . nn . CrossEntropyLoss () dataset = TensorDataset ( torch . from_numpy ( X_train_processed ) . float (), torch . from_numpy ( y_train_processed ) . long () ) dataloader = DataLoader ( dataset , batch_size = config [ \"batch_size\" ], shuffle = True ) total_steps = len ( dataloader ) * config [ \"epochs\" ] warmup_steps = len ( dataloader ) * config [ \"warmup_epochs\" ] def lr_lambda ( current_step ): if current_step < warmup_steps : return float ( current_step ) / float ( max ( 1 , warmup_steps )) return max ( 0.0 , float ( total_steps - current_step ) / float ( max ( 1 , total_steps - warmup_steps ))) scheduler = torch . optim . lr_scheduler . LambdaLR ( optimizer , lr_lambda ) step = 0 for epoch in range ( 1 , config [ \"epochs\" ] + 1 ): epoch_loss = 0.0 iterable = dataloader if config [ \"show_progress\" ]: iterable = tqdm ( dataloader , desc = f \"Pure SFT Epoch { epoch } \" , leave = False ) for X_batch , y_batch in iterable : X_batch = X_batch . to ( device ) y_batch = y_batch . to ( device ) # JIT PCA projection on GPU without affecting gradients if getattr ( model , \"feature_reduction\" , \"pca\" ) == \"pca\" and X_batch . shape [ - 1 ] > model . max_features and hasattr ( model , \"V\" ): with torch . no_grad (): X_batch = X_batch @ model . V X_support = X_batch . unsqueeze ( 1 ) y_support = y_batch . unsqueeze ( 1 ) X_query = X_batch . unsqueeze ( 1 ) X_support = pad_x ( X_support , model . max_features ) X_query = pad_x ( X_query , model . max_features ) x_src = torch . cat ([ X_support , X_query ], dim = 1 ) optimizer . zero_grad () logits = model . model ( x_src = x_src , y_src = y_support . unsqueeze ( - 1 ) . float (), task = 'cls' ) logits = logits [ ... , : num_classes ] # trim to observed classes cap if logits . dim () == 3 : logits = logits . squeeze ( 0 ) # normalize to [B, C] elif logits . dim () != 2 : raise ValueError ( f \"Unexpected logits shape: { tuple ( logits . shape ) } \" ) # CE requires targets in [0, C-1]; if head width < num_classes, drop OOR rows C_eff = logits . size ( - 1 ) y_batch = y_batch . long () valid = ( y_batch >= 0 ) & ( y_batch < C_eff ) if not valid . all (): # skip this minibatch if nothing valid remains if not valid . any (): continue logits = logits [ valid ] y_batch = y_batch [ valid ] loss = loss_fn ( logits , y_batch ) loss . backward () optimizer . step () scheduler . step () epoch_loss += loss . item () step += 1 if config [ \"show_progress\" ]: iterable . set_postfix ( loss = f \" { loss . item () : .4f } \" , lr = f \" { scheduler . get_last_lr ()[ 0 ] : .2e } \" ) avg_loss = epoch_loss / len ( dataloader ) logger . info ( f \"[TuningManager] Epoch [ { epoch } / { config [ 'epochs' ] } ]: \" f \"Avg Loss = { avg_loss : .4f } , \" f \"LR = { scheduler . get_last_lr ()[ 0 ] : .2e } \" ) model . model . eval () logger . info ( \"[TuningManager] TabDPT Pure Supervised Fine-Tuning Complete\" ) return model def _finetune_tabicl_simple_sft ( self , model , X_train_processed , y_train_processed , params = None , peft_config = None ): \"\"\" TabICL : Convert supervised batches to episodic format \"\"\" config = { 'device' : 'cuda' if torch . cuda . is_available () else 'cpu' , 'epochs' : 5 , 'learning_rate' : 1e-5 , 'batch_size' : 16 , 'show_progress' : True , } if params : config . update ( params ) device = torch . device ( config [ 'device' ]) # Initialize model . fit ( X_train_processed , y_train_processed ) model . _load_model () model . model_ . to ( device ) . train () C_out = None with torch . no_grad (): # make one tiny 1-class episode from the first few rows X_np = X_train_processed if isinstance ( X_train_processed , np . ndarray ) else X_train_processed . to_numpy () y_np = y_train_processed if isinstance ( y_train_processed , np . ndarray ) else y_train_processed . to_numpy () # pick a class that has >= (support_size + query_size) examples; fall back to any class s_sz = int ( config . get ( \"support_size\" , 48 )) q_sz = int ( config . get ( \"query_size\" , 32 )) need = s_sz + q_sz cls , idx = None , None for c in np . unique ( y_np ): cand = np . nonzero ( y_np == c )[ 0 ] if cand . size >= need : idx = cand [: need ] cls = c break if idx is None : idx = np . arange ( min ( need , len ( y_np ))) cls = y_np [ idx [ 0 ]] X_ep = torch . from_numpy ( X_np [ idx ]) . float () . unsqueeze ( 0 ) . to ( device ) # [1, S+Q, F] ys = torch . full (( s_sz ,), 0 , dtype = torch . long , device = device ) # all support -> class 0 # pack as your forward expects: first S as support, rest as query logits_probe = model . model_ ( X_ep , ys . unsqueeze ( 0 )) # [1, Q, C_eff] typically C_out = int ( logits_probe . squeeze ( 0 ) . size ( - 1 )) # safety if C_out <= 0 : raise RuntimeError ( \"Could not infer logits width (C_out).\" ) # Standard dataset dataset = TensorDataset ( torch . from_numpy ( X_train_processed ) . float (), torch . from_numpy ( y_train_processed ) . long () ) dataloader = DataLoader ( dataset , batch_size = config [ 'batch_size' ], shuffle = True ) optimizer = torch . optim . Adam ( model . model_ . parameters (), lr = config [ 'learning_rate' ]) loss_fn = torch . nn . CrossEntropyLoss () for epoch in range ( 1 , config [ 'epochs' ] + 1 ): iterable = tqdm ( dataloader , desc = f \"SFT Epoch { epoch } \" ) if config [ 'show_progress' ] else dataloader epoch_loss = 0 for X_batch , y_batch in iterable : batch_size = X_batch . shape [ 0 ] X_batch = X_batch . to ( device ) y_batch = y_batch . to ( device ) # Split batch in half: first half = support, second half = query mid = batch_size // 2 if mid == 0 : # Skip if batch too small continue X_support = X_batch [: mid ] y_support = y_batch [: mid ] X_query = X_batch [ mid :] y_query = y_batch [ mid :] # Ensure X_support and X_query are 2D [samples, features] before concatenation if X_support . dim () > 2 : X_support = X_support . view ( mid , - 1 ) # Flatten extra dimensions if X_query . dim () > 2 : X_query = X_query . view ( - 1 , X_query . shape [ - 1 ]) # Flatten extra dimensions X_episode = torch . cat ([ X_support , X_query ], dim = 0 ) . unsqueeze ( 0 ) # [1, batch_size, features] ys = y_support . squeeze ( 0 ) . long () if y_support . dim () > 1 else y_support . long () yq = y_query . squeeze ( 0 ) . long () if y_query . dim () > 1 else y_query . long () supp = torch . unique ( ys ) # keep at most C_out classes so the head can represent them keep = supp [: C_out ] # build map only for kept classes; others -> -1 (excluded) yq_m = torch . full_like ( yq , - 1 ) ys_m = torch . full_like ( ys , - 1 ) for i , c in enumerate ( keep ): ys_m [ ys == c ] = i yq_m [ yq == c ] = i # prune support rows that were dropped keep_mask = ( ys_m >= 0 ) if not keep_mask . any (): continue ys_m = ys_m [ keep_mask ] # Use mid directly for support size (before filtering) and apply keep_mask correctly # X_episode shape: [1, batch_size, features], mid is the original support size # Index support samples first, then apply keep_mask to avoid dimension issues X_support_all = X_episode [:, : mid , :] # [1, mid, F] X_support_kept = X_support_all [:, keep_mask , :] # [1, kept_support, F] X_query_part = X_episode [:, mid :, :] # [1, query_size, F] # Ensure both tensors have same number of dimensions (both should be 3D) X_episode = torch . cat ([ X_support_kept , X_query_part ], dim = 1 ) # if any query label was excluded, skip this episode (avoids OOB gathers) if ( yq_m < 0 ) . any (): continue # forward with episodic labels (contiguous, \u2264 C_out) logits = model . model_ ( X_episode , ys_m . unsqueeze ( 0 )) # [1, Q, <=C_out] logits = logits . squeeze ( 0 ) # [Q, <=C_out] # ensure mapping fits the actual head width (in case adapters changed it mid-run) if logits . size ( - 1 ) < yq_m . max () . item () + 1 : continue # skip this episode if it exceeds head capacity loss = loss_fn ( logits , yq_m ) loss . backward () optimizer . step () epoch_loss += loss . item () if config [ 'show_progress' ]: iterable . set_postfix ( loss = f \" { loss . item () : .4f } \" ) logger . info ( f \"[TuningManager] Epoch { epoch } : Loss = { epoch_loss / len ( dataloader ) : .4f } \" ) model . model_ . eval () return model load_checkpoint ( model , ckpt_path , map_location = 'cpu' ) \u00b6 Loads a checkpoint automatically to correct submodule. Source code in tabtune/TuningManager/tuning.py 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 def load_checkpoint ( self , model , ckpt_path : str , map_location = 'cpu' ): \"\"\"Loads a checkpoint automatically to correct submodule.\"\"\" if not os . path . exists ( ckpt_path ): logger . warning ( f \"[TuningManager] Checkpoint path { ckpt_path } not found\" ) return model state = torch . load ( ckpt_path , map_location = map_location ) state_dict = state . get ( 'model_state_dict' , state ) candidates = [ getattr ( model , 'model_' , None ), getattr ( model , 'model' , None ), model ] for candidate in candidates : if isinstance ( candidate , torch . nn . Module ): try : candidate . load_state_dict ( state_dict , strict = False ) logger . info ( f \"[TuningManager] Loaded checkpoint weights into { type ( candidate ) . __name__ } \" ) return model except Exception as e : logger . warning ( f \"[TuningManager] Could not load into { type ( candidate ) . __name__ } : { e } \" ) logger . error ( \"[TuningManager] Failed to load weights into model\" ) return model","title":"TuningManager"},{"location":"api/tuning-manager/#api-tuningmanager","text":"Handles the model adaptation process Source code in tabtune/TuningManager/tuning.py 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755 756 757 758 759 760 761 762 763 764 765 766 767 768 769 770 771 772 773 774 775 776 777 778 779 780 781 782 783 784 785 786 787 788 789 790 791 792 793 794 795 796 797 798 799 800 801 802 803 804 805 806 807 808 809 810 811 812 813 814 815 816 817 818 819 820 821 822 823 824 825 826 827 828 829 830 831 832 833 834 835 836 837 838 839 840 841 842 843 844 845 846 847 848 849 850 851 852 853 854 855 856 857 858 859 860 861 862 863 864 865 866 867 868 869 870 871 872 873 874 875 876 877 878 879 880 881 882 883 884 885 886 887 888 889 890 891 892 893 894 895 896 897 898 899 900 901 902 903 904 905 906 907 908 909 910 911 912 913 914 915 916 917 918 919 920 921 922 923 924 925 926 927 928 929 930 931 932 933 934 935 936 937 938 939 940 941 942 943 944 945 946 947 948 949 950 951 952 953 954 955 956 957 958 959 960 961 962 963 964 965 966 967 968 969 970 971 972 973 974 975 976 977 978 979 980 981 982 983 984 985 986 987 988 989 990 991 992 993 994 995 996 997 998 999 1000 1001 1002 1003 1004 1005 1006 1007 1008 1009 1010 1011 1012 1013 1014 1015 1016 1017 1018 1019 1020 1021 1022 1023 1024 1025 1026 1027 1028 1029 1030 1031 1032 1033 1034 1035 1036 1037 1038 1039 1040 1041 1042 1043 1044 1045 1046 1047 1048 1049 1050 1051 1052 1053 1054 1055 1056 1057 1058 1059 1060 1061 1062 1063 1064 1065 1066 1067 1068 1069 1070 1071 1072 1073 1074 1075 1076 1077 1078 1079 1080 1081 1082 1083 1084 1085 1086 1087 1088 1089 1090 1091 1092 1093 1094 1095 1096 1097 1098 1099 1100 1101 1102 1103 1104 1105 1106 1107 1108 1109 1110 1111 1112 1113 1114 1115 1116 1117 1118 1119 1120 1121 1122 1123 1124 1125 1126 1127 1128 1129 1130 1131 1132 1133 1134 1135 1136 1137 1138 1139 1140 1141 1142 1143 1144 1145 1146 1147 1148 1149 1150 1151 1152 1153 1154 1155 1156 1157 1158 1159 1160 1161 1162 1163 1164 1165 1166 1167 1168 1169 1170 1171 1172 1173 1174 1175 1176 1177 1178 1179 1180 1181 1182 1183 1184 1185 1186 1187 1188 1189 1190 1191 1192 1193 1194 1195 1196 1197 1198 1199 1200 1201 1202 1203 1204 1205 1206 1207 1208 1209 1210 1211 1212 1213 1214 1215 1216 1217 1218 1219 1220 1221 1222 1223 1224 1225 1226 1227 1228 1229 1230 1231 1232 1233 1234 1235 1236 1237 1238 1239 1240 1241 1242 1243 1244 1245 1246 1247 1248 1249 1250 1251 1252 1253 1254 1255 1256 1257 1258 1259 1260 1261 1262 1263 1264 1265 1266 1267 1268 1269 1270 1271 1272 1273 1274 1275 1276 1277 1278 1279 1280 1281 1282 1283 1284 1285 1286 1287 1288 1289 1290 1291 1292 1293 1294 1295 1296 1297 1298 1299 1300 1301 1302 1303 1304 1305 1306 1307 1308 1309 1310 1311 1312 1313 1314 1315 1316 1317 1318 1319 1320 1321 1322 1323 1324 1325 1326 1327 1328 1329 1330 1331 1332 1333 1334 1335 1336 1337 1338 1339 1340 1341 1342 1343 1344 1345 1346 1347 1348 1349 1350 1351 1352 1353 1354 1355 1356 1357 1358 1359 1360 1361 1362 1363 1364 1365 1366 1367 1368 1369 1370 1371 1372 1373 1374 1375 1376 1377 1378 1379 1380 1381 1382 1383 1384 1385 1386 1387 1388 1389 1390 1391 1392 1393 1394 1395 1396 1397 1398 1399 1400 1401 1402 1403 1404 1405 1406 1407 1408 1409 1410 1411 1412 1413 1414 1415 1416 1417 1418 1419 1420 1421 1422 1423 1424 1425 1426 1427 1428 1429 1430 1431 1432 1433 1434 1435 1436 1437 1438 1439 1440 1441 1442 1443 1444 1445 1446 1447 1448 1449 1450 1451 1452 1453 1454 1455 1456 1457 1458 1459 1460 1461 1462 1463 1464 1465 1466 1467 1468 1469 1470 1471 1472 1473 1474 1475 1476 1477 1478 1479 1480 1481 1482 1483 1484 1485 1486 1487 1488 1489 1490 1491 1492 1493 1494 1495 1496 1497 1498 1499 1500 1501 1502 1503 1504 1505 1506 1507 1508 1509 1510 1511 1512 1513 1514 1515 1516 1517 1518 1519 1520 1521 1522 1523 1524 1525 1526 1527 1528 1529 1530 1531 1532 1533 1534 1535 1536 1537 1538 1539 1540 1541 1542 1543 1544 1545 1546 1547 1548 1549 1550 1551 1552 1553 1554 1555 1556 1557 1558 1559 1560 1561 1562 1563 1564 class TuningManager : \"\"\" Handles the model adaptation process \"\"\" def tune ( self , model , X_train , y_train , strategy = 'inference' , params = None , processor = None ): params_copy = dict ( params ) if isinstance ( params , dict ) else {} finetune_mode = params_copy . pop ( 'finetune_mode' , 'meta-learning' ) save_checkpoint_path = params_copy . pop ( 'save_checkpoint_path' , None ) if save_checkpoint_path is None : default_dir = params_copy . get ( \"checkpoint_dir\" , \"./checkpoints\" ) if not os . path . exists ( default_dir ): os . makedirs ( default_dir ) save_checkpoint_path = os . path . join ( default_dir , f \" { type ( model ) . __name__ } _latest.pt\" ) # Strategy selection: accept either explicit 'peft' strategy or finetune_method='peft' finetune_method = params_copy . pop ( 'finetune_method' , None ) peft_config = params_copy . pop ( 'peft_config' , None ) selected_strategy = strategy if strategy == 'finetune' and finetune_method == 'peft' : selected_strategy = 'peft' elif strategy == 'finetune' : selected_strategy = 'base-ft' is_finetuned = False original_is_tab2d = isinstance ( model , Tab2D ) if ( isinstance ( model , Tab2D ) or original_is_tab2d ) and selected_strategy in ( 'finetune' , 'base-ft' , 'peft' ): if finetune_mode == 'sft' : logger . info ( \"[TuningManager] Using Pure SFT for Mitra (task-optimized)\" ) self . _finetune_mitra_pure_sft ( model , X_train , y_train , params = params_copy , peft_config = peft_config ) else : # default: 'meta-learning' logger . info ( \"[TuningManager] Using Episodic Meta-Learning for Mitra (default)\" ) self . _finetune_mitra ( model , X_train , y_train , params = params_copy , peft_config = peft_config ) is_finetuned = True elif isinstance ( model , TabPFNClassifier ) and selected_strategy in ( 'finetune' , 'base-ft' , 'peft' ): if finetune_mode == 'sft' : logger . info ( \"[TuningManager] Using Pure SFT for TabPFN (task-optimized)\" ) self . _finetune_tabpfn_pure_sft ( model , X_train , y_train , params = params_copy , peft_config = peft_config ) else : # default: 'meta-learning' logger . info ( \"[TuningManager] Using Episodic Meta-Learning for TabPFN (default)\" ) self . _finetune_tabpfn ( model , X_train , y_train , params = params_copy , peft_config = peft_config ) is_finetuned = True elif isinstance ( model , ( TabICLClassifier , OrionMSPClassifier , OrionBixClassifier )) and selected_strategy in ( 'finetune' , 'base-ft' , 'peft' ): if finetune_mode == 'meta-learning' : logger . info ( \"[TuningManager] Meta Learning based FT\" ) self . _finetune_tabicl ( model , X_train , y_train , params = params_copy , peft_config = peft_config ) else : logger . info ( \"[TuningManager] Performing SFT\" ) self . _finetune_tabicl_simple_sft ( model , X_train , y_train , params = params_copy , peft_config = peft_config ) is_finetuned = True elif isinstance ( model , ConTextTabClassifier ) and selected_strategy in ( 'finetune' , 'base-ft' , 'peft' ): self . _full_finetune_model ( model , X_train , y_train , params = params_copy , processor = processor , peft_config = peft_config ) is_finetuned = True elif isinstance ( model , TabDPTClassifier ) and selected_strategy in ( 'finetune' , 'base-ft' , 'peft' ): if finetune_mode == 'sft' : logger . info ( \"[TuningManager] Using Pure SFT for TabDPT (task-optimized)\" ) self . _finetune_tabdpt_pure_sft ( model , X_train , y_train , params = params_copy , processor = processor , peft_config = peft_config ) else : # default: 'meta-learning' logger . info ( \"[TuningManager] Using Episodic Meta-Learning for TabDPT (default)\" ) self . _finetune_tabdpt ( model , X_train , y_train , params = params_copy , processor = processor , peft_config = peft_config ) is_finetuned = True elif isinstance ( model , ( Tab2D )) and selected_strategy == 'inference' : logger . info ( \"[TuningManager] In-context learning model in inference mode. No training needed.\" ) pass elif isinstance ( model , ( TabICLClassifier , OrionMSPClassifier , OrionBixClassifier )) and selected_strategy == 'inference' : logger . info ( \"[TuningManager] Applying standard .fit() for TabICL setup (inference mode)\" ) model . fit ( X_train , y_train ) else : logger . info ( \"[TuningManager] Applying standard model fitting (.fit)\" ) model . fit ( X_train , y_train ) if is_finetuned and save_checkpoint_path : self . _save_checkpoint ( model , save_checkpoint_path ) logger . info ( f \"[TuningManager] Saved fine-tuned checkpoint to { save_checkpoint_path } \" ) model = self . load_checkpoint ( model , save_checkpoint_path , map_location = \"cuda\" if torch . cuda . is_available () else \"cpu\" ) logger . info ( \"[TuningManager] Reloaded fine-tuned weights into model for inference\" ) if isinstance ( model , torch . nn . Module ): model . eval () elif hasattr ( model , 'model' ): model . model . eval () elif hasattr ( model , 'model_' ): model . model_ . eval () logger . info ( \"[TuningManager] Reloaded fine-tuned weights and set model to eval mode\" ) return model def _maybe_save_epoch_ckpt ( self , model , ckpt_dir , ckpt_epochs , epoch , prefix ): if ckpt_dir and ( epoch in ckpt_epochs ): fname = f \" { prefix } _epoch { epoch } .pt\" path = os . path . join ( ckpt_dir , fname ) self . _save_checkpoint ( model , path ) def _save_checkpoint ( self , model , path : str ): logger . info ( f \"[TuningManager] Saving model checkpoint to { path } \" ) torch_model = None if hasattr ( model , 'model_' ): # For TabPFN, TabICL, OrionMSP, OrionBix torch_model = model . model_ elif hasattr ( model , 'model' ): # For ContextTab, TabDPT torch_model = model . model elif isinstance ( model , torch . nn . Module ): # For Mitra torch_model = model if torch_model : try : # Ensure path is a string here! if not isinstance ( path , str ): raise ValueError ( \"Checkpoint path must be a string\" ) torch . save ( torch_model . state_dict (), path ) logger . info ( f \"[TuningManager] Checkpoint saved successfully to { path } \" ) except Exception as e : logger . error ( f \"[TuningManager] Failed to save checkpoint: { e } \" ) else : logger . warning ( f \"[TuningManager] No compatible torch model found to save checkpoint\" ) def load_checkpoint ( self , model , ckpt_path : str , map_location = 'cpu' ): \"\"\"Loads a checkpoint automatically to correct submodule.\"\"\" if not os . path . exists ( ckpt_path ): logger . warning ( f \"[TuningManager] Checkpoint path { ckpt_path } not found\" ) return model state = torch . load ( ckpt_path , map_location = map_location ) state_dict = state . get ( 'model_state_dict' , state ) candidates = [ getattr ( model , 'model_' , None ), getattr ( model , 'model' , None ), model ] for candidate in candidates : if isinstance ( candidate , torch . nn . Module ): try : candidate . load_state_dict ( state_dict , strict = False ) logger . info ( f \"[TuningManager] Loaded checkpoint weights into { type ( candidate ) . __name__ } \" ) return model except Exception as e : logger . warning ( f \"[TuningManager] Could not load into { type ( candidate ) . __name__ } : { e } \" ) logger . error ( \"[TuningManager] Failed to load weights into model\" ) return model def _full_finetune_model ( self , model , X_train , y_train , params = None , processor = None , peft_config = None ): \"\"\" Performs a standard full fine-tuning loop. This has been refactored to use the model's own tokenizer for batch preparation, ensuring correctness. \"\"\" logger . info ( f \"[TuningManager] Starting full fine-tuning for { type ( model ) . __name__ } \" ) config = { \"device\" : \"cuda\" if torch . cuda . is_available () else \"cpu\" , \"epochs\" : 5 , \"learning_rate\" : 1e-4 , \"batch_size\" : 128 , \"show_progress\" : True } if params : config . update ( params ) logger . debug ( f \"[TuningManager] Using fine-tuning config: { config } \" ) is_contexttab = isinstance ( model , ConTextTabClassifier ) torch_model = model . model device = torch . device ( config [ \"device\" ]) torch_model . to ( device ) torch_model . train () for param in torch_model . parameters (): param . data = param . data . to ( device ) if is_contexttab : logger . info ( \"[TuningManager] Fitting the ConTextTab wrapper to set its data context\" ) model . fit ( X_train , y_train ) if peft_config : logger . warning ( \"[TuningManager] WARNING: ConTextTab PEFT support is currently experimental and may cause prediction issues\" ) logger . warning ( \"[TuningManager] ConTextTab's complex embedding pipeline may conflict with LoRA adapters\" ) logger . info ( \"[TuningManager] RECOMMENDATION: Use 'base-ft' strategy for ConTextTab instead of 'peft'\" ) logger . info ( \"[TuningManager] FALLBACK: Proceeding with standard base fine-tuning\" ) peft_config = None # Disable PEFT for ConTextTab optimizer = Adam ( torch_model . parameters (), lr = config [ \"learning_rate\" ]) loss_fn = torch . nn . CrossEntropyLoss () # Create a simple dataset of indices dataset = TensorDataset ( torch . arange ( len ( X_train ))) dataloader = DataLoader ( dataset , batch_size = config [ \"batch_size\" ], shuffle = True ) for epoch in range ( 1 , config [ \"epochs\" ] + 1 ): iterable = dataloader if config [ \"show_progress\" ]: iterable = tqdm ( dataloader , desc = f \"Finetuning Epoch { epoch } \" ) for batch_indices in iterable : # Get the raw data for the current batch if hasattr ( X_train , 'iloc' ): # DataFrame X_batch_raw = X_train . iloc [ batch_indices [ 0 ] . numpy ()] y_batch_raw = y_train . iloc [ batch_indices [ 0 ] . numpy ()] else : # numpy array X_batch_raw = X_train [ batch_indices [ 0 ] . numpy ()] y_batch_raw = y_train [ batch_indices [ 0 ] . numpy ()] optimizer . zero_grad () if is_contexttab : # Use the model's own tokenizer to prepare the batch # This guarantees the correct format. data_batch = model . get_tokenized_data ( X_batch_raw , bagging_index = epoch ) # Move tensors to the correct device for k , v in data_batch . items (): if isinstance ( v , torch . Tensor ): data_batch [ k ] = v . to ( device ) elif isinstance ( v , dict ): # Handle nested dicts like \u2060\u202fdata['data']\u202f\u2060 for k_inner , v_inner in v . items (): if isinstance ( v_inner , torch . Tensor ): v [ k_inner ] = v_inner . to ( device ) y_batch = data_batch [ 'data' ][ 'target' ] # Ensure y_batch is Long type for cross-entropy loss (ContextTab may return Float) if y_batch . dtype != torch . long : y_batch = y_batch . long () logits = torch_model ( ** data_batch ) else : # Fallback for other potential models X_batch_processed , y_batch_processed = processor . transform ( X_batch_raw , y_batch_raw ) X_batch = torch . tensor ( X_batch_processed , dtype = torch . float32 ) . to ( device ) y_batch = torch . tensor ( y_batch_processed , dtype = torch . long ) . to ( device ) logits = torch_model ( X_batch ) loss = loss_fn ( logits , y_batch ) loss . backward () optimizer . step () if config [ \"show_progress\" ]: iterable . set_postfix ( loss = f \" { loss . item () : .4f } \" ) logger . info ( \"[TuningManager] Full fine-tuning complete\" ) def _finetune_tabpfn ( self , model : TabPFNClassifier , X_train_processed : pd . DataFrame , y_train_processed : pd . Series , params : dict | None = None , peft_config = None ): logger . info ( \"[TuningManager] Starting advanced TabPFN fine-tuning\" ) config = { \"device\" : \"cuda\" if torch . cuda . is_available () else \"cpu\" , \"epochs\" : 3 , \"learning_rate\" : 1e-5 , \"batch_size\" : 256 , \"show_progress\" : True } if params : config . update ( params ) logger . debug ( f \"[TuningManager] Using fine-tuning config: { config } \" ) device = torch . device ( config [ \"device\" ]) model . model_ . to ( device ) for param in model . model_ . parameters (): param . data = param . data . to ( device ) if peft_config : logger . warning ( \"[TuningManager] WARNING: TabPFN PEFT support is currently experimental and unstable\" ) logger . warning ( \"[TuningManager] TabPFN's batched inference engine conflicts with LoRA adapter state\" ) logger . info ( \"[TuningManager] RECOMMENDATION: Use 'base-ft' strategy for TabPFN instead of 'peft'\" ) logger . info ( \"[TuningManager] FALLBACK: Proceeding with standard base fine-tuning\" ) peft_config = None # Disable PEFT for TabPFN optimizer = Adam ( model . model_ . parameters (), lr = config [ \"learning_rate\" ]) loss_function = torch . nn . CrossEntropyLoss () def stratified_splitter ( X , y ): \"\"\" A robust splitter that attempts to stratify and falls back gracefully. \"\"\" # Check if the target is multiclass and has at least 2 samples per class y_series = pd . Series ( y ) if y_series . nunique () > 1 and y_series . value_counts () . min () > 1 : # If stratification is possible, use it. return train_test_split ( X , y , test_size = 0.3 , stratify = y , random_state = 42 ) else : # Otherwise, use a standard random split. return train_test_split ( X , y , test_size = 0.3 , random_state = 42 ) # Use our new, robust splitter function directly. splitter = stratified_splitter #splitter = partial(train_test_split, test_size=0.3, stratify=None) training_datasets = model . get_preprocessed_datasets ( X_train_processed , y_train_processed , splitter , config [ \"batch_size\" ] ) finetuning_dataloader = DataLoader ( training_datasets , batch_size = 1 , collate_fn = meta_dataset_collator ) for epoch in range ( 1 , config [ \"epochs\" ] + 1 ): iterable = finetuning_dataloader if config [ \"show_progress\" ]: iterable = tqdm ( finetuning_dataloader , desc = f \"Finetuning Epoch { epoch } \" ) def _move_to_device ( item , target_device : torch . device ): if isinstance ( item , torch . Tensor ): return item . to ( target_device ) if isinstance ( item , list ): return [ _move_to_device ( x , target_device ) for x in item ] if isinstance ( item , tuple ): return tuple ( _move_to_device ( x , target_device ) for x in item ) if isinstance ( item , dict ): return { k : _move_to_device ( v , target_device ) for k , v in item . items ()} return item for ( X_train_batch , X_test_batch , y_train_batch , y_test_batch , cat_ixs , confs ) in iterable : if len ( np . unique ( y_train_batch )) != len ( np . unique ( y_test_batch )): logger . debug ( \"[TuningManager] Skipping batch with inconsistent number of classes between train and test splits\" ) continue X_train_batch = _move_to_device ( X_train_batch , device ) y_train_batch = _move_to_device ( y_train_batch , device ) X_test_batch = _move_to_device ( X_test_batch , device ) y_test_batch = _move_to_device ( y_test_batch , device ) optimizer . zero_grad () model . fit_from_preprocessed ( X_train_batch , y_train_batch , cat_ixs , confs ) predictions = model . forward ( X_test_batch , return_logits = True ) if isinstance ( predictions , torch . Tensor ) and predictions . device != device : predictions = predictions . to ( device ) # y_test_batch has already been moved above; in rare cases where it is a list # choose the first element (batch_size == 1 in our collator) if isinstance ( y_test_batch , list ) and len ( y_test_batch ) > 0 and isinstance ( y_test_batch [ 0 ], torch . Tensor ): target = y_test_batch [ 0 ] else : target = y_test_batch loss = loss_function ( predictions , target ) loss . backward () optimizer . step () if config [ \"show_progress\" ]: iterable . set_postfix ( loss = f \" { loss . item () : .4f } \" ) model . batched = False logger . info ( \"[TuningManager] Fine-tuning complete\" ) logger . debug ( \"[TuningManager] Setting fine-tuned model context for inference...\" ) #model.fit(X_train_processed, y_train_processed) def _finetune_tabpfn_pure_sft ( self , model : TabPFNClassifier , X_train_processed : pd . DataFrame , y_train_processed : pd . Series , params : dict | None = None , peft_config = None ): \"\"\" Performs SFT-style finetuning. This is different from the meta-learning loop by: 1. Using the *entire* dataset to create ONE single, large (Support, Query) episode. 2. Training repeatedly over this single episode for multiple epochs. This forces the model to specialize on the single task derived from the full dataset, giving the \"SFT sense\". \"\"\" import torch import numpy as np import pandas as pd from torch.optim import Adam from torch.utils.data import DataLoader from tqdm import tqdm from sklearn.model_selection import train_test_split from sklearn.preprocessing import LabelEncoder # This collator is required by the TabPFN API try : from ..models.tabpfn.utils import meta_dataset_collator except ImportError : logger . error ( \"[TuningManager] FATAL: meta_dataset_collator not found. Please fix the import path\" ) # Define a minimal fallback if import fails def meta_dataset_collator ( batch ): return batch [ 0 ] logger . warning ( \"[TuningManager] Using a placeholder meta_dataset_collator. This may fail\" ) # Helper to move tensors def _move_to_device ( item , target_device : torch . device ): if isinstance ( item , torch . Tensor ): return item . to ( target_device ) if isinstance ( item , list ): return [ _move_to_device ( x , target_device ) for x in item ] if isinstance ( item , tuple ): return tuple ( _move_to_device ( x , target_device ) for x in item ) if isinstance ( item , dict ): return { k : _move_to_device ( v , target_device ) for k , v in item . items ()} return item logger . info ( \"[TuningManager] Starting TabPFN SFT fine-tuning\" ) config = { \"device\" : \"cuda\" if torch . cuda . is_available () else \"cpu\" , \"epochs\" : 25 , # More epochs needed as we only have one \"batch\" \"learning_rate\" : 1e-5 , \"show_progress\" : True , \"max_episode_size\" : len ( X_train_processed ), \"query_set_ratio\" : 0.3 , \"weight_decay\" : 1e-4 } if params : # Allow user to override SFT defaults config . update ( params ) # Ensure max_episode_size isn't accidentally overridden by 'batch_size' if 'batch_size' in params : logger . warning ( \"[TuningManager] Ignoring 'batch_size' param, using 'max_episode_size' for SFT\" ) config . pop ( 'batch_size' , None ) logger . debug ( f \"[TuningManager] Using SFT-style config: { config } \" ) device = torch . device ( config [ \"device\" ]) model . model_ . to ( device ) model . model_ . train () # Set to train mode for param in model . model_ . parameters (): param . data = param . data . to ( device ) if peft_config : logger . warning ( \"[TuningManager] TabPFN PEFT not supported, falling back to base fine-tuning\" ) peft_config = None optimizer = Adam ( model . model_ . parameters (), lr = config [ \"learning_rate\" ], weight_decay = config [ \"weight_decay\" ]) loss_function = torch . nn . CrossEntropyLoss () # --- Data & Label Preprocessing --- # (This section is the same as the meta-learning function) if isinstance ( X_train_processed , pd . DataFrame ): X_train_processed_np = X_train_processed . to_numpy () else : X_train_processed_np = X_train_processed if isinstance ( y_train_processed , ( pd . Series , pd . DataFrame )): y_train_processed_np = y_train_processed . to_numpy () else : y_train_processed_np = y_train_processed if y_train_processed_np . dtype == object or not np . issubdtype ( y_train_processed_np . dtype , np . number ): logger . info ( \"[TuningManager] Converting non-numeric labels...\" ) le = LabelEncoder () y_train_processed_np = le . fit_transform ( y_train_processed_np ) if not hasattr ( model , 'label_encoder_' ): model . label_encoder_ = le def sft_episode_splitter ( X , y ): y_series = pd . Series ( y ) test_size = config [ \"query_set_ratio\" ] if y_series . nunique () > 1 and y_series . value_counts () . min () > 1 : return train_test_split ( X , y , test_size = test_size , stratify = y , random_state = 42 ) else : return train_test_split ( X , y , test_size = test_size , random_state = 42 ) logger . info ( f \"[TuningManager] Creating a single SFT task from { len ( X_train_processed_np ) } samples...\" ) training_datasets = model . get_preprocessed_datasets ( X_train_processed_np , y_train_processed_np , sft_episode_splitter , config [ \"max_episode_size\" ] # <-- This makes it ONE episode ) episode_dataloader = DataLoader ( training_datasets , batch_size = 1 , collate_fn = meta_dataset_collator , shuffle = False ) for epoch in range ( 1 , config [ \"epochs\" ] + 1 ): iterable = tqdm ( episode_dataloader , desc = f \"SFT Epoch { epoch } \" , leave = False ) epoch_losses = [] for ( X_support , X_query , y_support , y_query , cat_ixs , confs ) in iterable : if len ( np . unique ( y_support )) != len ( np . unique ( y_query )): logger . warning ( \"[TuningManager] Skipping epoch: Inconsistent classes in SFT split\" ) continue X_support = _move_to_device ( X_support , device ) y_support = _move_to_device ( y_support , device ) X_query = _move_to_device ( X_query , device ) y_query = _move_to_device ( y_query , device ) optimizer . zero_grad () # 1. Set the (large) Support Set as the prompt model . fit_from_preprocessed ( X_support , y_support , cat_ixs , confs ) # 2. Predict on the (large) Query Set predictions = model . forward ( X_query , return_logits = True ) if isinstance ( predictions , torch . Tensor ) and predictions . device != device : predictions = predictions . to ( device ) target = y_query [ 0 ] if isinstance ( y_query , list ) else y_query # 3. Calculate loss and backpropagate loss = loss_function ( predictions , target ) loss . backward () # SFT HINT 4: Add gradient clipping for stability torch . nn . utils . clip_grad_norm_ ( model . model_ . parameters (), max_norm = 1.0 ) optimizer . step () epoch_losses . append ( loss . item ()) iterable . set_postfix ( loss = f \" { loss . item () : .4f } \" ) avg_loss = np . mean ( epoch_losses ) if epoch_losses else float ( 'nan' ) logger . info ( f \"[TuningManager] Epoch [ { epoch } / { config [ 'epochs' ] } ]: Task Loss = { avg_loss : .4f } \" ) model . batched = False model . model_ . eval () logger . info ( \"[TuningManager] SFT-style finetuning complete\" ) return model def _finetune_tabicl ( self , model : ( TabICLClassifier , OrionMSPClassifier , OrionBixClassifier ), X_train_processed : np . ndarray , y_train_processed : np . ndarray , params : dict | None = None , peft_config = None ): logger . info ( \"[TuningManager] Starting advanced TabICL/OrionMSP/OrionBix fine-tuning\" ) config = { \"device\" : \"cuda\" if torch . cuda . is_available () else \"cpu\" , \"epochs\" : 5 , \"learning_rate\" : 2e-6 , \"show_progress\" : True , \"support_size\" : 48 , \"query_size\" : 32 , \"n_episodes\" : 1000 } if params : config . update ( params ) logger . debug ( f \"[TuningManager] Using fine-tuning config: { config } \" ) model . fit ( X_train_processed , y_train_processed ) model . _load_model () model . fit ( X_train_processed , y_train_processed ) device = torch . device ( config [ \"device\" ]) if peft_config : try : if isinstance ( model , OrionBixClassifier ): model_key = \"OrionBix\" elif isinstance ( model , OrionMSPClassifier ): model_key = \"OrionMSP\" else : model_key = \"TabICL\" model . model_ = apply_tabular_lora ( model_key , model . model_ , peft_config ) logger . info ( f \"[TuningManager] PEFT SUCCESS: Applied LoRA adapters to { model_key } model\" ) except Exception as e : logger . warning ( f \"[TuningManager] PEFT FAILED: TabICL/OrionMSP/OrionBix incompatible with PEFT: { e } \" ) logger . info ( \"[TuningManager] FALLBACK: Proceeding with base fine-tuning (fully supported)\" ) model . model_ . to ( device ) model . model_ . train () # --- discover the true logits width from a safe 1-class probe --- C_out = None with torch . no_grad (): # make one tiny 1-class episode from the first few rows X_np = X_train_processed if isinstance ( X_train_processed , np . ndarray ) else X_train_processed . to_numpy () y_np = y_train_processed if isinstance ( y_train_processed , np . ndarray ) else y_train_processed . to_numpy () # pick a class that has >= (support_size + query_size) examples; fall back to any class s_sz = int ( config . get ( \"support_size\" , 48 )) q_sz = int ( config . get ( \"query_size\" , 32 )) need = s_sz + q_sz cls , idx = None , None for c in np . unique ( y_np ): cand = np . nonzero ( y_np == c )[ 0 ] if cand . size >= need : idx = cand [: need ] cls = c break if idx is None : idx = np . arange ( min ( need , len ( y_np ))) cls = y_np [ idx [ 0 ]] X_ep = torch . from_numpy ( X_np [ idx ]) . float () . unsqueeze ( 0 ) . to ( device ) # [1, S+Q, F] ys = torch . full (( s_sz ,), 0 , dtype = torch . long , device = device ) # all support -> class 0 # pack as your forward expects: first S as support, rest as query logits_probe = model . model_ ( X_ep , ys . unsqueeze ( 0 )) # [1, Q, C_eff] typically C_out = int ( logits_probe . squeeze ( 0 ) . size ( - 1 )) # safety if C_out <= 0 : raise RuntimeError ( \"Could not infer logits width (C_out).\" ) for param in model . model_ . parameters (): param . data = param . data . to ( device ) optimizer = Adam ( model . model_ . parameters (), lr = config [ \"learning_rate\" ]) loss_fn = torch . nn . CrossEntropyLoss () meta_dataset = TabICLMetaDataset ( X_train_processed , y_train_processed , support_size = int ( config . get ( \"support_size\" , 48 )), query_size = int ( config . get ( \"query_size\" , 32 )), n_episodes = int ( config . get ( \"n_episodes\" , 1000 )) ) dataloader = DataLoader ( meta_dataset , batch_size = 1 , shuffle = True ) for epoch in range ( 1 , config [ \"epochs\" ] + 1 ): iterable = dataloader if config [ \"show_progress\" ]: iterable = tqdm ( dataloader , desc = f \"Finetuning Epoch { epoch } \" ) for X_episode , y_support , y_query in iterable : X_episode , y_support , y_query = X_episode . to ( device ), y_support . to ( device ), y_query . to ( device ) optimizer . zero_grad () ys = y_support . squeeze ( 0 ) . long () yq = y_query . squeeze ( 0 ) . long () supp = torch . unique ( ys ) # keep at most C_out classes so the head can represent them keep = supp [: C_out ] # build map only for kept classes; others -> -1 (excluded) yq_m = torch . full_like ( yq , - 1 ) ys_m = torch . full_like ( ys , - 1 ) for i , c in enumerate ( keep ): ys_m [ ys == c ] = i yq_m [ yq == c ] = i # prune support rows that were dropped keep_mask = ( ys_m >= 0 ) if not keep_mask . any (): continue ys_m = ys_m [ keep_mask ] X_support_kept = X_episode [:, : ys . shape [ 0 ], :][:, keep_mask , :] X_query_part = X_episode [:, ys . shape [ 0 ]:, :] X_episode = torch . cat ([ X_support_kept , X_query_part ], dim = 1 ) # if any query label was excluded, skip this episode (avoids OOB gathers) if ( yq_m < 0 ) . any (): continue # forward with episodic labels (contiguous, \u2264 C_out) logits = model . model_ ( X_episode , ys_m . unsqueeze ( 0 )) # [1, Q, <=C_out] logits = logits . squeeze ( 0 ) # [Q, <=C_out] # ensure mapping fits the actual head width (in case adapters changed it mid-run) if logits . size ( - 1 ) < yq_m . max () . item () + 1 : continue # skip this episode if it exceeds head capacity loss = loss_fn ( logits , yq_m ) loss . backward () optimizer . step () if config [ \"show_progress\" ]: iterable . set_postfix ( loss = f \" { loss . item () : .4f } \" ) logger . info ( \"[TuningManager] Fine-tuning complete\" ) def _finetune_tabicl_pure_sft ( self , model : ( TabICLClassifier , OrionMSPClassifier , OrionBixClassifier ) , X_train_processed , y_train_processed , params = None , peft_config = None ): \"\"\" PURE SFT FINE-TUNING (Not Recommended for TabICL) Standard supervised fine-tuning on full batches WITHOUT episodic structure. WARNING: This ignores TabICL's meta-learning design and may: - Reduce generalization to new tasks - Increase catastrophic forgetting - Overfit to the specific target task Use ONLY for: - Benchmarking against traditional fine-tuning - Comparison studies - Tasks where you explicitly want to sacrifice generalization for accuracy \"\"\" logger . warning ( \"[TuningManager] WARNING: Pure SFT on TabICL breaks its meta-learning design\" ) logger . warning ( \"[TuningManager] This approach may reduce generalization to new tasks\" ) logger . info ( \"[TuningManager] RECOMMENDATION: Use episodic or SFT-hybrid instead\" ) logger . info ( \"[TuningManager] PROCEED: Using pure SFT (use only for comparisons)\" ) config = { \"device\" : \"cuda\" if torch . cuda . is_available () else \"cpu\" , \"epochs\" : 10 , \"learning_rate\" : 1e-5 , \"batch_size\" : 32 , \"show_progress\" : True , \"weight_decay\" : 1e-4 , \"warmup_epochs\" : 1 } if params : config . update ( params ) logger . debug ( f \"[TuningManager] Using config: { config } \" ) device = torch . device ( config [ \"device\" ]) model . fit ( X_train_processed , y_train_processed ) model . _load_model () model . model_ . to ( device ) model . model_ . train () C_out = None with torch . no_grad (): # make one tiny 1-class episode from the first few rows X_np = X_train_processed if isinstance ( X_train_processed , np . ndarray ) else X_train_processed . to_numpy () y_np = y_train_processed if isinstance ( y_train_processed , np . ndarray ) else y_train_processed . to_numpy () # pick a class that has >= (support_size + query_size) examples; fall back to any class s_sz = int ( config . get ( \"support_size\" , 48 )) q_sz = int ( config . get ( \"query_size\" , 32 )) need = s_sz + q_sz cls , idx = None , None for c in np . unique ( y_np ): cand = np . nonzero ( y_np == c )[ 0 ] if cand . size >= need : idx = cand [: need ] cls = c break if idx is None : idx = np . arange ( min ( need , len ( y_np ))) cls = y_np [ idx [ 0 ]] X_ep = torch . from_numpy ( X_np [ idx ]) . float () . unsqueeze ( 0 ) . to ( device ) # [1, S+Q, F] ys = torch . full (( s_sz ,), 0 , dtype = torch . long , device = device ) # all support -> class 0 # pack as your forward expects: first S as support, rest as query logits_probe = model . model_ ( X_ep , ys . unsqueeze ( 0 )) # [1, Q, C_eff] typically C_out = int ( logits_probe . squeeze ( 0 ) . size ( - 1 )) # safety if C_out <= 0 : raise RuntimeError ( \"Could not infer logits width (C_out).\" ) for param in model . model_ . parameters (): param . data = param . data . to ( device ) if peft_config : try : model . model_ = apply_tabular_lora ( \"TabICL\" , model . model_ , peft_config ) logger . info ( \"[TuningManager] Applied LoRA adapters to TabICL (pure SFT)\" ) except Exception as e : logger . warning ( f \"[TuningManager] LoRA failed: { e } . Proceeding with base pure SFT fine-tuning\" ) # Create standard supervised dataset dataset = TensorDataset ( torch . from_numpy ( X_train_processed ) . float (), torch . from_numpy ( y_train_processed ) . long () ) dataloader = DataLoader ( dataset , batch_size = config [ \"batch_size\" ], shuffle = True ) optimizer = torch . optim . Adam ( model . model_ . parameters (), lr = config [ \"learning_rate\" ], weight_decay = config [ \"weight_decay\" ]) loss_fn = torch . nn . CrossEntropyLoss () # Optional: Learning rate scheduler total_steps = len ( dataloader ) * config [ \"epochs\" ] warmup_steps = len ( dataloader ) * config [ \"warmup_epochs\" ] def lr_lambda ( current_step ): if current_step < warmup_steps : return float ( current_step ) / float ( max ( 1 , warmup_steps )) return max ( 0.0 , float ( total_steps - current_step ) / float ( max ( 1 , total_steps - warmup_steps ))) scheduler = torch . optim . lr_scheduler . LambdaLR ( optimizer , lr_lambda ) step = 0 for epoch in range ( 1 , config [ \"epochs\" ] + 1 ): iterable = dataloader if config [ \"show_progress\" ]: iterable = tqdm ( dataloader , desc = f \"Pure SFT Epoch { epoch } \" , leave = False ) epoch_loss = 0 for X_batch , y_batch in iterable : X_batch = X_batch . to ( device ) y_batch = y_batch . to ( device ) # split batch into a small pseudo-episode: half support, half query mid = X_batch . size ( 0 ) // 2 X_support , y_support = X_batch [: mid ], y_batch [: mid ] X_query , y_query = X_batch [ mid :], y_batch [ mid :] X_episode = torch . cat ([ X_support , X_query ], dim = 0 ) . unsqueeze ( 0 ) ys = y_support . squeeze ( 0 ) . long () yq = y_query . squeeze ( 0 ) . long () supp = torch . unique ( ys ) # keep at most C_out classes so the head can represent them keep = supp [: C_out ] # build map only for kept classes; others -> -1 (excluded) yq_m = torch . full_like ( yq , - 1 ) ys_m = torch . full_like ( ys , - 1 ) for i , c in enumerate ( keep ): ys_m [ ys == c ] = i yq_m [ yq == c ] = i # prune support rows that were dropped keep_mask = ( ys_m >= 0 ) if not keep_mask . any (): continue ys_m = ys_m [ keep_mask ] X_support_kept = X_episode [:, : ys . shape [ 0 ], :][:, keep_mask , :] X_query_part = X_episode [:, ys . shape [ 0 ]:, :] X_episode = torch . cat ([ X_support_kept , X_query_part ], dim = 1 ) # if any query label was excluded, skip this episode (avoids OOB gathers) if ( yq_m < 0 ) . any (): continue # forward with episodic labels (contiguous, \u2264 C_out) logits = model . model_ ( X_episode , ys_m . unsqueeze ( 0 )) # [1, Q, <=C_out] logits = logits . squeeze ( 0 ) # [Q, <=C_out] # ensure mapping fits the actual head width (in case adapters changed it mid-run) if logits . size ( - 1 ) < yq_m . max () . item () + 1 : continue # skip this episode if it exceeds head capacity loss = loss_fn ( logits , yq_m ) loss . backward () optimizer . step () scheduler . step () epoch_loss += loss . item () step += 1 if config [ \"show_progress\" ]: iterable . set_postfix ( loss = f \" { loss . item () : .4f } \" , lr = f \" { scheduler . get_last_lr () : .2e } \" ) logger . info ( f \"[TuningManager] Epoch { epoch } : Avg Loss = { epoch_loss / len ( dataloader ) : .4f } , \" f \"LR = { scheduler . get_last_lr () : .2e } \" ) logger . warning ( \"[TuningManager] Pure SFT training complete (remember: not recommended for TabICL)\" ) def _finetune_mitra ( self , model , X_train_processed , y_train_processed , params = None , peft_config = None ): \"\"\" Performs episodic fine-tuning for in-context models like Mitra (Tab2D). \"\"\" logger . info ( f \"[TuningManager] Starting episodic fine-tuning for { type ( model ) . __name__ } \" ) config = { \"device\" : \"cuda\" if torch . cuda . is_available () else \"cpu\" , \"epochs\" : 3 , \"learning_rate\" : 1e-5 , \"batch_size\" : 4 , \"support_size\" : 128 , \"query_size\" : 128 , \"steps_per_epoch\" : 50 , \"show_progress\" : True } if params : config . update ( params ) logger . debug ( f \"[TuningManager] Using fine-tuning config: { config } \" ) device = torch . device ( config [ \"device\" ]) if peft_config : try : model = apply_tabular_lora ( \"Mitra\" , model , peft_config ) logger . info ( \"[TuningManager] PEFT SUCCESS: Applied LoRA adapters to Mitra (Tab2D) model\" ) except Exception as e : logger . warning ( f \"[TuningManager] PEFT FAILED: Mitra (Tab2D) incompatible with PEFT: { e } \" ) logger . info ( \"[TuningManager] FALLBACK: Proceeding with base fine-tuning (fully supported)\" ) model . to ( device ) model . train () for param in model . parameters (): param . data = param . data . to ( device ) optimizer = Adam ( model . parameters (), lr = config [ \"learning_rate\" ]) loss_fn = torch . nn . CrossEntropyLoss () n_samples = X_train_processed . shape [ 0 ] episode_size = config [ 'support_size' ] + config [ 'query_size' ] for epoch in range ( 1 , config [ \"epochs\" ] + 1 ): iterable = range ( config [ 'steps_per_epoch' ]) if config [ \"show_progress\" ]: iterable = tqdm ( iterable , desc = f \"Finetuning Epoch { epoch } \" ) for step in iterable : optimizer . zero_grad () X_episodes , y_episodes = [], [] for _ in range ( config [ 'batch_size' ]): # episode size does not exceed available samples if episode_size > n_samples : logger . warning ( f \"[TuningManager] Warning: Episode size ( { episode_size } ) is larger than the dataset size ( { n_samples } ). Using all samples\" ) indices = np . arange ( n_samples ) np . random . shuffle ( indices ) else : indices = np . random . choice ( n_samples , episode_size , replace = False ) X_episodes . append ( X_train_processed [ indices ]) y_episodes . append ( y_train_processed [ indices ]) X_batch = torch . from_numpy ( np . stack ( X_episodes )) . to ( device ) y_batch = torch . from_numpy ( np . stack ( y_episodes )) . long () . to ( device ) s_size = config [ 'support_size' ] X_support , X_query = X_batch [:, : s_size , :], X_batch [:, s_size :, :] y_support , y_query = y_batch [:, : s_size ], y_batch [:, s_size :] b , f = X_support . shape [ 0 ], X_support . shape [ 2 ] padding_features = torch . zeros ( b , f , dtype = torch . bool , device = device ) padding_obs_support = torch . zeros_like ( y_support , dtype = torch . bool , device = device ) padding_obs_query = torch . zeros ( b , X_query . shape [ 1 ], dtype = torch . bool , device = device ) logits = model ( x_support = X_support , y_support = y_support , x_query = X_query , padding_features = padding_features , padding_obs_support = padding_obs_support , padding_obs_query__ = padding_obs_query ) loss = loss_fn ( logits . reshape ( - 1 , logits . size ( - 1 )), y_query . reshape ( - 1 )) loss . backward () optimizer . step () if config [ \"show_progress\" ]: iterable . set_postfix ( loss = f \" { loss . item () : .4f } \" ) logger . info ( \"[TuningManager] Episodic fine-tuning complete\" ) def _finetune_tabdpt ( self , model : TabDPTClassifier , X_train_processed : np . ndarray , y_train_processed : np . ndarray , params : dict | None = None , processor = None , peft_config = None ): \"\"\" Performs episodic fine-tuning for the TabDPT model. \"\"\" logger . info ( f \"[TuningManager] Starting episodic fine-tuning for { type ( model ) . __name__ } \" ) # Determine number of classes from training data num_classes = len ( np . unique ( y_train_processed )) logger . info ( f \"[TuningManager] Detected { num_classes } classes in training data\" ) config = { \"device\" : \"cuda\" if torch . cuda . is_available () else \"cpu\" , \"epochs\" : 5 , \"learning_rate\" : 1e-5 , \"batch_size\" : 8 , \"support_size\" : 512 , \"query_size\" : 256 , \"steps_per_epoch\" : 100 , \"show_progress\" : True } if params : config . update ( params ) logger . debug ( f \"[TuningManager] Using fine-tuning config: { config } \" ) device = torch . device ( config [ \"device\" ]) if peft_config : try : model . model = apply_tabular_lora ( \"TabDPT\" , model . model , peft_config ) logger . info ( \"[TuningManager] PEFT SUCCESS: Applied LoRA to TabDPT model\" ) except Exception as e : logger . warning ( f \"[TuningManager] PEFT not compatible with TabDPT: { e } . Proceeding with base fine-tuning\" ) model . model . to ( device ) model . model . train () for param in model . model . parameters (): param . data = param . data . to ( device ) for buffer in model . model . buffers (): buffer . data = buffer . data . to ( device ) # Also ensure the model's device attribute is updated model . device = str ( device ) # TabDPT now handles projection internally, so only use model parameters trainable_params = list ( model . model . parameters ()) optimizer = torch . optim . Adam ( trainable_params , lr = config [ \"learning_rate\" ]) loss_fn = torch . nn . CrossEntropyLoss () n_samples = X_train_processed . shape [ 0 ] #episode_size = config['support_size'] + config['query_size'] # Compute PCA basis on GPU once, no autograd if getattr ( model , \"feature_reduction\" , \"pca\" ) == \"pca\" and X_train_processed . shape [ 1 ] > model . max_features : with torch . no_grad (): if not hasattr ( model , \"V\" ): x_dev = torch . from_numpy ( X_train_processed ) . to ( device ) . float () q = min ( x_dev . shape [ 0 ], model . max_features ) _ , _ , V = torch . pca_lowrank ( x_dev , q = q ) model . V = V model . V . requires_grad_ ( False ) for epoch in range ( 1 , config [ \"epochs\" ] + 1 ): iterable = range ( config [ 'steps_per_epoch' ]) if config [ \"show_progress\" ]: iterable = tqdm ( iterable , desc = f \"Finetuning Epoch { epoch } \" ) for step in iterable : optimizer . zero_grad () episode_size = config [ 'support_size' ] + config [ 'query_size' ] if episode_size > n_samples : scale = n_samples / float ( episode_size ) s = max ( 1 , int ( config [ 'support_size' ] * scale )) q = max ( 1 , int ( config [ 'query_size' ] * scale )) else : s , q = config [ 'support_size' ], config [ 'query_size' ] indices = np . random . choice ( n_samples , s + q , replace = False ) X_episode = torch . from_numpy ( X_train_processed [ indices ]) . float () . to ( device ) y_episode = torch . from_numpy ( y_train_processed [ indices ]) . long () . to ( device ) # JIT PCA projection on GPU without affecting gradients if getattr ( model , \"feature_reduction\" , \"pca\" ) == \"pca\" and X_episode . shape [ - 1 ] > model . max_features and hasattr ( model , \"V\" ): with torch . no_grad (): X_episode = X_episode @ model . V X_support = X_episode [: s ] . unsqueeze ( 0 ) y_support = y_episode [: s ] . unsqueeze ( 0 ) X_query = X_episode [ s :] . unsqueeze ( 0 ) y_query = y_episode [ s :] # Apply padding to match model's expected feature count X_support = pad_x ( X_support , model . max_features ) X_query = pad_x ( X_query , model . max_features ) x_src = torch . cat ([ X_support , X_query ], dim = 1 ) ys = y_support . squeeze ( 0 ) . long () yq = y_query . long () supp = torch . unique ( ys ) max_id = int ( max ( int ( ys . max ()), int ( yq . max ()))) emap = torch . full (( max_id + 1 ,), - 1 , dtype = torch . long , device = ys . device ) for i , c in enumerate ( supp ): emap [ int ( c )] = i ys_m = emap [ ys ] yq_m = emap [ yq ] # Skip episode if query label isn't in support (avoids OOB inside model/CE) if ( yq_m < 0 ) . any (): continue logits = model . model ( x_src = x_src , y_src = ys_m . unsqueeze ( 0 ) . unsqueeze ( - 1 ) . float (), task = 'cls' ) if logits . dim () == 3 : if logits . size ( 1 ) == 1 : logits = logits [:, 0 , :] elif logits . size ( 0 ) == 1 : logits = logits [ 0 , :, :] else : Q = yq_m . size ( 0 ) logits = logits [ - Q :, 0 , :] elif logits . dim () == 2 : pass elif logits . dim () == 1 : logits = logits . unsqueeze ( 0 ) else : raise ValueError ( f \"Unexpected logits shape { tuple ( logits . shape ) } ; expected 2D or 3D.\" ) # --- Guard CE range and compute loss with EPISODIC targets --- if int ( yq_m . max () . item ()) >= logits . size ( - 1 ): continue loss = loss_fn ( logits , yq_m ) loss . backward () optimizer . step () if config [ \"show_progress\" ]: iterable . set_postfix ( loss = f \" { loss . item () : .4f } \" ) # Clean up: ensure model is in eval mode and on correct device after finetuning model . model . eval () model . model . to ( device ) # Ensure all parameters and buffers are on the correct device for param in model . model . parameters (): param . data = param . data . to ( device ) for buffer in model . model . buffers (): buffer . data = buffer . data . to ( device ) logger . info ( \"[TuningManager] Episodic fine-tuning complete\" ) def _finetune_mitra_pure_sft ( self , model , X_train_processed , y_train_processed , params = None , peft_config = None ): \"\"\" PURE SFT FOR MITRA Unlike TabICL, pure SFT works naturally for Mitra because: 1. Forward method is flexible with sequence dimensions 2. Padding masks handle variable-length sequences 3. Better for task-specific optimization This is suitable when you want to fully optimize for target task accuracy. \"\"\" logger . info ( \"[TuningManager] Starting Mitra Pure SFT Fine-tuning\" ) config = { \"device\" : \"cuda\" if torch . cuda . is_available () else \"cpu\" , \"epochs\" : 5 , \"learning_rate\" : 1e-5 , \"batch_size\" : 128 , \"show_progress\" : True , \"weight_decay\" : 1e-4 , \"warmup_epochs\" : 1 } if params : config . update ( params ) logger . debug ( f \"[TuningManager] Using config: { config } \" ) device = torch . device ( config [ \"device\" ]) model . to ( device ) model . train () for param in model . parameters (): param . data = param . data . to ( device ) if peft_config : try : model = apply_tabular_lora ( \"Mitra\" , model , peft_config ) logger . info ( \"[TuningManager] Applied LoRA adapters to Mitra (pure SFT)\" ) except Exception as e : logger . warning ( f \"[TuningManager] LoRA failed: { e } \" ) # Create dataset dataset = TensorDataset ( torch . from_numpy ( X_train_processed ) . float (), torch . from_numpy ( y_train_processed ) . long () ) dataloader = DataLoader ( dataset , batch_size = config [ \"batch_size\" ], shuffle = True ) optimizer = torch . optim . Adam ( model . parameters (), lr = config [ \"learning_rate\" ], weight_decay = config [ \"weight_decay\" ]) loss_fn = torch . nn . CrossEntropyLoss () # LR scheduler total_steps = len ( dataloader ) * config [ \"epochs\" ] warmup_steps = len ( dataloader ) * config [ \"warmup_epochs\" ] def lr_lambda ( current_step ): if current_step < warmup_steps : return float ( current_step ) / float ( max ( 1 , warmup_steps )) return max ( 0.0 , float ( total_steps - current_step ) / float ( max ( 1 , total_steps - warmup_steps ))) scheduler = torch . optim . lr_scheduler . LambdaLR ( optimizer , lr_lambda ) for epoch in range ( 1 , config [ \"epochs\" ] + 1 ): iterable = dataloader if config [ \"show_progress\" ]: iterable = tqdm ( dataloader , desc = f \"Pure SFT Epoch { epoch } \" , leave = False ) epoch_loss = 0 for X_batch , y_batch in iterable : X_batch = X_batch . to ( device ) y_batch = y_batch . to ( device ) # Convert to episodic format for Mitra # [B, F] -> [B, 1, F] (treat entire batch as query with no support) X_support = X_batch . unsqueeze ( 1 ) y_support = y_batch . unsqueeze ( 1 ) X_query = X_batch . unsqueeze ( 1 ) b , f = X_support . shape [ 0 ], X_support . shape [ 2 ] padding_features = torch . zeros ( b , f , dtype = torch . bool , device = device ) padding_obs_support = torch . zeros_like ( y_support , dtype = torch . bool , device = device ) padding_obs_query = torch . zeros ( b , X_query . shape [ 1 ], dtype = torch . bool , device = device ) optimizer . zero_grad () logits = model ( x_support = X_support , y_support = y_support , x_query = X_query , padding_features = padding_features , padding_obs_support = padding_obs_support , padding_obs_query__ = padding_obs_query ) loss = loss_fn ( logits . reshape ( - 1 , logits . size ( - 1 )), y_batch ) loss . backward () optimizer . step () scheduler . step () epoch_loss += loss . item () if config [ \"show_progress\" ]: iterable . set_postfix ( loss = f \" { loss . item () : .4f } \" ) logger . info ( f \"[TuningManager] Epoch { epoch } : Avg Loss = { epoch_loss / len ( dataloader ) : .4f } \" ) logger . info ( \"[TuningManager] Pure SFT fine-tuning complete\" ) def _finetune_tabdpt_pure_sft ( self , model , X_train_processed , y_train_processed , params = None , processor = None , peft_config = None ): \"\"\" PURE SUPERVISED FINE-TUNING FOR TabDPT Standard batch-wise supervised training without episodic sampling. Works similarly to Mitra's pure SFT approach. Args: model: TabDPTClassifier instance X_train_processed: Preprocessed features (numpy array) y_train_processed: Target labels (numpy array) params: Fine-tuning hyperparameters processor: TabDPT processor with projector peft_config: PEFT configuration (optional) \"\"\" logger . info ( \"[TuningManager] Starting TabDPT Pure Supervised Fine-Tuning\" ) # Normalize labels to contiguous 0..C-1 IDs (prevents CE out-of-range) classes , y_train_processed = np . unique ( y_train_processed , return_inverse = True ) y_train_processed = y_train_processed . astype ( np . int64 ) num_classes = len ( classes ) logger . info ( f \"[TuningManager] Detected { num_classes } classes in training data (contiguous remap)\" ) # (Optional) keep mapping if you need to inverse-transform later model . classes_ = classes config = { \"device\" : \"cuda\" if torch . cuda . is_available () else \"cpu\" , \"epochs\" : 5 , \"learning_rate\" : 2e-5 , \"batch_size\" : 32 , \"show_progress\" : True , \"weight_decay\" : 1e-4 , \"warmup_epochs\" : 1 } if params : config . update ( params ) logger . debug ( f \"[TuningManager] Using config: { config } \" ) device = torch . device ( config [ \"device\" ]) if peft_config : try : model . model = apply_tabular_lora ( \"TabDPT\" , model . model , peft_config ) logger . info ( \"[TuningManager] Applied LoRA adapters to TabDPT (Pure SFT)\" ) except Exception as e : logger . warning ( f \"[TuningManager] PEFT failed: { e } . Proceeding with base fine-tuning\" ) model . model . to ( device ) model . model . train () for param in model . model . parameters (): param . data = param . data . to ( device ) for buffer in model . model . buffers (): buffer . data = buffer . data . to ( device ) model . device = str ( device ) # Compute PCA basis on GPU once, no autograd (only if needed) if getattr ( model , \"feature_reduction\" , \"pca\" ) == \"pca\" and X_train_processed . shape [ 1 ] > model . max_features : with torch . no_grad (): if not hasattr ( model , \"V\" ): x_dev = torch . from_numpy ( X_train_processed ) . to ( device ) . float () q = min ( x_dev . shape [ 0 ], model . max_features ) _ , _ , V = torch . pca_lowrank ( x_dev , q = q ) model . V = V model . V . requires_grad_ ( False ) trainable_params = list ( model . model . parameters ()) if processor and hasattr ( processor , 'custom_preprocessor_' ) and hasattr ( processor . custom_preprocessor_ , 'projector_' ): trainable_params += list ( processor . custom_preprocessor_ . projector_ . parameters ()) logger . info ( \"[TuningManager] Including projector parameters in optimizer\" ) optimizer = torch . optim . Adam ( trainable_params , lr = config [ \"learning_rate\" ], weight_decay = config [ \"weight_decay\" ] ) loss_fn = torch . nn . CrossEntropyLoss () dataset = TensorDataset ( torch . from_numpy ( X_train_processed ) . float (), torch . from_numpy ( y_train_processed ) . long () ) dataloader = DataLoader ( dataset , batch_size = config [ \"batch_size\" ], shuffle = True ) total_steps = len ( dataloader ) * config [ \"epochs\" ] warmup_steps = len ( dataloader ) * config [ \"warmup_epochs\" ] def lr_lambda ( current_step ): if current_step < warmup_steps : return float ( current_step ) / float ( max ( 1 , warmup_steps )) return max ( 0.0 , float ( total_steps - current_step ) / float ( max ( 1 , total_steps - warmup_steps ))) scheduler = torch . optim . lr_scheduler . LambdaLR ( optimizer , lr_lambda ) step = 0 for epoch in range ( 1 , config [ \"epochs\" ] + 1 ): epoch_loss = 0.0 iterable = dataloader if config [ \"show_progress\" ]: iterable = tqdm ( dataloader , desc = f \"Pure SFT Epoch { epoch } \" , leave = False ) for X_batch , y_batch in iterable : X_batch = X_batch . to ( device ) y_batch = y_batch . to ( device ) # JIT PCA projection on GPU without affecting gradients if getattr ( model , \"feature_reduction\" , \"pca\" ) == \"pca\" and X_batch . shape [ - 1 ] > model . max_features and hasattr ( model , \"V\" ): with torch . no_grad (): X_batch = X_batch @ model . V X_support = X_batch . unsqueeze ( 1 ) y_support = y_batch . unsqueeze ( 1 ) X_query = X_batch . unsqueeze ( 1 ) X_support = pad_x ( X_support , model . max_features ) X_query = pad_x ( X_query , model . max_features ) x_src = torch . cat ([ X_support , X_query ], dim = 1 ) optimizer . zero_grad () logits = model . model ( x_src = x_src , y_src = y_support . unsqueeze ( - 1 ) . float (), task = 'cls' ) logits = logits [ ... , : num_classes ] # trim to observed classes cap if logits . dim () == 3 : logits = logits . squeeze ( 0 ) # normalize to [B, C] elif logits . dim () != 2 : raise ValueError ( f \"Unexpected logits shape: { tuple ( logits . shape ) } \" ) # CE requires targets in [0, C-1]; if head width < num_classes, drop OOR rows C_eff = logits . size ( - 1 ) y_batch = y_batch . long () valid = ( y_batch >= 0 ) & ( y_batch < C_eff ) if not valid . all (): # skip this minibatch if nothing valid remains if not valid . any (): continue logits = logits [ valid ] y_batch = y_batch [ valid ] loss = loss_fn ( logits , y_batch ) loss . backward () optimizer . step () scheduler . step () epoch_loss += loss . item () step += 1 if config [ \"show_progress\" ]: iterable . set_postfix ( loss = f \" { loss . item () : .4f } \" , lr = f \" { scheduler . get_last_lr ()[ 0 ] : .2e } \" ) avg_loss = epoch_loss / len ( dataloader ) logger . info ( f \"[TuningManager] Epoch [ { epoch } / { config [ 'epochs' ] } ]: \" f \"Avg Loss = { avg_loss : .4f } , \" f \"LR = { scheduler . get_last_lr ()[ 0 ] : .2e } \" ) model . model . eval () logger . info ( \"[TuningManager] TabDPT Pure Supervised Fine-Tuning Complete\" ) return model def _finetune_tabicl_simple_sft ( self , model , X_train_processed , y_train_processed , params = None , peft_config = None ): \"\"\" TabICL : Convert supervised batches to episodic format \"\"\" config = { 'device' : 'cuda' if torch . cuda . is_available () else 'cpu' , 'epochs' : 5 , 'learning_rate' : 1e-5 , 'batch_size' : 16 , 'show_progress' : True , } if params : config . update ( params ) device = torch . device ( config [ 'device' ]) # Initialize model . fit ( X_train_processed , y_train_processed ) model . _load_model () model . model_ . to ( device ) . train () C_out = None with torch . no_grad (): # make one tiny 1-class episode from the first few rows X_np = X_train_processed if isinstance ( X_train_processed , np . ndarray ) else X_train_processed . to_numpy () y_np = y_train_processed if isinstance ( y_train_processed , np . ndarray ) else y_train_processed . to_numpy () # pick a class that has >= (support_size + query_size) examples; fall back to any class s_sz = int ( config . get ( \"support_size\" , 48 )) q_sz = int ( config . get ( \"query_size\" , 32 )) need = s_sz + q_sz cls , idx = None , None for c in np . unique ( y_np ): cand = np . nonzero ( y_np == c )[ 0 ] if cand . size >= need : idx = cand [: need ] cls = c break if idx is None : idx = np . arange ( min ( need , len ( y_np ))) cls = y_np [ idx [ 0 ]] X_ep = torch . from_numpy ( X_np [ idx ]) . float () . unsqueeze ( 0 ) . to ( device ) # [1, S+Q, F] ys = torch . full (( s_sz ,), 0 , dtype = torch . long , device = device ) # all support -> class 0 # pack as your forward expects: first S as support, rest as query logits_probe = model . model_ ( X_ep , ys . unsqueeze ( 0 )) # [1, Q, C_eff] typically C_out = int ( logits_probe . squeeze ( 0 ) . size ( - 1 )) # safety if C_out <= 0 : raise RuntimeError ( \"Could not infer logits width (C_out).\" ) # Standard dataset dataset = TensorDataset ( torch . from_numpy ( X_train_processed ) . float (), torch . from_numpy ( y_train_processed ) . long () ) dataloader = DataLoader ( dataset , batch_size = config [ 'batch_size' ], shuffle = True ) optimizer = torch . optim . Adam ( model . model_ . parameters (), lr = config [ 'learning_rate' ]) loss_fn = torch . nn . CrossEntropyLoss () for epoch in range ( 1 , config [ 'epochs' ] + 1 ): iterable = tqdm ( dataloader , desc = f \"SFT Epoch { epoch } \" ) if config [ 'show_progress' ] else dataloader epoch_loss = 0 for X_batch , y_batch in iterable : batch_size = X_batch . shape [ 0 ] X_batch = X_batch . to ( device ) y_batch = y_batch . to ( device ) # Split batch in half: first half = support, second half = query mid = batch_size // 2 if mid == 0 : # Skip if batch too small continue X_support = X_batch [: mid ] y_support = y_batch [: mid ] X_query = X_batch [ mid :] y_query = y_batch [ mid :] # Ensure X_support and X_query are 2D [samples, features] before concatenation if X_support . dim () > 2 : X_support = X_support . view ( mid , - 1 ) # Flatten extra dimensions if X_query . dim () > 2 : X_query = X_query . view ( - 1 , X_query . shape [ - 1 ]) # Flatten extra dimensions X_episode = torch . cat ([ X_support , X_query ], dim = 0 ) . unsqueeze ( 0 ) # [1, batch_size, features] ys = y_support . squeeze ( 0 ) . long () if y_support . dim () > 1 else y_support . long () yq = y_query . squeeze ( 0 ) . long () if y_query . dim () > 1 else y_query . long () supp = torch . unique ( ys ) # keep at most C_out classes so the head can represent them keep = supp [: C_out ] # build map only for kept classes; others -> -1 (excluded) yq_m = torch . full_like ( yq , - 1 ) ys_m = torch . full_like ( ys , - 1 ) for i , c in enumerate ( keep ): ys_m [ ys == c ] = i yq_m [ yq == c ] = i # prune support rows that were dropped keep_mask = ( ys_m >= 0 ) if not keep_mask . any (): continue ys_m = ys_m [ keep_mask ] # Use mid directly for support size (before filtering) and apply keep_mask correctly # X_episode shape: [1, batch_size, features], mid is the original support size # Index support samples first, then apply keep_mask to avoid dimension issues X_support_all = X_episode [:, : mid , :] # [1, mid, F] X_support_kept = X_support_all [:, keep_mask , :] # [1, kept_support, F] X_query_part = X_episode [:, mid :, :] # [1, query_size, F] # Ensure both tensors have same number of dimensions (both should be 3D) X_episode = torch . cat ([ X_support_kept , X_query_part ], dim = 1 ) # if any query label was excluded, skip this episode (avoids OOB gathers) if ( yq_m < 0 ) . any (): continue # forward with episodic labels (contiguous, \u2264 C_out) logits = model . model_ ( X_episode , ys_m . unsqueeze ( 0 )) # [1, Q, <=C_out] logits = logits . squeeze ( 0 ) # [Q, <=C_out] # ensure mapping fits the actual head width (in case adapters changed it mid-run) if logits . size ( - 1 ) < yq_m . max () . item () + 1 : continue # skip this episode if it exceeds head capacity loss = loss_fn ( logits , yq_m ) loss . backward () optimizer . step () epoch_loss += loss . item () if config [ 'show_progress' ]: iterable . set_postfix ( loss = f \" { loss . item () : .4f } \" ) logger . info ( f \"[TuningManager] Epoch { epoch } : Loss = { epoch_loss / len ( dataloader ) : .4f } \" ) model . model_ . eval () return model","title":"API: TuningManager"},{"location":"api/tuning-manager/#tabtune.TuningManager.tuning.TuningManager.load_checkpoint","text":"Loads a checkpoint automatically to correct submodule. Source code in tabtune/TuningManager/tuning.py 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 def load_checkpoint ( self , model , ckpt_path : str , map_location = 'cpu' ): \"\"\"Loads a checkpoint automatically to correct submodule.\"\"\" if not os . path . exists ( ckpt_path ): logger . warning ( f \"[TuningManager] Checkpoint path { ckpt_path } not found\" ) return model state = torch . load ( ckpt_path , map_location = map_location ) state_dict = state . get ( 'model_state_dict' , state ) candidates = [ getattr ( model , 'model_' , None ), getattr ( model , 'model' , None ), model ] for candidate in candidates : if isinstance ( candidate , torch . nn . Module ): try : candidate . load_state_dict ( state_dict , strict = False ) logger . info ( f \"[TuningManager] Loaded checkpoint weights into { type ( candidate ) . __name__ } \" ) return model except Exception as e : logger . warning ( f \"[TuningManager] Could not load into { type ( candidate ) . __name__ } : { e } \" ) logger . error ( \"[TuningManager] Failed to load weights into model\" ) return model","title":"load_checkpoint"},{"location":"contributing/documentation/","text":"Contributing: Documentation Guide \u00b6 Writing docs \u00b6 Keep intros concise; show runnable examples Prefer ::: path.to.symbol for API pages Cross-link related pages Building docs \u00b6 mkdocs build Style \u00b6 Use sentence case for headings Prefer lists over long paragraphs","title":"Documentation Guide"},{"location":"contributing/documentation/#contributing-documentation-guide","text":"","title":"Contributing: Documentation Guide"},{"location":"contributing/documentation/#writing-docs","text":"Keep intros concise; show runnable examples Prefer ::: path.to.symbol for API pages Cross-link related pages","title":"Writing docs"},{"location":"contributing/documentation/#building-docs","text":"mkdocs build","title":"Building docs"},{"location":"contributing/documentation/#style","text":"Use sentence case for headings Prefer lists over long paragraphs","title":"Style"},{"location":"contributing/new-models/","text":"Contributing: Adding New Models \u00b6 Steps \u00b6 Implement model under tabtune/models/<model_name>/ Add corresponding preprocessor in tabtune/Dataprocess/ Register model in TabularPipeline selection logic Document parameters and defaults Add example usage in docs/examples Checklist \u00b6 Inference path works on a small dataset Optional: fine-tuning via TuningManager Docs updated","title":"Adding New Models"},{"location":"contributing/new-models/#contributing-adding-new-models","text":"","title":"Contributing: Adding New Models"},{"location":"contributing/new-models/#steps","text":"Implement model under tabtune/models/<model_name>/ Add corresponding preprocessor in tabtune/Dataprocess/ Register model in TabularPipeline selection logic Document parameters and defaults Add example usage in docs/examples","title":"Steps"},{"location":"contributing/new-models/#checklist","text":"Inference path works on a small dataset Optional: fine-tuning via TuningManager Docs updated","title":"Checklist"},{"location":"contributing/setup/","text":"Contributing: Development Setup \u00b6 Prerequisites \u00b6 Python 3.10+ Git Setup \u00b6 git clone <your-fork> cd TabTune_Internal python -m venv .venv source .venv/bin/activate pip install -r requirements.txt pip install -r requirements-mkdocs.txt pip install -e . [ dev ] Running docs locally \u00b6 mkdocs serve","title":"Development Setup"},{"location":"contributing/setup/#contributing-development-setup","text":"","title":"Contributing: Development Setup"},{"location":"contributing/setup/#prerequisites","text":"Python 3.10+ Git","title":"Prerequisites"},{"location":"contributing/setup/#setup","text":"git clone <your-fork> cd TabTune_Internal python -m venv .venv source .venv/bin/activate pip install -r requirements.txt pip install -r requirements-mkdocs.txt pip install -e . [ dev ]","title":"Setup"},{"location":"contributing/setup/#running-docs-locally","text":"mkdocs serve","title":"Running docs locally"},{"location":"contributing/standards/","text":"Contributing: Code Standards \u00b6 Follow PEP8; use black and isort Type hints for public APIs Small, focused PRs with clear descriptions Add/adjust docs when changing APIs Prefer pure functions; avoid deep nesting Write minimal tests for new features","title":"Code Standards"},{"location":"contributing/standards/#contributing-code-standards","text":"Follow PEP8; use black and isort Type hints for public APIs Small, focused PRs with clear descriptions Add/adjust docs when changing APIs Prefer pure functions; avoid deep nesting Write minimal tests for new features","title":"Contributing: Code Standards"},{"location":"examples/benchmarking/","text":"Benchmarking with TabularLeaderboard \u00b6 Compare models and strategies on the same splits. from tabtune import TabularLeaderboard leaderboard = TabularLeaderboard ( X_train , X_test , y_train , y_test ) leaderboard . add_model ( model_name = 'TabICL' , tuning_strategy = 'inference' , model_params = { 'n_estimators' : 16 } ) leaderboard . add_model ( model_name = 'TabICL' , tuning_strategy = 'finetune' , model_params = { 'n_estimators' : 16 }, tuning_params = { 'epochs' : 5 , 'learning_rate' : 1e-5 } ) leaderboard . add_model ( model_name = 'TabPFN' , tuning_strategy = 'inference' ) leaderboard . run ( rank_by = 'roc_auc_score' )","title":"Benchmarking"},{"location":"examples/benchmarking/#benchmarking-with-tabularleaderboard","text":"Compare models and strategies on the same splits. from tabtune import TabularLeaderboard leaderboard = TabularLeaderboard ( X_train , X_test , y_train , y_test ) leaderboard . add_model ( model_name = 'TabICL' , tuning_strategy = 'inference' , model_params = { 'n_estimators' : 16 } ) leaderboard . add_model ( model_name = 'TabICL' , tuning_strategy = 'finetune' , model_params = { 'n_estimators' : 16 }, tuning_params = { 'epochs' : 5 , 'learning_rate' : 1e-5 } ) leaderboard . add_model ( model_name = 'TabPFN' , tuning_strategy = 'inference' ) leaderboard . run ( rank_by = 'roc_auc_score' )","title":"Benchmarking with TabularLeaderboard"},{"location":"examples/classification/","text":"Classification Examples: End-to-End Workflows with TabTune \u00b6 This document provides practical, complete examples for classification tasks using TabTune across various scenarios and complexity levels. 1. Quick Start Classification \u00b6 1.1 5-Minute Example \u00b6 Minimal code to get predictions: from tabtune import TabularPipeline from sklearn.datasets import load_breast_cancer from sklearn.model_selection import train_test_split # Load dataset X , y = load_breast_cancer ( return_X_y = True ) X_train , X_test , y_train , y_test = train_test_split ( X , y , test_size = 0.2 ) # Create and train pipeline = TabularPipeline ( model_name = 'TabPFN' , tuning_strategy = 'inference' ) pipeline . fit ( X_train , y_train ) # Predict predictions = pipeline . predict ( X_test ) metrics = pipeline . evaluate ( X_test , y_test ) print ( f \"Accuracy: { metrics [ 'accuracy' ] : .4f } \" ) 1.2 10-Minute Example with Fine-Tuning \u00b6 from tabtune import TabularPipeline import pandas as pd from sklearn.model_selection import train_test_split # Load your data df = pd . read_csv ( 'your_data.csv' ) X = df . drop ( 'target' , axis = 1 ) y = df [ 'target' ] X_train , X_test , y_train , y_test = train_test_split ( X , y , test_size = 0.2 ) # Create pipeline with fine-tuning pipeline = TabularPipeline ( model_name = 'TabICL' , tuning_strategy = 'base-ft' , tuning_params = { 'device' : 'cuda' , 'epochs' : 5 , 'learning_rate' : 2e-5 } ) # Train pipeline . fit ( X_train , y_train ) # Evaluate metrics = pipeline . evaluate ( X_test , y_test ) print ( f \"Accuracy: { metrics [ 'accuracy' ] : .4f } \" ) print ( f \"F1 Score: { metrics [ 'f1_score' ] : .4f } \" ) print ( f \"ROC AUC: { metrics [ 'roc_auc_score' ] : .4f } \" ) 2. Binary Classification \u00b6 2.1 Credit Card Fraud Detection \u00b6 Real-world binary classification example: import pandas as pd import numpy as np from sklearn.model_selection import train_test_split from tabtune import TabularPipeline # Load fraud detection dataset df = pd . read_csv ( 'creditcard.csv' ) # Separate features and target X = df . drop ( 'Class' , axis = 1 ) # Class: 0=normal, 1=fraud y = df [ 'Class' ] print ( f \"Class distribution: { y . value_counts () . to_dict () } \" ) # Split data X_train , X_test , y_train , y_test = train_test_split ( X , y , test_size = 0.2 , random_state = 42 , stratify = y ) # Train pipeline pipeline = TabularPipeline ( model_name = 'TabICL' , tuning_strategy = 'peft' , tuning_params = { 'device' : 'cuda' , 'epochs' : 5 , 'learning_rate' : 2e-4 , 'peft_config' : { 'r' : 8 , 'lora_alpha' : 16 } } ) print ( \"Training...\" ) pipeline . fit ( X_train , y_train ) # Evaluate metrics = pipeline . evaluate ( X_test , y_test ) print ( \" \\n === Results ===\" ) print ( f \"Accuracy: { metrics [ 'accuracy' ] : .4f } \" ) print ( f \"F1 Score: { metrics [ 'f1_score' ] : .4f } \" ) print ( f \"ROC AUC: { metrics [ 'roc_auc_score' ] : .4f } \" ) # Save for deployment pipeline . save ( 'fraud_detection_model.joblib' ) # In production loaded = TabularPipeline . load ( 'fraud_detection_model.joblib' ) new_transactions = pd . read_csv ( 'new_transactions.csv' ) fraud_predictions = loaded . predict ( new_transactions ) 2.2 Customer Churn Prediction \u00b6 import pandas as pd from tabtune import TabularPipeline from sklearn.model_selection import train_test_split # Load customer data df = pd . read_csv ( 'customer_churn.csv' ) # Preprocessing X = df . drop ([ 'CustomerID' , 'Churn' ], axis = 1 ) y = ( df [ 'Churn' ] == 'Yes' ) . astype ( int ) X_train , X_test , y_train , y_test = train_test_split ( X , y , test_size = 0.2 ) # Pipeline pipeline = TabularPipeline ( model_name = 'OrionMSP' , tuning_strategy = 'base-ft' , tuning_params = { 'device' : 'cuda' , 'epochs' : 5 , 'support_size' : 48 , 'query_size' : 32 , 'learning_rate' : 2e-5 } ) pipeline . fit ( X_train , y_train ) metrics = pipeline . evaluate ( X_test , y_test ) print ( f \"Churn Prediction Accuracy: { metrics [ 'accuracy' ] : .4f } \" ) # Identify high-risk customers churn_probs = pipeline . predict_proba ( X_test )[:, 1 ] high_risk = np . where ( churn_probs > 0.7 )[ 0 ] print ( f \"High-risk customers: { len ( high_risk ) } \" ) 3. Multi-Class Classification \u00b6 3.1 Iris Dataset (3-Class) \u00b6 Classic machine learning example: from sklearn.datasets import load_iris from sklearn.model_selection import train_test_split from tabtune import TabularPipeline # Load iris dataset X , y = load_iris ( return_X_y = True ) X_train , X_test , y_train , y_test = train_test_split ( X , y , test_size = 0.2 , random_state = 42 ) # Simple inference baseline print ( \"=== Inference Baseline ===\" ) pipeline_inf = TabularPipeline ( model_name = 'TabPFN' , tuning_strategy = 'inference' ) pipeline_inf . fit ( X_train , y_train ) inf_metrics = pipeline_inf . evaluate ( X_test , y_test ) print ( f \"Inference Accuracy: { inf_metrics [ 'accuracy' ] : .4f } \" ) # Fine-tuned model print ( \" \\n === Fine-Tuned Model ===\" ) pipeline_ft = TabularPipeline ( model_name = 'TabICL' , tuning_strategy = 'base-ft' , tuning_params = { 'device' : 'cuda' , 'epochs' : 5 , 'support_size' : 48 , 'query_size' : 32 } ) pipeline_ft . fit ( X_train , y_train ) ft_metrics = pipeline_ft . evaluate ( X_test , y_test ) print ( f \"Fine-Tuned Accuracy: { ft_metrics [ 'accuracy' ] : .4f } \" ) # Compare improvement = ( ft_metrics [ 'accuracy' ] - inf_metrics [ 'accuracy' ]) * 100 print ( f \" \\n Improvement: + { improvement : .2f } %\" ) 3.2 Multi-Class Document Classification \u00b6 import pandas as pd from tabtune import TabularPipeline from sklearn.feature_extraction.text import TfidfVectorizer from sklearn.model_selection import train_test_split # Load documents df = pd . read_csv ( 'documents.csv' ) # Columns: text, category # Feature extraction from text vectorizer = TfidfVectorizer ( max_features = 500 ) X = vectorizer . fit_transform ( df [ 'text' ]) . toarray () y = pd . factorize ( df [ 'category' ])[ 0 ] X_train , X_test , y_train , y_test = train_test_split ( X , y , test_size = 0.2 ) # Train classifier pipeline = TabularPipeline ( model_name = 'TabICL' , tuning_strategy = 'base-ft' , tuning_params = { 'epochs' : 5 } ) pipeline . fit ( X_train , y_train ) metrics = pipeline . evaluate ( X_test , y_test ) print ( f \"Classification Accuracy: { metrics [ 'accuracy' ] : .4f } \" ) print ( f \"Weighted F1: { metrics [ 'f1_score' ] : .4f } \" ) 5. Cross-Validation \u00b6 5.1 k-Fold Cross-Validation \u00b6 from sklearn.model_selection import StratifiedKFold import numpy as np from tabtune import TabularPipeline def cross_validate ( X , y , model_name , params , k = 5 ): \"\"\"Perform k-fold cross-validation.\"\"\" skf = StratifiedKFold ( n_splits = k , shuffle = True , random_state = 42 ) scores = [] f1_scores = [] for fold_idx , ( train_idx , val_idx ) in enumerate ( skf . split ( X , y )): X_train_fold = X . iloc [ train_idx ] X_val_fold = X . iloc [ val_idx ] y_train_fold = y . iloc [ train_idx ] y_val_fold = y . iloc [ val_idx ] # Train pipeline = TabularPipeline ( model_name = model_name , tuning_strategy = 'base-ft' , tuning_params = params ) pipeline . fit ( X_train_fold , y_train_fold ) metrics = pipeline . evaluate ( X_val_fold , y_val_fold ) scores . append ( metrics [ 'accuracy' ]) f1_scores . append ( metrics [ 'f1_score' ]) print ( f \"Fold { fold_idx + 1 } / { k } : Acc= { metrics [ 'accuracy' ] : .4f } , \" f \"F1= { metrics [ 'f1_score' ] : .4f } \" ) print ( f \" \\n Mean Accuracy: { np . mean ( scores ) : .4f } \u00b1 { np . std ( scores ) : .4f } \" ) print ( f \"Mean F1 Score: { np . mean ( f1_scores ) : .4f } \u00b1 { np . std ( f1_scores ) : .4f } \" ) return scores , f1_scores # Usage import pandas as pd df = pd . read_csv ( 'data.csv' ) X = df . drop ( 'target' , axis = 1 ) y = df [ 'target' ] scores , f1s = cross_validate ( X , y , model_name = 'TabICL' , params = { 'epochs' : 5 }, k = 5 ) 9. Real-World Datasets \u00b6 9.1 Adult Income Dataset \u00b6 Predicting income level (binary classification): import pandas as pd from tabtune import TabularPipeline from sklearn.model_selection import train_test_split import openml # Download from OpenML dataset = openml . datasets . get_dataset ( 1590 ) # Adult dataset X , y , _ , _ = dataset . get_data ( target = dataset . default_target_attribute ) X_train , X_test , y_train , y_test = train_test_split ( X , y , test_size = 0.2 ) # Train pipeline = TabularPipeline ( model_name = 'TabDPT' , tuning_strategy = 'base-ft' , tuning_params = { 'device' : 'cuda' , 'epochs' : 3 , 'support_size' : 1024 } ) pipeline . fit ( X_train , y_train ) metrics = pipeline . evaluate ( X_test , y_test ) print ( f \"Adult Dataset Results:\" ) print ( f \"Accuracy: { metrics [ 'accuracy' ] : .4f } \" ) print ( f \"F1 Score: { metrics [ 'f1_score' ] : .4f } \" ) 9.2 MNIST-Like Tabular Classification \u00b6 from sklearn.datasets import load_digits from tabtune import TabularPipeline from sklearn.model_selection import train_test_split # Load digits (multi-class: 0-9) X , y = load_digits ( return_X_y = True ) X_train , X_test , y_train , y_test = train_test_split ( X , y , test_size = 0.2 ) # Classify pipeline = TabularPipeline ( model_name = 'TabICL' , tuning_strategy = 'peft' , tuning_params = { 'device' : 'cuda' , 'epochs' : 3 , 'peft_config' : { 'r' : 8 } } ) pipeline . fit ( X_train , y_train ) metrics = pipeline . evaluate ( X_test , y_test ) print ( f \"Digit Recognition (0-9):\" ) print ( f \"Accuracy: { metrics [ 'accuracy' ] : .4f } \" ) print ( f \"Classes: { len ( set ( y )) } \" ) 10. Troubleshooting \u00b6 10.1 Common Issues \u00b6 # Issue: Low accuracy # Solution: Try different model and strategy from tabtune import TabularLeaderboard lb = TabularLeaderboard ( X_train , X_val , y_train , y_val ) for model in [ 'TabPFN' , 'TabICL' , 'TabDPT' ]: lb . add_model ( model , 'inference' ) results = lb . run () # Compare # Issue: Out of memory # Solution: Use PEFT pipeline = TabularPipeline ( model_name = 'TabICL' , tuning_strategy = 'peft' , tuning_params = { 'peft_config' : { 'r' : 4 }} ) # Issue: Slow training # Solution: Reduce batch size or use fewer epochs pipeline = TabularPipeline ( tuning_params = { 'batch_size' : 8 , 'epochs' : 3 } ) 11. Next Steps \u00b6 Model Selection Guide - Choose right model Hyperparameter Tuning - Optimize performance TabularLeaderboard - Compare models systematically Saving & Loading - Deploy models These examples cover the full spectrum of classification tasks with TabTune!","title":"Classification Tasks"},{"location":"examples/classification/#classification-examples-end-to-end-workflows-with-tabtune","text":"This document provides practical, complete examples for classification tasks using TabTune across various scenarios and complexity levels.","title":"Classification Examples: End-to-End Workflows with TabTune"},{"location":"examples/classification/#1-quick-start-classification","text":"","title":"1. Quick Start Classification"},{"location":"examples/classification/#11-5-minute-example","text":"Minimal code to get predictions: from tabtune import TabularPipeline from sklearn.datasets import load_breast_cancer from sklearn.model_selection import train_test_split # Load dataset X , y = load_breast_cancer ( return_X_y = True ) X_train , X_test , y_train , y_test = train_test_split ( X , y , test_size = 0.2 ) # Create and train pipeline = TabularPipeline ( model_name = 'TabPFN' , tuning_strategy = 'inference' ) pipeline . fit ( X_train , y_train ) # Predict predictions = pipeline . predict ( X_test ) metrics = pipeline . evaluate ( X_test , y_test ) print ( f \"Accuracy: { metrics [ 'accuracy' ] : .4f } \" )","title":"1.1 5-Minute Example"},{"location":"examples/classification/#12-10-minute-example-with-fine-tuning","text":"from tabtune import TabularPipeline import pandas as pd from sklearn.model_selection import train_test_split # Load your data df = pd . read_csv ( 'your_data.csv' ) X = df . drop ( 'target' , axis = 1 ) y = df [ 'target' ] X_train , X_test , y_train , y_test = train_test_split ( X , y , test_size = 0.2 ) # Create pipeline with fine-tuning pipeline = TabularPipeline ( model_name = 'TabICL' , tuning_strategy = 'base-ft' , tuning_params = { 'device' : 'cuda' , 'epochs' : 5 , 'learning_rate' : 2e-5 } ) # Train pipeline . fit ( X_train , y_train ) # Evaluate metrics = pipeline . evaluate ( X_test , y_test ) print ( f \"Accuracy: { metrics [ 'accuracy' ] : .4f } \" ) print ( f \"F1 Score: { metrics [ 'f1_score' ] : .4f } \" ) print ( f \"ROC AUC: { metrics [ 'roc_auc_score' ] : .4f } \" )","title":"1.2 10-Minute Example with Fine-Tuning"},{"location":"examples/classification/#2-binary-classification","text":"","title":"2. Binary Classification"},{"location":"examples/classification/#21-credit-card-fraud-detection","text":"Real-world binary classification example: import pandas as pd import numpy as np from sklearn.model_selection import train_test_split from tabtune import TabularPipeline # Load fraud detection dataset df = pd . read_csv ( 'creditcard.csv' ) # Separate features and target X = df . drop ( 'Class' , axis = 1 ) # Class: 0=normal, 1=fraud y = df [ 'Class' ] print ( f \"Class distribution: { y . value_counts () . to_dict () } \" ) # Split data X_train , X_test , y_train , y_test = train_test_split ( X , y , test_size = 0.2 , random_state = 42 , stratify = y ) # Train pipeline pipeline = TabularPipeline ( model_name = 'TabICL' , tuning_strategy = 'peft' , tuning_params = { 'device' : 'cuda' , 'epochs' : 5 , 'learning_rate' : 2e-4 , 'peft_config' : { 'r' : 8 , 'lora_alpha' : 16 } } ) print ( \"Training...\" ) pipeline . fit ( X_train , y_train ) # Evaluate metrics = pipeline . evaluate ( X_test , y_test ) print ( \" \\n === Results ===\" ) print ( f \"Accuracy: { metrics [ 'accuracy' ] : .4f } \" ) print ( f \"F1 Score: { metrics [ 'f1_score' ] : .4f } \" ) print ( f \"ROC AUC: { metrics [ 'roc_auc_score' ] : .4f } \" ) # Save for deployment pipeline . save ( 'fraud_detection_model.joblib' ) # In production loaded = TabularPipeline . load ( 'fraud_detection_model.joblib' ) new_transactions = pd . read_csv ( 'new_transactions.csv' ) fraud_predictions = loaded . predict ( new_transactions )","title":"2.1 Credit Card Fraud Detection"},{"location":"examples/classification/#22-customer-churn-prediction","text":"import pandas as pd from tabtune import TabularPipeline from sklearn.model_selection import train_test_split # Load customer data df = pd . read_csv ( 'customer_churn.csv' ) # Preprocessing X = df . drop ([ 'CustomerID' , 'Churn' ], axis = 1 ) y = ( df [ 'Churn' ] == 'Yes' ) . astype ( int ) X_train , X_test , y_train , y_test = train_test_split ( X , y , test_size = 0.2 ) # Pipeline pipeline = TabularPipeline ( model_name = 'OrionMSP' , tuning_strategy = 'base-ft' , tuning_params = { 'device' : 'cuda' , 'epochs' : 5 , 'support_size' : 48 , 'query_size' : 32 , 'learning_rate' : 2e-5 } ) pipeline . fit ( X_train , y_train ) metrics = pipeline . evaluate ( X_test , y_test ) print ( f \"Churn Prediction Accuracy: { metrics [ 'accuracy' ] : .4f } \" ) # Identify high-risk customers churn_probs = pipeline . predict_proba ( X_test )[:, 1 ] high_risk = np . where ( churn_probs > 0.7 )[ 0 ] print ( f \"High-risk customers: { len ( high_risk ) } \" )","title":"2.2 Customer Churn Prediction"},{"location":"examples/classification/#3-multi-class-classification","text":"","title":"3. Multi-Class Classification"},{"location":"examples/classification/#31-iris-dataset-3-class","text":"Classic machine learning example: from sklearn.datasets import load_iris from sklearn.model_selection import train_test_split from tabtune import TabularPipeline # Load iris dataset X , y = load_iris ( return_X_y = True ) X_train , X_test , y_train , y_test = train_test_split ( X , y , test_size = 0.2 , random_state = 42 ) # Simple inference baseline print ( \"=== Inference Baseline ===\" ) pipeline_inf = TabularPipeline ( model_name = 'TabPFN' , tuning_strategy = 'inference' ) pipeline_inf . fit ( X_train , y_train ) inf_metrics = pipeline_inf . evaluate ( X_test , y_test ) print ( f \"Inference Accuracy: { inf_metrics [ 'accuracy' ] : .4f } \" ) # Fine-tuned model print ( \" \\n === Fine-Tuned Model ===\" ) pipeline_ft = TabularPipeline ( model_name = 'TabICL' , tuning_strategy = 'base-ft' , tuning_params = { 'device' : 'cuda' , 'epochs' : 5 , 'support_size' : 48 , 'query_size' : 32 } ) pipeline_ft . fit ( X_train , y_train ) ft_metrics = pipeline_ft . evaluate ( X_test , y_test ) print ( f \"Fine-Tuned Accuracy: { ft_metrics [ 'accuracy' ] : .4f } \" ) # Compare improvement = ( ft_metrics [ 'accuracy' ] - inf_metrics [ 'accuracy' ]) * 100 print ( f \" \\n Improvement: + { improvement : .2f } %\" )","title":"3.1 Iris Dataset (3-Class)"},{"location":"examples/classification/#32-multi-class-document-classification","text":"import pandas as pd from tabtune import TabularPipeline from sklearn.feature_extraction.text import TfidfVectorizer from sklearn.model_selection import train_test_split # Load documents df = pd . read_csv ( 'documents.csv' ) # Columns: text, category # Feature extraction from text vectorizer = TfidfVectorizer ( max_features = 500 ) X = vectorizer . fit_transform ( df [ 'text' ]) . toarray () y = pd . factorize ( df [ 'category' ])[ 0 ] X_train , X_test , y_train , y_test = train_test_split ( X , y , test_size = 0.2 ) # Train classifier pipeline = TabularPipeline ( model_name = 'TabICL' , tuning_strategy = 'base-ft' , tuning_params = { 'epochs' : 5 } ) pipeline . fit ( X_train , y_train ) metrics = pipeline . evaluate ( X_test , y_test ) print ( f \"Classification Accuracy: { metrics [ 'accuracy' ] : .4f } \" ) print ( f \"Weighted F1: { metrics [ 'f1_score' ] : .4f } \" )","title":"3.2 Multi-Class Document Classification"},{"location":"examples/classification/#5-cross-validation","text":"","title":"5. Cross-Validation"},{"location":"examples/classification/#51-k-fold-cross-validation","text":"from sklearn.model_selection import StratifiedKFold import numpy as np from tabtune import TabularPipeline def cross_validate ( X , y , model_name , params , k = 5 ): \"\"\"Perform k-fold cross-validation.\"\"\" skf = StratifiedKFold ( n_splits = k , shuffle = True , random_state = 42 ) scores = [] f1_scores = [] for fold_idx , ( train_idx , val_idx ) in enumerate ( skf . split ( X , y )): X_train_fold = X . iloc [ train_idx ] X_val_fold = X . iloc [ val_idx ] y_train_fold = y . iloc [ train_idx ] y_val_fold = y . iloc [ val_idx ] # Train pipeline = TabularPipeline ( model_name = model_name , tuning_strategy = 'base-ft' , tuning_params = params ) pipeline . fit ( X_train_fold , y_train_fold ) metrics = pipeline . evaluate ( X_val_fold , y_val_fold ) scores . append ( metrics [ 'accuracy' ]) f1_scores . append ( metrics [ 'f1_score' ]) print ( f \"Fold { fold_idx + 1 } / { k } : Acc= { metrics [ 'accuracy' ] : .4f } , \" f \"F1= { metrics [ 'f1_score' ] : .4f } \" ) print ( f \" \\n Mean Accuracy: { np . mean ( scores ) : .4f } \u00b1 { np . std ( scores ) : .4f } \" ) print ( f \"Mean F1 Score: { np . mean ( f1_scores ) : .4f } \u00b1 { np . std ( f1_scores ) : .4f } \" ) return scores , f1_scores # Usage import pandas as pd df = pd . read_csv ( 'data.csv' ) X = df . drop ( 'target' , axis = 1 ) y = df [ 'target' ] scores , f1s = cross_validate ( X , y , model_name = 'TabICL' , params = { 'epochs' : 5 }, k = 5 )","title":"5.1 k-Fold Cross-Validation"},{"location":"examples/classification/#9-real-world-datasets","text":"","title":"9. Real-World Datasets"},{"location":"examples/classification/#91-adult-income-dataset","text":"Predicting income level (binary classification): import pandas as pd from tabtune import TabularPipeline from sklearn.model_selection import train_test_split import openml # Download from OpenML dataset = openml . datasets . get_dataset ( 1590 ) # Adult dataset X , y , _ , _ = dataset . get_data ( target = dataset . default_target_attribute ) X_train , X_test , y_train , y_test = train_test_split ( X , y , test_size = 0.2 ) # Train pipeline = TabularPipeline ( model_name = 'TabDPT' , tuning_strategy = 'base-ft' , tuning_params = { 'device' : 'cuda' , 'epochs' : 3 , 'support_size' : 1024 } ) pipeline . fit ( X_train , y_train ) metrics = pipeline . evaluate ( X_test , y_test ) print ( f \"Adult Dataset Results:\" ) print ( f \"Accuracy: { metrics [ 'accuracy' ] : .4f } \" ) print ( f \"F1 Score: { metrics [ 'f1_score' ] : .4f } \" )","title":"9.1 Adult Income Dataset"},{"location":"examples/classification/#92-mnist-like-tabular-classification","text":"from sklearn.datasets import load_digits from tabtune import TabularPipeline from sklearn.model_selection import train_test_split # Load digits (multi-class: 0-9) X , y = load_digits ( return_X_y = True ) X_train , X_test , y_train , y_test = train_test_split ( X , y , test_size = 0.2 ) # Classify pipeline = TabularPipeline ( model_name = 'TabICL' , tuning_strategy = 'peft' , tuning_params = { 'device' : 'cuda' , 'epochs' : 3 , 'peft_config' : { 'r' : 8 } } ) pipeline . fit ( X_train , y_train ) metrics = pipeline . evaluate ( X_test , y_test ) print ( f \"Digit Recognition (0-9):\" ) print ( f \"Accuracy: { metrics [ 'accuracy' ] : .4f } \" ) print ( f \"Classes: { len ( set ( y )) } \" )","title":"9.2 MNIST-Like Tabular Classification"},{"location":"examples/classification/#10-troubleshooting","text":"","title":"10. Troubleshooting"},{"location":"examples/classification/#101-common-issues","text":"# Issue: Low accuracy # Solution: Try different model and strategy from tabtune import TabularLeaderboard lb = TabularLeaderboard ( X_train , X_val , y_train , y_val ) for model in [ 'TabPFN' , 'TabICL' , 'TabDPT' ]: lb . add_model ( model , 'inference' ) results = lb . run () # Compare # Issue: Out of memory # Solution: Use PEFT pipeline = TabularPipeline ( model_name = 'TabICL' , tuning_strategy = 'peft' , tuning_params = { 'peft_config' : { 'r' : 4 }} ) # Issue: Slow training # Solution: Reduce batch size or use fewer epochs pipeline = TabularPipeline ( tuning_params = { 'batch_size' : 8 , 'epochs' : 3 } )","title":"10.1 Common Issues"},{"location":"examples/classification/#11-next-steps","text":"Model Selection Guide - Choose right model Hyperparameter Tuning - Optimize performance TabularLeaderboard - Compare models systematically Saving & Loading - Deploy models These examples cover the full spectrum of classification tasks with TabTune!","title":"11. Next Steps"},{"location":"examples/peft-examples/","text":"PEFT Examples: Practical Parameter-Efficient Fine-Tuning Workflows \u00b6 This document provides practical, production-ready examples for using LoRA and PEFT techniques with TabTune across various scenarios and constraints. 1. Quick Start PEFT \u00b6 1.1 5-Minute PEFT Example \u00b6 Minimal code to use LoRA: from tabtune import TabularPipeline from sklearn.datasets import load_breast_cancer from sklearn.model_selection import train_test_split # Load dataset X , y = load_breast_cancer ( return_X_y = True ) X_train , X_test , y_train , y_test = train_test_split ( X , y , test_size = 0.2 ) # Create PEFT pipeline pipeline = TabularPipeline ( model_name = 'TabICL' , tuning_strategy = 'peft' , tuning_params = { 'device' : 'cuda' , 'epochs' : 5 , 'learning_rate' : 2e-4 , 'peft_config' : { 'r' : 8 , 'lora_alpha' : 16 , 'lora_dropout' : 0.05 } } ) # Train (much faster and lighter than base-ft) pipeline . fit ( X_train , y_train ) # Evaluate metrics = pipeline . evaluate ( X_test , y_test ) print ( f \"Accuracy: { metrics [ 'accuracy' ] : .4f } \" ) print ( f \"Model size: 1-2% of full model\" ) 1.2 Comparing Base-FT vs PEFT \u00b6 import time import torch from tabtune import TabularPipeline X_train , X_test , y_train , y_test = load_data () # Method 1: Base Fine-Tuning print ( \"=== Base Fine-Tuning ===\" ) torch . cuda . reset_peak_memory_stats () start = time . time () pipeline_base = TabularPipeline ( model_name = 'TabICL' , tuning_strategy = 'base-ft' , tuning_params = { 'epochs' : 5 } ) pipeline_base . fit ( X_train , y_train ) base_time = time . time () - start base_memory = torch . cuda . max_memory_allocated () / 1e9 metrics_base = pipeline_base . evaluate ( X_test , y_test ) print ( f \"Time: { base_time : .1f } s\" ) print ( f \"Memory: { base_memory : .1f } GB\" ) print ( f \"Accuracy: { metrics_base [ 'accuracy' ] : .4f } \" ) # Method 2: PEFT print ( \" \\n === PEFT (LoRA) ===\" ) torch . cuda . reset_peak_memory_stats () start = time . time () pipeline_peft = TabularPipeline ( model_name = 'TabICL' , tuning_strategy = 'peft' , tuning_params = { 'epochs' : 5 , 'peft_config' : { 'r' : 8 } } ) pipeline_peft . fit ( X_train , y_train ) peft_time = time . time () - start peft_memory = torch . cuda . max_memory_allocated () / 1e9 metrics_peft = pipeline_peft . evaluate ( X_test , y_test ) print ( f \"Time: { peft_time : .1f } s\" ) print ( f \"Memory: { peft_memory : .1f } GB\" ) print ( f \"Accuracy: { metrics_peft [ 'accuracy' ] : .4f } \" ) # Comparison print ( \" \\n === Comparison ===\" ) print ( f \"Speedup: { base_time / peft_time : .1f } x\" ) print ( f \"Memory savings: { ( 1 - peft_memory / base_memory ) * 100 : .0f } %\" ) print ( f \"Accuracy loss: { ( metrics_base [ 'accuracy' ] - metrics_peft [ 'accuracy' ]) * 100 : .2f } %\" ) 2. Memory-Constrained Training \u00b6 2.1 Training on Limited GPU (4GB) \u00b6 import torch from tabtune import TabularPipeline # Check available GPU memory available_memory = torch . cuda . get_device_properties ( 0 ) . total_memory / 1e9 print ( f \"Available GPU memory: { available_memory : .1f } GB\" ) if available_memory < 4 : print ( \"Using ultra-efficient PEFT configuration...\" ) pipeline = TabularPipeline ( model_name = 'TabICL' , tuning_strategy = 'peft' , tuning_params = { 'device' : 'cuda' , 'epochs' : 3 , 'learning_rate' : 2e-4 , 'batch_size' : 4 , # Small batch 'support_size' : 32 , # Small context 'query_size' : 16 , 'num_workers' : 0 , # No parallel loading 'peft_config' : { 'r' : 4 , # Very low rank 'lora_alpha' : 8 , 'lora_dropout' : 0.1 } } ) elif available_memory < 8 : print ( \"Using efficient PEFT configuration...\" ) pipeline = TabularPipeline ( model_name = 'TabICL' , tuning_strategy = 'peft' , tuning_params = { 'device' : 'cuda' , 'epochs' : 5 , 'learning_rate' : 2e-4 , 'batch_size' : 8 , 'support_size' : 64 , 'query_size' : 32 , 'peft_config' : { 'r' : 8 , 'lora_alpha' : 16 , 'lora_dropout' : 0.05 } } ) else : print ( \"Sufficient memory. Using standard PEFT...\" ) pipeline = TabularPipeline ( model_name = 'TabICL' , tuning_strategy = 'peft' , tuning_params = { 'device' : 'cuda' , 'epochs' : 5 , 'learning_rate' : 2e-4 , 'peft_config' : { 'r' : 8 } } ) # Train pipeline . fit ( X_train , y_train ) 4. LoRA Rank Selection \u00b6 4.1 Choosing Optimal Rank \u00b6 import numpy as np from tabtune import TabularPipeline def evaluate_rank ( X_train , X_test , y_train , y_test , rank ): \"\"\"Evaluate specific LoRA rank.\"\"\" pipeline = TabularPipeline ( model_name = 'TabICL' , tuning_strategy = 'peft' , tuning_params = { 'device' : 'cuda' , 'epochs' : 3 , 'peft_config' : { 'r' : rank , 'lora_alpha' : 2 * rank , 'lora_dropout' : 0.05 } } ) pipeline . fit ( X_train , y_train ) metrics = pipeline . evaluate ( X_test , y_test ) return metrics # Evaluate multiple ranks ranks = [ 2 , 4 , 8 , 16 , 32 ] results = {} print ( \"Evaluating LoRA ranks...\" ) for r in ranks : print ( f \" Testing rank { r } ...\" , end = '' , flush = True ) metrics = evaluate_rank ( X_train , X_test , y_train , y_test , r ) results [ r ] = metrics [ 'accuracy' ] print ( f \" Accuracy: { metrics [ 'accuracy' ] : .4f } \" ) # Find optimal rank optimal_rank = max ( results , key = results . get ) optimal_acc = results [ optimal_rank ] print ( f \" \\n Optimal rank: { optimal_rank } (Accuracy: { optimal_acc : .4f } )\" ) # Plot results import matplotlib.pyplot as plt plt . figure ( figsize = ( 10 , 6 )) plt . plot ( ranks , [ results [ r ] for r in ranks ], 'o-' , linewidth = 2 , markersize = 8 ) plt . xlabel ( 'LoRA Rank' ) plt . ylabel ( 'Accuracy' ) plt . title ( 'LoRA Rank vs Model Accuracy' ) plt . grid ( True , alpha = 0.3 ) plt . savefig ( 'rank_analysis.png' ) 6. Advanced PEFT Techniques \u00b6 6.1 Custom Target Modules \u00b6 from tabtune import TabularPipeline # Train only specific layers pipeline = TabularPipeline ( model_name = 'TabICL' , tuning_strategy = 'peft' , tuning_params = { 'device' : 'cuda' , 'epochs' : 5 , 'learning_rate' : 2e-4 , 'peft_config' : { 'r' : 8 , 'lora_alpha' : 16 , 'target_modules' : [ 'col_embedder.tf_col' , # Column embedder only 'icl_predictor.decoder' # Plus decoder ] } } ) pipeline . fit ( X_train , y_train ) 6.2 LoRA with Different Learning Rates \u00b6 from tabtune import TabularPipeline # Higher learning rate for smaller LoRA modules pipeline = TabularPipeline ( model_name = 'TabICL' , tuning_strategy = 'peft' , tuning_params = { 'device' : 'cuda' , 'epochs' : 5 , 'learning_rate' : 1e-3 , # 10x higher for PEFT 'peft_config' : { 'r' : 8 , 'lora_alpha' : 16 } } ) pipeline . fit ( X_train , y_train ) 9. Troubleshooting PEFT \u00b6 9.1 Common PEFT Issues \u00b6 # Issue 1: PEFT accuracy much lower than base-ft # Solution: Increase rank peft_config = { 'r' : 16 , # Instead of 4 'lora_alpha' : 32 } # Issue 2: PEFT training diverging # Solution: Reduce learning rate tuning_params = { 'learning_rate' : 1e-4 # Instead of 2e-4 } # Issue 3: PEFT still using too much memory # Solution: Combine PEFT + mixed precision + gradient accumulation tuning_params = { 'learning_rate' : 2e-4 , 'batch_size' : 4 , 'gradient_accumulation_steps' : 8 , 'mixed_precision' : 'fp16' , 'peft_config' : { 'r' : 4 } } # Issue 4: PEFT slower than expected # Solution: Verify LoRA is applied print ( pipeline . model ) # Check for LoRA modules 10. PEFT Best Practices \u00b6 \u2705 Do's \u00b6 \u2705 Start with r=8 (good default) \u2705 Use 2x learning rate for PEFT \u2705 Include warmup steps \u2705 Monitor gradient norms \u2705 Use gradient clipping \u2705 Test rank selection \u2705 Save adapter weights separately \u274c Don'ts \u00b6 \u274c Don't use same learning rate as base-ft \u274c Don't use rank < 2 \u274c Don't skip regularization \u274c Don't forget to scale learning rate \u274c Don't train very long (overfit risk) 11. PEFT Performance Summary \u00b6 Typical Results on 100K Sample Classification Task: Base Fine-Tuning: Training Time: 30 minutes Memory: 12 GB Accuracy: 90.5% Model Size: 500 MB PEFT (r=8): Training Time: 10 minutes (3x faster) Memory: 3 GB (75% reduction) Accuracy: 89.8% (0.7% loss) Model Size: 5 MB (100x smaller) Trade-off Analysis: Speed: 3x faster Memory: 75% reduction Storage: 100x smaller Accuracy: Only 0.7% lower RECOMMENDATION: Use PEFT for most scenarios 12. Quick Reference \u00b6 Scenario r alpha dropout LR Notes Memory constrained 4 8 0.1 1e-4 Ultra-low resource Standard 8 16 0.05 2e-4 Default, balanced High accuracy 16 32 0.02 1e-4 Best results Large data (1M) 8 16 0.05 2e-4 TabDPT recommended 13. Next Steps \u00b6 PEFT & LoRA - Theory and mathematics Memory Optimization - Memory techniques Hyperparameter Tuning - Optimization Classification Examples - Complete workflows Master PEFT for efficient, production-ready fine-tuning!","title":"PEFT Fine-Tuning"},{"location":"examples/peft-examples/#peft-examples-practical-parameter-efficient-fine-tuning-workflows","text":"This document provides practical, production-ready examples for using LoRA and PEFT techniques with TabTune across various scenarios and constraints.","title":"PEFT Examples: Practical Parameter-Efficient Fine-Tuning Workflows"},{"location":"examples/peft-examples/#1-quick-start-peft","text":"","title":"1. Quick Start PEFT"},{"location":"examples/peft-examples/#11-5-minute-peft-example","text":"Minimal code to use LoRA: from tabtune import TabularPipeline from sklearn.datasets import load_breast_cancer from sklearn.model_selection import train_test_split # Load dataset X , y = load_breast_cancer ( return_X_y = True ) X_train , X_test , y_train , y_test = train_test_split ( X , y , test_size = 0.2 ) # Create PEFT pipeline pipeline = TabularPipeline ( model_name = 'TabICL' , tuning_strategy = 'peft' , tuning_params = { 'device' : 'cuda' , 'epochs' : 5 , 'learning_rate' : 2e-4 , 'peft_config' : { 'r' : 8 , 'lora_alpha' : 16 , 'lora_dropout' : 0.05 } } ) # Train (much faster and lighter than base-ft) pipeline . fit ( X_train , y_train ) # Evaluate metrics = pipeline . evaluate ( X_test , y_test ) print ( f \"Accuracy: { metrics [ 'accuracy' ] : .4f } \" ) print ( f \"Model size: 1-2% of full model\" )","title":"1.1 5-Minute PEFT Example"},{"location":"examples/peft-examples/#12-comparing-base-ft-vs-peft","text":"import time import torch from tabtune import TabularPipeline X_train , X_test , y_train , y_test = load_data () # Method 1: Base Fine-Tuning print ( \"=== Base Fine-Tuning ===\" ) torch . cuda . reset_peak_memory_stats () start = time . time () pipeline_base = TabularPipeline ( model_name = 'TabICL' , tuning_strategy = 'base-ft' , tuning_params = { 'epochs' : 5 } ) pipeline_base . fit ( X_train , y_train ) base_time = time . time () - start base_memory = torch . cuda . max_memory_allocated () / 1e9 metrics_base = pipeline_base . evaluate ( X_test , y_test ) print ( f \"Time: { base_time : .1f } s\" ) print ( f \"Memory: { base_memory : .1f } GB\" ) print ( f \"Accuracy: { metrics_base [ 'accuracy' ] : .4f } \" ) # Method 2: PEFT print ( \" \\n === PEFT (LoRA) ===\" ) torch . cuda . reset_peak_memory_stats () start = time . time () pipeline_peft = TabularPipeline ( model_name = 'TabICL' , tuning_strategy = 'peft' , tuning_params = { 'epochs' : 5 , 'peft_config' : { 'r' : 8 } } ) pipeline_peft . fit ( X_train , y_train ) peft_time = time . time () - start peft_memory = torch . cuda . max_memory_allocated () / 1e9 metrics_peft = pipeline_peft . evaluate ( X_test , y_test ) print ( f \"Time: { peft_time : .1f } s\" ) print ( f \"Memory: { peft_memory : .1f } GB\" ) print ( f \"Accuracy: { metrics_peft [ 'accuracy' ] : .4f } \" ) # Comparison print ( \" \\n === Comparison ===\" ) print ( f \"Speedup: { base_time / peft_time : .1f } x\" ) print ( f \"Memory savings: { ( 1 - peft_memory / base_memory ) * 100 : .0f } %\" ) print ( f \"Accuracy loss: { ( metrics_base [ 'accuracy' ] - metrics_peft [ 'accuracy' ]) * 100 : .2f } %\" )","title":"1.2 Comparing Base-FT vs PEFT"},{"location":"examples/peft-examples/#2-memory-constrained-training","text":"","title":"2. Memory-Constrained Training"},{"location":"examples/peft-examples/#21-training-on-limited-gpu-4gb","text":"import torch from tabtune import TabularPipeline # Check available GPU memory available_memory = torch . cuda . get_device_properties ( 0 ) . total_memory / 1e9 print ( f \"Available GPU memory: { available_memory : .1f } GB\" ) if available_memory < 4 : print ( \"Using ultra-efficient PEFT configuration...\" ) pipeline = TabularPipeline ( model_name = 'TabICL' , tuning_strategy = 'peft' , tuning_params = { 'device' : 'cuda' , 'epochs' : 3 , 'learning_rate' : 2e-4 , 'batch_size' : 4 , # Small batch 'support_size' : 32 , # Small context 'query_size' : 16 , 'num_workers' : 0 , # No parallel loading 'peft_config' : { 'r' : 4 , # Very low rank 'lora_alpha' : 8 , 'lora_dropout' : 0.1 } } ) elif available_memory < 8 : print ( \"Using efficient PEFT configuration...\" ) pipeline = TabularPipeline ( model_name = 'TabICL' , tuning_strategy = 'peft' , tuning_params = { 'device' : 'cuda' , 'epochs' : 5 , 'learning_rate' : 2e-4 , 'batch_size' : 8 , 'support_size' : 64 , 'query_size' : 32 , 'peft_config' : { 'r' : 8 , 'lora_alpha' : 16 , 'lora_dropout' : 0.05 } } ) else : print ( \"Sufficient memory. Using standard PEFT...\" ) pipeline = TabularPipeline ( model_name = 'TabICL' , tuning_strategy = 'peft' , tuning_params = { 'device' : 'cuda' , 'epochs' : 5 , 'learning_rate' : 2e-4 , 'peft_config' : { 'r' : 8 } } ) # Train pipeline . fit ( X_train , y_train )","title":"2.1 Training on Limited GPU (4GB)"},{"location":"examples/peft-examples/#4-lora-rank-selection","text":"","title":"4. LoRA Rank Selection"},{"location":"examples/peft-examples/#41-choosing-optimal-rank","text":"import numpy as np from tabtune import TabularPipeline def evaluate_rank ( X_train , X_test , y_train , y_test , rank ): \"\"\"Evaluate specific LoRA rank.\"\"\" pipeline = TabularPipeline ( model_name = 'TabICL' , tuning_strategy = 'peft' , tuning_params = { 'device' : 'cuda' , 'epochs' : 3 , 'peft_config' : { 'r' : rank , 'lora_alpha' : 2 * rank , 'lora_dropout' : 0.05 } } ) pipeline . fit ( X_train , y_train ) metrics = pipeline . evaluate ( X_test , y_test ) return metrics # Evaluate multiple ranks ranks = [ 2 , 4 , 8 , 16 , 32 ] results = {} print ( \"Evaluating LoRA ranks...\" ) for r in ranks : print ( f \" Testing rank { r } ...\" , end = '' , flush = True ) metrics = evaluate_rank ( X_train , X_test , y_train , y_test , r ) results [ r ] = metrics [ 'accuracy' ] print ( f \" Accuracy: { metrics [ 'accuracy' ] : .4f } \" ) # Find optimal rank optimal_rank = max ( results , key = results . get ) optimal_acc = results [ optimal_rank ] print ( f \" \\n Optimal rank: { optimal_rank } (Accuracy: { optimal_acc : .4f } )\" ) # Plot results import matplotlib.pyplot as plt plt . figure ( figsize = ( 10 , 6 )) plt . plot ( ranks , [ results [ r ] for r in ranks ], 'o-' , linewidth = 2 , markersize = 8 ) plt . xlabel ( 'LoRA Rank' ) plt . ylabel ( 'Accuracy' ) plt . title ( 'LoRA Rank vs Model Accuracy' ) plt . grid ( True , alpha = 0.3 ) plt . savefig ( 'rank_analysis.png' )","title":"4.1 Choosing Optimal Rank"},{"location":"examples/peft-examples/#6-advanced-peft-techniques","text":"","title":"6. Advanced PEFT Techniques"},{"location":"examples/peft-examples/#61-custom-target-modules","text":"from tabtune import TabularPipeline # Train only specific layers pipeline = TabularPipeline ( model_name = 'TabICL' , tuning_strategy = 'peft' , tuning_params = { 'device' : 'cuda' , 'epochs' : 5 , 'learning_rate' : 2e-4 , 'peft_config' : { 'r' : 8 , 'lora_alpha' : 16 , 'target_modules' : [ 'col_embedder.tf_col' , # Column embedder only 'icl_predictor.decoder' # Plus decoder ] } } ) pipeline . fit ( X_train , y_train )","title":"6.1 Custom Target Modules"},{"location":"examples/peft-examples/#62-lora-with-different-learning-rates","text":"from tabtune import TabularPipeline # Higher learning rate for smaller LoRA modules pipeline = TabularPipeline ( model_name = 'TabICL' , tuning_strategy = 'peft' , tuning_params = { 'device' : 'cuda' , 'epochs' : 5 , 'learning_rate' : 1e-3 , # 10x higher for PEFT 'peft_config' : { 'r' : 8 , 'lora_alpha' : 16 } } ) pipeline . fit ( X_train , y_train )","title":"6.2 LoRA with Different Learning Rates"},{"location":"examples/peft-examples/#9-troubleshooting-peft","text":"","title":"9. Troubleshooting PEFT"},{"location":"examples/peft-examples/#91-common-peft-issues","text":"# Issue 1: PEFT accuracy much lower than base-ft # Solution: Increase rank peft_config = { 'r' : 16 , # Instead of 4 'lora_alpha' : 32 } # Issue 2: PEFT training diverging # Solution: Reduce learning rate tuning_params = { 'learning_rate' : 1e-4 # Instead of 2e-4 } # Issue 3: PEFT still using too much memory # Solution: Combine PEFT + mixed precision + gradient accumulation tuning_params = { 'learning_rate' : 2e-4 , 'batch_size' : 4 , 'gradient_accumulation_steps' : 8 , 'mixed_precision' : 'fp16' , 'peft_config' : { 'r' : 4 } } # Issue 4: PEFT slower than expected # Solution: Verify LoRA is applied print ( pipeline . model ) # Check for LoRA modules","title":"9.1 Common PEFT Issues"},{"location":"examples/peft-examples/#10-peft-best-practices","text":"","title":"10. PEFT Best Practices"},{"location":"examples/peft-examples/#dos","text":"\u2705 Start with r=8 (good default) \u2705 Use 2x learning rate for PEFT \u2705 Include warmup steps \u2705 Monitor gradient norms \u2705 Use gradient clipping \u2705 Test rank selection \u2705 Save adapter weights separately","title":"\u2705 Do's"},{"location":"examples/peft-examples/#donts","text":"\u274c Don't use same learning rate as base-ft \u274c Don't use rank < 2 \u274c Don't skip regularization \u274c Don't forget to scale learning rate \u274c Don't train very long (overfit risk)","title":"\u274c Don'ts"},{"location":"examples/peft-examples/#11-peft-performance-summary","text":"Typical Results on 100K Sample Classification Task: Base Fine-Tuning: Training Time: 30 minutes Memory: 12 GB Accuracy: 90.5% Model Size: 500 MB PEFT (r=8): Training Time: 10 minutes (3x faster) Memory: 3 GB (75% reduction) Accuracy: 89.8% (0.7% loss) Model Size: 5 MB (100x smaller) Trade-off Analysis: Speed: 3x faster Memory: 75% reduction Storage: 100x smaller Accuracy: Only 0.7% lower RECOMMENDATION: Use PEFT for most scenarios","title":"11. PEFT Performance Summary"},{"location":"examples/peft-examples/#12-quick-reference","text":"Scenario r alpha dropout LR Notes Memory constrained 4 8 0.1 1e-4 Ultra-low resource Standard 8 16 0.05 2e-4 Default, balanced High accuracy 16 32 0.02 1e-4 Best results Large data (1M) 8 16 0.05 2e-4 TabDPT recommended","title":"12. Quick Reference"},{"location":"examples/peft-examples/#13-next-steps","text":"PEFT & LoRA - Theory and mathematics Memory Optimization - Memory techniques Hyperparameter Tuning - Optimization Classification Examples - Complete workflows Master PEFT for efficient, production-ready fine-tuning!","title":"13. Next Steps"},{"location":"getting-started/basic-concepts/","text":"Basic Concepts \u00b6 This page introduces the core ideas, architecture, and workflow used across TabTune. Understanding these concepts will help you effectively use TabTune for your tabular machine learning tasks. 1. Tasks \u00b6 TabTune currently focuses on classification tasks: Classification : Multi-class and binary classification (fully supported) Regression : Planned for future releases 2. Core Components \u00b6 TabTune is built on four main components that work together seamlessly: 2.1 TabularPipeline \u00b6 The main user-facing interface that provides a scikit-learn-compatible API. Key Methods: - .fit(X, y) : Train/preprocess the pipeline on training data - .predict(X) : Generate predictions on new data - .predict_proba(X) : Get probability predictions (classification) - .evaluate(X, y) : Calculate performance metrics - .save(path) : Serialize the entire pipeline - .load(path) : Load a saved pipeline Purpose : Simplifies the entire workflow into a single, easy-to-use interface. 2.2 DataProcessor \u00b6 A smart, model-aware data preparation engine that automatically handles preprocessing based on your chosen model. Responsibilities: - Imputation : Handle missing values (mean, median, mode, etc.) - Scaling : Normalize numerical features (standard, min-max, robust) - Encoding : Transform categorical features (one-hot, ordinal, target encoding) - Model-Specific Processing : Apply specialized transformations for each model type - Feature Selection : Optional dimensionality reduction Key Feature : Automatically selects the correct preprocessing strategy based on model_name , eliminating manual configuration. 2.3 TuningManager \u00b6 The computational core that manages model adaptation and training strategies. Strategies: - inference : Zero-shot predictions using pre-trained weights (no training) - base-ft : Full fine-tuning of all model parameters - peft : Parameter-efficient fine-tuning using LoRA adapters (memory-efficient) Purpose : Abstracts away complex training loops, episodic fine-tuning, and optimization details. 2.4 TabularLeaderboard \u00b6 A benchmarking tool for comparing multiple models and strategies on the same dataset splits. Features: - Compare multiple models simultaneously - Test different tuning strategies - Rank results by any metric - Export results for analysis Purpose : Streamline model selection and hyperparameter exploration. 3. Component Relationships \u00b6 flowchart TB A[Raw Data: X, y] --> B[TabularPipeline] B --> C[DataProcessor] C --> D{Model-Specific?} D -->|Yes| E[Custom Preprocessor] D -->|No| F[Standard Pipeline] E --> G[Transformed Data] F --> G G --> H[TuningManager] H --> I{Strategy?} I -->|inference| J[Load Pre-trained] I -->|base-ft| K[Full Fine-tuning] I -->|peft| L[LoRA Fine-tuning] J --> M[Trained Model] K --> M L --> M M --> N[Predictions / Metrics] style B fill:#e1f5ff style C fill:#fff4e1 style H fill:#f0e1ff style M fill:#e1ffe1 Flow Explanation: 1. Raw data enters TabularPipeline 2. DataProcessor applies model-aware preprocessing 3. TuningManager executes the chosen training strategy 4. Final model generates predictions and metrics 4. Workflow \u00b6 Here's the typical workflow for using TabTune: Step 1: Prepare Your Data \u00b6 import pandas as pd from sklearn.model_selection import train_test_split # Load your data df = pd . read_csv ( 'your_data.csv' ) X = df . drop ( 'target' , axis = 1 ) # Features y = df [ 'target' ] # Labels # Split into train/test X_train , X_test , y_train , y_test = train_test_split ( X , y , test_size = 0.2 , random_state = 42 ) Step 2: Initialize the Pipeline \u00b6 from tabtune import TabularPipeline pipeline = TabularPipeline ( model_name = \"TabICL\" , # Choose your model task_type = \"classification\" , # Task type tuning_strategy = \"base-ft\" , # Training strategy tuning_params = { \"device\" : \"cuda\" , \"epochs\" : 5 , \"learning_rate\" : 2e-5 } ) Step 3: Train the Model \u00b6 # This automatically: # 1. Fits the DataProcessor (learns preprocessing transformations) # 2. Applies preprocessing to training data # 3. Trains the model using TuningManager pipeline . fit ( X_train , y_train ) Step 4: Make Predictions \u00b6 # Generate predictions predictions = pipeline . predict ( X_test ) # Get probability scores probabilities = pipeline . predict_proba ( X_test ) # Evaluate performance metrics = pipeline . evaluate ( X_test , y_test ) print ( f \"Accuracy: { metrics [ 'accuracy' ] : .4f } \" ) print ( f \"F1 Score: { metrics [ 'f1_score' ] : .4f } \" ) Step 5: Save and Reload \u00b6 # Save the entire pipeline (including preprocessing and model state) pipeline . save ( \"my_pipeline.joblib\" ) # Later, reload for inference loaded_pipeline = TabularPipeline . load ( \"my_pipeline.joblib\" ) new_predictions = loaded_pipeline . predict ( new_data ) 5. Models \u00b6 TabTune supports multiple state-of-the-art tabular foundation models: Model Family Best Use Case TabPFN PFN/ICL Small datasets (<10K rows), quick baselines TabICL Scalable ICL General-purpose, balanced performance OrionMSP Scalable ICL Large datasets, strong generalization OrionBix Scalable ICL High-accuracy requirements TabDPT Denoising Transformer Very large datasets (>100K rows) Mitra 2D Attention Complex patterns, mixed data types ContextTab Semantic ICL Text-heavy tabular features See the Models Overview for detailed comparisons and selection guidance. 6. Tuning Strategies Explained \u00b6 Inference ( inference ) \u00b6 What : Uses pre-trained model weights without any training When : Quick baselines, zero-shot evaluation Training Time : 0 seconds Memory : Minimal Base Fine-Tuning ( base-ft ) \u00b6 What : Updates all model parameters during training When : Maximum accuracy is priority, ample compute resources Training Time : Moderate to long (depending on dataset size) Memory : High (requires full model in memory) PEFT ( peft ) \u00b6 What : Parameter-efficient fine-tuning using LoRA adapters When : Memory-constrained environments, fast iteration Training Time : Faster than base-ft Memory : Low (only adapter weights updated) See Tuning Strategies for detailed comparisons. 7. Key Principles \u00b6 Model-Aware Processing : The DataProcessor automatically selects optimal preprocessing based on your chosen model Unified API : Same interface ( .fit() , .predict() ) for all models and strategies Automatic Optimization : TuningManager handles complex training loops, learning rate schedules, and optimization Reproducibility : Save entire pipelines (including preprocessing) for consistent results 8. Next Steps \u00b6 Installation Guide : Set up TabTune in your environment Quick Start : Run your first end-to-end example Pipeline Overview : Deep dive into TabularPipeline Model Selection : Choose the right model for your task Tuning Strategies : Understand training options 9. Common Patterns \u00b6 Pattern 1: Quick Baseline \u00b6 pipeline = TabularPipeline ( model_name = \"TabPFN\" , tuning_strategy = \"inference\" ) pipeline . fit ( X_train , y_train ) metrics = pipeline . evaluate ( X_test , y_test ) Pattern 2: Production Model \u00b6 pipeline = TabularPipeline ( model_name = \"OrionBix\" , tuning_strategy = \"base-ft\" , tuning_params = { \"epochs\" : 10 , \"learning_rate\" : 2e-5 } ) pipeline . fit ( X_train , y_train ) pipeline . save ( \"production_model.joblib\" ) Pattern 3: Memory-Efficient Training \u00b6 pipeline = TabularPipeline ( model_name = \"TabICL\" , tuning_strategy = \"peft\" , tuning_params = { \"peft_config\" : { \"r\" : 8 }} ) pipeline . fit ( X_train , y_train ) Understanding these basic concepts will help you effectively leverage TabTune's capabilities for your tabular machine learning projects.","title":"Basic Concepts"},{"location":"getting-started/basic-concepts/#basic-concepts","text":"This page introduces the core ideas, architecture, and workflow used across TabTune. Understanding these concepts will help you effectively use TabTune for your tabular machine learning tasks.","title":"Basic Concepts"},{"location":"getting-started/basic-concepts/#1-tasks","text":"TabTune currently focuses on classification tasks: Classification : Multi-class and binary classification (fully supported) Regression : Planned for future releases","title":"1. Tasks"},{"location":"getting-started/basic-concepts/#2-core-components","text":"TabTune is built on four main components that work together seamlessly:","title":"2. Core Components"},{"location":"getting-started/basic-concepts/#21-tabularpipeline","text":"The main user-facing interface that provides a scikit-learn-compatible API. Key Methods: - .fit(X, y) : Train/preprocess the pipeline on training data - .predict(X) : Generate predictions on new data - .predict_proba(X) : Get probability predictions (classification) - .evaluate(X, y) : Calculate performance metrics - .save(path) : Serialize the entire pipeline - .load(path) : Load a saved pipeline Purpose : Simplifies the entire workflow into a single, easy-to-use interface.","title":"2.1 TabularPipeline"},{"location":"getting-started/basic-concepts/#22-dataprocessor","text":"A smart, model-aware data preparation engine that automatically handles preprocessing based on your chosen model. Responsibilities: - Imputation : Handle missing values (mean, median, mode, etc.) - Scaling : Normalize numerical features (standard, min-max, robust) - Encoding : Transform categorical features (one-hot, ordinal, target encoding) - Model-Specific Processing : Apply specialized transformations for each model type - Feature Selection : Optional dimensionality reduction Key Feature : Automatically selects the correct preprocessing strategy based on model_name , eliminating manual configuration.","title":"2.2 DataProcessor"},{"location":"getting-started/basic-concepts/#23-tuningmanager","text":"The computational core that manages model adaptation and training strategies. Strategies: - inference : Zero-shot predictions using pre-trained weights (no training) - base-ft : Full fine-tuning of all model parameters - peft : Parameter-efficient fine-tuning using LoRA adapters (memory-efficient) Purpose : Abstracts away complex training loops, episodic fine-tuning, and optimization details.","title":"2.3 TuningManager"},{"location":"getting-started/basic-concepts/#24-tabularleaderboard","text":"A benchmarking tool for comparing multiple models and strategies on the same dataset splits. Features: - Compare multiple models simultaneously - Test different tuning strategies - Rank results by any metric - Export results for analysis Purpose : Streamline model selection and hyperparameter exploration.","title":"2.4 TabularLeaderboard"},{"location":"getting-started/basic-concepts/#3-component-relationships","text":"flowchart TB A[Raw Data: X, y] --> B[TabularPipeline] B --> C[DataProcessor] C --> D{Model-Specific?} D -->|Yes| E[Custom Preprocessor] D -->|No| F[Standard Pipeline] E --> G[Transformed Data] F --> G G --> H[TuningManager] H --> I{Strategy?} I -->|inference| J[Load Pre-trained] I -->|base-ft| K[Full Fine-tuning] I -->|peft| L[LoRA Fine-tuning] J --> M[Trained Model] K --> M L --> M M --> N[Predictions / Metrics] style B fill:#e1f5ff style C fill:#fff4e1 style H fill:#f0e1ff style M fill:#e1ffe1 Flow Explanation: 1. Raw data enters TabularPipeline 2. DataProcessor applies model-aware preprocessing 3. TuningManager executes the chosen training strategy 4. Final model generates predictions and metrics","title":"3. Component Relationships"},{"location":"getting-started/basic-concepts/#4-workflow","text":"Here's the typical workflow for using TabTune:","title":"4. Workflow"},{"location":"getting-started/basic-concepts/#step-1-prepare-your-data","text":"import pandas as pd from sklearn.model_selection import train_test_split # Load your data df = pd . read_csv ( 'your_data.csv' ) X = df . drop ( 'target' , axis = 1 ) # Features y = df [ 'target' ] # Labels # Split into train/test X_train , X_test , y_train , y_test = train_test_split ( X , y , test_size = 0.2 , random_state = 42 )","title":"Step 1: Prepare Your Data"},{"location":"getting-started/basic-concepts/#step-2-initialize-the-pipeline","text":"from tabtune import TabularPipeline pipeline = TabularPipeline ( model_name = \"TabICL\" , # Choose your model task_type = \"classification\" , # Task type tuning_strategy = \"base-ft\" , # Training strategy tuning_params = { \"device\" : \"cuda\" , \"epochs\" : 5 , \"learning_rate\" : 2e-5 } )","title":"Step 2: Initialize the Pipeline"},{"location":"getting-started/basic-concepts/#step-3-train-the-model","text":"# This automatically: # 1. Fits the DataProcessor (learns preprocessing transformations) # 2. Applies preprocessing to training data # 3. Trains the model using TuningManager pipeline . fit ( X_train , y_train )","title":"Step 3: Train the Model"},{"location":"getting-started/basic-concepts/#step-4-make-predictions","text":"# Generate predictions predictions = pipeline . predict ( X_test ) # Get probability scores probabilities = pipeline . predict_proba ( X_test ) # Evaluate performance metrics = pipeline . evaluate ( X_test , y_test ) print ( f \"Accuracy: { metrics [ 'accuracy' ] : .4f } \" ) print ( f \"F1 Score: { metrics [ 'f1_score' ] : .4f } \" )","title":"Step 4: Make Predictions"},{"location":"getting-started/basic-concepts/#step-5-save-and-reload","text":"# Save the entire pipeline (including preprocessing and model state) pipeline . save ( \"my_pipeline.joblib\" ) # Later, reload for inference loaded_pipeline = TabularPipeline . load ( \"my_pipeline.joblib\" ) new_predictions = loaded_pipeline . predict ( new_data )","title":"Step 5: Save and Reload"},{"location":"getting-started/basic-concepts/#5-models","text":"TabTune supports multiple state-of-the-art tabular foundation models: Model Family Best Use Case TabPFN PFN/ICL Small datasets (<10K rows), quick baselines TabICL Scalable ICL General-purpose, balanced performance OrionMSP Scalable ICL Large datasets, strong generalization OrionBix Scalable ICL High-accuracy requirements TabDPT Denoising Transformer Very large datasets (>100K rows) Mitra 2D Attention Complex patterns, mixed data types ContextTab Semantic ICL Text-heavy tabular features See the Models Overview for detailed comparisons and selection guidance.","title":"5. Models"},{"location":"getting-started/basic-concepts/#6-tuning-strategies-explained","text":"","title":"6. Tuning Strategies Explained"},{"location":"getting-started/basic-concepts/#inference-inference","text":"What : Uses pre-trained model weights without any training When : Quick baselines, zero-shot evaluation Training Time : 0 seconds Memory : Minimal","title":"Inference (inference)"},{"location":"getting-started/basic-concepts/#base-fine-tuning-base-ft","text":"What : Updates all model parameters during training When : Maximum accuracy is priority, ample compute resources Training Time : Moderate to long (depending on dataset size) Memory : High (requires full model in memory)","title":"Base Fine-Tuning (base-ft)"},{"location":"getting-started/basic-concepts/#peft-peft","text":"What : Parameter-efficient fine-tuning using LoRA adapters When : Memory-constrained environments, fast iteration Training Time : Faster than base-ft Memory : Low (only adapter weights updated) See Tuning Strategies for detailed comparisons.","title":"PEFT (peft)"},{"location":"getting-started/basic-concepts/#7-key-principles","text":"Model-Aware Processing : The DataProcessor automatically selects optimal preprocessing based on your chosen model Unified API : Same interface ( .fit() , .predict() ) for all models and strategies Automatic Optimization : TuningManager handles complex training loops, learning rate schedules, and optimization Reproducibility : Save entire pipelines (including preprocessing) for consistent results","title":"7. Key Principles"},{"location":"getting-started/basic-concepts/#8-next-steps","text":"Installation Guide : Set up TabTune in your environment Quick Start : Run your first end-to-end example Pipeline Overview : Deep dive into TabularPipeline Model Selection : Choose the right model for your task Tuning Strategies : Understand training options","title":"8. Next Steps"},{"location":"getting-started/basic-concepts/#9-common-patterns","text":"","title":"9. Common Patterns"},{"location":"getting-started/basic-concepts/#pattern-1-quick-baseline","text":"pipeline = TabularPipeline ( model_name = \"TabPFN\" , tuning_strategy = \"inference\" ) pipeline . fit ( X_train , y_train ) metrics = pipeline . evaluate ( X_test , y_test )","title":"Pattern 1: Quick Baseline"},{"location":"getting-started/basic-concepts/#pattern-2-production-model","text":"pipeline = TabularPipeline ( model_name = \"OrionBix\" , tuning_strategy = \"base-ft\" , tuning_params = { \"epochs\" : 10 , \"learning_rate\" : 2e-5 } ) pipeline . fit ( X_train , y_train ) pipeline . save ( \"production_model.joblib\" )","title":"Pattern 2: Production Model"},{"location":"getting-started/basic-concepts/#pattern-3-memory-efficient-training","text":"pipeline = TabularPipeline ( model_name = \"TabICL\" , tuning_strategy = \"peft\" , tuning_params = { \"peft_config\" : { \"r\" : 8 }} ) pipeline . fit ( X_train , y_train ) Understanding these basic concepts will help you effectively leverage TabTune's capabilities for your tabular machine learning projects.","title":"Pattern 3: Memory-Efficient Training"},{"location":"getting-started/installation/","text":"Installation \u00b6 This guide will walk you through installing TabTune and its dependencies for optimal performance across different environments. System Requirements \u00b6 Python Version \u00b6 Python 3.10+ (required) Python 3.12+ (recommended for best performance) Hardware \u00b6 Minimum : 8GB RAM, 2GB free disk space Recommended : 16GB+ RAM, NVIDIA GPU with 8GB+ VRAM For Large Datasets : 32GB+ RAM, multiple GPUs Installation Methods \u00b6 Method 1: Install from Source (Recommended) \u00b6 Clone the repository git clone https://github.com/Lexsi-Labs/TabTune.git pip install -r requirements.txt cd TabTune pip install -e . Create virtual environment # Using venv python -m venv tabtune-env source tabtune-env/bin/activate # Linux/macOS # tabtune-env\\Scripts\\activate # Windows # Or using conda conda create -n tabtune python = 3 .11 conda activate tabtune GPU Support For optimal performance with large models, install CUDA-enabled PyTorch. Check your CUDA version with nvidia-smi . Core Dependencies \u00b6 The following packages are automatically installed with TabTune: Essential Packages \u00b6 # Core ML libraries torch> = 2 .0.0 numpy> = 1 .21.0 pandas> = 1 .3.0 scikit-learn> = 1 .0.0 # Data handling openml> = 0 .12.0 datasets> = 2 .0.0 # PEFT support peft> = 0 .4.0 accelerate> = 0 .20.0 transformers> = 4 .30.0 # Utilities joblib> = 1 .0.0 tqdm> = 4 .60.0 Model-Specific Dependencies \u00b6 # For ContextTab (requires HuggingFace Hub access) huggingface-hub> = 0 .15.0 sentence-transformers> = 2 .2.0 # For advanced preprocessing category-encoders> = 2 .5.0 Verify Installation \u00b6 Quick Verification \u00b6 import torch print ( f \"PyTorch version: { torch . __version__ } \" ) print ( f \"CUDA available: { torch . cuda . is_available () } \" ) # Test TabTune import from tabtune import TabularPipeline print ( \"\u2705 TabTune successfully installed!\" ) GPU Verification \u00b6 import torch if torch . cuda . is_available (): print ( f \"\u2705 CUDA available: { torch . cuda . get_device_name ( 0 ) } \" ) print ( f \"GPU Memory: { torch . cuda . get_device_properties ( 0 ) . total_memory / 1e9 : .1f } GB\" ) else : print ( \"\u2139\ufe0f CUDA not available, using CPU\" ) Model Loading Test \u00b6 from tabtune import TabularPipeline import pandas as pd # Quick smoke test df = pd . DataFrame ({ 'a' : [ 1 , 2 , 3 ], 'b' : [ 4 , 5 , 6 ]}) y = pd . Series ([ 0 , 1 , 0 ]) pipeline = TabularPipeline ( model_name = \"TabPFN\" , tuning_strategy = \"inference\" ) print ( \"\u2705 Pipeline creation successful!\" ) Troubleshooting \u00b6 Common Installation Issues \u00b6 Issue: ModuleNotFoundError for torch \u00b6 # Solution: Reinstall PyTorch with correct CUDA version pip uninstall torch torchvision torchaudio pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/check_version_in_requirements Issue: CUDA out of memory during model loading \u00b6 # Solution: Use smaller batch sizes or CPU fallback pipeline = TabularPipeline ( model_name = \"TabPFN\" , tuning_strategy = \"inference\" , tuning_params = { \"device\" : \"cpu\" , \"batch_size\" : 16 } ) Issue: ContextTab model access denied \u00b6 # Solution: Set up HuggingFace token export HF_TOKEN = \"your_huggingface_token\" # Or login interactively huggingface-cli login Issue: Permission denied on Windows \u00b6 # Solution: Run as administrator or use --user flag pip install --user -e . Memory Issues \u00b6 Large Dataset Handling \u00b6 # Use chunked processing for large datasets tuning_params = { \"batch_size\" : 8 , # Reduce batch size \"gradient_accumulation_steps\" : 4 , # Maintain effective batch size \"device\" : \"cuda\" } PEFT Memory Optimization \u00b6 # Use PEFT for memory-efficient fine-tuning pipeline = TabularPipeline ( model_name = \"TabICL\" , tuning_strategy = \"peft\" , tuning_params = { \"peft_config\" : { \"r\" : 4 } # Lower rank for less memory } ) Environment Variables \u00b6 Set these environment variables for optimal performance: # HuggingFace token for gated models export HF_TOKEN = \"your_token_here\" # Disable tokenizers parallelism warnings export TOKENIZERS_PARALLELISM = false # CUDA memory management export PYTORCH_CUDA_ALLOC_CONF = max_split_size_mb:512 # For debugging export CUDA_LAUNCH_BLOCKING = 1 Next Steps \u00b6 After successful installation: Quick Start Guide - Run your first tabtune example Basic Concepts - Understand the core architecture Model Selection - Choose the right model for your task Installation Complete You're now ready to start using TabTune! If you encounter any issues, please check our FAQ or open an issue on GitHub .","title":"Installation"},{"location":"getting-started/installation/#installation","text":"This guide will walk you through installing TabTune and its dependencies for optimal performance across different environments.","title":"Installation"},{"location":"getting-started/installation/#system-requirements","text":"","title":"System Requirements"},{"location":"getting-started/installation/#python-version","text":"Python 3.10+ (required) Python 3.12+ (recommended for best performance)","title":"Python Version"},{"location":"getting-started/installation/#hardware","text":"Minimum : 8GB RAM, 2GB free disk space Recommended : 16GB+ RAM, NVIDIA GPU with 8GB+ VRAM For Large Datasets : 32GB+ RAM, multiple GPUs","title":"Hardware"},{"location":"getting-started/installation/#installation-methods","text":"","title":"Installation Methods"},{"location":"getting-started/installation/#method-1-install-from-source-recommended","text":"Clone the repository git clone https://github.com/Lexsi-Labs/TabTune.git pip install -r requirements.txt cd TabTune pip install -e . Create virtual environment # Using venv python -m venv tabtune-env source tabtune-env/bin/activate # Linux/macOS # tabtune-env\\Scripts\\activate # Windows # Or using conda conda create -n tabtune python = 3 .11 conda activate tabtune GPU Support For optimal performance with large models, install CUDA-enabled PyTorch. Check your CUDA version with nvidia-smi .","title":"Method 1: Install from Source (Recommended)"},{"location":"getting-started/installation/#core-dependencies","text":"The following packages are automatically installed with TabTune:","title":"Core Dependencies"},{"location":"getting-started/installation/#essential-packages","text":"# Core ML libraries torch> = 2 .0.0 numpy> = 1 .21.0 pandas> = 1 .3.0 scikit-learn> = 1 .0.0 # Data handling openml> = 0 .12.0 datasets> = 2 .0.0 # PEFT support peft> = 0 .4.0 accelerate> = 0 .20.0 transformers> = 4 .30.0 # Utilities joblib> = 1 .0.0 tqdm> = 4 .60.0","title":"Essential Packages"},{"location":"getting-started/installation/#model-specific-dependencies","text":"# For ContextTab (requires HuggingFace Hub access) huggingface-hub> = 0 .15.0 sentence-transformers> = 2 .2.0 # For advanced preprocessing category-encoders> = 2 .5.0","title":"Model-Specific Dependencies"},{"location":"getting-started/installation/#verify-installation","text":"","title":"Verify Installation"},{"location":"getting-started/installation/#quick-verification","text":"import torch print ( f \"PyTorch version: { torch . __version__ } \" ) print ( f \"CUDA available: { torch . cuda . is_available () } \" ) # Test TabTune import from tabtune import TabularPipeline print ( \"\u2705 TabTune successfully installed!\" )","title":"Quick Verification"},{"location":"getting-started/installation/#gpu-verification","text":"import torch if torch . cuda . is_available (): print ( f \"\u2705 CUDA available: { torch . cuda . get_device_name ( 0 ) } \" ) print ( f \"GPU Memory: { torch . cuda . get_device_properties ( 0 ) . total_memory / 1e9 : .1f } GB\" ) else : print ( \"\u2139\ufe0f CUDA not available, using CPU\" )","title":"GPU Verification"},{"location":"getting-started/installation/#model-loading-test","text":"from tabtune import TabularPipeline import pandas as pd # Quick smoke test df = pd . DataFrame ({ 'a' : [ 1 , 2 , 3 ], 'b' : [ 4 , 5 , 6 ]}) y = pd . Series ([ 0 , 1 , 0 ]) pipeline = TabularPipeline ( model_name = \"TabPFN\" , tuning_strategy = \"inference\" ) print ( \"\u2705 Pipeline creation successful!\" )","title":"Model Loading Test"},{"location":"getting-started/installation/#troubleshooting","text":"","title":"Troubleshooting"},{"location":"getting-started/installation/#common-installation-issues","text":"","title":"Common Installation Issues"},{"location":"getting-started/installation/#issue-modulenotfounderror-for-torch","text":"# Solution: Reinstall PyTorch with correct CUDA version pip uninstall torch torchvision torchaudio pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/check_version_in_requirements","title":"Issue: ModuleNotFoundError for torch"},{"location":"getting-started/installation/#issue-cuda-out-of-memory-during-model-loading","text":"# Solution: Use smaller batch sizes or CPU fallback pipeline = TabularPipeline ( model_name = \"TabPFN\" , tuning_strategy = \"inference\" , tuning_params = { \"device\" : \"cpu\" , \"batch_size\" : 16 } )","title":"Issue: CUDA out of memory during model loading"},{"location":"getting-started/installation/#issue-contexttab-model-access-denied","text":"# Solution: Set up HuggingFace token export HF_TOKEN = \"your_huggingface_token\" # Or login interactively huggingface-cli login","title":"Issue: ContextTab model access denied"},{"location":"getting-started/installation/#issue-permission-denied-on-windows","text":"# Solution: Run as administrator or use --user flag pip install --user -e .","title":"Issue: Permission denied on Windows"},{"location":"getting-started/installation/#memory-issues","text":"","title":"Memory Issues"},{"location":"getting-started/installation/#large-dataset-handling","text":"# Use chunked processing for large datasets tuning_params = { \"batch_size\" : 8 , # Reduce batch size \"gradient_accumulation_steps\" : 4 , # Maintain effective batch size \"device\" : \"cuda\" }","title":"Large Dataset Handling"},{"location":"getting-started/installation/#peft-memory-optimization","text":"# Use PEFT for memory-efficient fine-tuning pipeline = TabularPipeline ( model_name = \"TabICL\" , tuning_strategy = \"peft\" , tuning_params = { \"peft_config\" : { \"r\" : 4 } # Lower rank for less memory } )","title":"PEFT Memory Optimization"},{"location":"getting-started/installation/#environment-variables","text":"Set these environment variables for optimal performance: # HuggingFace token for gated models export HF_TOKEN = \"your_token_here\" # Disable tokenizers parallelism warnings export TOKENIZERS_PARALLELISM = false # CUDA memory management export PYTORCH_CUDA_ALLOC_CONF = max_split_size_mb:512 # For debugging export CUDA_LAUNCH_BLOCKING = 1","title":"Environment Variables"},{"location":"getting-started/installation/#next-steps","text":"After successful installation: Quick Start Guide - Run your first tabtune example Basic Concepts - Understand the core architecture Model Selection - Choose the right model for your task Installation Complete You're now ready to start using TabTune! If you encounter any issues, please check our FAQ or open an issue on GitHub .","title":"Next Steps"},{"location":"getting-started/quick-start/","text":"Quick Start \u00b6 This quick start guide demonstrates how to run a complete end-to-end workflow with TabTune in just a few steps. 1. Prepare Your Environment \u00b6 Ensure you have installed TabTune and its dependencies as per the Installation Guide . Activate your virtual environment: # If using venv source tabtune-env/bin/activate # If using conda conda activate tabtune 2. Load a Dataset \u00b6 We use the Telco Customer Churn dataset from OpenML for this example. import openml from sklearn.model_selection import train_test_split # Load dataset dataset = openml . datasets . get_dataset ( 42178 ) X , y , _ , _ = dataset . get_data ( target = dataset . default_target_attribute ) # Split into train and test sets X_train , X_test , y_train , y_test = train_test_split ( X , y , test_size = 0.25 , random_state = 42 ) print ( \"Train shape:\" , X_train . shape ) print ( \"Test shape:\" , X_test . shape ) 3. Initialize and Configure the Pipeline \u00b6 Use the TabularPipeline class to define your model, task type, and tuning strategy. from tabtune import TabularPipeline # Base fine-tuning example pipeline = TabularPipeline ( model_name = \"TabPFN\" , task_type = \"classification\" , tuning_strategy = \"base-ft\" , tuning_params = { \"device\" : \"cuda\" , # or \"cpu\" if GPU unavailable \"epochs\" : 3 , \"learning_rate\" : 2e-5 , \"show_progress\" : True } ) 4. Fit the Model \u00b6 Train your pipeline on the training data. pipeline . fit ( X_train , y_train ) During training, TabTune will automatically handle data preprocessing and apply the chosen tuning strategy. 5. Evaluate and Predict \u00b6 After training, evaluate performance and generate predictions: # Evaluate on test set metrics = pipeline . evaluate ( X_test , y_test ) print ( \"Evaluation metrics:\" , metrics ) # Make predictions predictions = pipeline . predict ( X_test ) Supported metrics include: Accuracy , Weighted F1 Score , and ROC AUC Score . 6. Save and Load the Pipeline \u00b6 Persist your trained pipeline for later use: # Save to disk pipeline . save ( \"tabtune_pipeline.joblib\" ) # Load from disk loaded_pipeline = TabularPipeline . load ( \"tabtune_pipeline.joblib\" ) results = loaded_pipeline . predict ( X_test ) 7. Try PEFT (LoRA) Strategy \u00b6 Switch to parameter-efficient fine-tuning with minimal code changes: # PEFT fine-tuning peft_pipeline = TabularPipeline ( model_name = \"TabPFN\" , task_type = \"classification\" , tuning_strategy = \"peft\" , tuning_params = { \"device\" : \"cuda\" , \"epochs\" : 3 , \"learning_rate\" : 2e-4 , \"peft_config\" : { \"r\" : 8 , \"lora_alpha\" : 16 , \"lora_dropout\" : 0.05 } } ) peft_pipeline . fit ( X_train , y_train ) metrics_peft = peft_pipeline . evaluate ( X_test , y_test ) print ( \"PEFT metrics:\" , metrics_peft ) Next Steps \u00b6 Explore advanced configurations in the User Guide Compare multiple models with the TabularLeaderboard Dive into PEFT internals in PEFT & LoRA","title":"Quick Start"},{"location":"getting-started/quick-start/#quick-start","text":"This quick start guide demonstrates how to run a complete end-to-end workflow with TabTune in just a few steps.","title":"Quick Start"},{"location":"getting-started/quick-start/#1-prepare-your-environment","text":"Ensure you have installed TabTune and its dependencies as per the Installation Guide . Activate your virtual environment: # If using venv source tabtune-env/bin/activate # If using conda conda activate tabtune","title":"1. Prepare Your Environment"},{"location":"getting-started/quick-start/#2-load-a-dataset","text":"We use the Telco Customer Churn dataset from OpenML for this example. import openml from sklearn.model_selection import train_test_split # Load dataset dataset = openml . datasets . get_dataset ( 42178 ) X , y , _ , _ = dataset . get_data ( target = dataset . default_target_attribute ) # Split into train and test sets X_train , X_test , y_train , y_test = train_test_split ( X , y , test_size = 0.25 , random_state = 42 ) print ( \"Train shape:\" , X_train . shape ) print ( \"Test shape:\" , X_test . shape )","title":"2. Load a Dataset"},{"location":"getting-started/quick-start/#3-initialize-and-configure-the-pipeline","text":"Use the TabularPipeline class to define your model, task type, and tuning strategy. from tabtune import TabularPipeline # Base fine-tuning example pipeline = TabularPipeline ( model_name = \"TabPFN\" , task_type = \"classification\" , tuning_strategy = \"base-ft\" , tuning_params = { \"device\" : \"cuda\" , # or \"cpu\" if GPU unavailable \"epochs\" : 3 , \"learning_rate\" : 2e-5 , \"show_progress\" : True } )","title":"3. Initialize and Configure the Pipeline"},{"location":"getting-started/quick-start/#4-fit-the-model","text":"Train your pipeline on the training data. pipeline . fit ( X_train , y_train ) During training, TabTune will automatically handle data preprocessing and apply the chosen tuning strategy.","title":"4. Fit the Model"},{"location":"getting-started/quick-start/#5-evaluate-and-predict","text":"After training, evaluate performance and generate predictions: # Evaluate on test set metrics = pipeline . evaluate ( X_test , y_test ) print ( \"Evaluation metrics:\" , metrics ) # Make predictions predictions = pipeline . predict ( X_test ) Supported metrics include: Accuracy , Weighted F1 Score , and ROC AUC Score .","title":"5. Evaluate and Predict"},{"location":"getting-started/quick-start/#6-save-and-load-the-pipeline","text":"Persist your trained pipeline for later use: # Save to disk pipeline . save ( \"tabtune_pipeline.joblib\" ) # Load from disk loaded_pipeline = TabularPipeline . load ( \"tabtune_pipeline.joblib\" ) results = loaded_pipeline . predict ( X_test )","title":"6. Save and Load the Pipeline"},{"location":"getting-started/quick-start/#7-try-peft-lora-strategy","text":"Switch to parameter-efficient fine-tuning with minimal code changes: # PEFT fine-tuning peft_pipeline = TabularPipeline ( model_name = \"TabPFN\" , task_type = \"classification\" , tuning_strategy = \"peft\" , tuning_params = { \"device\" : \"cuda\" , \"epochs\" : 3 , \"learning_rate\" : 2e-4 , \"peft_config\" : { \"r\" : 8 , \"lora_alpha\" : 16 , \"lora_dropout\" : 0.05 } } ) peft_pipeline . fit ( X_train , y_train ) metrics_peft = peft_pipeline . evaluate ( X_test , y_test ) print ( \"PEFT metrics:\" , metrics_peft )","title":"7. Try PEFT (LoRA) Strategy"},{"location":"getting-started/quick-start/#next-steps","text":"Explore advanced configurations in the User Guide Compare multiple models with the TabularLeaderboard Dive into PEFT internals in PEFT & LoRA","title":"Next Steps"},{"location":"models/contexttab/","text":"ContextTab: Semantics-Aware In-Context Learning \u00b6 ContextTab is a semantically-aware tabular model that integrates modality-specific embeddings to leverage semantic information from feature names, descriptions, and mixed data types. This document provides comprehensive guidance for using ContextTab with TabTune. 1. Introduction \u00b6 What is ContextTab? ContextTab (ConTextTabClassifier) is an advanced in-context learning model uniquely designed to: Leverage Feature Semantics : Understands column names and semantic meaning Text Integration : Handles free-text features naturally Modality-Aware Processing : Different encoders for different data modalities Semantic Embeddings : Uses pre-trained text embeddings for features Heterogeneous Data : Mixed numerical, categorical, and text features Key Innovation : Combines tabular features with semantic embeddings of feature names, enabling the model to understand what features represent semantically. 2. Architecture \u00b6 2.1 High-Level Design \u00b6 flowchart LR A[Raw Data] --> B[Feature Names] A --> C[Feature Values] B --> D[Text Encoder] C --> E[Value Encoder] D --> F[Semantic Embeddings] E --> G[Value Embeddings] F --> H[Fusion Layer] G --> H H --> I[ICL Module] I --> J[Context Processing] J --> K[Predictions] 2.3 Semantic Processing Pipeline \u00b6 Feature Names Feature Values \u2193 \u2193 Text Encoder Value Encoder \u2193 \u2193 Semantic Vectors Value Vectors \u2193 \u2193 \u251c\u2500\u2500\u2500\u2500\u2500Semantic Fusion\u2500\u2500\u2500\u2500\u2500\u2524 \u2193 Joint Representation \u2193 ICL Predictor \u2193 Predictions 3. Inference Parameters \u00b6 3.1 Complete Parameter Reference \u00b6 model_params = { # Text encoding 'text_encoder' : 'sentence-transformers/all-MiniLM-L6-v2' , # BERT model 'text_dim' : 384 , # Text embedding dimension # Value encoding 'value_dim' : 64 , # Value embedding dimension 'categorical_encoding' : 'embedding' , # 'embedding' or 'onehot' # Fusion and processing 'fusion_dim' : 128 , # Fused embedding dimension 'dropout' : 0.1 , # Dropout rate # Training behavior 'use_cache' : True , # Cache embeddings 'seed' : 42 # Reproducibility } 3.2 Parameter Descriptions \u00b6 Parameter Type Default Description text_encoder str 'all-MiniLM-L6-v2' Hugging Face model ID text_dim int 384 Output dimension of text encoder value_dim int 64 Dimension for value embeddings categorical_encoding str 'embedding' How to encode categoricals fusion_dim int 128 Fused representation dimension dropout float 0.1 Dropout probability use_cache bool True Cache computed embeddings seed int 42 Random seed 3.3 Text Encoder Options \u00b6 # Different pre-trained models (trade-off: speed vs quality) text_encoders = { 'all-MiniLM-L6-v2' : { 'dim' : 384 , 'speed' : 'Fast' , 'quality' : 'Good' }, 'all-MiniLM-L12-v2' : { 'dim' : 384 , 'speed' : 'Medium' , 'quality' : 'Better' }, 'all-mpnet-base-v2' : { 'dim' : 768 , 'speed' : 'Slow' , 'quality' : 'Best' } } 4. Fine-Tuning with ContextTab \u00b6 ContextTab supports base fine-tuning via episodic training. 4.1 Fine-Tuning Parameters \u00b6 tuning_params = { 'device' : 'cuda' , 'epochs' : 10 , # More epochs typically needed 'learning_rate' : 1e-4 , # Higher than other models 'optimizer' : 'adamw' , # Optimizer type 'batch_size' : 8 , # Standard batch size 'show_progress' : True # Progress bar } 4.2 Fine-Tuning Best Practices \u00b6 Epochs : 5-15 (longer than most models) Learning Rate : 1e-4 to 5e-4 (higher than TabICL) Warmup : Include warmup for stability Scheduler : Use cosine decay for better convergence Early Stopping : Important due to text embedding complexity 4.3 Fine-Tuning Stability \u00b6 ContextTab training can be unstable due to: - Complex text-value fusion - High-dimensional embeddings - Cross-modality interactions Recommendations : tuning_params = { 'device' : 'cuda' , 'epochs' : 10 , 'learning_rate' : 1e-4 , 'show_progress' : True } 5. Setup Requirements \u00b6 5.1 HuggingFace Hub Access \u00b6 ContextTab requires access to gated models on Hugging Face: # Install HuggingFace CLI pip install huggingface-hub # Login with your token huggingface-cli login # Or set environment variable export HF_TOKEN = 'your_token_here' 5.2 Verify Setup \u00b6 from huggingface_hub import login import os # Check token hf_token = os . getenv ( 'HF_TOKEN' ) if hf_token : login ( hf_token ) print ( \"\u2705 Logged into Hugging Face Hub\" ) else : print ( \"\u26a0\ufe0f HF_TOKEN not set - may fail for gated models\" ) 5.3 Model Download \u00b6 First-time usage downloads model (~2GB): # First use will download model pipeline = TabularPipeline ( model_name = 'ContextTab' , tuning_strategy = 'inference' ) # ... downloads and caches model 6. Usage Patterns \u00b6 6.1 Inference Only \u00b6 from tabtune import TabularPipeline import os # Set HF token os . environ [ 'HF_TOKEN' ] = 'your_token' pipeline = TabularPipeline ( model_name = 'ContextTab' , tuning_strategy = 'inference' , model_params = { 'text_encoder' : 'sentence-transformers/all-MiniLM-L6-v2' , 'fusion_dim' : 128 } ) pipeline . fit ( X_train , y_train ) predictions = pipeline . predict ( X_test ) 6.2 Base Fine-Tuning \u00b6 pipeline = TabularPipeline ( model_name = 'ContextTab' , tuning_strategy = 'base-ft' , tuning_params = { 'device' : 'cuda' , 'epochs' : 10 , 'learning_rate' : 1e-4 , 'warmup_steps' : 200 , 'batch_size' : 8 , 'show_progress' : True } ) pipeline . fit ( X_train , y_train ) metrics = pipeline . evaluate ( X_test , y_test ) 6.3 PEFT Fine-Tuning (Experimental) \u00b6 # \u26a0\ufe0f PEFT support is experimental for ContextTab pipeline = TabularPipeline ( model_name = 'ContextTab' , tuning_strategy = 'peft' , tuning_params = { 'device' : 'cuda' , 'epochs' : 10 , 'learning_rate' : 2e-4 , 'peft_config' : { 'r' : 8 , 'lora_alpha' : 16 , 'lora_dropout' : 0.05 } } ) # May have issues - base-ft recommended pipeline . fit ( X_train , y_train ) 7. LoRA Target Modules (Experimental) \u00b6 When using PEFT, ContextTab targets: target_modules = [ 'in_context_encoder' , # Text encoder 'dense' , # Value encoder 'output_head' , # Prediction head 'embeddings' # Embedding layers ] 7.1 PEFT Status \u00b6 \u26a0\ufe0f Experimental : PEFT support for ContextTab is experimental because: - Complex embedding pipeline - Cross-modality fusion issues - Potential prediction inconsistencies Recommendation : Use base-ft strategy instead of peft 8. Complete Examples \u00b6 8.1 Text-Heavy Dataset \u00b6 from tabtune import TabularPipeline import os # Example: Customer survey data with text responses os . environ [ 'HF_TOKEN' ] = 'your_token' # X contains columns like: # - age (numerical) # - category (categorical) # - feedback_text (text) # - rating (numerical) pipeline = TabularPipeline ( model_name = 'ContextTab' , tuning_strategy = 'base-ft' , tuning_params = { 'device' : 'cuda' , 'epochs' : 10 , 'learning_rate' : 1e-4 , 'warmup_steps' : 200 , 'show_progress' : True } ) pipeline . fit ( X_train , y_train ) metrics = pipeline . evaluate ( X_test , y_test ) print ( f \"Accuracy: { metrics [ 'accuracy' ] : .4f } \" ) 8.2 Semantic Feature Names \u00b6 ContextTab leverages meaningful feature names: # Good: Descriptive feature names (ContextTab works well) X = pd . DataFrame ({ 'customer_age' : [ 25 , 30 , 45 ], 'total_purchases_amount' : [ 100 , 250 , 5000 ], 'years_as_customer' : [ 1 , 5 , 10 ], 'product_category_preference' : [ 'electronics' , 'books' , 'home' ] }) # Less Good: Generic feature names (ContextTab has less semantic info) X = pd . DataFrame ({ 'f1' : [ 25 , 30 , 45 ], 'f2' : [ 100 , 250 , 5000 ], 'f3' : [ 1 , 5 , 10 ], 'f4' : [ 'electronics' , 'books' , 'home' ] }) 8.3 Production Deployment - Saving using joblib \u00b6 import joblib import os os . environ [ 'HF_TOKEN' ] = 'your_token' # Train pipeline = TabularPipeline ( model_name = 'ContextTab' , tuning_strategy = 'base-ft' , tuning_params = { 'epochs' : 10 , 'learning_rate' : 1e-4 } ) pipeline . fit ( X_train , y_train ) metrics = pipeline . evaluate ( X_test , y_test ) # Save pipeline . save ( 'contexttab_production.joblib' ) # In production (ensure HF_TOKEN is set) loaded = TabularPipeline . load ( 'contexttab_production.joblib' ) predictions = loaded . predict ( X_new ) 9. Performance Characteristics \u00b6 9.1 Speed Benchmarks \u00b6 Operation Time Notes Inference (batch=1000) 2-4s Text encoding overhead Fine-tuning (10 epochs, 100K) 30-45m Longer training Prediction latency 20-100ms Per sample Text embedding cache 1-2s One-time at startup 9.2 Memory Usage \u00b6 Scenario Memory GPU VRAM Inference 6-8 GB 4GB minimum Fine-tuning 10-14 GB 8GB recommended Large text dim Up to 16 GB 10GB+ needed With caching Add 1-2 GB For embeddings 11. Troubleshooting \u00b6 Issue: \"HuggingFace login required\" \u00b6 Solution : export HF_TOKEN = 'hf_xxxxxxxxxxxx' # or huggingface-cli login Issue: \"Model download fails\" \u00b6 Solution : Check internet and token from huggingface_hub import model_info try : info = model_info ( 'sentence-transformers/all-MiniLM-L6-v2' ) print ( \"\u2705 Model accessible\" ) except Exception as e : print ( f \"\u274c Model access failed: { e } \" ) Issue: \"Slow inference due to text encoding\" \u00b6 Solution : Use faster text encoder model_params = { 'text_encoder' : 'sentence-transformers/all-MiniLM-L6-v2' , # Fastest # instead of 'all-mpnet-base-v2' # Slowest } Issue: \"Training unstable or diverging\" \u00b6 Solution : Increase regularization tuning_params = { 'learning_rate' : 5e-5 , # Reduce 'warmup_steps' : 500 , # Increase 'weight_decay' : 0.1 , # Increase 'gradient_clip_value' : 0.5 # Tighter } Issue: \"Out of memory during training\" \u00b6 Solution : Reduce batch size tuning_params = { 'batch_size' : 4 # Instead of 8 } 12. Best Practices \u00b6 \u2705 Do's \u00b6 \u2705 Use descriptive feature names \u2705 Include text columns when available \u2705 Set HF_TOKEN before use \u2705 Use base-ft strategy (not peft) \u2705 Include longer warmup phases \u2705 Cache embeddings for repeated use \u274c Don'ts \u00b6 \u274c Don't use PEFT (experimental) \u274c Don't use on pure numerical data (use TabICL) \u274c Don't forget to set HF_TOKEN \u274c Don't use very large batch sizes \u274c Don't skip gradient clipping \u274c Don't use without semantic feature names 13. When to Use ContextTab \u00b6 Use ContextTab when : - \u2705 Dataset has text columns/features - \u2705 Feature names are semantic/meaningful - \u2705 Mixed data types (numerical + categorical + text) - \u2705 You have HuggingFace Hub access - \u2705 Accuracy is priority over speed Don't use ContextTab for : - \u274c Pure numerical data (use TabDPT) - \u274c Generic feature names (limited benefit) - \u274c Memory-constrained environments - \u274c When you need fast training - \u274c Without HuggingFace access 14. Comparison with Other Models \u00b6 Aspect ContextTab TabICL TabDPT Mitra Text Support \u2705 Excellent \u274c No \u274c No \u274c No Semantic Names \u2705 Uses \u274c Ignores \u274c Ignores \u274c Ignores Speed Medium Fast Slow Slow Memory Moderate Moderate High Very High Mixed Data \u2705 Excellent Good Good Good Accuracy High Good Excellent Excellent PEFT \u26a0\ufe0f Exp \u2705 Full \u2705 Full \u2705 Full 15. Quick Reference \u00b6 Task Strategy Config Epochs Quick baseline inference default 0 Mixed data base-ft learning_rate=1e-4 10 Text-heavy base-ft warmup=200 10 Memory limited base-ft batch_size=4 10 Max accuracy base-ft full tune 15 16. Next Steps \u00b6 Model Selection - Compare with other models Tuning Strategies - Fine-tuning details TabularLeaderboard - Benchmark ContextTab HuggingFace Hub - Access gated models ContextTab excels with text-enriched tabular data and semantic feature understanding. Use it when your data includes text or has meaningful feature names!","title":"ConTextTab"},{"location":"models/contexttab/#contexttab-semantics-aware-in-context-learning","text":"ContextTab is a semantically-aware tabular model that integrates modality-specific embeddings to leverage semantic information from feature names, descriptions, and mixed data types. This document provides comprehensive guidance for using ContextTab with TabTune.","title":"ContextTab: Semantics-Aware In-Context Learning"},{"location":"models/contexttab/#1-introduction","text":"What is ContextTab? ContextTab (ConTextTabClassifier) is an advanced in-context learning model uniquely designed to: Leverage Feature Semantics : Understands column names and semantic meaning Text Integration : Handles free-text features naturally Modality-Aware Processing : Different encoders for different data modalities Semantic Embeddings : Uses pre-trained text embeddings for features Heterogeneous Data : Mixed numerical, categorical, and text features Key Innovation : Combines tabular features with semantic embeddings of feature names, enabling the model to understand what features represent semantically.","title":"1. Introduction"},{"location":"models/contexttab/#2-architecture","text":"","title":"2. Architecture"},{"location":"models/contexttab/#21-high-level-design","text":"flowchart LR A[Raw Data] --> B[Feature Names] A --> C[Feature Values] B --> D[Text Encoder] C --> E[Value Encoder] D --> F[Semantic Embeddings] E --> G[Value Embeddings] F --> H[Fusion Layer] G --> H H --> I[ICL Module] I --> J[Context Processing] J --> K[Predictions]","title":"2.1 High-Level Design"},{"location":"models/contexttab/#23-semantic-processing-pipeline","text":"Feature Names Feature Values \u2193 \u2193 Text Encoder Value Encoder \u2193 \u2193 Semantic Vectors Value Vectors \u2193 \u2193 \u251c\u2500\u2500\u2500\u2500\u2500Semantic Fusion\u2500\u2500\u2500\u2500\u2500\u2524 \u2193 Joint Representation \u2193 ICL Predictor \u2193 Predictions","title":"2.3 Semantic Processing Pipeline"},{"location":"models/contexttab/#3-inference-parameters","text":"","title":"3. Inference Parameters"},{"location":"models/contexttab/#31-complete-parameter-reference","text":"model_params = { # Text encoding 'text_encoder' : 'sentence-transformers/all-MiniLM-L6-v2' , # BERT model 'text_dim' : 384 , # Text embedding dimension # Value encoding 'value_dim' : 64 , # Value embedding dimension 'categorical_encoding' : 'embedding' , # 'embedding' or 'onehot' # Fusion and processing 'fusion_dim' : 128 , # Fused embedding dimension 'dropout' : 0.1 , # Dropout rate # Training behavior 'use_cache' : True , # Cache embeddings 'seed' : 42 # Reproducibility }","title":"3.1 Complete Parameter Reference"},{"location":"models/contexttab/#32-parameter-descriptions","text":"Parameter Type Default Description text_encoder str 'all-MiniLM-L6-v2' Hugging Face model ID text_dim int 384 Output dimension of text encoder value_dim int 64 Dimension for value embeddings categorical_encoding str 'embedding' How to encode categoricals fusion_dim int 128 Fused representation dimension dropout float 0.1 Dropout probability use_cache bool True Cache computed embeddings seed int 42 Random seed","title":"3.2 Parameter Descriptions"},{"location":"models/contexttab/#33-text-encoder-options","text":"# Different pre-trained models (trade-off: speed vs quality) text_encoders = { 'all-MiniLM-L6-v2' : { 'dim' : 384 , 'speed' : 'Fast' , 'quality' : 'Good' }, 'all-MiniLM-L12-v2' : { 'dim' : 384 , 'speed' : 'Medium' , 'quality' : 'Better' }, 'all-mpnet-base-v2' : { 'dim' : 768 , 'speed' : 'Slow' , 'quality' : 'Best' } }","title":"3.3 Text Encoder Options"},{"location":"models/contexttab/#4-fine-tuning-with-contexttab","text":"ContextTab supports base fine-tuning via episodic training.","title":"4. Fine-Tuning with ContextTab"},{"location":"models/contexttab/#41-fine-tuning-parameters","text":"tuning_params = { 'device' : 'cuda' , 'epochs' : 10 , # More epochs typically needed 'learning_rate' : 1e-4 , # Higher than other models 'optimizer' : 'adamw' , # Optimizer type 'batch_size' : 8 , # Standard batch size 'show_progress' : True # Progress bar }","title":"4.1 Fine-Tuning Parameters"},{"location":"models/contexttab/#42-fine-tuning-best-practices","text":"Epochs : 5-15 (longer than most models) Learning Rate : 1e-4 to 5e-4 (higher than TabICL) Warmup : Include warmup for stability Scheduler : Use cosine decay for better convergence Early Stopping : Important due to text embedding complexity","title":"4.2 Fine-Tuning Best Practices"},{"location":"models/contexttab/#43-fine-tuning-stability","text":"ContextTab training can be unstable due to: - Complex text-value fusion - High-dimensional embeddings - Cross-modality interactions Recommendations : tuning_params = { 'device' : 'cuda' , 'epochs' : 10 , 'learning_rate' : 1e-4 , 'show_progress' : True }","title":"4.3 Fine-Tuning Stability"},{"location":"models/contexttab/#5-setup-requirements","text":"","title":"5. Setup Requirements"},{"location":"models/contexttab/#51-huggingface-hub-access","text":"ContextTab requires access to gated models on Hugging Face: # Install HuggingFace CLI pip install huggingface-hub # Login with your token huggingface-cli login # Or set environment variable export HF_TOKEN = 'your_token_here'","title":"5.1 HuggingFace Hub Access"},{"location":"models/contexttab/#52-verify-setup","text":"from huggingface_hub import login import os # Check token hf_token = os . getenv ( 'HF_TOKEN' ) if hf_token : login ( hf_token ) print ( \"\u2705 Logged into Hugging Face Hub\" ) else : print ( \"\u26a0\ufe0f HF_TOKEN not set - may fail for gated models\" )","title":"5.2 Verify Setup"},{"location":"models/contexttab/#53-model-download","text":"First-time usage downloads model (~2GB): # First use will download model pipeline = TabularPipeline ( model_name = 'ContextTab' , tuning_strategy = 'inference' ) # ... downloads and caches model","title":"5.3 Model Download"},{"location":"models/contexttab/#6-usage-patterns","text":"","title":"6. Usage Patterns"},{"location":"models/contexttab/#61-inference-only","text":"from tabtune import TabularPipeline import os # Set HF token os . environ [ 'HF_TOKEN' ] = 'your_token' pipeline = TabularPipeline ( model_name = 'ContextTab' , tuning_strategy = 'inference' , model_params = { 'text_encoder' : 'sentence-transformers/all-MiniLM-L6-v2' , 'fusion_dim' : 128 } ) pipeline . fit ( X_train , y_train ) predictions = pipeline . predict ( X_test )","title":"6.1 Inference Only"},{"location":"models/contexttab/#62-base-fine-tuning","text":"pipeline = TabularPipeline ( model_name = 'ContextTab' , tuning_strategy = 'base-ft' , tuning_params = { 'device' : 'cuda' , 'epochs' : 10 , 'learning_rate' : 1e-4 , 'warmup_steps' : 200 , 'batch_size' : 8 , 'show_progress' : True } ) pipeline . fit ( X_train , y_train ) metrics = pipeline . evaluate ( X_test , y_test )","title":"6.2 Base Fine-Tuning"},{"location":"models/contexttab/#63-peft-fine-tuning-experimental","text":"# \u26a0\ufe0f PEFT support is experimental for ContextTab pipeline = TabularPipeline ( model_name = 'ContextTab' , tuning_strategy = 'peft' , tuning_params = { 'device' : 'cuda' , 'epochs' : 10 , 'learning_rate' : 2e-4 , 'peft_config' : { 'r' : 8 , 'lora_alpha' : 16 , 'lora_dropout' : 0.05 } } ) # May have issues - base-ft recommended pipeline . fit ( X_train , y_train )","title":"6.3 PEFT Fine-Tuning (Experimental)"},{"location":"models/contexttab/#7-lora-target-modules-experimental","text":"When using PEFT, ContextTab targets: target_modules = [ 'in_context_encoder' , # Text encoder 'dense' , # Value encoder 'output_head' , # Prediction head 'embeddings' # Embedding layers ]","title":"7. LoRA Target Modules (Experimental)"},{"location":"models/contexttab/#71-peft-status","text":"\u26a0\ufe0f Experimental : PEFT support for ContextTab is experimental because: - Complex embedding pipeline - Cross-modality fusion issues - Potential prediction inconsistencies Recommendation : Use base-ft strategy instead of peft","title":"7.1 PEFT Status"},{"location":"models/contexttab/#8-complete-examples","text":"","title":"8. Complete Examples"},{"location":"models/contexttab/#81-text-heavy-dataset","text":"from tabtune import TabularPipeline import os # Example: Customer survey data with text responses os . environ [ 'HF_TOKEN' ] = 'your_token' # X contains columns like: # - age (numerical) # - category (categorical) # - feedback_text (text) # - rating (numerical) pipeline = TabularPipeline ( model_name = 'ContextTab' , tuning_strategy = 'base-ft' , tuning_params = { 'device' : 'cuda' , 'epochs' : 10 , 'learning_rate' : 1e-4 , 'warmup_steps' : 200 , 'show_progress' : True } ) pipeline . fit ( X_train , y_train ) metrics = pipeline . evaluate ( X_test , y_test ) print ( f \"Accuracy: { metrics [ 'accuracy' ] : .4f } \" )","title":"8.1 Text-Heavy Dataset"},{"location":"models/contexttab/#82-semantic-feature-names","text":"ContextTab leverages meaningful feature names: # Good: Descriptive feature names (ContextTab works well) X = pd . DataFrame ({ 'customer_age' : [ 25 , 30 , 45 ], 'total_purchases_amount' : [ 100 , 250 , 5000 ], 'years_as_customer' : [ 1 , 5 , 10 ], 'product_category_preference' : [ 'electronics' , 'books' , 'home' ] }) # Less Good: Generic feature names (ContextTab has less semantic info) X = pd . DataFrame ({ 'f1' : [ 25 , 30 , 45 ], 'f2' : [ 100 , 250 , 5000 ], 'f3' : [ 1 , 5 , 10 ], 'f4' : [ 'electronics' , 'books' , 'home' ] })","title":"8.2 Semantic Feature Names"},{"location":"models/contexttab/#83-production-deployment-saving-using-joblib","text":"import joblib import os os . environ [ 'HF_TOKEN' ] = 'your_token' # Train pipeline = TabularPipeline ( model_name = 'ContextTab' , tuning_strategy = 'base-ft' , tuning_params = { 'epochs' : 10 , 'learning_rate' : 1e-4 } ) pipeline . fit ( X_train , y_train ) metrics = pipeline . evaluate ( X_test , y_test ) # Save pipeline . save ( 'contexttab_production.joblib' ) # In production (ensure HF_TOKEN is set) loaded = TabularPipeline . load ( 'contexttab_production.joblib' ) predictions = loaded . predict ( X_new )","title":"8.3 Production Deployment - Saving using joblib"},{"location":"models/contexttab/#9-performance-characteristics","text":"","title":"9. Performance Characteristics"},{"location":"models/contexttab/#91-speed-benchmarks","text":"Operation Time Notes Inference (batch=1000) 2-4s Text encoding overhead Fine-tuning (10 epochs, 100K) 30-45m Longer training Prediction latency 20-100ms Per sample Text embedding cache 1-2s One-time at startup","title":"9.1 Speed Benchmarks"},{"location":"models/contexttab/#92-memory-usage","text":"Scenario Memory GPU VRAM Inference 6-8 GB 4GB minimum Fine-tuning 10-14 GB 8GB recommended Large text dim Up to 16 GB 10GB+ needed With caching Add 1-2 GB For embeddings","title":"9.2 Memory Usage"},{"location":"models/contexttab/#11-troubleshooting","text":"","title":"11. Troubleshooting"},{"location":"models/contexttab/#issue-huggingface-login-required","text":"Solution : export HF_TOKEN = 'hf_xxxxxxxxxxxx' # or huggingface-cli login","title":"Issue: \"HuggingFace login required\""},{"location":"models/contexttab/#issue-model-download-fails","text":"Solution : Check internet and token from huggingface_hub import model_info try : info = model_info ( 'sentence-transformers/all-MiniLM-L6-v2' ) print ( \"\u2705 Model accessible\" ) except Exception as e : print ( f \"\u274c Model access failed: { e } \" )","title":"Issue: \"Model download fails\""},{"location":"models/contexttab/#issue-slow-inference-due-to-text-encoding","text":"Solution : Use faster text encoder model_params = { 'text_encoder' : 'sentence-transformers/all-MiniLM-L6-v2' , # Fastest # instead of 'all-mpnet-base-v2' # Slowest }","title":"Issue: \"Slow inference due to text encoding\""},{"location":"models/contexttab/#issue-training-unstable-or-diverging","text":"Solution : Increase regularization tuning_params = { 'learning_rate' : 5e-5 , # Reduce 'warmup_steps' : 500 , # Increase 'weight_decay' : 0.1 , # Increase 'gradient_clip_value' : 0.5 # Tighter }","title":"Issue: \"Training unstable or diverging\""},{"location":"models/contexttab/#issue-out-of-memory-during-training","text":"Solution : Reduce batch size tuning_params = { 'batch_size' : 4 # Instead of 8 }","title":"Issue: \"Out of memory during training\""},{"location":"models/contexttab/#12-best-practices","text":"","title":"12. Best Practices"},{"location":"models/contexttab/#dos","text":"\u2705 Use descriptive feature names \u2705 Include text columns when available \u2705 Set HF_TOKEN before use \u2705 Use base-ft strategy (not peft) \u2705 Include longer warmup phases \u2705 Cache embeddings for repeated use","title":"\u2705 Do's"},{"location":"models/contexttab/#donts","text":"\u274c Don't use PEFT (experimental) \u274c Don't use on pure numerical data (use TabICL) \u274c Don't forget to set HF_TOKEN \u274c Don't use very large batch sizes \u274c Don't skip gradient clipping \u274c Don't use without semantic feature names","title":"\u274c Don'ts"},{"location":"models/contexttab/#13-when-to-use-contexttab","text":"Use ContextTab when : - \u2705 Dataset has text columns/features - \u2705 Feature names are semantic/meaningful - \u2705 Mixed data types (numerical + categorical + text) - \u2705 You have HuggingFace Hub access - \u2705 Accuracy is priority over speed Don't use ContextTab for : - \u274c Pure numerical data (use TabDPT) - \u274c Generic feature names (limited benefit) - \u274c Memory-constrained environments - \u274c When you need fast training - \u274c Without HuggingFace access","title":"13. When to Use ContextTab"},{"location":"models/contexttab/#14-comparison-with-other-models","text":"Aspect ContextTab TabICL TabDPT Mitra Text Support \u2705 Excellent \u274c No \u274c No \u274c No Semantic Names \u2705 Uses \u274c Ignores \u274c Ignores \u274c Ignores Speed Medium Fast Slow Slow Memory Moderate Moderate High Very High Mixed Data \u2705 Excellent Good Good Good Accuracy High Good Excellent Excellent PEFT \u26a0\ufe0f Exp \u2705 Full \u2705 Full \u2705 Full","title":"14. Comparison with Other Models"},{"location":"models/contexttab/#15-quick-reference","text":"Task Strategy Config Epochs Quick baseline inference default 0 Mixed data base-ft learning_rate=1e-4 10 Text-heavy base-ft warmup=200 10 Memory limited base-ft batch_size=4 10 Max accuracy base-ft full tune 15","title":"15. Quick Reference"},{"location":"models/contexttab/#16-next-steps","text":"Model Selection - Compare with other models Tuning Strategies - Fine-tuning details TabularLeaderboard - Benchmark ContextTab HuggingFace Hub - Access gated models ContextTab excels with text-enriched tabular data and semantic feature understanding. Use it when your data includes text or has meaningful feature names!","title":"16. Next Steps"},{"location":"models/mitra/","text":"Mitra: 2D Cross-Attention for Tabular Data \u00b6 Mitra (also known as Tab2D) is a sophisticated tabular model featuring 2D cross-attention mechanisms for modeling both row-wise and column-wise dependencies. This document provides comprehensive guidance for using Mitra with TabTune. 1. Introduction \u00b6 What is Mitra? Mitra is an advanced in-context learning model that captures complex interactions through: 2D Cross-Attention : Simultaneous row (sample) and column (feature) modeling Synthetic Priors : Pre-trained representations for tabular data Mixed-Type Feature Handling : Natural support for numerical and categorical data Episodic Training : Task-specific adaptation via meta-learning Key Innovation : 2D attention mechanism simultaneously models relationships both across rows (samples) and columns (features), enabling superior pattern discovery. 2. Architecture \u00b6 2.1 High-Level Design \u00b6 flowchart LR A[Input Features] --> B[Feature Embeddings] B --> C[Row Embeddings] C --> D[2D Cross-Attention] D --> E[Feature-Sample Interactions] E --> F[Prediction Head] F --> G[Logits] G --> H[Final Predictions] 2.3 2D Attention Mechanism \u00b6 Row Embeddings Feature Embeddings \u2193 \u2193 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25002D Cross-Attention\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2193 Joint Row-Feature Representation \u2193 Prediction Head \u2193 Output 2.4 Synthetic Priors \u00b6 Mitra incorporates synthetic prior knowledge: - Pre-trained on diverse tabular data - Captures common patterns - Accelerates learning on new tasks - Improves generalization 3. Inference Parameters \u00b6 3.1 Complete Parameter Reference \u00b6 model_params = { # Architecture parameters 'd_model' : 64 , # Feature embedding dimension 'd_ff' : 128 , # Feedforward hidden dimension 'num_heads' : 4 , # Attention heads 'num_layers' : 2 , # Stacked layers # Training behavior 'dropout' : 0.1 , # Dropout rate 'use_synthetic_prior' : True , # Use pre-trained prior 'seed' : 42 # Reproducibility } 3.2 Parameter Descriptions \u00b6 Parameter Type Default Range Description d_model int 64 32-256 Feature embedding dimension d_ff int 128 64-512 Feedforward network hidden size num_heads int 4 2-8 Number of attention heads num_layers int 2 1-4 Number of transformer layers dropout float 0.1 0.0-0.3 Dropout probability seed int 42 0+ Random seed 3.3 Architecture Tuning \u00b6 Config Speed Accuracy Memory Small: d_model=32, num_layers=1 \u2b50\u2b50\u2b50\u2b50\u2b50 \u2b50\u2b50 \u2b50 Medium: d_model=64, num_layers=2 \u2b50\u2b50\u2b50\u2b50 \u2b50\u2b50\u2b50 \u2b50\u2b50 Large: d_model=128, num_layers=4 \u2b50\u2b50\u2b50 \u2b50\u2b50\u2b50\u2b50\u2b50 \u2b50\u2b50\u2b50 4. Fine-Tuning with Mitra \u00b6 Mitra uses episodic fine-tuning for task-specific adaptation. 4.1 Fine-Tuning Parameters \u00b6 tuning_params = { 'device' : 'cuda' , 'epochs' : 3 , # Training epochs 'learning_rate' : 1e-5 , # Optimizer learning rate 'optimizer' : 'adamw' , # Optimizer type # Episodic parameters 'support_size' : 128 , # Support set size 'query_size' : 128 , # Query set size 'n_episodes' : 500 , # Episodes per epoch 'steps_per_epoch' : 50 , # Gradient steps per epoch 'batch_size' : 4 , # Episodes per batch (small!) 'show_progress' : True # Progress bar } 4.2 Key Parameters \u00b6 Parameter Type Default Description support_size int 128 Context samples per episode query_size int 128 Query samples per episode n_episodes int 500 Total episodes for training steps_per_epoch int 50 Gradient updates per epoch batch_size int 4 Episodes per batch (keep small) 4.3 Why Small Batch Size? \u00b6 Mitra's 2D attention is computationally expensive: - Attention complexity: (O(n^2)) for both rows and columns - Memory grows rapidly with batch size - Empirically: batch_size=4-8 is optimal - Larger: Use gradient accumulation instead 4.4 Fine-Tuning Guidelines \u00b6 Support/Query Balance : support_size = 128 # Larger context for pattern discovery query_size = 128 # Balance for gradient signal Learning Rate Strategy : - Start: 1e-5 - If converging slowly: increase to 2e-5 - If diverging: decrease to 5e-6 Episode Count : - Small dataset (10K): 500 episodes - Medium dataset (100K): 1000 episodes - Large dataset (500K): 2000 episodes 5. LoRA Target Modules \u00b6 When using PEFT, Mitra targets these modules: target_modules = [ 'x_embedding' , # Feature embedder 'layers' , # Attention layers 'final_layer' # Prediction head ] 5.1 Default PEFT Configuration \u00b6 peft_config = { 'r' : 8 , 'lora_alpha' : 16 , 'lora_dropout' : 0.05 , 'target_modules' : None # Uses defaults above } 5.2 PEFT Rank Guidelines \u00b6 Rank Memory Speed Accuracy r=4 Minimal Fast Good r=8 Low Moderate Better r=16 Moderate Slower Best 6. Usage Patterns \u00b6 6.1 Inference Only \u00b6 from tabtune import TabularPipeline pipeline = TabularPipeline ( model_name = 'Mitra' , tuning_strategy = 'inference' , model_params = { 'd_model' : 64 , 'num_layers' : 2 , 'use_synthetic_prior' : True } ) pipeline . fit ( X_train , y_train ) # Preprocessing only predictions = pipeline . predict ( X_test ) 6.2 Base Fine-Tuning \u00b6 pipeline = TabularPipeline ( model_name = 'Mitra' , tuning_strategy = 'base-ft' , tuning_params = { 'device' : 'cuda' , 'epochs' : 3 , 'learning_rate' : 1e-5 , 'support_size' : 128 , 'query_size' : 128 , 'batch_size' : 4 , # Small! 'show_progress' : True } ) pipeline . fit ( X_train , y_train ) metrics = pipeline . evaluate ( X_test , y_test ) 6.3 PEFT Fine-Tuning \u00b6 pipeline = TabularPipeline ( model_name = 'Mitra' , tuning_strategy = 'peft' , tuning_params = { 'device' : 'cuda' , 'epochs' : 3 , 'learning_rate' : 2e-5 , # Higher for PEFT 'support_size' : 64 , # Reduced for memory 'query_size' : 64 , 'batch_size' : 2 , # Very small 'peft_config' : { 'r' : 8 , 'lora_alpha' : 16 , 'lora_dropout' : 0.05 } } ) pipeline . fit ( X_train , y_train ) 6.4 Architecture Customization \u00b6 pipeline = TabularPipeline ( model_name = 'Mitra' , tuning_strategy = 'base-ft' , model_params = { 'd_model' : 128 , # Larger embeddings 'num_layers' : 4 , # More layers 'num_heads' : 8 , # More heads 'd_ff' : 256 }, tuning_params = { 'epochs' : 5 , 'learning_rate' : 1e-5 } ) 7. Complete Examples \u00b6 7.1 Basic Workflow \u00b6 from tabtune import TabularPipeline from sklearn.model_selection import train_test_split import pandas as pd # Load data df = pd . read_csv ( 'structured_data.csv' ) X = df . drop ( 'target' , axis = 1 ) y = df [ 'target' ] # Split X_train , X_test , y_train , y_test = train_test_split ( X , y , test_size = 0.2 , random_state = 42 ) # Train with Mitra pipeline = TabularPipeline ( model_name = 'Mitra' , tuning_strategy = 'base-ft' , tuning_params = { 'device' : 'cuda' , 'epochs' : 3 , 'support_size' : 128 , 'query_size' : 128 , 'batch_size' : 4 , 'learning_rate' : 1e-5 } ) pipeline . fit ( X_train , y_train ) metrics = pipeline . evaluate ( X_test , y_test ) print ( f \"Accuracy: { metrics [ 'accuracy' ] : .4f } \" ) print ( f \"F1 Score: { metrics [ 'f1_score' ] : .4f } \" ) 7.2 PEFT for Memory Constraints \u00b6 # Use PEFT when memory is limited pipeline = TabularPipeline ( model_name = 'Mitra' , tuning_strategy = 'peft' , tuning_params = { 'device' : 'cuda' , 'epochs' : 3 , 'learning_rate' : 2e-5 , 'support_size' : 64 , # Reduced 'query_size' : 64 , # Reduced 'batch_size' : 2 , # Very small 'steps_per_epoch' : 30 , # Fewer steps 'peft_config' : { 'r' : 4 , # Lower rank 'lora_alpha' : 8 , 'lora_dropout' : 0.1 } } ) pipeline . fit ( X_train , y_train ) 7.3 Architecture Search \u00b6 from tabtune import TabularLeaderboard # Compare architectures lb = TabularLeaderboard ( X_train , X_test , y_train , y_test ) # Small model lb . add_model ( 'Mitra' , 'base-ft' , name = 'Mitra-Small' , model_params = { 'd_model' : 32 , 'num_layers' : 1 }, tuning_params = { 'epochs' : 3 } ) # Medium model lb . add_model ( 'Mitra' , 'base-ft' , name = 'Mitra-Medium' , model_params = { 'd_model' : 64 , 'num_layers' : 2 }, tuning_params = { 'epochs' : 3 } ) # Large model lb . add_model ( 'Mitra' , 'base-ft' , name = 'Mitra-Large' , model_params = { 'd_model' : 128 , 'num_layers' : 4 }, tuning_params = { 'epochs' : 3 } ) lb . run ( rank_by = 'accuracy' ) 7.4 Production Deployment - Saving using joblib \u00b6 import joblib # Train optimal model pipeline = TabularPipeline ( model_name = 'Mitra' , tuning_strategy = 'base-ft' , tuning_params = { 'device' : 'cuda' , 'epochs' : 5 , 'support_size' : 128 , 'batch_size' : 4 , 'learning_rate' : 1e-5 } ) pipeline . fit ( X_train , y_train ) metrics = pipeline . evaluate ( X_test , y_test ) # Save for deployment pipeline . save ( 'mitra_production.joblib' ) # In production loaded = TabularPipeline . load ( 'mitra_production.joblib' ) predictions = loaded . predict ( X_new ) 8. Performance Characteristics \u00b6 8.1 Speed Benchmarks \u00b6 Operation Time Notes Inference (batch=1000) 3-5s 2D attention overhead Base FT (3 epochs, 100K) 45-60m Slow but powerful PEFT (3 epochs, 100K) 20-30m Better speed Prediction latency 20-100ms Per sample 8.2 Memory Usage \u00b6 Scenario Memory GPU VRAM Inference 8-10 GB 6GB minimum Base FT 16-20 GB 12GB recommended PEFT 10-12 GB 6-8GB sufficient Large model Up to 24 GB 16GB+ needed 9. Best Practices \u00b6 \u2705 Do's \u00b6 \u2705 Use small batch sizes (2-4) \u2705 Start with medium architecture (d_model=64) \u2705 Monitor memory usage actively \u2705 Use PEFT on constrained systems \u2705 Increase support/query sizes for pattern discovery \u2705 Use synthetic priors (faster convergence) \u274c Don'ts \u00b6 \u274c Don't use large batch sizes (causes OOM) \u274c Don't use very large models on small GPUs \u274c Don't skip warmup steps \u274c Don't disable gradient clipping \u274c Don't train for too many epochs (overfit risk) 10. Troubleshooting \u00b6 Issue: \"CUDA out of memory\" \u00b6 Solution 1 : Reduce batch size tuning_params = { 'batch_size' : 2 , # Instead of 4 'support_size' : 64 , # Instead of 128 'query_size' : 64 } Solution 2 : Use PEFT tuning_strategy = 'peft' Issue: \"Training very slow\" \u00b6 Solution : Reduce model size model_params = { 'd_model' : 32 , # Instead of 64 'num_layers' : 1 # Instead of 2 } Issue: \"Low accuracy\" \u00b6 Solution : Increase support set size tuning_params = { 'support_size' : 256 , # More context 'query_size' : 256 , 'n_episodes' : 1000 # More training } Issue: \"Overfitting on small datasets\" \u00b6 Solution : Use regularization tuning_params = { 'weight_decay' : 0.1 , # Increase regularization 'dropout' : 0.2 # In model_params } 11. Comparison with Other Models \u00b6 Aspect Mitra TabICL TabDPT OrionMSP OrionBix Speed Slow Fast Slow Moderate Medium Memory Very High Moderate High Moderate-High High Accuracy Excellent Good Excellent Very Good Very Good Complexity Complex Simple Medium Medium Medium Small Data Good Good Okay \u26a0\ufe0f Good Large Data Good Good Excellent Excellent Good PEFT \u2705 Full \u2705 Full \u2705 Full \u2705 Full \u2705 Full 12. Quick Reference \u00b6 Use Case Config Batch Size Support Small data (10K) d_model=64, layers=2 4 128 Medium data (100K) d_model=64, layers=2 4 256 Large data (500K) d_model=128, layers=4 2 512 Memory limited PEFT, r=4 2 64 Max accuracy d_model=128, layers=4 4 512 13. Next Steps \u00b6 Model Selection - Compare with other models Tuning Strategies - Fine-tuning details Advanced PEFT - LoRA optimization TabularLeaderboard - Benchmark Mitra Mitra excels at capturing complex 2D patterns in structured tabular data. Use it when maximum accuracy and pattern discovery are priorities!","title":"Mitra"},{"location":"models/mitra/#mitra-2d-cross-attention-for-tabular-data","text":"Mitra (also known as Tab2D) is a sophisticated tabular model featuring 2D cross-attention mechanisms for modeling both row-wise and column-wise dependencies. This document provides comprehensive guidance for using Mitra with TabTune.","title":"Mitra: 2D Cross-Attention for Tabular Data"},{"location":"models/mitra/#1-introduction","text":"What is Mitra? Mitra is an advanced in-context learning model that captures complex interactions through: 2D Cross-Attention : Simultaneous row (sample) and column (feature) modeling Synthetic Priors : Pre-trained representations for tabular data Mixed-Type Feature Handling : Natural support for numerical and categorical data Episodic Training : Task-specific adaptation via meta-learning Key Innovation : 2D attention mechanism simultaneously models relationships both across rows (samples) and columns (features), enabling superior pattern discovery.","title":"1. Introduction"},{"location":"models/mitra/#2-architecture","text":"","title":"2. Architecture"},{"location":"models/mitra/#21-high-level-design","text":"flowchart LR A[Input Features] --> B[Feature Embeddings] B --> C[Row Embeddings] C --> D[2D Cross-Attention] D --> E[Feature-Sample Interactions] E --> F[Prediction Head] F --> G[Logits] G --> H[Final Predictions]","title":"2.1 High-Level Design"},{"location":"models/mitra/#23-2d-attention-mechanism","text":"Row Embeddings Feature Embeddings \u2193 \u2193 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25002D Cross-Attention\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2193 Joint Row-Feature Representation \u2193 Prediction Head \u2193 Output","title":"2.3 2D Attention Mechanism"},{"location":"models/mitra/#24-synthetic-priors","text":"Mitra incorporates synthetic prior knowledge: - Pre-trained on diverse tabular data - Captures common patterns - Accelerates learning on new tasks - Improves generalization","title":"2.4 Synthetic Priors"},{"location":"models/mitra/#3-inference-parameters","text":"","title":"3. Inference Parameters"},{"location":"models/mitra/#31-complete-parameter-reference","text":"model_params = { # Architecture parameters 'd_model' : 64 , # Feature embedding dimension 'd_ff' : 128 , # Feedforward hidden dimension 'num_heads' : 4 , # Attention heads 'num_layers' : 2 , # Stacked layers # Training behavior 'dropout' : 0.1 , # Dropout rate 'use_synthetic_prior' : True , # Use pre-trained prior 'seed' : 42 # Reproducibility }","title":"3.1 Complete Parameter Reference"},{"location":"models/mitra/#32-parameter-descriptions","text":"Parameter Type Default Range Description d_model int 64 32-256 Feature embedding dimension d_ff int 128 64-512 Feedforward network hidden size num_heads int 4 2-8 Number of attention heads num_layers int 2 1-4 Number of transformer layers dropout float 0.1 0.0-0.3 Dropout probability seed int 42 0+ Random seed","title":"3.2 Parameter Descriptions"},{"location":"models/mitra/#33-architecture-tuning","text":"Config Speed Accuracy Memory Small: d_model=32, num_layers=1 \u2b50\u2b50\u2b50\u2b50\u2b50 \u2b50\u2b50 \u2b50 Medium: d_model=64, num_layers=2 \u2b50\u2b50\u2b50\u2b50 \u2b50\u2b50\u2b50 \u2b50\u2b50 Large: d_model=128, num_layers=4 \u2b50\u2b50\u2b50 \u2b50\u2b50\u2b50\u2b50\u2b50 \u2b50\u2b50\u2b50","title":"3.3 Architecture Tuning"},{"location":"models/mitra/#4-fine-tuning-with-mitra","text":"Mitra uses episodic fine-tuning for task-specific adaptation.","title":"4. Fine-Tuning with Mitra"},{"location":"models/mitra/#41-fine-tuning-parameters","text":"tuning_params = { 'device' : 'cuda' , 'epochs' : 3 , # Training epochs 'learning_rate' : 1e-5 , # Optimizer learning rate 'optimizer' : 'adamw' , # Optimizer type # Episodic parameters 'support_size' : 128 , # Support set size 'query_size' : 128 , # Query set size 'n_episodes' : 500 , # Episodes per epoch 'steps_per_epoch' : 50 , # Gradient steps per epoch 'batch_size' : 4 , # Episodes per batch (small!) 'show_progress' : True # Progress bar }","title":"4.1 Fine-Tuning Parameters"},{"location":"models/mitra/#42-key-parameters","text":"Parameter Type Default Description support_size int 128 Context samples per episode query_size int 128 Query samples per episode n_episodes int 500 Total episodes for training steps_per_epoch int 50 Gradient updates per epoch batch_size int 4 Episodes per batch (keep small)","title":"4.2 Key Parameters"},{"location":"models/mitra/#43-why-small-batch-size","text":"Mitra's 2D attention is computationally expensive: - Attention complexity: (O(n^2)) for both rows and columns - Memory grows rapidly with batch size - Empirically: batch_size=4-8 is optimal - Larger: Use gradient accumulation instead","title":"4.3 Why Small Batch Size?"},{"location":"models/mitra/#44-fine-tuning-guidelines","text":"Support/Query Balance : support_size = 128 # Larger context for pattern discovery query_size = 128 # Balance for gradient signal Learning Rate Strategy : - Start: 1e-5 - If converging slowly: increase to 2e-5 - If diverging: decrease to 5e-6 Episode Count : - Small dataset (10K): 500 episodes - Medium dataset (100K): 1000 episodes - Large dataset (500K): 2000 episodes","title":"4.4 Fine-Tuning Guidelines"},{"location":"models/mitra/#5-lora-target-modules","text":"When using PEFT, Mitra targets these modules: target_modules = [ 'x_embedding' , # Feature embedder 'layers' , # Attention layers 'final_layer' # Prediction head ]","title":"5. LoRA Target Modules"},{"location":"models/mitra/#51-default-peft-configuration","text":"peft_config = { 'r' : 8 , 'lora_alpha' : 16 , 'lora_dropout' : 0.05 , 'target_modules' : None # Uses defaults above }","title":"5.1 Default PEFT Configuration"},{"location":"models/mitra/#52-peft-rank-guidelines","text":"Rank Memory Speed Accuracy r=4 Minimal Fast Good r=8 Low Moderate Better r=16 Moderate Slower Best","title":"5.2 PEFT Rank Guidelines"},{"location":"models/mitra/#6-usage-patterns","text":"","title":"6. Usage Patterns"},{"location":"models/mitra/#61-inference-only","text":"from tabtune import TabularPipeline pipeline = TabularPipeline ( model_name = 'Mitra' , tuning_strategy = 'inference' , model_params = { 'd_model' : 64 , 'num_layers' : 2 , 'use_synthetic_prior' : True } ) pipeline . fit ( X_train , y_train ) # Preprocessing only predictions = pipeline . predict ( X_test )","title":"6.1 Inference Only"},{"location":"models/mitra/#62-base-fine-tuning","text":"pipeline = TabularPipeline ( model_name = 'Mitra' , tuning_strategy = 'base-ft' , tuning_params = { 'device' : 'cuda' , 'epochs' : 3 , 'learning_rate' : 1e-5 , 'support_size' : 128 , 'query_size' : 128 , 'batch_size' : 4 , # Small! 'show_progress' : True } ) pipeline . fit ( X_train , y_train ) metrics = pipeline . evaluate ( X_test , y_test )","title":"6.2 Base Fine-Tuning"},{"location":"models/mitra/#63-peft-fine-tuning","text":"pipeline = TabularPipeline ( model_name = 'Mitra' , tuning_strategy = 'peft' , tuning_params = { 'device' : 'cuda' , 'epochs' : 3 , 'learning_rate' : 2e-5 , # Higher for PEFT 'support_size' : 64 , # Reduced for memory 'query_size' : 64 , 'batch_size' : 2 , # Very small 'peft_config' : { 'r' : 8 , 'lora_alpha' : 16 , 'lora_dropout' : 0.05 } } ) pipeline . fit ( X_train , y_train )","title":"6.3 PEFT Fine-Tuning"},{"location":"models/mitra/#64-architecture-customization","text":"pipeline = TabularPipeline ( model_name = 'Mitra' , tuning_strategy = 'base-ft' , model_params = { 'd_model' : 128 , # Larger embeddings 'num_layers' : 4 , # More layers 'num_heads' : 8 , # More heads 'd_ff' : 256 }, tuning_params = { 'epochs' : 5 , 'learning_rate' : 1e-5 } )","title":"6.4 Architecture Customization"},{"location":"models/mitra/#7-complete-examples","text":"","title":"7. Complete Examples"},{"location":"models/mitra/#71-basic-workflow","text":"from tabtune import TabularPipeline from sklearn.model_selection import train_test_split import pandas as pd # Load data df = pd . read_csv ( 'structured_data.csv' ) X = df . drop ( 'target' , axis = 1 ) y = df [ 'target' ] # Split X_train , X_test , y_train , y_test = train_test_split ( X , y , test_size = 0.2 , random_state = 42 ) # Train with Mitra pipeline = TabularPipeline ( model_name = 'Mitra' , tuning_strategy = 'base-ft' , tuning_params = { 'device' : 'cuda' , 'epochs' : 3 , 'support_size' : 128 , 'query_size' : 128 , 'batch_size' : 4 , 'learning_rate' : 1e-5 } ) pipeline . fit ( X_train , y_train ) metrics = pipeline . evaluate ( X_test , y_test ) print ( f \"Accuracy: { metrics [ 'accuracy' ] : .4f } \" ) print ( f \"F1 Score: { metrics [ 'f1_score' ] : .4f } \" )","title":"7.1 Basic Workflow"},{"location":"models/mitra/#72-peft-for-memory-constraints","text":"# Use PEFT when memory is limited pipeline = TabularPipeline ( model_name = 'Mitra' , tuning_strategy = 'peft' , tuning_params = { 'device' : 'cuda' , 'epochs' : 3 , 'learning_rate' : 2e-5 , 'support_size' : 64 , # Reduced 'query_size' : 64 , # Reduced 'batch_size' : 2 , # Very small 'steps_per_epoch' : 30 , # Fewer steps 'peft_config' : { 'r' : 4 , # Lower rank 'lora_alpha' : 8 , 'lora_dropout' : 0.1 } } ) pipeline . fit ( X_train , y_train )","title":"7.2 PEFT for Memory Constraints"},{"location":"models/mitra/#73-architecture-search","text":"from tabtune import TabularLeaderboard # Compare architectures lb = TabularLeaderboard ( X_train , X_test , y_train , y_test ) # Small model lb . add_model ( 'Mitra' , 'base-ft' , name = 'Mitra-Small' , model_params = { 'd_model' : 32 , 'num_layers' : 1 }, tuning_params = { 'epochs' : 3 } ) # Medium model lb . add_model ( 'Mitra' , 'base-ft' , name = 'Mitra-Medium' , model_params = { 'd_model' : 64 , 'num_layers' : 2 }, tuning_params = { 'epochs' : 3 } ) # Large model lb . add_model ( 'Mitra' , 'base-ft' , name = 'Mitra-Large' , model_params = { 'd_model' : 128 , 'num_layers' : 4 }, tuning_params = { 'epochs' : 3 } ) lb . run ( rank_by = 'accuracy' )","title":"7.3 Architecture Search"},{"location":"models/mitra/#74-production-deployment-saving-using-joblib","text":"import joblib # Train optimal model pipeline = TabularPipeline ( model_name = 'Mitra' , tuning_strategy = 'base-ft' , tuning_params = { 'device' : 'cuda' , 'epochs' : 5 , 'support_size' : 128 , 'batch_size' : 4 , 'learning_rate' : 1e-5 } ) pipeline . fit ( X_train , y_train ) metrics = pipeline . evaluate ( X_test , y_test ) # Save for deployment pipeline . save ( 'mitra_production.joblib' ) # In production loaded = TabularPipeline . load ( 'mitra_production.joblib' ) predictions = loaded . predict ( X_new )","title":"7.4 Production Deployment - Saving using joblib"},{"location":"models/mitra/#8-performance-characteristics","text":"","title":"8. Performance Characteristics"},{"location":"models/mitra/#81-speed-benchmarks","text":"Operation Time Notes Inference (batch=1000) 3-5s 2D attention overhead Base FT (3 epochs, 100K) 45-60m Slow but powerful PEFT (3 epochs, 100K) 20-30m Better speed Prediction latency 20-100ms Per sample","title":"8.1 Speed Benchmarks"},{"location":"models/mitra/#82-memory-usage","text":"Scenario Memory GPU VRAM Inference 8-10 GB 6GB minimum Base FT 16-20 GB 12GB recommended PEFT 10-12 GB 6-8GB sufficient Large model Up to 24 GB 16GB+ needed","title":"8.2 Memory Usage"},{"location":"models/mitra/#9-best-practices","text":"","title":"9. Best Practices"},{"location":"models/mitra/#dos","text":"\u2705 Use small batch sizes (2-4) \u2705 Start with medium architecture (d_model=64) \u2705 Monitor memory usage actively \u2705 Use PEFT on constrained systems \u2705 Increase support/query sizes for pattern discovery \u2705 Use synthetic priors (faster convergence)","title":"\u2705 Do's"},{"location":"models/mitra/#donts","text":"\u274c Don't use large batch sizes (causes OOM) \u274c Don't use very large models on small GPUs \u274c Don't skip warmup steps \u274c Don't disable gradient clipping \u274c Don't train for too many epochs (overfit risk)","title":"\u274c Don'ts"},{"location":"models/mitra/#10-troubleshooting","text":"","title":"10. Troubleshooting"},{"location":"models/mitra/#issue-cuda-out-of-memory","text":"Solution 1 : Reduce batch size tuning_params = { 'batch_size' : 2 , # Instead of 4 'support_size' : 64 , # Instead of 128 'query_size' : 64 } Solution 2 : Use PEFT tuning_strategy = 'peft'","title":"Issue: \"CUDA out of memory\""},{"location":"models/mitra/#issue-training-very-slow","text":"Solution : Reduce model size model_params = { 'd_model' : 32 , # Instead of 64 'num_layers' : 1 # Instead of 2 }","title":"Issue: \"Training very slow\""},{"location":"models/mitra/#issue-low-accuracy","text":"Solution : Increase support set size tuning_params = { 'support_size' : 256 , # More context 'query_size' : 256 , 'n_episodes' : 1000 # More training }","title":"Issue: \"Low accuracy\""},{"location":"models/mitra/#issue-overfitting-on-small-datasets","text":"Solution : Use regularization tuning_params = { 'weight_decay' : 0.1 , # Increase regularization 'dropout' : 0.2 # In model_params }","title":"Issue: \"Overfitting on small datasets\""},{"location":"models/mitra/#11-comparison-with-other-models","text":"Aspect Mitra TabICL TabDPT OrionMSP OrionBix Speed Slow Fast Slow Moderate Medium Memory Very High Moderate High Moderate-High High Accuracy Excellent Good Excellent Very Good Very Good Complexity Complex Simple Medium Medium Medium Small Data Good Good Okay \u26a0\ufe0f Good Large Data Good Good Excellent Excellent Good PEFT \u2705 Full \u2705 Full \u2705 Full \u2705 Full \u2705 Full","title":"11. Comparison with Other Models"},{"location":"models/mitra/#12-quick-reference","text":"Use Case Config Batch Size Support Small data (10K) d_model=64, layers=2 4 128 Medium data (100K) d_model=64, layers=2 4 256 Large data (500K) d_model=128, layers=4 2 512 Memory limited PEFT, r=4 2 64 Max accuracy d_model=128, layers=4 4 512","title":"12. Quick Reference"},{"location":"models/mitra/#13-next-steps","text":"Model Selection - Compare with other models Tuning Strategies - Fine-tuning details Advanced PEFT - LoRA optimization TabularLeaderboard - Benchmark Mitra Mitra excels at capturing complex 2D patterns in structured tabular data. Use it when maximum accuracy and pattern discovery are priorities!","title":"13. Next Steps"},{"location":"models/orion-bix/","text":"Orion BIX \u00b6 Orion BIX (Biaxial Interaction eXpert) enhances scalable ICL with biaxial (row \u00d7 column) interaction modules to better capture complex feature dependencies. Key ideas \u00b6 Biaxial interaction layers improve feature\u2013feature and sample\u2013sample coupling Robust ensembling across transformed feature orders Designed for accuracy on complex datasets while remaining scalable When to use \u00b6 Medium to large datasets (\u226550K rows) with strong cross\u2011feature interactions Accuracy\u2011critical workloads where TabICL underfits Inference parameters \u00b6 n_estimators (int): ensemble size across transformed views softmax_temperature (float): post\u2011logit scaling average_logits (bool): average logits vs. probabilities feat_shuffle_method (str): feature permutation policy (e.g., latin ) norm_methods (list[str]): normalization strategies per view batch_size (int): ensemble batch size Fine\u2011tuning parameters (via TuningManager) \u00b6 epochs (int): 3\u20138 typical learning_rate (float): 1e\u20115 \u2013 3e\u20114 (AdamW) support_size / query_size : episodic context/query sizes n_episodes (int): total adaptation episodes batch_size (int): episode batch size Usage \u00b6 from tabtune import TabularPipeline # Inference pipe = TabularPipeline ( model_name = \"OrionBix\" , task_type = \"classification\" , tuning_strategy = \"inference\" , model_params = { \"n_estimators\" : 24 , \"softmax_temperature\" : 0.9 , \"average_logits\" : True , \"feat_shuffle_method\" : \"latin\" , \"norm_methods\" : [ \"none\" , \"power\" ], }, ) pipe . fit ( X_train , y_train ) y_pred = pipe . predict ( X_test ) # PEFT fine\u2011tuning peft_pipe = TabularPipeline ( model_name = \"OrionBix\" , task_type = \"classification\" , tuning_strategy = \"peft\" , tuning_params = { \"epochs\" : 5 , \"learning_rate\" : 2e-5 , \"support_size\" : 1024 , \"query_size\" : 256 , \"batch_size\" : 8 , \"peft_config\" : { \"r\" : 8 , \"lora_alpha\" : 16 , \"lora_dropout\" : 0.05 }, }, ) peft_pipe . fit ( X_train , y_train ) Notes & caveats \u00b6 Slightly higher memory than OrionMSP due to biaxial layers Increase n_estimators for more stable calibration; reduce for speed Ensure preprocessing/encodings are consistent between splits","title":"Orion BIX"},{"location":"models/orion-bix/#orion-bix","text":"Orion BIX (Biaxial Interaction eXpert) enhances scalable ICL with biaxial (row \u00d7 column) interaction modules to better capture complex feature dependencies.","title":"Orion BIX"},{"location":"models/orion-bix/#key-ideas","text":"Biaxial interaction layers improve feature\u2013feature and sample\u2013sample coupling Robust ensembling across transformed feature orders Designed for accuracy on complex datasets while remaining scalable","title":"Key ideas"},{"location":"models/orion-bix/#when-to-use","text":"Medium to large datasets (\u226550K rows) with strong cross\u2011feature interactions Accuracy\u2011critical workloads where TabICL underfits","title":"When to use"},{"location":"models/orion-bix/#inference-parameters","text":"n_estimators (int): ensemble size across transformed views softmax_temperature (float): post\u2011logit scaling average_logits (bool): average logits vs. probabilities feat_shuffle_method (str): feature permutation policy (e.g., latin ) norm_methods (list[str]): normalization strategies per view batch_size (int): ensemble batch size","title":"Inference parameters"},{"location":"models/orion-bix/#finetuning-parameters-via-tuningmanager","text":"epochs (int): 3\u20138 typical learning_rate (float): 1e\u20115 \u2013 3e\u20114 (AdamW) support_size / query_size : episodic context/query sizes n_episodes (int): total adaptation episodes batch_size (int): episode batch size","title":"Fine\u2011tuning parameters (via TuningManager)"},{"location":"models/orion-bix/#usage","text":"from tabtune import TabularPipeline # Inference pipe = TabularPipeline ( model_name = \"OrionBix\" , task_type = \"classification\" , tuning_strategy = \"inference\" , model_params = { \"n_estimators\" : 24 , \"softmax_temperature\" : 0.9 , \"average_logits\" : True , \"feat_shuffle_method\" : \"latin\" , \"norm_methods\" : [ \"none\" , \"power\" ], }, ) pipe . fit ( X_train , y_train ) y_pred = pipe . predict ( X_test ) # PEFT fine\u2011tuning peft_pipe = TabularPipeline ( model_name = \"OrionBix\" , task_type = \"classification\" , tuning_strategy = \"peft\" , tuning_params = { \"epochs\" : 5 , \"learning_rate\" : 2e-5 , \"support_size\" : 1024 , \"query_size\" : 256 , \"batch_size\" : 8 , \"peft_config\" : { \"r\" : 8 , \"lora_alpha\" : 16 , \"lora_dropout\" : 0.05 }, }, ) peft_pipe . fit ( X_train , y_train )","title":"Usage"},{"location":"models/orion-bix/#notes-caveats","text":"Slightly higher memory than OrionMSP due to biaxial layers Increase n_estimators for more stable calibration; reduce for speed Ensure preprocessing/encodings are consistent between splits","title":"Notes &amp; caveats"},{"location":"models/orion-msp/","text":"OrionMSP \u00b6 OrionMSP is a scalable in-context learning model that leverages multi\u2011scale synthetic priors and episodic adaptation for robust tabular classification. Key ideas \u00b6 Multi\u2011scale synthetic priors to initialize inductive biases Column\u2011then\u2011row attention for efficient context reasoning Ensemble over transformed views for stability Works with standard tabular dtypes: numeric, categorical (after encoding) When to use \u00b6 Datasets from 50K to multi\u2011million rows where ICL scales better than PFN Strong generalization with modest fine\u2011tuning budget Inference parameters \u00b6 n_estimators (int): number of transformed views to ensemble softmax_temperature (float): post\u2011logit scaling (0.5\u20131.5) average_logits (bool): average logits vs. probabilities batch_size (int): batch size for ensembling Fine\u2011tuning parameters (via TuningManager) \u00b6 epochs (int): number of epochs (typ. 3\u201310) learning_rate (float): AdamW LR (1e\u20115 \u2013 5e\u20114) support_size (int): context samples per episode (e.g., 512\u20134096) query_size (int): query samples per episode (e.g., 128\u2013512) n_episodes (int): total adaptation episodes batch_size (int): episode batch size Usage \u00b6 from tabtune import TabularPipeline # Inference / zero-shot pipe = TabularPipeline ( model_name = \"OrionMSP\" , task_type = \"classification\" , tuning_strategy = \"inference\" , model_params = { \"n_estimators\" : 16 , \"softmax_temperature\" : 0.9 , \"average_logits\" : True , }, ) pipe . fit ( X_train , y_train ) y_pred = pipe . predict ( X_test ) # Base fine-tuning ft_pipe = TabularPipeline ( model_name = \"OrionMSP\" , task_type = \"classification\" , tuning_strategy = \"base-ft\" , tuning_params = { \"epochs\" : 5 , \"learning_rate\" : 2e-5 , \"support_size\" : 1024 , \"query_size\" : 256 , \"batch_size\" : 8 , }, ) ft_pipe . fit ( X_train , y_train ) PEFT (LoRA) \u00b6 Supported; target linear projections in attention and MLPs are typical. Start with r=8, lora_alpha=16, lora_dropout=0.05 . Notes & caveats \u00b6 Ensure categorical encoding is consistent between train/test splits. For very small datasets (<20K), consider TabPFN or TabICL instead. Increase n_estimators for stability on noisy datasets.","title":"Orion MSP"},{"location":"models/orion-msp/#orionmsp","text":"OrionMSP is a scalable in-context learning model that leverages multi\u2011scale synthetic priors and episodic adaptation for robust tabular classification.","title":"OrionMSP"},{"location":"models/orion-msp/#key-ideas","text":"Multi\u2011scale synthetic priors to initialize inductive biases Column\u2011then\u2011row attention for efficient context reasoning Ensemble over transformed views for stability Works with standard tabular dtypes: numeric, categorical (after encoding)","title":"Key ideas"},{"location":"models/orion-msp/#when-to-use","text":"Datasets from 50K to multi\u2011million rows where ICL scales better than PFN Strong generalization with modest fine\u2011tuning budget","title":"When to use"},{"location":"models/orion-msp/#inference-parameters","text":"n_estimators (int): number of transformed views to ensemble softmax_temperature (float): post\u2011logit scaling (0.5\u20131.5) average_logits (bool): average logits vs. probabilities batch_size (int): batch size for ensembling","title":"Inference parameters"},{"location":"models/orion-msp/#finetuning-parameters-via-tuningmanager","text":"epochs (int): number of epochs (typ. 3\u201310) learning_rate (float): AdamW LR (1e\u20115 \u2013 5e\u20114) support_size (int): context samples per episode (e.g., 512\u20134096) query_size (int): query samples per episode (e.g., 128\u2013512) n_episodes (int): total adaptation episodes batch_size (int): episode batch size","title":"Fine\u2011tuning parameters (via TuningManager)"},{"location":"models/orion-msp/#usage","text":"from tabtune import TabularPipeline # Inference / zero-shot pipe = TabularPipeline ( model_name = \"OrionMSP\" , task_type = \"classification\" , tuning_strategy = \"inference\" , model_params = { \"n_estimators\" : 16 , \"softmax_temperature\" : 0.9 , \"average_logits\" : True , }, ) pipe . fit ( X_train , y_train ) y_pred = pipe . predict ( X_test ) # Base fine-tuning ft_pipe = TabularPipeline ( model_name = \"OrionMSP\" , task_type = \"classification\" , tuning_strategy = \"base-ft\" , tuning_params = { \"epochs\" : 5 , \"learning_rate\" : 2e-5 , \"support_size\" : 1024 , \"query_size\" : 256 , \"batch_size\" : 8 , }, ) ft_pipe . fit ( X_train , y_train )","title":"Usage"},{"location":"models/orion-msp/#peft-lora","text":"Supported; target linear projections in attention and MLPs are typical. Start with r=8, lora_alpha=16, lora_dropout=0.05 .","title":"PEFT (LoRA)"},{"location":"models/orion-msp/#notes-caveats","text":"Ensure categorical encoding is consistent between train/test splits. For very small datasets (<20K), consider TabPFN or TabICL instead. Increase n_estimators for stability on noisy datasets.","title":"Notes &amp; caveats"},{"location":"models/overview/","text":"Supported Models Overview \u00b6 TabTune integrates state-of-the-art tabular foundation models, each with unique architectural properties, strengths, and use cases. This document provides a comprehensive overview of all supported models. 1. Model Ecosystem \u00b6 flowchart TD A[Tabular Foundation Models] --> B[ICL-Based Models] A --> C[Transformer-Based Models] A --> D[PFN-Based Models] B --> E[TabICL] B --> F[OrionMSP] B --> G[Orion BIX] B --> H[Mitra] B --> I[ContextTab] C --> J[TabDPT] D --> K[TabPFN] 2. Model Comparison Matrix \u00b6 Model Paradigm Architecture Best For Scaling Speed Memory PEFT TabPFN PFN/ICL Approximate Bayesian Small datasets <10K \u2b50\u2b50\u2b50\u2b50\u2b50 \u2b50\u2b50 \u26a0\ufe0f TabICL Scalable ICL Column-Row Attention Balanced 10K-1M \u2b50\u2b50\u2b50\u2b50 \u2b50\u2b50\u2b50 \u2705 OrionMSP Scalable ICL Multi\u2011scale priors Generalization 50K-2M+ \u2b50\u2b50\u2b50 \u2b50\u2b50\u2b50 \u2705 Orion BIX Scalable ICL Biaxial interactions Accuracy 50K-2M+ \u2b50\u2b50 \u2b50\u2b50\u2b50\u2b50 \u2705 TabDPT Denoising Transformer Large Datasets 100K-5M \u2b50\u2b50\u2b50 \u2b50\u2b50\u2b50\u2b50 \u2705 Mitra 2D Attention Cross\u2011Attention Complex Patterns 10K-500K \u2b50\u2b50 \u2b50\u2b50\u2b50\u2b50\u2b50 \u2705 ContextTab Semantic ICL Text + Embeddings Text-Heavy 10K-500K \u2b50\u2b50 \u2b50\u2b50\u2b50 \u26a0\ufe0f 3. Selection Quick Tips \u00b6 <10K rows: TabPFN (inference) or TabICL (base\u2011ft) 50K\u20132M rows: OrionMSP (balanced) or Orion BIX (accuracy\u2011oriented) 2M rows: TabDPT (base\u2011ft/PEFT) Text\u2011heavy features: ContextTab 4. Feature Support Matrix \u00b6 Feature TabPFN TabICL OrionMSP OrionBix TabDPT Mitra ContextTab Numerical Features \u2705 \u2705 \u2705 \u2705 \u2705 \u2705 \u2705 Categorical Features \u2705 \u2705 \u2705 \u2705 \u2705 \u2705 \u2705 Missing Values \u2705 \u2705 \u2705 \u2705 \u2705 \u2705 \u2705 Text Features \u274c \u274c \u274c \u274c \u274c \u274c \u2705 Large Datasets (>1M) \u274c \u2705 \u2705 \u2705 \u2705 \u26a0\ufe0f \u274c Small Datasets (<10K) \u2705 \u2705 \u26a0\ufe0f \u26a0\ufe0f \u26a0\ufe0f \u2705 \u2705 PEFT Support \u26a0\ufe0f \u2705 \u2705 \u2705 \u2705 \u2705 \u26a0\ufe0f Multi-GPU Training \u274c \u2705 \u2705 \u2705 \u2705 \u2705 \u2705 5. Performance Benchmarks \u00b6 Performance characteristics vary significantly based on dataset size, hardware, and hyperparameters. The following benchmarks provide rough estimates based on typical configurations. Benchmark Disclaimer All benchmarks are approximate and depend on: - Hardware (GPU model, CPU, memory) - Dataset characteristics (size, features, class distribution) - Hyperparameter settings - Software versions Use these as rough guidelines for relative comparisons. 5.1 Accuracy Benchmarks \u00b6 Typical accuracy ranges on standard OpenML datasets (medium-sized, ~10K-50K samples): Model Strategy Accuracy Range Notes TabPFN inference 0.75-0.85 Best on small, clean datasets TabICL base-ft 0.80-0.92 Balanced performance TabICL peft 0.78-0.90 ~2-5% below base-ft OrionMSP base-ft 0.82-0.93 Strong generalization OrionBix base-ft 0.85-0.94 Highest accuracy potential TabDPT base-ft 0.83-0.92 Excellent on large datasets Mitra base-ft 0.84-0.93 Complex pattern handling ContextTab base-ft 0.75-0.88 Best with text features Notes: - Ranges represent typical performance on diverse datasets - Your results may vary significantly based on dataset characteristics - Fine-tuning (base-ft/peft) generally outperforms inference by 5-15% 5.2 Training Time Benchmarks \u00b6 Approximate training times (5 epochs, medium dataset ~20K samples, NVIDIA RTX 3090): Model Strategy Training Time Speed Factor TabPFN inference 0s Instant TabPFN base-ft 10-30 min Fast TabICL inference <1 min Very fast TabICL base-ft 15-45 min Moderate TabICL peft 8-20 min Fast OrionMSP base-ft 30-90 min Moderate-Slow OrionBix base-ft 45-120 min Slow TabDPT base-ft 20-60 min Moderate Mitra base-ft 60-180 min Slow ContextTab base-ft 30-90 min Moderate-Slow Factors affecting training time: - Dataset size (rows \u00d7 features) - Number of epochs - Batch size - GPU/CPU speed - Model complexity 5.3 Memory Usage Estimates \u00b6 Peak memory usage during training (approximate, GPU memory): Model Strategy Memory Range Notes TabPFN inference 2-4 GB Small datasets TabICL inference 3-6 GB Moderate TabICL base-ft 8-16 GB Full model TabICL peft 4-8 GB 40-50% reduction OrionMSP base-ft 10-20 GB Large context OrionBix base-ft 12-24 GB Biaxial layers TabDPT base-ft 12-28 GB Large transformer Mitra base-ft 16-32 GB 2D attention ContextTab base-ft 8-16 GB Embedding overhead Memory optimization tips: - Use PEFT strategy (reduces memory by 40-60%) - Reduce batch size - Use gradient accumulation - Process large datasets in chunks 5.4 Inference Latency \u00b6 Average inference time per batch (batch_size=32, GPU): Model Latency (ms/batch) Throughput (samples/s) TabPFN 10-50 640-3200 TabICL 20-80 400-1600 OrionMSP 40-120 267-800 OrionBix 60-150 213-533 TabDPT 30-100 320-1067 Mitra 80-200 160-400 ContextTab 100-300 107-320 Note: Latency increases with dataset size (for ICL models that use training data as context). 5.5 Benchmark Methodology \u00b6 When comparing models: Use same dataset splits : Ensure train/test consistency Same preprocessing : Use identical DataProcessor settings Multiple runs : Average over 3-5 runs with different seeds Hardware consistency : Same GPU/CPU for fair comparison Hyperparameter tuning : Optimize each model fairly Recommended benchmark datasets: - OpenML datasets (42178, 1489, etc.) - Your domain-specific datasets - Standard UCI ML datasets Each model excels in different scenarios. Use this overview to pick the best fit for your task.","title":"Overview"},{"location":"models/overview/#supported-models-overview","text":"TabTune integrates state-of-the-art tabular foundation models, each with unique architectural properties, strengths, and use cases. This document provides a comprehensive overview of all supported models.","title":"Supported Models Overview"},{"location":"models/overview/#1-model-ecosystem","text":"flowchart TD A[Tabular Foundation Models] --> B[ICL-Based Models] A --> C[Transformer-Based Models] A --> D[PFN-Based Models] B --> E[TabICL] B --> F[OrionMSP] B --> G[Orion BIX] B --> H[Mitra] B --> I[ContextTab] C --> J[TabDPT] D --> K[TabPFN]","title":"1. Model Ecosystem"},{"location":"models/overview/#2-model-comparison-matrix","text":"Model Paradigm Architecture Best For Scaling Speed Memory PEFT TabPFN PFN/ICL Approximate Bayesian Small datasets <10K \u2b50\u2b50\u2b50\u2b50\u2b50 \u2b50\u2b50 \u26a0\ufe0f TabICL Scalable ICL Column-Row Attention Balanced 10K-1M \u2b50\u2b50\u2b50\u2b50 \u2b50\u2b50\u2b50 \u2705 OrionMSP Scalable ICL Multi\u2011scale priors Generalization 50K-2M+ \u2b50\u2b50\u2b50 \u2b50\u2b50\u2b50 \u2705 Orion BIX Scalable ICL Biaxial interactions Accuracy 50K-2M+ \u2b50\u2b50 \u2b50\u2b50\u2b50\u2b50 \u2705 TabDPT Denoising Transformer Large Datasets 100K-5M \u2b50\u2b50\u2b50 \u2b50\u2b50\u2b50\u2b50 \u2705 Mitra 2D Attention Cross\u2011Attention Complex Patterns 10K-500K \u2b50\u2b50 \u2b50\u2b50\u2b50\u2b50\u2b50 \u2705 ContextTab Semantic ICL Text + Embeddings Text-Heavy 10K-500K \u2b50\u2b50 \u2b50\u2b50\u2b50 \u26a0\ufe0f","title":"2. Model Comparison Matrix"},{"location":"models/overview/#3-selection-quick-tips","text":"<10K rows: TabPFN (inference) or TabICL (base\u2011ft) 50K\u20132M rows: OrionMSP (balanced) or Orion BIX (accuracy\u2011oriented) 2M rows: TabDPT (base\u2011ft/PEFT) Text\u2011heavy features: ContextTab","title":"3. Selection Quick Tips"},{"location":"models/overview/#4-feature-support-matrix","text":"Feature TabPFN TabICL OrionMSP OrionBix TabDPT Mitra ContextTab Numerical Features \u2705 \u2705 \u2705 \u2705 \u2705 \u2705 \u2705 Categorical Features \u2705 \u2705 \u2705 \u2705 \u2705 \u2705 \u2705 Missing Values \u2705 \u2705 \u2705 \u2705 \u2705 \u2705 \u2705 Text Features \u274c \u274c \u274c \u274c \u274c \u274c \u2705 Large Datasets (>1M) \u274c \u2705 \u2705 \u2705 \u2705 \u26a0\ufe0f \u274c Small Datasets (<10K) \u2705 \u2705 \u26a0\ufe0f \u26a0\ufe0f \u26a0\ufe0f \u2705 \u2705 PEFT Support \u26a0\ufe0f \u2705 \u2705 \u2705 \u2705 \u2705 \u26a0\ufe0f Multi-GPU Training \u274c \u2705 \u2705 \u2705 \u2705 \u2705 \u2705","title":"4. Feature Support Matrix"},{"location":"models/overview/#5-performance-benchmarks","text":"Performance characteristics vary significantly based on dataset size, hardware, and hyperparameters. The following benchmarks provide rough estimates based on typical configurations. Benchmark Disclaimer All benchmarks are approximate and depend on: - Hardware (GPU model, CPU, memory) - Dataset characteristics (size, features, class distribution) - Hyperparameter settings - Software versions Use these as rough guidelines for relative comparisons.","title":"5. Performance Benchmarks"},{"location":"models/overview/#51-accuracy-benchmarks","text":"Typical accuracy ranges on standard OpenML datasets (medium-sized, ~10K-50K samples): Model Strategy Accuracy Range Notes TabPFN inference 0.75-0.85 Best on small, clean datasets TabICL base-ft 0.80-0.92 Balanced performance TabICL peft 0.78-0.90 ~2-5% below base-ft OrionMSP base-ft 0.82-0.93 Strong generalization OrionBix base-ft 0.85-0.94 Highest accuracy potential TabDPT base-ft 0.83-0.92 Excellent on large datasets Mitra base-ft 0.84-0.93 Complex pattern handling ContextTab base-ft 0.75-0.88 Best with text features Notes: - Ranges represent typical performance on diverse datasets - Your results may vary significantly based on dataset characteristics - Fine-tuning (base-ft/peft) generally outperforms inference by 5-15%","title":"5.1 Accuracy Benchmarks"},{"location":"models/overview/#52-training-time-benchmarks","text":"Approximate training times (5 epochs, medium dataset ~20K samples, NVIDIA RTX 3090): Model Strategy Training Time Speed Factor TabPFN inference 0s Instant TabPFN base-ft 10-30 min Fast TabICL inference <1 min Very fast TabICL base-ft 15-45 min Moderate TabICL peft 8-20 min Fast OrionMSP base-ft 30-90 min Moderate-Slow OrionBix base-ft 45-120 min Slow TabDPT base-ft 20-60 min Moderate Mitra base-ft 60-180 min Slow ContextTab base-ft 30-90 min Moderate-Slow Factors affecting training time: - Dataset size (rows \u00d7 features) - Number of epochs - Batch size - GPU/CPU speed - Model complexity","title":"5.2 Training Time Benchmarks"},{"location":"models/overview/#53-memory-usage-estimates","text":"Peak memory usage during training (approximate, GPU memory): Model Strategy Memory Range Notes TabPFN inference 2-4 GB Small datasets TabICL inference 3-6 GB Moderate TabICL base-ft 8-16 GB Full model TabICL peft 4-8 GB 40-50% reduction OrionMSP base-ft 10-20 GB Large context OrionBix base-ft 12-24 GB Biaxial layers TabDPT base-ft 12-28 GB Large transformer Mitra base-ft 16-32 GB 2D attention ContextTab base-ft 8-16 GB Embedding overhead Memory optimization tips: - Use PEFT strategy (reduces memory by 40-60%) - Reduce batch size - Use gradient accumulation - Process large datasets in chunks","title":"5.3 Memory Usage Estimates"},{"location":"models/overview/#54-inference-latency","text":"Average inference time per batch (batch_size=32, GPU): Model Latency (ms/batch) Throughput (samples/s) TabPFN 10-50 640-3200 TabICL 20-80 400-1600 OrionMSP 40-120 267-800 OrionBix 60-150 213-533 TabDPT 30-100 320-1067 Mitra 80-200 160-400 ContextTab 100-300 107-320 Note: Latency increases with dataset size (for ICL models that use training data as context).","title":"5.4 Inference Latency"},{"location":"models/overview/#55-benchmark-methodology","text":"When comparing models: Use same dataset splits : Ensure train/test consistency Same preprocessing : Use identical DataProcessor settings Multiple runs : Average over 3-5 runs with different seeds Hardware consistency : Same GPU/CPU for fair comparison Hyperparameter tuning : Optimize each model fairly Recommended benchmark datasets: - OpenML datasets (42178, 1489, etc.) - Your domain-specific datasets - Standard UCI ML datasets Each model excels in different scenarios. Use this overview to pick the best fit for your task.","title":"5.5 Benchmark Methodology"},{"location":"models/tabdpt/","text":"TabDPT: Tabular Denoising Pre-trained Transformer \u00b6 TabDPT is a large-scale tabular model pre-trained via denoising objectives on diverse datasets. This document provides comprehensive guidance for using TabDPT with TabTune for maximum scalability and robustness. 1. Introduction \u00b6 What is TabDPT? TabDPT (Tabular Denoising Pre-trained Transformer) is a state-of-the-art model designed for: Large-Scale Learning : Scales to datasets with millions of samples Robust Feature Learning : Pre-trained on denoising objectives Noise Resilience : Handles missing and corrupted features Context-Aware Predictions : k-NN based context selection Strong Generalization : Pre-trained on diverse tabular corpora Key Innovation : Pre-training via masked feature prediction (denoising) enables robust feature representations and strong generalization to new tasks. 2. Architecture \u00b6 2.1 High-Level Design \u00b6 flowchart LR A[Input Features] --> B[Masking Layer] B --> C[Noisy Features] C --> D[Transformer Encoder] D --> E[Hidden Representations] E --> F[k-NN Context Retrieval] F --> G[Context Features] G --> H[Transformer Decoder] H --> I[Reconstructed Features] I --> J[Prediction Head] J --> K[Output] 2.3 Pre-training Strategy \u00b6 Pre-training Phase (on diverse data): 1. Mask random features (30-50%) 2. Encode remaining features 3. Retrieve k-NN context 4. Predict masked features 5. Loss = MSE(predicted, actual) Fine-tuning Phase (on your task): 1. Replace prediction head 2. Fine-tune on task labels 3. Use pre-trained encoder 3. Inference Parameters \u00b6 3.1 Complete Parameter Reference \u00b6 model_params = { # Architecture 'd_model' : 256 , # Embedding dimension 'num_heads' : 8 , # Attention heads 'num_layers' : 4 , # Transformer layers 'hidden_size' : 512 , # Feedforward hidden size 'dropout' : 0.1 , # Dropout probability # Context retrieval 'k_neighbors' : 5 , # Number of neighbors for context 'context_mode' : 'mixed' , # 'mixed' or 'features_only' # Inference behavior 'n_ensembles' : 8 , # Multiple runs 'temperature' : 0.3 , # Output scaling 'mask_ratio' : 0.3 , # Feature masking during inference # Training 'use_pretrain' : True , # Use pre-trained weights 'seed' : 42 # Reproducibility } 3.2 Parameter Descriptions \u00b6 Parameter Type Default Range Description d_model int 256 128-512 Transformer embedding dimension num_heads int 8 4-16 Number of attention heads num_layers int 4 2-8 Number of transformer layers hidden_size int 512 256-1024 Feedforward hidden dimension dropout float 0.1 0.0-0.3 Dropout probability k_neighbors int 5 1-50 k-NN context neighbors context_mode str 'mixed' 'mixed', 'features_only' How to use context n_ensembles int 8 1-16 Number of ensemble runs temperature float 0.3 0.1-1.0 Output temperature use_pretrain bool True True/False Use pre-trained weights 3.3 Architecture Tuning \u00b6 Config Speed Accuracy Memory Best For Small: d=128, layers=2 \u2b50\u2b50\u2b50\u2b50\u2b50 \u2b50\u2b50 \u2b50 Quick baseline Medium: d=256, layers=4 \u2b50\u2b50\u2b50\u2b50 \u2b50\u2b50\u2b50\u2b50 \u2b50\u2b50 Balanced Large: d=512, layers=8 \u2b50\u2b50\u2b50 \u2b50\u2b50\u2b50\u2b50\u2b50 \u2b50\u2b50\u2b50 Max accuracy 3.4 Context Modes \u00b6 context_modes = { 'mixed' : 'Use both context features and their representations' , 'features_only' : 'Use only context features, not representations' } # Typically 'mixed' is better model_params = { 'context_mode' : 'mixed' } 4. Fine-Tuning with TabDPT \u00b6 TabDPT uses episodic fine-tuning with large context windows. 4.1 Fine-Tuning Parameters \u00b6 tuning_params = { 'device' : 'cuda' , 'epochs' : 3 , # Few epochs needed (pre-trained) 'learning_rate' : 2e-5 , # Conservative learning rate 'optimizer' : 'adamw' , # Optimizer type 'scheduler' : 'linear' , # Learning rate scheduler 'warmup_steps' : 500 , # Extended warmup 'weight_decay' : 0.01 , # L2 regularization 'gradient_clip_value' : 1.0 , # Gradient clipping # Large context for TabDPT 'support_size' : 1024 , # Large context 'query_size' : 256 , # Prediction samples 'steps_per_epoch' : 15 , # Gradient steps 'batch_size' : 32 , # Standard batch 'show_progress' : True # Progress bar } 4.2 Key Parameters \u00b6 Parameter Type Default Description support_size int 1024 Large context for k-NN query_size int 256 Query samples per episode steps_per_epoch int 15 Optimization steps batch_size int 32 Samples per batch 4.3 Fine-Tuning Guidelines \u00b6 Large Context Windows : # TabDPT benefits from large context tuning_params = { 'support_size' : 1024 , # Large context (TabDPT strength) 'query_size' : 256 , # Balance for gradients 'batch_size' : 32 # Process in parallel } Learning Rate Strategy : - 1e-5: Conservative, safe - 2e-5: Balanced (default) - 5e-5: Aggressive Pre-training Advantage : - TabDPT needs fewer epochs due to pre-training - Typically 3-5 epochs sufficient - Convergence faster than TabICL 4.4 Dataset Recommendations \u00b6 # TabDPT shines with large datasets dataset_sizes = { '10K' : 'Acceptable, TabICL better' , '100K' : 'Good fit for TabDPT' , '1M' : 'Excellent for TabDPT' , '5M+' : 'Perfect use case' } 5. LoRA Target Modules \u00b6 When using PEFT, TabDPT targets these modules: target_modules = [ 'transformer_encoder' , # Main encoder 'encoder' , # Additional encoder 'y_encoder' , # Label encoder 'head' # Prediction head ] 5.1 Default PEFT Configuration \u00b6 peft_config = { 'r' : 8 , 'lora_alpha' : 16 , 'lora_dropout' : 0.05 , 'target_modules' : None # Uses defaults } 5.2 PEFT for Large Models \u00b6 # PEFT works well with TabDPT's large architecture pipeline = TabularPipeline ( model_name = 'TabDPT' , tuning_strategy = 'peft' , tuning_params = { 'device' : 'cuda' , 'epochs' : 3 , 'learning_rate' : 2e-4 , 'support_size' : 512 , # Still large 'peft_config' : { 'r' : 16 } # Higher rank acceptable } ) 6. Usage Patterns \u00b6 6.1 Inference Only \u00b6 from tabtune import TabularPipeline pipeline = TabularPipeline ( model_name = 'TabDPT' , tuning_strategy = 'inference' , ) pipeline . fit ( X_train , y_train ) predictions = pipeline . predict ( X_test ) 6.2 Base Fine-Tuning on Large Dataset \u00b6 # TabDPT excels with large datasets pipeline = TabularPipeline ( model_name = 'TabDPT' , tuning_strategy = 'base-ft' , tuning_params = { 'device' : 'cuda' , 'epochs' : 3 , 'learning_rate' : 2e-5 , 'support_size' : 1024 , # Large context 'query_size' : 256 , 'steps_per_epoch' : 15 , 'batch_size' : 32 , 'show_progress' : True } ) pipeline . fit ( X_train , y_train ) # 100K+ samples ideal metrics = pipeline . evaluate ( X_test , y_test ) 6.3 PEFT Fine-Tuning \u00b6 pipeline = TabularPipeline ( model_name = 'TabDPT' , tuning_strategy = 'peft' , tuning_params = { 'device' : 'cuda' , 'epochs' : 3 , 'learning_rate' : 2e-4 , 'support_size' : 512 , 'query_size' : 256 , 'peft_config' : { 'r' : 8 , 'lora_alpha' : 16 , 'lora_dropout' : 0.05 } } ) pipeline . fit ( X_train , y_train ) 7. Complete Examples \u00b6 7.1 Large Dataset Workflow \u00b6 from tabtune import TabularPipeline from sklearn.model_selection import train_test_split import pandas as pd # Load large dataset (1M+ rows) df = pd . read_csv ( 'large_dataset.csv' ) # 1M+ rows X = df . drop ( 'target' , axis = 1 ) y = df [ 'target' ] # Split X_train , X_test , y_train , y_test = train_test_split ( X , y , test_size = 0.1 , random_state = 42 ) # Train TabDPT pipeline = TabularPipeline ( model_name = 'TabDPT' , tuning_strategy = 'base-ft' , tuning_params = { 'device' : 'cuda' , 'epochs' : 3 , 'learning_rate' : 2e-5 , 'support_size' : 1024 , 'query_size' : 256 , 'batch_size' : 32 , 'show_progress' : True } ) pipeline . fit ( X_train , y_train ) metrics = pipeline . evaluate ( X_test , y_test ) print ( f \"Accuracy: { metrics [ 'accuracy' ] : .4f } \" ) print ( f \"Training completed on { len ( X_train ) } samples\" ) 7.2 Production Model with PEFT - Saving using joblib \u00b6 # PEFT for production deployment pipeline = TabularPipeline ( model_name = 'TabDPT' , tuning_strategy = 'peft' , tuning_params = { 'device' : 'cuda' , 'epochs' : 3 , 'learning_rate' : 2e-4 , 'support_size' : 512 , 'peft_config' : { 'r' : 8 } } ) pipeline . fit ( X_train , y_train ) metrics = pipeline . evaluate ( X_test , y_test ) # Save for deployment pipeline . save ( 'tabdpt_production.joblib' ) 7.3 Architecture Comparison \u00b6 from tabtune import TabularLeaderboard # Compare architectures lb = TabularLeaderboard ( X_train , X_test , y_train , y_test ) # Small lb . add_model ( 'TabDPT' , 'base-ft' , name = 'TabDPT-Small' , model_params = { 'd_model' : 128 , 'num_layers' : 2 }, tuning_params = { 'epochs' : 3 } ) # Medium lb . add_model ( 'TabDPT' , 'base-ft' , name = 'TabDPT-Medium' , model_params = { 'd_model' : 256 , 'num_layers' : 4 }, tuning_params = { 'epochs' : 3 } ) # Large lb . add_model ( 'TabDPT' , 'base-ft' , name = 'TabDPT-Large' , model_params = { 'd_model' : 512 , 'num_layers' : 8 }, tuning_params = { 'epochs' : 3 } ) lb . run ( rank_by = 'accuracy' ) 9. Best Practices \u00b6 \u2705 Do's \u00b6 \u2705 Use large context windows (support_size >= 512) \u2705 Use on datasets with 100K+ samples \u2705 Leverage pre-trained weights \u2705 Use few epochs (3-5) due to pre-training \u2705 Monitor for overfitting with regularization \u2705 Use PEFT for faster training \u2705 Include k-NN context \u274c Don'ts \u00b6 \u274c Don't use on small datasets (<10K) \u274c Don't use without pre-training \u274c Don't train for too many epochs \u274c Don't use small context windows \u274c Don't disable masking (helps robustness) 10. Troubleshooting \u00b6 Issue: \"Context retrieval slow\" \u00b6 Solution : Reduce k_neighbors or use approximate k-NN model_params = { 'k_neighbors' : 3 , # Instead of 5 'context_mode' : 'features_only' } Issue: \"Out of memory with large support_size\" \u00b6 Solution : Use PEFT or reduce support size tuning_params = { 'support_size' : 512 , # Instead of 1024 'batch_size' : 16 # Smaller batch } Issue: \"Accuracy plateauing\" \u00b6 Solution : Increase training budget tuning_params = { 'epochs' : 5 , # More epochs 'steps_per_epoch' : 20 , # More steps 'warmup_steps' : 1000 # Longer warmup } Issue: \"Prediction latency too high\" \u00b6 Solution : Use smaller ensemble model_params = { 'n_ensembles' : 2 , # Instead of 8 'k_neighbors' : 3 # Fewer neighbors } 11. Comparison with Other Models \u00b6 Aspect TabDPT TabICL Mitra TabPFN Small data (10K) Poor Good Good Excellent Large data (1M) Excellent Good Okay N/A Accuracy Excellent Good Excellent Medium Speed Slow Fast Slow Fastest Memory High Moderate Very High Low Pre-training Yes No No Yes PEFT \u2705 Full \u2705 Full \u2705 Full \u26a0\ufe0f Exp 12. When to Use TabDPT \u00b6 Use TabDPT when : - \u2705 Dataset has 100K+ samples - \u2705 Maximum accuracy is priority - \u2705 Data has missing/noisy values - \u2705 You have sufficient memory - \u2705 Training time is not critical - \u2705 Deployment can handle context retrieval Don't use TabDPT for : - \u274c Small datasets (<50K) - \u274c When prediction speed critical - \u274c Very memory-constrained systems - \u274c When training time is limited 14. Quick Reference \u00b6 Use Case Strategy Config Support Baseline inference default 1024 Production (1M data) base-ft default 1024 Memory limited peft r=8 512 Max accuracy base-ft large model 2048 Fast inference peft r=4 256 15. Next Steps \u00b6 Model Selection - Compare with other models Tuning Strategies - Fine-tuning details Advanced PEFT - LoRA optimization TabularLeaderboard - Benchmark TabDPT TabDPT excels at large-scale tabular learning with pre-trained robustness. Use it for production systems with millions of samples!","title":"TabDPT"},{"location":"models/tabdpt/#tabdpt-tabular-denoising-pre-trained-transformer","text":"TabDPT is a large-scale tabular model pre-trained via denoising objectives on diverse datasets. This document provides comprehensive guidance for using TabDPT with TabTune for maximum scalability and robustness.","title":"TabDPT: Tabular Denoising Pre-trained Transformer"},{"location":"models/tabdpt/#1-introduction","text":"What is TabDPT? TabDPT (Tabular Denoising Pre-trained Transformer) is a state-of-the-art model designed for: Large-Scale Learning : Scales to datasets with millions of samples Robust Feature Learning : Pre-trained on denoising objectives Noise Resilience : Handles missing and corrupted features Context-Aware Predictions : k-NN based context selection Strong Generalization : Pre-trained on diverse tabular corpora Key Innovation : Pre-training via masked feature prediction (denoising) enables robust feature representations and strong generalization to new tasks.","title":"1. Introduction"},{"location":"models/tabdpt/#2-architecture","text":"","title":"2. Architecture"},{"location":"models/tabdpt/#21-high-level-design","text":"flowchart LR A[Input Features] --> B[Masking Layer] B --> C[Noisy Features] C --> D[Transformer Encoder] D --> E[Hidden Representations] E --> F[k-NN Context Retrieval] F --> G[Context Features] G --> H[Transformer Decoder] H --> I[Reconstructed Features] I --> J[Prediction Head] J --> K[Output]","title":"2.1 High-Level Design"},{"location":"models/tabdpt/#23-pre-training-strategy","text":"Pre-training Phase (on diverse data): 1. Mask random features (30-50%) 2. Encode remaining features 3. Retrieve k-NN context 4. Predict masked features 5. Loss = MSE(predicted, actual) Fine-tuning Phase (on your task): 1. Replace prediction head 2. Fine-tune on task labels 3. Use pre-trained encoder","title":"2.3 Pre-training Strategy"},{"location":"models/tabdpt/#3-inference-parameters","text":"","title":"3. Inference Parameters"},{"location":"models/tabdpt/#31-complete-parameter-reference","text":"model_params = { # Architecture 'd_model' : 256 , # Embedding dimension 'num_heads' : 8 , # Attention heads 'num_layers' : 4 , # Transformer layers 'hidden_size' : 512 , # Feedforward hidden size 'dropout' : 0.1 , # Dropout probability # Context retrieval 'k_neighbors' : 5 , # Number of neighbors for context 'context_mode' : 'mixed' , # 'mixed' or 'features_only' # Inference behavior 'n_ensembles' : 8 , # Multiple runs 'temperature' : 0.3 , # Output scaling 'mask_ratio' : 0.3 , # Feature masking during inference # Training 'use_pretrain' : True , # Use pre-trained weights 'seed' : 42 # Reproducibility }","title":"3.1 Complete Parameter Reference"},{"location":"models/tabdpt/#32-parameter-descriptions","text":"Parameter Type Default Range Description d_model int 256 128-512 Transformer embedding dimension num_heads int 8 4-16 Number of attention heads num_layers int 4 2-8 Number of transformer layers hidden_size int 512 256-1024 Feedforward hidden dimension dropout float 0.1 0.0-0.3 Dropout probability k_neighbors int 5 1-50 k-NN context neighbors context_mode str 'mixed' 'mixed', 'features_only' How to use context n_ensembles int 8 1-16 Number of ensemble runs temperature float 0.3 0.1-1.0 Output temperature use_pretrain bool True True/False Use pre-trained weights","title":"3.2 Parameter Descriptions"},{"location":"models/tabdpt/#33-architecture-tuning","text":"Config Speed Accuracy Memory Best For Small: d=128, layers=2 \u2b50\u2b50\u2b50\u2b50\u2b50 \u2b50\u2b50 \u2b50 Quick baseline Medium: d=256, layers=4 \u2b50\u2b50\u2b50\u2b50 \u2b50\u2b50\u2b50\u2b50 \u2b50\u2b50 Balanced Large: d=512, layers=8 \u2b50\u2b50\u2b50 \u2b50\u2b50\u2b50\u2b50\u2b50 \u2b50\u2b50\u2b50 Max accuracy","title":"3.3 Architecture Tuning"},{"location":"models/tabdpt/#34-context-modes","text":"context_modes = { 'mixed' : 'Use both context features and their representations' , 'features_only' : 'Use only context features, not representations' } # Typically 'mixed' is better model_params = { 'context_mode' : 'mixed' }","title":"3.4 Context Modes"},{"location":"models/tabdpt/#4-fine-tuning-with-tabdpt","text":"TabDPT uses episodic fine-tuning with large context windows.","title":"4. Fine-Tuning with TabDPT"},{"location":"models/tabdpt/#41-fine-tuning-parameters","text":"tuning_params = { 'device' : 'cuda' , 'epochs' : 3 , # Few epochs needed (pre-trained) 'learning_rate' : 2e-5 , # Conservative learning rate 'optimizer' : 'adamw' , # Optimizer type 'scheduler' : 'linear' , # Learning rate scheduler 'warmup_steps' : 500 , # Extended warmup 'weight_decay' : 0.01 , # L2 regularization 'gradient_clip_value' : 1.0 , # Gradient clipping # Large context for TabDPT 'support_size' : 1024 , # Large context 'query_size' : 256 , # Prediction samples 'steps_per_epoch' : 15 , # Gradient steps 'batch_size' : 32 , # Standard batch 'show_progress' : True # Progress bar }","title":"4.1 Fine-Tuning Parameters"},{"location":"models/tabdpt/#42-key-parameters","text":"Parameter Type Default Description support_size int 1024 Large context for k-NN query_size int 256 Query samples per episode steps_per_epoch int 15 Optimization steps batch_size int 32 Samples per batch","title":"4.2 Key Parameters"},{"location":"models/tabdpt/#43-fine-tuning-guidelines","text":"Large Context Windows : # TabDPT benefits from large context tuning_params = { 'support_size' : 1024 , # Large context (TabDPT strength) 'query_size' : 256 , # Balance for gradients 'batch_size' : 32 # Process in parallel } Learning Rate Strategy : - 1e-5: Conservative, safe - 2e-5: Balanced (default) - 5e-5: Aggressive Pre-training Advantage : - TabDPT needs fewer epochs due to pre-training - Typically 3-5 epochs sufficient - Convergence faster than TabICL","title":"4.3 Fine-Tuning Guidelines"},{"location":"models/tabdpt/#44-dataset-recommendations","text":"# TabDPT shines with large datasets dataset_sizes = { '10K' : 'Acceptable, TabICL better' , '100K' : 'Good fit for TabDPT' , '1M' : 'Excellent for TabDPT' , '5M+' : 'Perfect use case' }","title":"4.4 Dataset Recommendations"},{"location":"models/tabdpt/#5-lora-target-modules","text":"When using PEFT, TabDPT targets these modules: target_modules = [ 'transformer_encoder' , # Main encoder 'encoder' , # Additional encoder 'y_encoder' , # Label encoder 'head' # Prediction head ]","title":"5. LoRA Target Modules"},{"location":"models/tabdpt/#51-default-peft-configuration","text":"peft_config = { 'r' : 8 , 'lora_alpha' : 16 , 'lora_dropout' : 0.05 , 'target_modules' : None # Uses defaults }","title":"5.1 Default PEFT Configuration"},{"location":"models/tabdpt/#52-peft-for-large-models","text":"# PEFT works well with TabDPT's large architecture pipeline = TabularPipeline ( model_name = 'TabDPT' , tuning_strategy = 'peft' , tuning_params = { 'device' : 'cuda' , 'epochs' : 3 , 'learning_rate' : 2e-4 , 'support_size' : 512 , # Still large 'peft_config' : { 'r' : 16 } # Higher rank acceptable } )","title":"5.2 PEFT for Large Models"},{"location":"models/tabdpt/#6-usage-patterns","text":"","title":"6. Usage Patterns"},{"location":"models/tabdpt/#61-inference-only","text":"from tabtune import TabularPipeline pipeline = TabularPipeline ( model_name = 'TabDPT' , tuning_strategy = 'inference' , ) pipeline . fit ( X_train , y_train ) predictions = pipeline . predict ( X_test )","title":"6.1 Inference Only"},{"location":"models/tabdpt/#62-base-fine-tuning-on-large-dataset","text":"# TabDPT excels with large datasets pipeline = TabularPipeline ( model_name = 'TabDPT' , tuning_strategy = 'base-ft' , tuning_params = { 'device' : 'cuda' , 'epochs' : 3 , 'learning_rate' : 2e-5 , 'support_size' : 1024 , # Large context 'query_size' : 256 , 'steps_per_epoch' : 15 , 'batch_size' : 32 , 'show_progress' : True } ) pipeline . fit ( X_train , y_train ) # 100K+ samples ideal metrics = pipeline . evaluate ( X_test , y_test )","title":"6.2 Base Fine-Tuning on Large Dataset"},{"location":"models/tabdpt/#63-peft-fine-tuning","text":"pipeline = TabularPipeline ( model_name = 'TabDPT' , tuning_strategy = 'peft' , tuning_params = { 'device' : 'cuda' , 'epochs' : 3 , 'learning_rate' : 2e-4 , 'support_size' : 512 , 'query_size' : 256 , 'peft_config' : { 'r' : 8 , 'lora_alpha' : 16 , 'lora_dropout' : 0.05 } } ) pipeline . fit ( X_train , y_train )","title":"6.3 PEFT Fine-Tuning"},{"location":"models/tabdpt/#7-complete-examples","text":"","title":"7. Complete Examples"},{"location":"models/tabdpt/#71-large-dataset-workflow","text":"from tabtune import TabularPipeline from sklearn.model_selection import train_test_split import pandas as pd # Load large dataset (1M+ rows) df = pd . read_csv ( 'large_dataset.csv' ) # 1M+ rows X = df . drop ( 'target' , axis = 1 ) y = df [ 'target' ] # Split X_train , X_test , y_train , y_test = train_test_split ( X , y , test_size = 0.1 , random_state = 42 ) # Train TabDPT pipeline = TabularPipeline ( model_name = 'TabDPT' , tuning_strategy = 'base-ft' , tuning_params = { 'device' : 'cuda' , 'epochs' : 3 , 'learning_rate' : 2e-5 , 'support_size' : 1024 , 'query_size' : 256 , 'batch_size' : 32 , 'show_progress' : True } ) pipeline . fit ( X_train , y_train ) metrics = pipeline . evaluate ( X_test , y_test ) print ( f \"Accuracy: { metrics [ 'accuracy' ] : .4f } \" ) print ( f \"Training completed on { len ( X_train ) } samples\" )","title":"7.1 Large Dataset Workflow"},{"location":"models/tabdpt/#72-production-model-with-peft-saving-using-joblib","text":"# PEFT for production deployment pipeline = TabularPipeline ( model_name = 'TabDPT' , tuning_strategy = 'peft' , tuning_params = { 'device' : 'cuda' , 'epochs' : 3 , 'learning_rate' : 2e-4 , 'support_size' : 512 , 'peft_config' : { 'r' : 8 } } ) pipeline . fit ( X_train , y_train ) metrics = pipeline . evaluate ( X_test , y_test ) # Save for deployment pipeline . save ( 'tabdpt_production.joblib' )","title":"7.2 Production Model with PEFT - Saving using joblib"},{"location":"models/tabdpt/#73-architecture-comparison","text":"from tabtune import TabularLeaderboard # Compare architectures lb = TabularLeaderboard ( X_train , X_test , y_train , y_test ) # Small lb . add_model ( 'TabDPT' , 'base-ft' , name = 'TabDPT-Small' , model_params = { 'd_model' : 128 , 'num_layers' : 2 }, tuning_params = { 'epochs' : 3 } ) # Medium lb . add_model ( 'TabDPT' , 'base-ft' , name = 'TabDPT-Medium' , model_params = { 'd_model' : 256 , 'num_layers' : 4 }, tuning_params = { 'epochs' : 3 } ) # Large lb . add_model ( 'TabDPT' , 'base-ft' , name = 'TabDPT-Large' , model_params = { 'd_model' : 512 , 'num_layers' : 8 }, tuning_params = { 'epochs' : 3 } ) lb . run ( rank_by = 'accuracy' )","title":"7.3 Architecture Comparison"},{"location":"models/tabdpt/#9-best-practices","text":"","title":"9. Best Practices"},{"location":"models/tabdpt/#dos","text":"\u2705 Use large context windows (support_size >= 512) \u2705 Use on datasets with 100K+ samples \u2705 Leverage pre-trained weights \u2705 Use few epochs (3-5) due to pre-training \u2705 Monitor for overfitting with regularization \u2705 Use PEFT for faster training \u2705 Include k-NN context","title":"\u2705 Do's"},{"location":"models/tabdpt/#donts","text":"\u274c Don't use on small datasets (<10K) \u274c Don't use without pre-training \u274c Don't train for too many epochs \u274c Don't use small context windows \u274c Don't disable masking (helps robustness)","title":"\u274c Don'ts"},{"location":"models/tabdpt/#10-troubleshooting","text":"","title":"10. Troubleshooting"},{"location":"models/tabdpt/#issue-context-retrieval-slow","text":"Solution : Reduce k_neighbors or use approximate k-NN model_params = { 'k_neighbors' : 3 , # Instead of 5 'context_mode' : 'features_only' }","title":"Issue: \"Context retrieval slow\""},{"location":"models/tabdpt/#issue-out-of-memory-with-large-support_size","text":"Solution : Use PEFT or reduce support size tuning_params = { 'support_size' : 512 , # Instead of 1024 'batch_size' : 16 # Smaller batch }","title":"Issue: \"Out of memory with large support_size\""},{"location":"models/tabdpt/#issue-accuracy-plateauing","text":"Solution : Increase training budget tuning_params = { 'epochs' : 5 , # More epochs 'steps_per_epoch' : 20 , # More steps 'warmup_steps' : 1000 # Longer warmup }","title":"Issue: \"Accuracy plateauing\""},{"location":"models/tabdpt/#issue-prediction-latency-too-high","text":"Solution : Use smaller ensemble model_params = { 'n_ensembles' : 2 , # Instead of 8 'k_neighbors' : 3 # Fewer neighbors }","title":"Issue: \"Prediction latency too high\""},{"location":"models/tabdpt/#11-comparison-with-other-models","text":"Aspect TabDPT TabICL Mitra TabPFN Small data (10K) Poor Good Good Excellent Large data (1M) Excellent Good Okay N/A Accuracy Excellent Good Excellent Medium Speed Slow Fast Slow Fastest Memory High Moderate Very High Low Pre-training Yes No No Yes PEFT \u2705 Full \u2705 Full \u2705 Full \u26a0\ufe0f Exp","title":"11. Comparison with Other Models"},{"location":"models/tabdpt/#12-when-to-use-tabdpt","text":"Use TabDPT when : - \u2705 Dataset has 100K+ samples - \u2705 Maximum accuracy is priority - \u2705 Data has missing/noisy values - \u2705 You have sufficient memory - \u2705 Training time is not critical - \u2705 Deployment can handle context retrieval Don't use TabDPT for : - \u274c Small datasets (<50K) - \u274c When prediction speed critical - \u274c Very memory-constrained systems - \u274c When training time is limited","title":"12. When to Use TabDPT"},{"location":"models/tabdpt/#14-quick-reference","text":"Use Case Strategy Config Support Baseline inference default 1024 Production (1M data) base-ft default 1024 Memory limited peft r=8 512 Max accuracy base-ft large model 2048 Fast inference peft r=4 256","title":"14. Quick Reference"},{"location":"models/tabdpt/#15-next-steps","text":"Model Selection - Compare with other models Tuning Strategies - Fine-tuning details Advanced PEFT - LoRA optimization TabularLeaderboard - Benchmark TabDPT TabDPT excels at large-scale tabular learning with pre-trained robustness. Use it for production systems with millions of samples!","title":"15. Next Steps"},{"location":"models/tabicl/","text":"TabICL: In-Context Learning for Tabular Data \u00b6 TabICL is a scalable, ensemble-based in-context learning model designed for general-purpose tabular classification. This document provides comprehensive guidance for using TabICL with TabTune. 1. Introduction \u00b6 What is TabICL? TabICL (Tabular In-Context Learning) is a neural model that leverages in-context learning principles adapted for tabular data. Unlike traditional models, TabICL learns to: Process feature relationships dynamically Adapt to task-specific patterns via fine-tuning Generate robust predictions via ensemble methods Handle mixed data types naturally Key Innovation : Two-stage attention mechanism (column \u2192 row) enabling efficient feature processing and interaction modeling. 2. Architecture \u00b6 2.1 High-Level Design \u00b6 flowchart LR A[Input Features] --> B[Column Embedder] B --> C[Feature-wise Embeddings] C --> D[Row Interactor] D --> E[Feature Interactions] E --> F[ICL Predictor] F --> G[Predictions] G --> H[Ensemble Aggregation] H --> I[Final Output] 2.3 Two-Stage Attention \u00b6 Stage 1: Column Attention (Feature-wise) \u2193 Per-feature processing Feature extraction Stage 2: Row Attention (Sample-wise) \u2193 Feature interaction Context modeling 3. Inference Parameters \u00b6 3.1 Complete Parameter Reference \u00b6 model_params = { 'n_estimators' : 32 , # Ensemble size (views) 'softmax_temperature' : 0.9 , # Prediction confidence 'average_logits' : True , # Aggregation method 'norm_methods' : [ 'none' , 'power' ], # Feature normalization 'feat_shuffle_method' : 'latin' , # Feature permutation strategy 'batch_size' : 8 , # Ensemble batch size 'seed' : 42 # Reproducibility } 3.2 Parameter Descriptions \u00b6 Parameter Type Default Range Description n_estimators int 32 4-128 Number of ensemble members; more = robust but slower softmax_temperature float 0.9 0.1-2.0 Scaling before softmax; lower = sharper predictions average_logits bool True True/False Average logits vs probabilities norm_methods list ['none', 'power'] Varies Feature normalization techniques feat_shuffle_method str 'latin' 'random', 'latin', 'sequential' Feature permutation strategy batch_size int 8 1-32 Ensemble members per batch seed int 42 0+ Random seed for reproducibility 3.3 Ensemble Size Effects \u00b6 n_estimators Speed Robustness Memory 4-8 \u2b50\u2b50\u2b50\u2b50\u2b50 \u2b50\u2b50 \u2b50 16-32 \u2b50\u2b50\u2b50\u2b50 \u2b50\u2b50\u2b50\u2b50 \u2b50\u2b50 64-128 \u2b50\u2b50\u2b50 \u2b50\u2b50\u2b50\u2b50\u2b50 \u2b50\u2b50\u2b50 3.4 Feature Normalization Methods \u00b6 norm_methods = [ 'none' , # No normalization 'power' , # Power transformation 'quantile' , # Quantile normalization 'minmax' , # Min-max scaling 'standard' # Standardization ] 3.5 Feature Shuffle Methods \u00b6 feat_shuffle_methods = { 'random' : 'Random permutation each time' , 'latin' : 'Latin square design (balanced)' , 'sequential' : 'Fixed sequential order' } 4. Fine-Tuning with TabICL \u00b6 TabICL supports episodic fine-tuning where training occurs via task-like episodes. 4.1 Episodic Training Parameters \u00b6 tuning_params = { 'device' : 'cuda' , 'epochs' : 5 , # Training epochs 'learning_rate' : 2e-5 , # Optimizer learning rate 'optimizer' : 'adamw' , # Optimizer type # Episodic parameters 'support_size' : 48 , # Support set samples 'query_size' : 32 , # Query set samples 'n_episodes' : 1000 , # Episodes per epoch 'batch_size' : 8 , # Episodes per batch 'show_progress' : True # Progress bar } 4.2 Parameter Descriptions \u00b6 Parameter Type Default Description support_size int 48 Number of samples per support set query_size int 32 Number of samples per query set n_episodes int 1000 Total episodes for training batch_size int 8 Episodes per batch gradient update 4.3 Episodic Training Concept \u00b6 One Episode: \u251c\u2500 Support Set (48 samples) \u2502 \u2514\u2500 Used as training context \u251c\u2500 Query Set (32 samples) \u2502 \u2514\u2500 Used for evaluation \u2514\u2500 Loss computed on Query Epoch = 1000 episodes with gradient updates 4.4 Fine-Tuning Guidelines \u00b6 Support/Query Size Balance : - Larger support \u2192 more context but slower - Larger query \u2192 better gradient signal - Typical: support:query = 3:2 Number of Episodes : - 500-1000: Good for medium datasets - 1000-5000: Better for large datasets - Adjust based on dataset size: (n_{episodes} = \\frac{\\text{dataset_size}}{100}) Learning Rate : - 1e-5: Conservative, safe - 2e-5: Balanced (default) - 5e-5: Aggressive, higher variance 5. Usage Patterns \u00b6 5.1 Inference Only \u00b6 from tabtune import TabularPipeline # Zero-shot with pre-trained weights pipeline = TabularPipeline ( model_name = 'TabICL' , tuning_strategy = 'inference' , model_params = { 'n_estimators' : 32 } ) pipeline . fit ( X_train , y_train ) # Only preprocesses predictions = pipeline . predict ( X_test ) 5.2 Base Fine-Tuning (Full Parameters) \u00b6 pipeline = TabularPipeline ( model_name = 'TabICL' , tuning_strategy = 'base-ft' , tuning_params = { 'device' : 'cuda' , 'epochs' : 5 , 'support_size' : 48 , 'query_size' : 32 , 'n_episodes' : 1000 } ) pipeline . fit ( X_train , y_train ) metrics = pipeline . evaluate ( X_test , y_test ) 5.3 PEFT Fine-Tuning (LoRA Adapters) \u00b6 pipeline = TabularPipeline ( model_name = 'TabICL' , tuning_strategy = 'peft' , tuning_params = { 'device' : 'cuda' , 'epochs' : 5 , 'learning_rate' : 2e-4 , # Higher for PEFT 'support_size' : 24 , # Smaller for memory 'query_size' : 16 , 'peft_config' : { 'r' : 8 , 'lora_alpha' : 16 , 'lora_dropout' : 0.05 } } ) pipeline . fit ( X_train , y_train ) 5.4 Ensemble Configuration \u00b6 # Increase ensemble for robustness pipeline = TabularPipeline ( model_name = 'TabICL' , tuning_strategy = 'inference' , model_params = { 'n_estimators' : 128 , # Large ensemble 'batch_size' : 16 # Parallel processing } ) 6. LoRA Target Modules \u00b6 When using PEFT, TabICL automatically targets these modules: target_modules = [ 'col_embedder.tf_col' , # Column transformer 'col_embedder.in_linear' , # Input projection 'row_interactor' , # Interaction layers 'icl_predictor.tf_icl' , # Prediction transformer 'icl_predictor.decoder' # Decoder head ] Default PEFT Config : peft_config = { 'r' : 8 , 'lora_alpha' : 16 , 'lora_dropout' : 0.05 , 'target_modules' : None # Uses defaults above } 7. Complete Examples \u00b6 7.1 Basic Workflow \u00b6 from tabtune import TabularPipeline from sklearn.model_selection import train_test_split import pandas as pd # Load data df = pd . read_csv ( 'data.csv' ) X = df . drop ( 'target' , axis = 1 ) y = df [ 'target' ] # Split X_train , X_test , y_train , y_test = train_test_split ( X , y , test_size = 0.2 , random_state = 42 ) # Create and train pipeline = TabularPipeline ( model_name = 'TabICL' , tuning_strategy = 'base-ft' , tuning_params = { 'device' : 'cuda' , 'epochs' : 5 , 'learning_rate' : 2e-5 , 'n_episodes' : 1000 } ) pipeline . fit ( X_train , y_train ) metrics = pipeline . evaluate ( X_test , y_test ) print ( f \"Accuracy: { metrics [ 'accuracy' ] : .4f } \" ) print ( f \"F1 Score: { metrics [ 'f1_score' ] : .4f } \" ) 7.2 PEFT for Memory-Constrained Environments \u00b6 # Fit large model in limited memory with PEFT pipeline = TabularPipeline ( model_name = 'TabICL' , tuning_strategy = 'peft' , tuning_params = { 'device' : 'cuda' , 'epochs' : 5 , 'learning_rate' : 2e-4 , 'support_size' : 24 , # Reduced 'query_size' : 16 , # Reduced 'batch_size' : 4 , # Smaller batches 'peft_config' : { 'r' : 4 , # Lower rank 'lora_alpha' : 8 , 'lora_dropout' : 0.1 } } ) pipeline . fit ( X_train , y_train ) 7.3 Hyperparameter Tuning \u00b6 from tabtune import TabularLeaderboard # Compare different configurations lb = TabularLeaderboard ( X_train , X_test , y_train , y_test ) # Ensemble size comparison for n_est in [ 16 , 32 , 64 ]: lb . add_model ( 'TabICL' , 'inference' , name = f 'TabICL-n { n_est } ' , model_params = { 'n_estimators' : n_est } ) # LoRA rank comparison for r in [ 4 , 8 , 16 ]: lb . add_model ( 'TabICL' , 'peft' , name = f 'TabICL-PEFT-r { r } ' , tuning_params = { 'epochs' : 3 , 'peft_config' : { 'r' : r , 'lora_alpha' : 2 * r } } ) lb . run ( rank_by = 'accuracy' ) 9. Troubleshooting \u00b6 Issue: \"Out of memory during training\" \u00b6 Solution 1 : Reduce support/query sizes tuning_params = { 'support_size' : 24 , # Instead of 48 'query_size' : 16 # Instead of 32 } Solution 2 : Use PEFT instead of base-ft tuning_strategy = 'peft' # Lower memory Issue: \"Model not converging\" \u00b6 Solution : Adjust learning rate and epochs tuning_params = { 'learning_rate' : 5e-5 , # Increase 'epochs' : 10 , # More epochs 'n_episodes' : 2000 # More training } Issue: \"Inference too slow\" \u00b6 Solution : Reduce ensemble size model_params = { 'n_estimators' : 8 # Instead of 32 } Issue: \"Low accuracy on small datasets\" \u00b6 Solution : Use larger support set tuning_params = { 'support_size' : 96 , # Larger context 'query_size' : 64 } 10. Advanced Topics \u00b6 11. Quick Reference \u00b6 Use Case Strategy Config Time Quick test inference n_est=16 <1s Rapid proto peft r=8, epochs=3 5-10m Production base-ft epochs=5 20-30m Max accuracy base-ft epochs=10 40-60m Memory limited peft r=4 5-10m 13. Next Steps \u00b6 Model Selection - Compare with other models Tuning Strategies - Deep dive into strategies Advanced PEFT - LoRA deep dive TabularLeaderboard - Benchmark TabICL TabICL offers an excellent balance of speed, accuracy, and scalability. Use it for most tabular classification tasks!","title":"TabICL"},{"location":"models/tabicl/#tabicl-in-context-learning-for-tabular-data","text":"TabICL is a scalable, ensemble-based in-context learning model designed for general-purpose tabular classification. This document provides comprehensive guidance for using TabICL with TabTune.","title":"TabICL: In-Context Learning for Tabular Data"},{"location":"models/tabicl/#1-introduction","text":"What is TabICL? TabICL (Tabular In-Context Learning) is a neural model that leverages in-context learning principles adapted for tabular data. Unlike traditional models, TabICL learns to: Process feature relationships dynamically Adapt to task-specific patterns via fine-tuning Generate robust predictions via ensemble methods Handle mixed data types naturally Key Innovation : Two-stage attention mechanism (column \u2192 row) enabling efficient feature processing and interaction modeling.","title":"1. Introduction"},{"location":"models/tabicl/#2-architecture","text":"","title":"2. Architecture"},{"location":"models/tabicl/#21-high-level-design","text":"flowchart LR A[Input Features] --> B[Column Embedder] B --> C[Feature-wise Embeddings] C --> D[Row Interactor] D --> E[Feature Interactions] E --> F[ICL Predictor] F --> G[Predictions] G --> H[Ensemble Aggregation] H --> I[Final Output]","title":"2.1 High-Level Design"},{"location":"models/tabicl/#23-two-stage-attention","text":"Stage 1: Column Attention (Feature-wise) \u2193 Per-feature processing Feature extraction Stage 2: Row Attention (Sample-wise) \u2193 Feature interaction Context modeling","title":"2.3 Two-Stage Attention"},{"location":"models/tabicl/#3-inference-parameters","text":"","title":"3. Inference Parameters"},{"location":"models/tabicl/#31-complete-parameter-reference","text":"model_params = { 'n_estimators' : 32 , # Ensemble size (views) 'softmax_temperature' : 0.9 , # Prediction confidence 'average_logits' : True , # Aggregation method 'norm_methods' : [ 'none' , 'power' ], # Feature normalization 'feat_shuffle_method' : 'latin' , # Feature permutation strategy 'batch_size' : 8 , # Ensemble batch size 'seed' : 42 # Reproducibility }","title":"3.1 Complete Parameter Reference"},{"location":"models/tabicl/#32-parameter-descriptions","text":"Parameter Type Default Range Description n_estimators int 32 4-128 Number of ensemble members; more = robust but slower softmax_temperature float 0.9 0.1-2.0 Scaling before softmax; lower = sharper predictions average_logits bool True True/False Average logits vs probabilities norm_methods list ['none', 'power'] Varies Feature normalization techniques feat_shuffle_method str 'latin' 'random', 'latin', 'sequential' Feature permutation strategy batch_size int 8 1-32 Ensemble members per batch seed int 42 0+ Random seed for reproducibility","title":"3.2 Parameter Descriptions"},{"location":"models/tabicl/#33-ensemble-size-effects","text":"n_estimators Speed Robustness Memory 4-8 \u2b50\u2b50\u2b50\u2b50\u2b50 \u2b50\u2b50 \u2b50 16-32 \u2b50\u2b50\u2b50\u2b50 \u2b50\u2b50\u2b50\u2b50 \u2b50\u2b50 64-128 \u2b50\u2b50\u2b50 \u2b50\u2b50\u2b50\u2b50\u2b50 \u2b50\u2b50\u2b50","title":"3.3 Ensemble Size Effects"},{"location":"models/tabicl/#34-feature-normalization-methods","text":"norm_methods = [ 'none' , # No normalization 'power' , # Power transformation 'quantile' , # Quantile normalization 'minmax' , # Min-max scaling 'standard' # Standardization ]","title":"3.4 Feature Normalization Methods"},{"location":"models/tabicl/#35-feature-shuffle-methods","text":"feat_shuffle_methods = { 'random' : 'Random permutation each time' , 'latin' : 'Latin square design (balanced)' , 'sequential' : 'Fixed sequential order' }","title":"3.5 Feature Shuffle Methods"},{"location":"models/tabicl/#4-fine-tuning-with-tabicl","text":"TabICL supports episodic fine-tuning where training occurs via task-like episodes.","title":"4. Fine-Tuning with TabICL"},{"location":"models/tabicl/#41-episodic-training-parameters","text":"tuning_params = { 'device' : 'cuda' , 'epochs' : 5 , # Training epochs 'learning_rate' : 2e-5 , # Optimizer learning rate 'optimizer' : 'adamw' , # Optimizer type # Episodic parameters 'support_size' : 48 , # Support set samples 'query_size' : 32 , # Query set samples 'n_episodes' : 1000 , # Episodes per epoch 'batch_size' : 8 , # Episodes per batch 'show_progress' : True # Progress bar }","title":"4.1 Episodic Training Parameters"},{"location":"models/tabicl/#42-parameter-descriptions","text":"Parameter Type Default Description support_size int 48 Number of samples per support set query_size int 32 Number of samples per query set n_episodes int 1000 Total episodes for training batch_size int 8 Episodes per batch gradient update","title":"4.2 Parameter Descriptions"},{"location":"models/tabicl/#43-episodic-training-concept","text":"One Episode: \u251c\u2500 Support Set (48 samples) \u2502 \u2514\u2500 Used as training context \u251c\u2500 Query Set (32 samples) \u2502 \u2514\u2500 Used for evaluation \u2514\u2500 Loss computed on Query Epoch = 1000 episodes with gradient updates","title":"4.3 Episodic Training Concept"},{"location":"models/tabicl/#44-fine-tuning-guidelines","text":"Support/Query Size Balance : - Larger support \u2192 more context but slower - Larger query \u2192 better gradient signal - Typical: support:query = 3:2 Number of Episodes : - 500-1000: Good for medium datasets - 1000-5000: Better for large datasets - Adjust based on dataset size: (n_{episodes} = \\frac{\\text{dataset_size}}{100}) Learning Rate : - 1e-5: Conservative, safe - 2e-5: Balanced (default) - 5e-5: Aggressive, higher variance","title":"4.4 Fine-Tuning Guidelines"},{"location":"models/tabicl/#5-usage-patterns","text":"","title":"5. Usage Patterns"},{"location":"models/tabicl/#51-inference-only","text":"from tabtune import TabularPipeline # Zero-shot with pre-trained weights pipeline = TabularPipeline ( model_name = 'TabICL' , tuning_strategy = 'inference' , model_params = { 'n_estimators' : 32 } ) pipeline . fit ( X_train , y_train ) # Only preprocesses predictions = pipeline . predict ( X_test )","title":"5.1 Inference Only"},{"location":"models/tabicl/#52-base-fine-tuning-full-parameters","text":"pipeline = TabularPipeline ( model_name = 'TabICL' , tuning_strategy = 'base-ft' , tuning_params = { 'device' : 'cuda' , 'epochs' : 5 , 'support_size' : 48 , 'query_size' : 32 , 'n_episodes' : 1000 } ) pipeline . fit ( X_train , y_train ) metrics = pipeline . evaluate ( X_test , y_test )","title":"5.2 Base Fine-Tuning (Full Parameters)"},{"location":"models/tabicl/#53-peft-fine-tuning-lora-adapters","text":"pipeline = TabularPipeline ( model_name = 'TabICL' , tuning_strategy = 'peft' , tuning_params = { 'device' : 'cuda' , 'epochs' : 5 , 'learning_rate' : 2e-4 , # Higher for PEFT 'support_size' : 24 , # Smaller for memory 'query_size' : 16 , 'peft_config' : { 'r' : 8 , 'lora_alpha' : 16 , 'lora_dropout' : 0.05 } } ) pipeline . fit ( X_train , y_train )","title":"5.3 PEFT Fine-Tuning (LoRA Adapters)"},{"location":"models/tabicl/#54-ensemble-configuration","text":"# Increase ensemble for robustness pipeline = TabularPipeline ( model_name = 'TabICL' , tuning_strategy = 'inference' , model_params = { 'n_estimators' : 128 , # Large ensemble 'batch_size' : 16 # Parallel processing } )","title":"5.4 Ensemble Configuration"},{"location":"models/tabicl/#6-lora-target-modules","text":"When using PEFT, TabICL automatically targets these modules: target_modules = [ 'col_embedder.tf_col' , # Column transformer 'col_embedder.in_linear' , # Input projection 'row_interactor' , # Interaction layers 'icl_predictor.tf_icl' , # Prediction transformer 'icl_predictor.decoder' # Decoder head ] Default PEFT Config : peft_config = { 'r' : 8 , 'lora_alpha' : 16 , 'lora_dropout' : 0.05 , 'target_modules' : None # Uses defaults above }","title":"6. LoRA Target Modules"},{"location":"models/tabicl/#7-complete-examples","text":"","title":"7. Complete Examples"},{"location":"models/tabicl/#71-basic-workflow","text":"from tabtune import TabularPipeline from sklearn.model_selection import train_test_split import pandas as pd # Load data df = pd . read_csv ( 'data.csv' ) X = df . drop ( 'target' , axis = 1 ) y = df [ 'target' ] # Split X_train , X_test , y_train , y_test = train_test_split ( X , y , test_size = 0.2 , random_state = 42 ) # Create and train pipeline = TabularPipeline ( model_name = 'TabICL' , tuning_strategy = 'base-ft' , tuning_params = { 'device' : 'cuda' , 'epochs' : 5 , 'learning_rate' : 2e-5 , 'n_episodes' : 1000 } ) pipeline . fit ( X_train , y_train ) metrics = pipeline . evaluate ( X_test , y_test ) print ( f \"Accuracy: { metrics [ 'accuracy' ] : .4f } \" ) print ( f \"F1 Score: { metrics [ 'f1_score' ] : .4f } \" )","title":"7.1 Basic Workflow"},{"location":"models/tabicl/#72-peft-for-memory-constrained-environments","text":"# Fit large model in limited memory with PEFT pipeline = TabularPipeline ( model_name = 'TabICL' , tuning_strategy = 'peft' , tuning_params = { 'device' : 'cuda' , 'epochs' : 5 , 'learning_rate' : 2e-4 , 'support_size' : 24 , # Reduced 'query_size' : 16 , # Reduced 'batch_size' : 4 , # Smaller batches 'peft_config' : { 'r' : 4 , # Lower rank 'lora_alpha' : 8 , 'lora_dropout' : 0.1 } } ) pipeline . fit ( X_train , y_train )","title":"7.2 PEFT for Memory-Constrained Environments"},{"location":"models/tabicl/#73-hyperparameter-tuning","text":"from tabtune import TabularLeaderboard # Compare different configurations lb = TabularLeaderboard ( X_train , X_test , y_train , y_test ) # Ensemble size comparison for n_est in [ 16 , 32 , 64 ]: lb . add_model ( 'TabICL' , 'inference' , name = f 'TabICL-n { n_est } ' , model_params = { 'n_estimators' : n_est } ) # LoRA rank comparison for r in [ 4 , 8 , 16 ]: lb . add_model ( 'TabICL' , 'peft' , name = f 'TabICL-PEFT-r { r } ' , tuning_params = { 'epochs' : 3 , 'peft_config' : { 'r' : r , 'lora_alpha' : 2 * r } } ) lb . run ( rank_by = 'accuracy' )","title":"7.3 Hyperparameter Tuning"},{"location":"models/tabicl/#9-troubleshooting","text":"","title":"9. Troubleshooting"},{"location":"models/tabicl/#issue-out-of-memory-during-training","text":"Solution 1 : Reduce support/query sizes tuning_params = { 'support_size' : 24 , # Instead of 48 'query_size' : 16 # Instead of 32 } Solution 2 : Use PEFT instead of base-ft tuning_strategy = 'peft' # Lower memory","title":"Issue: \"Out of memory during training\""},{"location":"models/tabicl/#issue-model-not-converging","text":"Solution : Adjust learning rate and epochs tuning_params = { 'learning_rate' : 5e-5 , # Increase 'epochs' : 10 , # More epochs 'n_episodes' : 2000 # More training }","title":"Issue: \"Model not converging\""},{"location":"models/tabicl/#issue-inference-too-slow","text":"Solution : Reduce ensemble size model_params = { 'n_estimators' : 8 # Instead of 32 }","title":"Issue: \"Inference too slow\""},{"location":"models/tabicl/#issue-low-accuracy-on-small-datasets","text":"Solution : Use larger support set tuning_params = { 'support_size' : 96 , # Larger context 'query_size' : 64 }","title":"Issue: \"Low accuracy on small datasets\""},{"location":"models/tabicl/#10-advanced-topics","text":"","title":"10. Advanced Topics"},{"location":"models/tabicl/#11-quick-reference","text":"Use Case Strategy Config Time Quick test inference n_est=16 <1s Rapid proto peft r=8, epochs=3 5-10m Production base-ft epochs=5 20-30m Max accuracy base-ft epochs=10 40-60m Memory limited peft r=4 5-10m","title":"11. Quick Reference"},{"location":"models/tabicl/#13-next-steps","text":"Model Selection - Compare with other models Tuning Strategies - Deep dive into strategies Advanced PEFT - LoRA deep dive TabularLeaderboard - Benchmark TabICL TabICL offers an excellent balance of speed, accuracy, and scalability. Use it for most tabular classification tasks!","title":"13. Next Steps"},{"location":"models/tabpfn/","text":"TabPFN: Prior-Fitted Network \u00b6 TabPFN is a revolutionary tabular model that demonstrates strong zero-shot performance without any fine-tuning. This document provides an in-depth guide to using TabPFN with TabTune. 1. Introduction \u00b6 What is TabPFN? TabPFN (Prior-Fitted Network) is a neural network trained via in-context learning on thousands of synthetic datasets. It approximates Bayesian posterior inference, making it uniquely suited for: Quick baseline predictions Small dataset learning Uncertainty quantification Few-shot adaptation Key Innovation : Rather than training on a specific task, TabPFN learns to solve tasks as a sequence-to-sequence problem , making it excel in in-context learning scenarios. 2. Architecture \u00b6 2.1 High-Level Design \u00b6 flowchart LR A[Input Features] --> B[Feature Encoding] B --> C[Support Set Processing] C --> D[Transformer Stack] D --> E[Bayesian Inference] E --> F[Predictions + Uncertainty] 2.2 Core Components \u00b6 Feature Encoder : Converts tabular features to embedding space Support Set Processor : Handles training examples as context Transformer Stack : Self-attention over support + query samples Bayesian Head : Produces mean and variance estimates 2.3 Inference Process \u00b6 1. Encode support set (training data) 2. Encode query point (test sample) 3. Process through transformer layers 4. Output Bayesian posterior (mean + variance) 5. Generate predictions with uncertainty 3. Inference Parameters \u00b6 3.1 Complete Parameter Reference \u00b6 model_params = { 'n_estimators' : 16 , # Ensemble size 'softmax_temperature' : 0.9 , # Prediction confidence 'average_logits' : True , # Aggregation method 'prior_strength' : 1.0 , # Bayesian prior weight 'normalize_input' : True , # Feature normalization 'seed' : 42 # Reproducibility } 3.2 Parameter Descriptions \u00b6 Parameter Type Default Range Description n_estimators int 16 1-32 Number of ensemble members; higher = more robust softmax_temperature float 0.9 0.1-2.0 Scaling of logits before softmax; lower = sharper predictions average_logits bool True True/False Average logits vs probabilities across ensemble prior_strength float 1.0 0.5-2.0 Weight of Bayesian prior relative to data normalize_input bool True True/False Apply input normalization seed int 42 0+ Random seed for reproducibility 3.3 Parameter Tuning Guidelines \u00b6 Ensemble Size ( n_estimators ) : - 8-16 : Fast inference, good uncertainty - 16-32 : Robust predictions, slower Temperature ( softmax_temperature ) : - < 0.5 : Very confident predictions (may overfit) - 0.5 - 1.0 : Default, balanced confidence - > 1.0 : Softer predictions, lower confidence Average Method ( average_logits ) : - True : Better for class imbalance - False : Better for probability calibration 4. Fine-Tuning with TabPFN \u00b6 TabPFN supports full fine-tuning (base-ft strategy) for task adaptation. 4.1 Base Fine-Tuning Parameters \u00b6 tuning_params = { 'device' : 'cuda' , 'epochs' : 3 , 'learning_rate' : 1e-5 , 'batch_size' : 512 , 'optimizer' : 'adamw' , 'scheduler' : 'linear' , 'warmup_steps' : 100 , 'weight_decay' : 0.01 , 'show_progress' : True } 4.2 Fine-Tuning Best Practices \u00b6 Learning Rate : Start with 1e-5, increase if needed Epochs : 3-5 epochs typically sufficient Batch Size : 256-512 works well Warmup : Use 5-10% of total steps Early Stopping : Monitor validation metric 4.3 Fine-Tuning Example \u00b6 from tabtune import TabularPipeline pipeline = TabularPipeline ( model_name = 'TabPFN' , tuning_strategy = 'base-ft' , tuning_params = { 'device' : 'cuda' , 'epochs' : 5 , 'learning_rate' : 2e-5 , 'batch_size' : 256 , 'scheduler' : 'cosine' , 'show_progress' : True } ) # Fine-tune on your data pipeline . fit ( X_train , y_train ) # Evaluate metrics = pipeline . evaluate ( X_test , y_test ) print ( f \"Accuracy: { metrics [ 'accuracy' ] : .4f } \" ) 5. Inference-Only Usage \u00b6 5.1 Zero-Shot Predictions \u00b6 Use TabPFN's pre-trained weights for immediate predictions without training: from tabtune import TabularPipeline # Create pipeline with inference strategy pipeline = TabularPipeline ( model_name = 'TabPFN' , tuning_strategy = 'inference' , model_params = { 'n_estimators' : 16 , 'softmax_temperature' : 0.9 } ) # No training needed - just preprocess and predict pipeline . fit ( X_train , y_train ) # Only does preprocessing predictions = pipeline . predict ( X_test ) uncertainty = pipeline . get_uncertainty ( X_test ) 5.2 Uncertainty Estimation \u00b6 # Get predictions with uncertainty predictions , std_dev = pipeline . predict_with_uncertainty ( X_test ) # Filter predictions by confidence high_conf_idx = std_dev < np . percentile ( std_dev , 25 ) print ( f \"High confidence predictions: { high_conf_idx . sum () } / { len ( predictions ) } \" ) 6. Usage Scenarios \u00b6 6.1 Quick Baseline \u00b6 from tabtune import TabularPipeline # Establish baseline in seconds pipeline = TabularPipeline ( model_name = 'TabPFN' , tuning_strategy = 'inference' ) pipeline . fit ( X_train , y_train ) baseline_score = pipeline . evaluate ( X_test , y_test ) print ( f \"Baseline accuracy: { baseline_score [ 'accuracy' ] : .4f } \" ) 6.2 Small Dataset Learning \u00b6 # For datasets < 10K rows, TabPFN excels X_train , X_test , y_train , y_test = train_test_split ( X , y , test_size = 0.2 , random_state = 42 ) pipeline = TabularPipeline ( model_name = 'TabPFN' , tuning_strategy = 'base-ft' , tuning_params = { 'epochs' : 3 } ) pipeline . fit ( X_train , y_train ) 7. Limitations and Constraints \u00b6 7.1 Data Constraints \u00b6 Constraint Limit Impact Max Rows ~10K Exceeding causes performance degradation Max Features ~100 More features \u2192 longer processing time Min Features 2 Single-feature prediction not supported Max Classes 10 Binary/multi-class up to 10 classes 7.2 Feature Type Constraints \u00b6 Supported : Numerical, categorical, mixed Not Supported : Text, images, time-series Preprocessing : One-hot encoding recommended for categoricals 7.3 Task Type Constraints \u00b6 \u2705 Binary Classification \u2705 Multi-class Classification \u274c Regression \u274c Multi-output \u274c Multi-label 8. PEFT (LoRA) Support \u00b6 8.1 Current Status \u00b6 \u26a0\ufe0f Experimental : LoRA support for TabPFN is experimental due to: - Batched inference engine architecture - Adapter state management conflicts - Potential prediction inconsistencies 8.2 When to Use PEFT \u00b6 Not Recommended for TabPFN. Use base-ft strategy instead: # \u274c Not recommended pipeline = TabularPipeline ( model_name = 'TabPFN' , tuning_strategy = 'peft' # May have issues - will override to base-ft ) # \u2705 Recommended pipeline = TabularPipeline ( model_name = 'TabPFN' , tuning_strategy = 'base-ft' # Fully supported ) 10. Troubleshooting \u00b6 Issue: \"Dataset too large for TabPFN\" \u00b6 Solution : Use TabICL for datasets >10K rows if len ( X_train ) > 10000 : model = 'TabICL' else : model = 'TabPFN' Issue: \"Out of memory during inference\" \u00b6 Solution : Reduce batch size tuning_params = { 'batch_size' : 128 # Instead of 512 } Issue: \"Predictions too confident (low uncertainty)\" \u00b6 Solution : Increase temperature model_params = { 'softmax_temperature' : 1.5 # Instead of 0.9 } Issue: \"PEFT causing prediction errors\" \u00b6 Solution : Use base-ft strategy instead pipeline = TabularPipeline ( model_name = 'TabPFN' , tuning_strategy = 'base-ft' # Not peft ) 11. Complete Example Workflow \u00b6 from tabtune import TabularPipeline , TabularLeaderboard from sklearn.model_selection import train_test_split import pandas as pd # 1. Load data df = pd . read_csv ( 'small_dataset.csv' ) # <10K rows ideal X = df . drop ( 'target' , axis = 1 ) y = df [ 'target' ] # 2. Split X_train , X_test , y_train , y_test = train_test_split ( X , y , test_size = 0.2 , random_state = 42 ) # 3. Strategy 1: Zero-shot baseline print ( \"=== Zero-Shot Baseline ===\" ) baseline = TabularPipeline ( model_name = 'TabPFN' , tuning_strategy = 'inference' ) baseline . fit ( X_train , y_train ) baseline_metrics = baseline . evaluate ( X_test , y_test ) print ( f \"Baseline Accuracy: { baseline_metrics [ 'accuracy' ] : .4f } \" ) # 4. Strategy 2: Fine-tuned print ( \" \\n === Fine-Tuned ===\" ) finetuned = TabularPipeline ( model_name = 'TabPFN' , tuning_strategy = 'base-ft' , tuning_params = { 'device' : 'cuda' , 'epochs' : 5 , 'learning_rate' : 2e-5 , 'show_progress' : True } ) finetuned . fit ( X_train , y_train ) finetuned_metrics = finetuned . evaluate ( X_test , y_test ) print ( f \"Fine-tuned Accuracy: { finetuned_metrics [ 'accuracy' ] : .4f } \" ) # 5. Compare with other models print ( \" \\n === Model Comparison ===\" ) lb = TabularLeaderboard ( X_train , X_test , y_train , y_test ) lb . add_model ( 'TabPFN' , 'inference' , name = 'TabPFN-Inference' ) lb . add_model ( 'TabPFN' , 'base-ft' , name = 'TabPFN-FineTune' , tuning_params = { 'epochs' : 5 }) lb . add_model ( 'TabICL' , 'peft' , name = 'TabICL-PEFT' ) lb . run ( rank_by = 'accuracy' ) 12. Quick Reference \u00b6 Task Strategy Time Accuracy Instant baseline inference <1s Medium Rapid prototyping base-ft + 3 epochs 5m Good Production model base-ft + 5 epochs 15m High Uncertainty estimation inference <1s With uncertainty 13. Next Steps \u00b6 Model Selection - Compare with other models Tuning Strategies - Fine-tuning details TabularLeaderboard - Benchmark TabPFN vs other models API Reference - Complete API docs TabPFN excels at quick learning on small datasets. Use it for rapid experimentation and as a strong baseline!","title":"TabPFN"},{"location":"models/tabpfn/#tabpfn-prior-fitted-network","text":"TabPFN is a revolutionary tabular model that demonstrates strong zero-shot performance without any fine-tuning. This document provides an in-depth guide to using TabPFN with TabTune.","title":"TabPFN: Prior-Fitted Network"},{"location":"models/tabpfn/#1-introduction","text":"What is TabPFN? TabPFN (Prior-Fitted Network) is a neural network trained via in-context learning on thousands of synthetic datasets. It approximates Bayesian posterior inference, making it uniquely suited for: Quick baseline predictions Small dataset learning Uncertainty quantification Few-shot adaptation Key Innovation : Rather than training on a specific task, TabPFN learns to solve tasks as a sequence-to-sequence problem , making it excel in in-context learning scenarios.","title":"1. Introduction"},{"location":"models/tabpfn/#2-architecture","text":"","title":"2. Architecture"},{"location":"models/tabpfn/#21-high-level-design","text":"flowchart LR A[Input Features] --> B[Feature Encoding] B --> C[Support Set Processing] C --> D[Transformer Stack] D --> E[Bayesian Inference] E --> F[Predictions + Uncertainty]","title":"2.1 High-Level Design"},{"location":"models/tabpfn/#22-core-components","text":"Feature Encoder : Converts tabular features to embedding space Support Set Processor : Handles training examples as context Transformer Stack : Self-attention over support + query samples Bayesian Head : Produces mean and variance estimates","title":"2.2 Core Components"},{"location":"models/tabpfn/#23-inference-process","text":"1. Encode support set (training data) 2. Encode query point (test sample) 3. Process through transformer layers 4. Output Bayesian posterior (mean + variance) 5. Generate predictions with uncertainty","title":"2.3 Inference Process"},{"location":"models/tabpfn/#3-inference-parameters","text":"","title":"3. Inference Parameters"},{"location":"models/tabpfn/#31-complete-parameter-reference","text":"model_params = { 'n_estimators' : 16 , # Ensemble size 'softmax_temperature' : 0.9 , # Prediction confidence 'average_logits' : True , # Aggregation method 'prior_strength' : 1.0 , # Bayesian prior weight 'normalize_input' : True , # Feature normalization 'seed' : 42 # Reproducibility }","title":"3.1 Complete Parameter Reference"},{"location":"models/tabpfn/#32-parameter-descriptions","text":"Parameter Type Default Range Description n_estimators int 16 1-32 Number of ensemble members; higher = more robust softmax_temperature float 0.9 0.1-2.0 Scaling of logits before softmax; lower = sharper predictions average_logits bool True True/False Average logits vs probabilities across ensemble prior_strength float 1.0 0.5-2.0 Weight of Bayesian prior relative to data normalize_input bool True True/False Apply input normalization seed int 42 0+ Random seed for reproducibility","title":"3.2 Parameter Descriptions"},{"location":"models/tabpfn/#33-parameter-tuning-guidelines","text":"Ensemble Size ( n_estimators ) : - 8-16 : Fast inference, good uncertainty - 16-32 : Robust predictions, slower Temperature ( softmax_temperature ) : - < 0.5 : Very confident predictions (may overfit) - 0.5 - 1.0 : Default, balanced confidence - > 1.0 : Softer predictions, lower confidence Average Method ( average_logits ) : - True : Better for class imbalance - False : Better for probability calibration","title":"3.3 Parameter Tuning Guidelines"},{"location":"models/tabpfn/#4-fine-tuning-with-tabpfn","text":"TabPFN supports full fine-tuning (base-ft strategy) for task adaptation.","title":"4. Fine-Tuning with TabPFN"},{"location":"models/tabpfn/#41-base-fine-tuning-parameters","text":"tuning_params = { 'device' : 'cuda' , 'epochs' : 3 , 'learning_rate' : 1e-5 , 'batch_size' : 512 , 'optimizer' : 'adamw' , 'scheduler' : 'linear' , 'warmup_steps' : 100 , 'weight_decay' : 0.01 , 'show_progress' : True }","title":"4.1 Base Fine-Tuning Parameters"},{"location":"models/tabpfn/#42-fine-tuning-best-practices","text":"Learning Rate : Start with 1e-5, increase if needed Epochs : 3-5 epochs typically sufficient Batch Size : 256-512 works well Warmup : Use 5-10% of total steps Early Stopping : Monitor validation metric","title":"4.2 Fine-Tuning Best Practices"},{"location":"models/tabpfn/#43-fine-tuning-example","text":"from tabtune import TabularPipeline pipeline = TabularPipeline ( model_name = 'TabPFN' , tuning_strategy = 'base-ft' , tuning_params = { 'device' : 'cuda' , 'epochs' : 5 , 'learning_rate' : 2e-5 , 'batch_size' : 256 , 'scheduler' : 'cosine' , 'show_progress' : True } ) # Fine-tune on your data pipeline . fit ( X_train , y_train ) # Evaluate metrics = pipeline . evaluate ( X_test , y_test ) print ( f \"Accuracy: { metrics [ 'accuracy' ] : .4f } \" )","title":"4.3 Fine-Tuning Example"},{"location":"models/tabpfn/#5-inference-only-usage","text":"","title":"5. Inference-Only Usage"},{"location":"models/tabpfn/#51-zero-shot-predictions","text":"Use TabPFN's pre-trained weights for immediate predictions without training: from tabtune import TabularPipeline # Create pipeline with inference strategy pipeline = TabularPipeline ( model_name = 'TabPFN' , tuning_strategy = 'inference' , model_params = { 'n_estimators' : 16 , 'softmax_temperature' : 0.9 } ) # No training needed - just preprocess and predict pipeline . fit ( X_train , y_train ) # Only does preprocessing predictions = pipeline . predict ( X_test ) uncertainty = pipeline . get_uncertainty ( X_test )","title":"5.1 Zero-Shot Predictions"},{"location":"models/tabpfn/#52-uncertainty-estimation","text":"# Get predictions with uncertainty predictions , std_dev = pipeline . predict_with_uncertainty ( X_test ) # Filter predictions by confidence high_conf_idx = std_dev < np . percentile ( std_dev , 25 ) print ( f \"High confidence predictions: { high_conf_idx . sum () } / { len ( predictions ) } \" )","title":"5.2 Uncertainty Estimation"},{"location":"models/tabpfn/#6-usage-scenarios","text":"","title":"6. Usage Scenarios"},{"location":"models/tabpfn/#61-quick-baseline","text":"from tabtune import TabularPipeline # Establish baseline in seconds pipeline = TabularPipeline ( model_name = 'TabPFN' , tuning_strategy = 'inference' ) pipeline . fit ( X_train , y_train ) baseline_score = pipeline . evaluate ( X_test , y_test ) print ( f \"Baseline accuracy: { baseline_score [ 'accuracy' ] : .4f } \" )","title":"6.1 Quick Baseline"},{"location":"models/tabpfn/#62-small-dataset-learning","text":"# For datasets < 10K rows, TabPFN excels X_train , X_test , y_train , y_test = train_test_split ( X , y , test_size = 0.2 , random_state = 42 ) pipeline = TabularPipeline ( model_name = 'TabPFN' , tuning_strategy = 'base-ft' , tuning_params = { 'epochs' : 3 } ) pipeline . fit ( X_train , y_train )","title":"6.2 Small Dataset Learning"},{"location":"models/tabpfn/#7-limitations-and-constraints","text":"","title":"7. Limitations and Constraints"},{"location":"models/tabpfn/#71-data-constraints","text":"Constraint Limit Impact Max Rows ~10K Exceeding causes performance degradation Max Features ~100 More features \u2192 longer processing time Min Features 2 Single-feature prediction not supported Max Classes 10 Binary/multi-class up to 10 classes","title":"7.1 Data Constraints"},{"location":"models/tabpfn/#72-feature-type-constraints","text":"Supported : Numerical, categorical, mixed Not Supported : Text, images, time-series Preprocessing : One-hot encoding recommended for categoricals","title":"7.2 Feature Type Constraints"},{"location":"models/tabpfn/#73-task-type-constraints","text":"\u2705 Binary Classification \u2705 Multi-class Classification \u274c Regression \u274c Multi-output \u274c Multi-label","title":"7.3 Task Type Constraints"},{"location":"models/tabpfn/#8-peft-lora-support","text":"","title":"8. PEFT (LoRA) Support"},{"location":"models/tabpfn/#81-current-status","text":"\u26a0\ufe0f Experimental : LoRA support for TabPFN is experimental due to: - Batched inference engine architecture - Adapter state management conflicts - Potential prediction inconsistencies","title":"8.1 Current Status"},{"location":"models/tabpfn/#82-when-to-use-peft","text":"Not Recommended for TabPFN. Use base-ft strategy instead: # \u274c Not recommended pipeline = TabularPipeline ( model_name = 'TabPFN' , tuning_strategy = 'peft' # May have issues - will override to base-ft ) # \u2705 Recommended pipeline = TabularPipeline ( model_name = 'TabPFN' , tuning_strategy = 'base-ft' # Fully supported )","title":"8.2 When to Use PEFT"},{"location":"models/tabpfn/#10-troubleshooting","text":"","title":"10. Troubleshooting"},{"location":"models/tabpfn/#issue-dataset-too-large-for-tabpfn","text":"Solution : Use TabICL for datasets >10K rows if len ( X_train ) > 10000 : model = 'TabICL' else : model = 'TabPFN'","title":"Issue: \"Dataset too large for TabPFN\""},{"location":"models/tabpfn/#issue-out-of-memory-during-inference","text":"Solution : Reduce batch size tuning_params = { 'batch_size' : 128 # Instead of 512 }","title":"Issue: \"Out of memory during inference\""},{"location":"models/tabpfn/#issue-predictions-too-confident-low-uncertainty","text":"Solution : Increase temperature model_params = { 'softmax_temperature' : 1.5 # Instead of 0.9 }","title":"Issue: \"Predictions too confident (low uncertainty)\""},{"location":"models/tabpfn/#issue-peft-causing-prediction-errors","text":"Solution : Use base-ft strategy instead pipeline = TabularPipeline ( model_name = 'TabPFN' , tuning_strategy = 'base-ft' # Not peft )","title":"Issue: \"PEFT causing prediction errors\""},{"location":"models/tabpfn/#11-complete-example-workflow","text":"from tabtune import TabularPipeline , TabularLeaderboard from sklearn.model_selection import train_test_split import pandas as pd # 1. Load data df = pd . read_csv ( 'small_dataset.csv' ) # <10K rows ideal X = df . drop ( 'target' , axis = 1 ) y = df [ 'target' ] # 2. Split X_train , X_test , y_train , y_test = train_test_split ( X , y , test_size = 0.2 , random_state = 42 ) # 3. Strategy 1: Zero-shot baseline print ( \"=== Zero-Shot Baseline ===\" ) baseline = TabularPipeline ( model_name = 'TabPFN' , tuning_strategy = 'inference' ) baseline . fit ( X_train , y_train ) baseline_metrics = baseline . evaluate ( X_test , y_test ) print ( f \"Baseline Accuracy: { baseline_metrics [ 'accuracy' ] : .4f } \" ) # 4. Strategy 2: Fine-tuned print ( \" \\n === Fine-Tuned ===\" ) finetuned = TabularPipeline ( model_name = 'TabPFN' , tuning_strategy = 'base-ft' , tuning_params = { 'device' : 'cuda' , 'epochs' : 5 , 'learning_rate' : 2e-5 , 'show_progress' : True } ) finetuned . fit ( X_train , y_train ) finetuned_metrics = finetuned . evaluate ( X_test , y_test ) print ( f \"Fine-tuned Accuracy: { finetuned_metrics [ 'accuracy' ] : .4f } \" ) # 5. Compare with other models print ( \" \\n === Model Comparison ===\" ) lb = TabularLeaderboard ( X_train , X_test , y_train , y_test ) lb . add_model ( 'TabPFN' , 'inference' , name = 'TabPFN-Inference' ) lb . add_model ( 'TabPFN' , 'base-ft' , name = 'TabPFN-FineTune' , tuning_params = { 'epochs' : 5 }) lb . add_model ( 'TabICL' , 'peft' , name = 'TabICL-PEFT' ) lb . run ( rank_by = 'accuracy' )","title":"11. Complete Example Workflow"},{"location":"models/tabpfn/#12-quick-reference","text":"Task Strategy Time Accuracy Instant baseline inference <1s Medium Rapid prototyping base-ft + 3 epochs 5m Good Production model base-ft + 5 epochs 15m High Uncertainty estimation inference <1s With uncertainty","title":"12. Quick Reference"},{"location":"models/tabpfn/#13-next-steps","text":"Model Selection - Compare with other models Tuning Strategies - Fine-tuning details TabularLeaderboard - Benchmark TabPFN vs other models API Reference - Complete API docs TabPFN excels at quick learning on small datasets. Use it for rapid experimentation and as a strong baseline!","title":"13. Next Steps"},{"location":"user-guide/data-processing/","text":"Data Processing \u00b6 The DataProcessor is TabTune's intelligent, model-aware data preparation engine. It automatically handles imputation, scaling, categorical encoding, feature selection, and resampling based on your chosen model's requirements. 1. Overview \u00b6 The DataProcessor integrates two preprocessing pathways: Custom Model-Specific Preprocessing : For models requiring specialized transformations (TabPFN, TabICL, ContextTab, Mitra, TabDPT, OrionMSP, OrionBix). Standard Preprocessing Pipeline : For general tabular models using scikit-learn-compatible transformations. 2. Architecture \u00b6 flowchart TD A[Raw DataFrame] --> B{Model-Specific?} B -->|Yes| C[Custom Preprocessor] B -->|No| D[Standard Pipeline] C --> E[Transformed Data] D --> F[Imputation] F --> G[Categorical Encoding] G --> H[Scaling] H --> I[Feature Selection] I --> J[Resampling] J --> E 3. Initialization Parameters \u00b6 DataProcessor ( model_name = None , override_types = None , imputation_strategy = 'mean' , categorical_encoding = 'onehot' , scaling_strategy = 'standard' , resampling_strategy = None , feature_selection_strategy = None , feature_selection_k = 10 , model_params = None ) Parameter Reference \u00b6 Parameter Type Default Description model_name str None Model identifier to trigger custom preprocessing override_types dict None Manual column type specifications imputation_strategy str 'mean' Strategy for missing value imputation categorical_encoding str 'onehot' Categorical encoding method scaling_strategy str 'standard' Numerical feature scaling method resampling_strategy str None Class imbalance correction method feature_selection_strategy str None Feature selection algorithm feature_selection_k int 10 Number of features to select model_params dict None Additional model-specific parameters 4. Model-Aware Defaults \u00b6 When model_name is specified, the processor automatically configures optimal preprocessing: Model Categorical Encoding Special Handling TabPFN tabpfn_special Integer encoding, limited feature count TabICL tabicl_special Normalization + shuffling support OrionMSP orionmsp_special Multi-scale prior preprocessing OrionBix tab_biaxial_special Custom biaxial attention preprocessing TabDPT tabdpt_special Context-aware transformations Mitra mitra_special 2D attention preprocessing ContextTab contexttab_special Text embedding integration 5. Standard Preprocessing Pipeline \u00b6 5.1 Missing Value Imputation \u00b6 Available strategies: mean : Replace with column mean (default) median : Replace with column median iterative : Multivariate imputation using IterativeImputer knn : K-Nearest Neighbors imputation processor = DataProcessor ( imputation_strategy = 'median' ) 5.2 Categorical Encoding \u00b6 Supported methods: onehot : One-hot encoding (default, handles unknown categories) ordinal : Ordinal encoding with unknown value handling target : Target encoding (uses target statistics) hashing : Hashing encoder for high-cardinality features binary : Binary encoding for memory efficiency processor = DataProcessor ( categorical_encoding = 'target' ) 5.3 Numerical Scaling \u00b6 Available scalers: standard : Standardization (zero mean, unit variance) minmax : Min-max normalization to [0, 1] robust : Robust scaling using median and IQR power_transform : Power transformation for normality processor = DataProcessor ( scaling_strategy = 'robust' ) 5.4 Feature Selection \u00b6 Methods: variance : Remove low-variance features select_k_best_anova : ANOVA F-test for classification select_k_best_chi2 : Chi-squared test (requires non-negative features) correlation : Remove highly correlated features (threshold=0.9) processor = DataProcessor ( feature_selection_strategy = 'select_k_best_anova' , feature_selection_k = 20 ) 5.5 Class Resampling \u00b6 Imbalanced data handling: smote : Synthetic Minority Over-sampling Technique random_over : Random oversampling of minority class random_under : Random undersampling of majority class tomek : Tomek links removal kmeans : Cluster centroids undersampling knn : Neighborhood cleaning rule processor = DataProcessor ( resampling_strategy = 'smote' ) 6. get_processing_summary() function \u00b6 Returns a detailed report of all applied transformations. summary = processor . get_processing_summary () print ( summary ) Example Output : --- Data Processing Summary --- [Standard Preprocessing Pipeline] 1. Imputation (Strategy: 'median') - Applied to 12 numerical features: `age`, `salary`, ... 2. Categorical Encoding (Strategy: 'onehot') - Applied to 5 categorical features: `city`, `occupation`, ... 3. Scaling (Strategy: 'standard') - Applied to 27 features (original numerical + encoded categorical). 4. Feature Selection (Strategy: 'select_k_best_anova') - Removed 7 features: `feature_3`, `feature_8`, ... [Resampling] - Strategy: 'smote' applied to the training data. 7. Custom Model Preprocessing \u00b6 For models with special requirements, TabTune automatically loads the appropriate custom preprocessor: TabPFN Preprocessing \u00b6 Converts all features to integer codes Limits feature count (max 100) Handles categorical and numerical separately TabICL Preprocessing \u00b6 Applies multiple normalization methods Supports feature shuffling strategies Prepares data for episodic training OrionMSP Preprocessing \u00b6 Multi-scale prior transformations Column-then-row attention preparation Ensemble view transformations OrionBix Preprocessing \u00b6 Biaxial attention preprocessing Custom feature interaction handling Similar to TabICL with enhanced interactions ContextTab Preprocessing \u00b6 Generates text embeddings for categorical features Integrates semantic information Handles mixed-type features TabDPT Preprocessing \u00b6 Context-based transformations Supports k-NN context selection Prepares permuted features Mitra Preprocessing \u00b6 2D attention-compatible format Row and column embeddings Synthetic prior integration 8. Usage Examples \u00b6 Example 1: Basic Standard Pipeline \u00b6 from tabtune import DataProcessor processor = DataProcessor ( imputation_strategy = 'median' , categorical_encoding = 'onehot' , scaling_strategy = 'standard' ) # Fit on training data processor . fit ( X_train , y_train ) # Transform training and test data X_train_processed , y_train_processed = processor . transform ( X_train , y_train ) X_test_processed = processor . transform ( X_test ) Example 3: Handling Imbalanced Data \u00b6 processor = DataProcessor ( imputation_strategy = 'mean' , categorical_encoding = 'target' , scaling_strategy = 'robust' , resampling_strategy = 'smote' ) # Resampling is applied during fit_transform X_resampled , y_resampled = processor . fit_transform ( X_train , y_train ) 9. Best Practices \u00b6 Always fit on training data only : Never fit on test data to avoid data leakage. Use model-specific preprocessing : Let model_name trigger optimal defaults. Check processing summary : Verify applied transformations before training. Handle missing values explicitly : Choose imputation strategy based on data distribution. Scale after encoding : Standard pipeline handles this automatically. Test resampling impact : Compare with and without resampling for imbalanced tasks. 10. Troubleshooting \u00b6 Issue: \"Must call fit() before calling transform()\" \u00b6 Solution : Ensure .fit() is called on training data before .transform() . Issue: Feature count mismatch after encoding \u00b6 Solution : Use handle_unknown='ignore' in encoder or ensure test data has same categories. Issue: NaN values after transformation \u00b6 Solution : Check imputation strategy; use 'iterative' for complex missing patterns. Issue: Memory errors with large datasets \u00b6 Solution : Use 'hashing' or 'binary' encoding; avoid one-hot for high cardinality. 11. Next Steps \u00b6 Tuning Strategies - Learn about training workflows Model Selection - Choose the right model API Reference - Complete DataProcessor API The DataProcessor ensures your data is optimally prepared for any TabTune model with minimal configuration.","title":"Data Processing"},{"location":"user-guide/data-processing/#data-processing","text":"The DataProcessor is TabTune's intelligent, model-aware data preparation engine. It automatically handles imputation, scaling, categorical encoding, feature selection, and resampling based on your chosen model's requirements.","title":"Data Processing"},{"location":"user-guide/data-processing/#1-overview","text":"The DataProcessor integrates two preprocessing pathways: Custom Model-Specific Preprocessing : For models requiring specialized transformations (TabPFN, TabICL, ContextTab, Mitra, TabDPT, OrionMSP, OrionBix). Standard Preprocessing Pipeline : For general tabular models using scikit-learn-compatible transformations.","title":"1. Overview"},{"location":"user-guide/data-processing/#2-architecture","text":"flowchart TD A[Raw DataFrame] --> B{Model-Specific?} B -->|Yes| C[Custom Preprocessor] B -->|No| D[Standard Pipeline] C --> E[Transformed Data] D --> F[Imputation] F --> G[Categorical Encoding] G --> H[Scaling] H --> I[Feature Selection] I --> J[Resampling] J --> E","title":"2. Architecture"},{"location":"user-guide/data-processing/#3-initialization-parameters","text":"DataProcessor ( model_name = None , override_types = None , imputation_strategy = 'mean' , categorical_encoding = 'onehot' , scaling_strategy = 'standard' , resampling_strategy = None , feature_selection_strategy = None , feature_selection_k = 10 , model_params = None )","title":"3. Initialization Parameters"},{"location":"user-guide/data-processing/#parameter-reference","text":"Parameter Type Default Description model_name str None Model identifier to trigger custom preprocessing override_types dict None Manual column type specifications imputation_strategy str 'mean' Strategy for missing value imputation categorical_encoding str 'onehot' Categorical encoding method scaling_strategy str 'standard' Numerical feature scaling method resampling_strategy str None Class imbalance correction method feature_selection_strategy str None Feature selection algorithm feature_selection_k int 10 Number of features to select model_params dict None Additional model-specific parameters","title":"Parameter Reference"},{"location":"user-guide/data-processing/#4-model-aware-defaults","text":"When model_name is specified, the processor automatically configures optimal preprocessing: Model Categorical Encoding Special Handling TabPFN tabpfn_special Integer encoding, limited feature count TabICL tabicl_special Normalization + shuffling support OrionMSP orionmsp_special Multi-scale prior preprocessing OrionBix tab_biaxial_special Custom biaxial attention preprocessing TabDPT tabdpt_special Context-aware transformations Mitra mitra_special 2D attention preprocessing ContextTab contexttab_special Text embedding integration","title":"4. Model-Aware Defaults"},{"location":"user-guide/data-processing/#5-standard-preprocessing-pipeline","text":"","title":"5. Standard Preprocessing Pipeline"},{"location":"user-guide/data-processing/#51-missing-value-imputation","text":"Available strategies: mean : Replace with column mean (default) median : Replace with column median iterative : Multivariate imputation using IterativeImputer knn : K-Nearest Neighbors imputation processor = DataProcessor ( imputation_strategy = 'median' )","title":"5.1 Missing Value Imputation"},{"location":"user-guide/data-processing/#52-categorical-encoding","text":"Supported methods: onehot : One-hot encoding (default, handles unknown categories) ordinal : Ordinal encoding with unknown value handling target : Target encoding (uses target statistics) hashing : Hashing encoder for high-cardinality features binary : Binary encoding for memory efficiency processor = DataProcessor ( categorical_encoding = 'target' )","title":"5.2 Categorical Encoding"},{"location":"user-guide/data-processing/#53-numerical-scaling","text":"Available scalers: standard : Standardization (zero mean, unit variance) minmax : Min-max normalization to [0, 1] robust : Robust scaling using median and IQR power_transform : Power transformation for normality processor = DataProcessor ( scaling_strategy = 'robust' )","title":"5.3 Numerical Scaling"},{"location":"user-guide/data-processing/#54-feature-selection","text":"Methods: variance : Remove low-variance features select_k_best_anova : ANOVA F-test for classification select_k_best_chi2 : Chi-squared test (requires non-negative features) correlation : Remove highly correlated features (threshold=0.9) processor = DataProcessor ( feature_selection_strategy = 'select_k_best_anova' , feature_selection_k = 20 )","title":"5.4 Feature Selection"},{"location":"user-guide/data-processing/#55-class-resampling","text":"Imbalanced data handling: smote : Synthetic Minority Over-sampling Technique random_over : Random oversampling of minority class random_under : Random undersampling of majority class tomek : Tomek links removal kmeans : Cluster centroids undersampling knn : Neighborhood cleaning rule processor = DataProcessor ( resampling_strategy = 'smote' )","title":"5.5 Class Resampling"},{"location":"user-guide/data-processing/#6-get_processing_summary-function","text":"Returns a detailed report of all applied transformations. summary = processor . get_processing_summary () print ( summary ) Example Output : --- Data Processing Summary --- [Standard Preprocessing Pipeline] 1. Imputation (Strategy: 'median') - Applied to 12 numerical features: `age`, `salary`, ... 2. Categorical Encoding (Strategy: 'onehot') - Applied to 5 categorical features: `city`, `occupation`, ... 3. Scaling (Strategy: 'standard') - Applied to 27 features (original numerical + encoded categorical). 4. Feature Selection (Strategy: 'select_k_best_anova') - Removed 7 features: `feature_3`, `feature_8`, ... [Resampling] - Strategy: 'smote' applied to the training data.","title":"6. get_processing_summary() function"},{"location":"user-guide/data-processing/#7-custom-model-preprocessing","text":"For models with special requirements, TabTune automatically loads the appropriate custom preprocessor:","title":"7. Custom Model Preprocessing"},{"location":"user-guide/data-processing/#tabpfn-preprocessing","text":"Converts all features to integer codes Limits feature count (max 100) Handles categorical and numerical separately","title":"TabPFN Preprocessing"},{"location":"user-guide/data-processing/#tabicl-preprocessing","text":"Applies multiple normalization methods Supports feature shuffling strategies Prepares data for episodic training","title":"TabICL Preprocessing"},{"location":"user-guide/data-processing/#orionmsp-preprocessing","text":"Multi-scale prior transformations Column-then-row attention preparation Ensemble view transformations","title":"OrionMSP Preprocessing"},{"location":"user-guide/data-processing/#orionbix-preprocessing","text":"Biaxial attention preprocessing Custom feature interaction handling Similar to TabICL with enhanced interactions","title":"OrionBix Preprocessing"},{"location":"user-guide/data-processing/#contexttab-preprocessing","text":"Generates text embeddings for categorical features Integrates semantic information Handles mixed-type features","title":"ContextTab Preprocessing"},{"location":"user-guide/data-processing/#tabdpt-preprocessing","text":"Context-based transformations Supports k-NN context selection Prepares permuted features","title":"TabDPT Preprocessing"},{"location":"user-guide/data-processing/#mitra-preprocessing","text":"2D attention-compatible format Row and column embeddings Synthetic prior integration","title":"Mitra Preprocessing"},{"location":"user-guide/data-processing/#8-usage-examples","text":"","title":"8. Usage Examples"},{"location":"user-guide/data-processing/#example-1-basic-standard-pipeline","text":"from tabtune import DataProcessor processor = DataProcessor ( imputation_strategy = 'median' , categorical_encoding = 'onehot' , scaling_strategy = 'standard' ) # Fit on training data processor . fit ( X_train , y_train ) # Transform training and test data X_train_processed , y_train_processed = processor . transform ( X_train , y_train ) X_test_processed = processor . transform ( X_test )","title":"Example 1: Basic Standard Pipeline"},{"location":"user-guide/data-processing/#example-3-handling-imbalanced-data","text":"processor = DataProcessor ( imputation_strategy = 'mean' , categorical_encoding = 'target' , scaling_strategy = 'robust' , resampling_strategy = 'smote' ) # Resampling is applied during fit_transform X_resampled , y_resampled = processor . fit_transform ( X_train , y_train )","title":"Example 3: Handling Imbalanced Data"},{"location":"user-guide/data-processing/#9-best-practices","text":"Always fit on training data only : Never fit on test data to avoid data leakage. Use model-specific preprocessing : Let model_name trigger optimal defaults. Check processing summary : Verify applied transformations before training. Handle missing values explicitly : Choose imputation strategy based on data distribution. Scale after encoding : Standard pipeline handles this automatically. Test resampling impact : Compare with and without resampling for imbalanced tasks.","title":"9. Best Practices"},{"location":"user-guide/data-processing/#10-troubleshooting","text":"","title":"10. Troubleshooting"},{"location":"user-guide/data-processing/#issue-must-call-fit-before-calling-transform","text":"Solution : Ensure .fit() is called on training data before .transform() .","title":"Issue: \"Must call fit() before calling transform()\""},{"location":"user-guide/data-processing/#issue-feature-count-mismatch-after-encoding","text":"Solution : Use handle_unknown='ignore' in encoder or ensure test data has same categories.","title":"Issue: Feature count mismatch after encoding"},{"location":"user-guide/data-processing/#issue-nan-values-after-transformation","text":"Solution : Check imputation strategy; use 'iterative' for complex missing patterns.","title":"Issue: NaN values after transformation"},{"location":"user-guide/data-processing/#issue-memory-errors-with-large-datasets","text":"Solution : Use 'hashing' or 'binary' encoding; avoid one-hot for high cardinality.","title":"Issue: Memory errors with large datasets"},{"location":"user-guide/data-processing/#11-next-steps","text":"Tuning Strategies - Learn about training workflows Model Selection - Choose the right model API Reference - Complete DataProcessor API The DataProcessor ensures your data is optimally prepared for any TabTune model with minimal configuration.","title":"11. Next Steps"},{"location":"user-guide/leaderboard/","text":"Model Comparison with TabularLeaderboard \u00b6 The TabularLeaderboard is a powerful benchmarking tool that enables systematic comparison of multiple models and tuning strategies on your dataset. This guide shows how to use it effectively. 1. Overview \u00b6 TabularLeaderboard simplifies the process of comparing different TabTune models and strategies: \u2705 Compare multiple models simultaneously \u2705 Test different tuning strategies (inference, base-ft, peft) \u2705 Rank results by any evaluation metric \u2705 Export results for analysis \u2705 Track experiment metadata \u2705 Visualize performance comparisons 2. Core Concepts \u00b6 2.1 Workflow \u00b6 flowchart LR A[Initialize Leaderboard] --> B[Add Models] B --> C[Configure Strategies] C --> D[Run Benchmarks] D --> E[Evaluate & Rank] E --> F[Export Results] 2.2 Key Components \u00b6 Leaderboard : Container for managing multiple model configurations Model Entry : Single model + strategy + hyperparameter combination Benchmark : Complete evaluation run across all added models Results : Ranked comparison with metrics and metadata 3. Basic Usage \u00b6 3.1 Initialize Leaderboard \u00b6 from tabtune import TabularLeaderboard from sklearn.model_selection import train_test_split # Prepare data splits X , y = load_your_data () X_train , X_test , y_train , y_test = train_test_split ( X , y , test_size = 0.2 , random_state = 42 ) # Initialize leaderboard leaderboard = TabularLeaderboard ( X_train = X_train , X_test = X_test , y_train = y_train , y_test = y_test , task_type = 'classification' ) 3.2 Add Models to Leaderboard \u00b6 # Add single model with default settings leaderboard . add_model ( model_name = 'TabPFN' , tuning_strategy = 'inference' ) # Add model with custom tuning parameters leaderboard . add_model ( model_name = 'TabICL' , tuning_strategy = 'base-ft' , model_params = { 'n_estimators' : 16 }, tuning_params = { 'epochs' : 5 , 'learning_rate' : 2e-5 } ) # Add same model with different strategy leaderboard . add_model ( model_name = 'TabICL' , tuning_strategy = 'peft' , model_params = { 'n_estimators' : 16 }, tuning_params = { 'epochs' : 5 , 'learning_rate' : 2e-4 , 'peft_config' : { 'r' : 8 , 'lora_alpha' : 16 } } ) 3.3 Run Benchmarks \u00b6 # Run all added models results = leaderboard . run () # Run with custom settings results = leaderboard . run ( rank_by = 'roc_auc_score' , # Metric to rank by verbose = True , # Print progress n_jobs = 1 # Parallel jobs (1 = sequential) ) 4. Complete Example \u00b6 4.1 Full Comparison Workflow \u00b6 from tabtune import TabularLeaderboard import pandas as pd from sklearn.model_selection import train_test_split # 1. Load data df = pd . read_csv ( 'dataset.csv' ) X = df . drop ( 'target' , axis = 1 ) y = df [ 'target' ] # 2. Split data X_train , X_test , y_train , y_test = train_test_split ( X , y , test_size = 0.2 ) # 3. Initialize leaderboard leaderboard = TabularLeaderboard ( X_train , X_test , y_train , y_test ) # 4. Add models - Inference Baseline leaderboard . add_model ( 'TabPFN' , 'inference' , name = 'TabPFN-Inference' ) # 5. Add models - PEFT Strategies for model in [ 'TabICL' , 'OrionMSP' , 'OrionBix' , 'Mitra' ]: leaderboard . add_model ( model , 'peft' , name = f ' { model } -PEFT' , tuning_params = { 'epochs' : 3 , 'peft_config' : { 'r' : 8 , 'lora_alpha' : 16 } } ) # 6. Add models - Base Fine-Tuning (for comparison) leaderboard . add_model ( 'TabICL' , 'base-ft' , name = 'TabICL-BaseFT' , tuning_params = { 'epochs' : 5 , 'learning_rate' : 2e-5 } ) # 7. Run benchmarks results = leaderboard . run ( rank_by = 'accuracy' , verbose = True ) 5. Class Reference \u00b6 5.1 TabularLeaderboard Constructor \u00b6 TabularLeaderboard ( X_train : pd . DataFrame , X_test : pd . DataFrame , y_train : pd . Series , y_test : pd . Series , task_type : str = 'classification' , validation_split : float = 0.1 , random_state : int = 42 ) Parameters : Parameter Type Default Description X_train DataFrame Required Training features X_test DataFrame Required Test features y_train Series Required Training labels y_test Series Required Test labels task_type str 'classification' 'classification' or 'regression' validation_split float 0.1 Fraction for validation random_state int 42 Random seed for reproducibility 5.2 add_model() Method \u00b6 leaderboard . add_model ( model_name : str , tuning_strategy : str , name : str = None , model_params : dict = None , tuning_params : dict = None , processor_params : dict = None ) Parameters : Parameter Type Default Description model_name str Required Model to add (TabPFN, TabICL, etc.) tuning_strategy str Required 'inference', 'base-ft', or 'peft' name str None Custom name for leaderboard (auto-generated if None) model_params dict None Model-specific hyperparameters tuning_params dict None Training hyperparameters processor_params dict None Preprocessing parameters 5.3 run() Method \u00b6 results = leaderboard . run ( rank_by : str = 'accuracy' , verbose : bool = True , timeout : float = None ) Parameters : Parameter Type Default Description rank_by str 'accuracy' Metric to rank models verbose bool True Print progress timeout float None Timeout per model in seconds Return : LeaderboardResults object with all benchmarks 6. Advanced Usage \u00b6 6.1 Custom Model Names \u00b6 # Add with meaningful names leaderboard . add_model ( 'TabICL' , 'peft' , name = 'TabICL-PEFT-r8' , tuning_params = { 'peft_config' : { 'r' : 8 }} ) leaderboard . add_model ( 'TabICL' , 'peft' , name = 'TabICL-PEFT-r16' , tuning_params = { 'peft_config' : { 'r' : 16 }} ) results = leaderboard . run ( rank_by = 'accuracy' ) 6.2 Hyperparameter Grid \u00b6 Test multiple hyperparameter combinations: from itertools import product # Define hyperparameter grid learning_rates = [ 1e-5 , 2e-5 , 5e-5 ] ranks = [ 4 , 8 , 16 ] # Grid search for lr , r in product ( learning_rates , ranks ): leaderboard . add_model ( 'TabICL' , 'peft' , name = f 'TabICL-lr { lr } -r { r } ' , tuning_params = { 'learning_rate' : lr , 'peft_config' : { 'r' : r , 'lora_alpha' : 2 * r } } ) results = leaderboard . run ( rank_by = 'f1_score' , verbose = True ) print ( leaderboard . get_ranking ()) 6.3 Strategy Comparison \u00b6 Compare all three strategies for a model: model = 'TabDPT' # Inference baseline leaderboard . add_model ( model , 'inference' , name = f ' { model } -Inference' ) # Base fine-tuning leaderboard . add_model ( model , 'base-ft' , name = f ' { model } -BaseFT' , tuning_params = { 'epochs' : 5 } ) # PEFT fine-tuning leaderboard . add_model ( model , 'peft' , name = f ' { model } -PEFT' , tuning_params = { 'epochs' : 5 , 'peft_config' : { 'r' : 8 }} ) results = leaderboard . run () # Extract strategy comparison for name , score in results . items (): strategy = name . split ( '-' )[ - 1 ] print ( f \" { strategy } : { score [ 'accuracy' ] : .4f } \" ) 6.4 Cross-Validation \u00b6 Test stability across multiple folds: from sklearn.model_selection import KFold kf = KFold ( n_splits = 5 ) fold_results = [] for fold_idx , ( train_idx , test_idx ) in enumerate ( kf . split ( X )): X_train_fold , X_test_fold = X . iloc [ train_idx ], X . iloc [ test_idx ] y_train_fold , y_test_fold = y . iloc [ train_idx ], y . iloc [ test_idx ] # Create leaderboard for this fold lb = TabularLeaderboard ( X_train_fold , X_test_fold , y_train_fold , y_test_fold ) # Add models lb . add_model ( 'TabICL' , 'peft' ) lb . add_model ( 'OrionBix' , 'base-ft' ) # Run and store results = lb . run () fold_results . append ( results ) # Aggregate results import pandas as pd all_results = pd . concat ( fold_results ) print ( all_results . groupby ( 'model' ) . mean ()) 7. Evaluation Metrics \u00b6 Supported metrics for ranking: Classification Metrics : - accuracy : Fraction of correct predictions - f1_score : Harmonic mean of precision and recall (weighted) - roc_auc_score : Area under ROC curve - precision_score : True positives / (TP + FP) - recall_score : True positives / (TP + FN) - balanced_accuracy : Average per-class accuracy Regression Metrics : - mse : Mean squared error (lower is better) - rmse : Root mean squared error - mae : Mean absolute error - r2_score : Coefficient of determination # Rank by different metrics results_acc = leaderboard . run ( rank_by = 'accuracy' ) results_f1 = leaderboard . run ( rank_by = 'f1_score' ) results_auc = leaderboard . run ( rank_by = 'roc_auc_score' ) 8. Best Practices \u00b6 \u2705 Do's \u00b6 \u2705 Use consistent data splits across all models \u2705 Fix random seed for reproducibility \u2705 Include inference baseline for comparison \u2705 Test multiple strategies for each model \u2705 Save results to disk \u2705 Use reasonable timeout values \u2705 Export results as CSV for further analysis \u274c Don'ts \u00b6 \u274c Don't change data between runs \u274c Don't use training data for validation \u274c Don't tune hyperparameters on test set \u274c Don't mix different task types in one leaderboard \u274c Don't run without timeout protection \u274c Don't forget to save best model config 9. Complete Workflow Example \u00b6 from tabtune import TabularLeaderboard import pandas as pd from sklearn.model_selection import train_test_split # Step 1: Load and prepare data print ( \"Loading data...\" ) df = pd . read_csv ( 'data.csv' ) X = df . drop ( 'target' , axis = 1 ) y = df [ 'target' ] X_train , X_test , y_train , y_test = train_test_split ( X , y , test_size = 0.2 , random_state = 42 ) # Step 2: Initialize leaderboard print ( \"Initializing leaderboard...\" ) lb = TabularLeaderboard ( X_train , X_test , y_train , y_test ) # Step 3: Add baseline print ( \"Adding inference baseline...\" ) lb . add_model ( 'TabPFN' , 'inference' ) # Step 4: Add PEFT models print ( \"Adding PEFT models...\" ) for model in [ 'TabICL' , 'OrionMSP' , 'OrionBix' , 'TabDPT' ]: lb . add_model ( model , 'peft' , tuning_params = { 'epochs' : 3 , 'peft_config' : { 'r' : 8 }} ) # Step 5: Run benchmarks print ( \"Running benchmarks...\" ) results = lb . run ( rank_by = 'accuracy' , verbose = True , n_jobs =- 1 ) 10. Next Steps \u00b6 Model Selection - Guide for choosing models Tuning Strategies - Deep dive into strategies Examples - More examples The TabularLeaderboard streamlines model selection by enabling systematic, reproducible benchmarking!","title":"Model Comparison"},{"location":"user-guide/leaderboard/#model-comparison-with-tabularleaderboard","text":"The TabularLeaderboard is a powerful benchmarking tool that enables systematic comparison of multiple models and tuning strategies on your dataset. This guide shows how to use it effectively.","title":"Model Comparison with TabularLeaderboard"},{"location":"user-guide/leaderboard/#1-overview","text":"TabularLeaderboard simplifies the process of comparing different TabTune models and strategies: \u2705 Compare multiple models simultaneously \u2705 Test different tuning strategies (inference, base-ft, peft) \u2705 Rank results by any evaluation metric \u2705 Export results for analysis \u2705 Track experiment metadata \u2705 Visualize performance comparisons","title":"1. Overview"},{"location":"user-guide/leaderboard/#2-core-concepts","text":"","title":"2. Core Concepts"},{"location":"user-guide/leaderboard/#21-workflow","text":"flowchart LR A[Initialize Leaderboard] --> B[Add Models] B --> C[Configure Strategies] C --> D[Run Benchmarks] D --> E[Evaluate & Rank] E --> F[Export Results]","title":"2.1 Workflow"},{"location":"user-guide/leaderboard/#22-key-components","text":"Leaderboard : Container for managing multiple model configurations Model Entry : Single model + strategy + hyperparameter combination Benchmark : Complete evaluation run across all added models Results : Ranked comparison with metrics and metadata","title":"2.2 Key Components"},{"location":"user-guide/leaderboard/#3-basic-usage","text":"","title":"3. Basic Usage"},{"location":"user-guide/leaderboard/#31-initialize-leaderboard","text":"from tabtune import TabularLeaderboard from sklearn.model_selection import train_test_split # Prepare data splits X , y = load_your_data () X_train , X_test , y_train , y_test = train_test_split ( X , y , test_size = 0.2 , random_state = 42 ) # Initialize leaderboard leaderboard = TabularLeaderboard ( X_train = X_train , X_test = X_test , y_train = y_train , y_test = y_test , task_type = 'classification' )","title":"3.1 Initialize Leaderboard"},{"location":"user-guide/leaderboard/#32-add-models-to-leaderboard","text":"# Add single model with default settings leaderboard . add_model ( model_name = 'TabPFN' , tuning_strategy = 'inference' ) # Add model with custom tuning parameters leaderboard . add_model ( model_name = 'TabICL' , tuning_strategy = 'base-ft' , model_params = { 'n_estimators' : 16 }, tuning_params = { 'epochs' : 5 , 'learning_rate' : 2e-5 } ) # Add same model with different strategy leaderboard . add_model ( model_name = 'TabICL' , tuning_strategy = 'peft' , model_params = { 'n_estimators' : 16 }, tuning_params = { 'epochs' : 5 , 'learning_rate' : 2e-4 , 'peft_config' : { 'r' : 8 , 'lora_alpha' : 16 } } )","title":"3.2 Add Models to Leaderboard"},{"location":"user-guide/leaderboard/#33-run-benchmarks","text":"# Run all added models results = leaderboard . run () # Run with custom settings results = leaderboard . run ( rank_by = 'roc_auc_score' , # Metric to rank by verbose = True , # Print progress n_jobs = 1 # Parallel jobs (1 = sequential) )","title":"3.3 Run Benchmarks"},{"location":"user-guide/leaderboard/#4-complete-example","text":"","title":"4. Complete Example"},{"location":"user-guide/leaderboard/#41-full-comparison-workflow","text":"from tabtune import TabularLeaderboard import pandas as pd from sklearn.model_selection import train_test_split # 1. Load data df = pd . read_csv ( 'dataset.csv' ) X = df . drop ( 'target' , axis = 1 ) y = df [ 'target' ] # 2. Split data X_train , X_test , y_train , y_test = train_test_split ( X , y , test_size = 0.2 ) # 3. Initialize leaderboard leaderboard = TabularLeaderboard ( X_train , X_test , y_train , y_test ) # 4. Add models - Inference Baseline leaderboard . add_model ( 'TabPFN' , 'inference' , name = 'TabPFN-Inference' ) # 5. Add models - PEFT Strategies for model in [ 'TabICL' , 'OrionMSP' , 'OrionBix' , 'Mitra' ]: leaderboard . add_model ( model , 'peft' , name = f ' { model } -PEFT' , tuning_params = { 'epochs' : 3 , 'peft_config' : { 'r' : 8 , 'lora_alpha' : 16 } } ) # 6. Add models - Base Fine-Tuning (for comparison) leaderboard . add_model ( 'TabICL' , 'base-ft' , name = 'TabICL-BaseFT' , tuning_params = { 'epochs' : 5 , 'learning_rate' : 2e-5 } ) # 7. Run benchmarks results = leaderboard . run ( rank_by = 'accuracy' , verbose = True )","title":"4.1 Full Comparison Workflow"},{"location":"user-guide/leaderboard/#5-class-reference","text":"","title":"5. Class Reference"},{"location":"user-guide/leaderboard/#51-tabularleaderboard-constructor","text":"TabularLeaderboard ( X_train : pd . DataFrame , X_test : pd . DataFrame , y_train : pd . Series , y_test : pd . Series , task_type : str = 'classification' , validation_split : float = 0.1 , random_state : int = 42 ) Parameters : Parameter Type Default Description X_train DataFrame Required Training features X_test DataFrame Required Test features y_train Series Required Training labels y_test Series Required Test labels task_type str 'classification' 'classification' or 'regression' validation_split float 0.1 Fraction for validation random_state int 42 Random seed for reproducibility","title":"5.1 TabularLeaderboard Constructor"},{"location":"user-guide/leaderboard/#52-add_model-method","text":"leaderboard . add_model ( model_name : str , tuning_strategy : str , name : str = None , model_params : dict = None , tuning_params : dict = None , processor_params : dict = None ) Parameters : Parameter Type Default Description model_name str Required Model to add (TabPFN, TabICL, etc.) tuning_strategy str Required 'inference', 'base-ft', or 'peft' name str None Custom name for leaderboard (auto-generated if None) model_params dict None Model-specific hyperparameters tuning_params dict None Training hyperparameters processor_params dict None Preprocessing parameters","title":"5.2 add_model() Method"},{"location":"user-guide/leaderboard/#53-run-method","text":"results = leaderboard . run ( rank_by : str = 'accuracy' , verbose : bool = True , timeout : float = None ) Parameters : Parameter Type Default Description rank_by str 'accuracy' Metric to rank models verbose bool True Print progress timeout float None Timeout per model in seconds Return : LeaderboardResults object with all benchmarks","title":"5.3 run() Method"},{"location":"user-guide/leaderboard/#6-advanced-usage","text":"","title":"6. Advanced Usage"},{"location":"user-guide/leaderboard/#61-custom-model-names","text":"# Add with meaningful names leaderboard . add_model ( 'TabICL' , 'peft' , name = 'TabICL-PEFT-r8' , tuning_params = { 'peft_config' : { 'r' : 8 }} ) leaderboard . add_model ( 'TabICL' , 'peft' , name = 'TabICL-PEFT-r16' , tuning_params = { 'peft_config' : { 'r' : 16 }} ) results = leaderboard . run ( rank_by = 'accuracy' )","title":"6.1 Custom Model Names"},{"location":"user-guide/leaderboard/#62-hyperparameter-grid","text":"Test multiple hyperparameter combinations: from itertools import product # Define hyperparameter grid learning_rates = [ 1e-5 , 2e-5 , 5e-5 ] ranks = [ 4 , 8 , 16 ] # Grid search for lr , r in product ( learning_rates , ranks ): leaderboard . add_model ( 'TabICL' , 'peft' , name = f 'TabICL-lr { lr } -r { r } ' , tuning_params = { 'learning_rate' : lr , 'peft_config' : { 'r' : r , 'lora_alpha' : 2 * r } } ) results = leaderboard . run ( rank_by = 'f1_score' , verbose = True ) print ( leaderboard . get_ranking ())","title":"6.2 Hyperparameter Grid"},{"location":"user-guide/leaderboard/#63-strategy-comparison","text":"Compare all three strategies for a model: model = 'TabDPT' # Inference baseline leaderboard . add_model ( model , 'inference' , name = f ' { model } -Inference' ) # Base fine-tuning leaderboard . add_model ( model , 'base-ft' , name = f ' { model } -BaseFT' , tuning_params = { 'epochs' : 5 } ) # PEFT fine-tuning leaderboard . add_model ( model , 'peft' , name = f ' { model } -PEFT' , tuning_params = { 'epochs' : 5 , 'peft_config' : { 'r' : 8 }} ) results = leaderboard . run () # Extract strategy comparison for name , score in results . items (): strategy = name . split ( '-' )[ - 1 ] print ( f \" { strategy } : { score [ 'accuracy' ] : .4f } \" )","title":"6.3 Strategy Comparison"},{"location":"user-guide/leaderboard/#64-cross-validation","text":"Test stability across multiple folds: from sklearn.model_selection import KFold kf = KFold ( n_splits = 5 ) fold_results = [] for fold_idx , ( train_idx , test_idx ) in enumerate ( kf . split ( X )): X_train_fold , X_test_fold = X . iloc [ train_idx ], X . iloc [ test_idx ] y_train_fold , y_test_fold = y . iloc [ train_idx ], y . iloc [ test_idx ] # Create leaderboard for this fold lb = TabularLeaderboard ( X_train_fold , X_test_fold , y_train_fold , y_test_fold ) # Add models lb . add_model ( 'TabICL' , 'peft' ) lb . add_model ( 'OrionBix' , 'base-ft' ) # Run and store results = lb . run () fold_results . append ( results ) # Aggregate results import pandas as pd all_results = pd . concat ( fold_results ) print ( all_results . groupby ( 'model' ) . mean ())","title":"6.4 Cross-Validation"},{"location":"user-guide/leaderboard/#7-evaluation-metrics","text":"Supported metrics for ranking: Classification Metrics : - accuracy : Fraction of correct predictions - f1_score : Harmonic mean of precision and recall (weighted) - roc_auc_score : Area under ROC curve - precision_score : True positives / (TP + FP) - recall_score : True positives / (TP + FN) - balanced_accuracy : Average per-class accuracy Regression Metrics : - mse : Mean squared error (lower is better) - rmse : Root mean squared error - mae : Mean absolute error - r2_score : Coefficient of determination # Rank by different metrics results_acc = leaderboard . run ( rank_by = 'accuracy' ) results_f1 = leaderboard . run ( rank_by = 'f1_score' ) results_auc = leaderboard . run ( rank_by = 'roc_auc_score' )","title":"7. Evaluation Metrics"},{"location":"user-guide/leaderboard/#8-best-practices","text":"","title":"8. Best Practices"},{"location":"user-guide/leaderboard/#dos","text":"\u2705 Use consistent data splits across all models \u2705 Fix random seed for reproducibility \u2705 Include inference baseline for comparison \u2705 Test multiple strategies for each model \u2705 Save results to disk \u2705 Use reasonable timeout values \u2705 Export results as CSV for further analysis","title":"\u2705 Do's"},{"location":"user-guide/leaderboard/#donts","text":"\u274c Don't change data between runs \u274c Don't use training data for validation \u274c Don't tune hyperparameters on test set \u274c Don't mix different task types in one leaderboard \u274c Don't run without timeout protection \u274c Don't forget to save best model config","title":"\u274c Don'ts"},{"location":"user-guide/leaderboard/#9-complete-workflow-example","text":"from tabtune import TabularLeaderboard import pandas as pd from sklearn.model_selection import train_test_split # Step 1: Load and prepare data print ( \"Loading data...\" ) df = pd . read_csv ( 'data.csv' ) X = df . drop ( 'target' , axis = 1 ) y = df [ 'target' ] X_train , X_test , y_train , y_test = train_test_split ( X , y , test_size = 0.2 , random_state = 42 ) # Step 2: Initialize leaderboard print ( \"Initializing leaderboard...\" ) lb = TabularLeaderboard ( X_train , X_test , y_train , y_test ) # Step 3: Add baseline print ( \"Adding inference baseline...\" ) lb . add_model ( 'TabPFN' , 'inference' ) # Step 4: Add PEFT models print ( \"Adding PEFT models...\" ) for model in [ 'TabICL' , 'OrionMSP' , 'OrionBix' , 'TabDPT' ]: lb . add_model ( model , 'peft' , tuning_params = { 'epochs' : 3 , 'peft_config' : { 'r' : 8 }} ) # Step 5: Run benchmarks print ( \"Running benchmarks...\" ) results = lb . run ( rank_by = 'accuracy' , verbose = True , n_jobs =- 1 )","title":"9. Complete Workflow Example"},{"location":"user-guide/leaderboard/#10-next-steps","text":"Model Selection - Guide for choosing models Tuning Strategies - Deep dive into strategies Examples - More examples The TabularLeaderboard streamlines model selection by enabling systematic, reproducible benchmarking!","title":"10. Next Steps"},{"location":"user-guide/model-selection/","text":"Model Selection \u00b6 Choosing the right model for your tabular task is crucial for achieving optimal performance. This guide helps you navigate TabTune's model ecosystem and select the best model for your specific use case. 1. Model Overview \u00b6 Model Family Best For Dataset Size PEFT Support Training Speed TabPFN PFN/ICL Small datasets, quick experiments <10K rows \u26a0\ufe0f Experimental \u2b50\u2b50\u2b50\u2b50\u2b50 TabICL Scalable ICL General tabular, balanced performance 10K-1M rows \u2705 Full \u2b50\u2b50\u2b50\u2b50 OrionMSP Scalable ICL Balanced generalization 50K-2M+ rows \u2705 Full \u2b50\u2b50\u2b50 OrionBix Scalable ICL High-accuracy scenarios 10K-1M rows \u2705 Full \u2b50\u2b50\u2b50 TabDPT Denoising Large datasets, robust features 100K-5M rows \u2705 Full \u2b50\u2b50\u2b50 Mitra 2D Attention Complex patterns, mixed types 10K-500K rows \u2705 Full \u2b50\u2b50 ContextTab Semantic ICL Text-heavy features, semantics 10K-500K rows \u26a0\ufe0f Experimental \u2b50\u2b50 2. Decision Framework \u00b6 2.1 By Dataset Size \u00b6 flowchart TD A[Dataset Size?] --> B{ < 10K rows} A --> C{10K - 100K rows} A --> D{100K - 1M rows} A --> E{> 1M rows} B --> F[TabPFN] C --> G[TabICL or Mitra] D --> H[OrionBix or TabDPT] E --> I[TabDPT] Small (<10K rows) - Recommended : TabPFN, Mitra - Alternative : TabICL with small n_estimators Medium (10K-100K rows) - Recommended : TabICL - Alternatives : Mitra, OrionMSP, OrionBix Large (100K-1M rows) - Recommended : OrionMSP, OrionBix, TabDPT - Alternative : TabICL with larger n_estimators Very Large (>1M rows) - Recommended : TabDPT - Alternative : OrionBix with chunked training 2.2 By Feature Types \u00b6 Primarily Numerical - Best : TabDPT, TabICL - Reason : Efficient scaling and normalization pipelines Primarily Categorical - Best : TabPFN (if small), ContextTab - Reason : Specialized categorical encoding Mixed (Numerical + Categorical) - Best : TabICL, OrionMSP, OrionBix, Mitra - Reason : Balanced handling of both types Text/Semantic Features - Best : ContextTab - Reason : Built-in text embedding support 2.3 By Computational Budget \u00b6 Limited Resources (<8GB GPU) - Recommended : TabPFN (inference), TabICL (PEFT) - Strategy : Use peft tuning strategy Moderate Resources (8-16GB GPU) - Recommended : TabICL, OrionMSP, OrionBix - Strategy : base-ft or peft Ample Resources (>16GB GPU) - Recommended : TabDPT, Mitra, OrionBix - Strategy : base-ft with mixed precision 2.4 By Use Case \u00b6 Quick Prototyping - Model : TabPFN, TabICL - Strategy : inference - Reason : Zero-shot predictions, instant results Production Deployment - Model : OrionMSP, OrionBix, TabDPT - Strategy : base-ft - Reason : Highest accuracy, stable performance Research/Experimentation - Model : Any with peft - Strategy : peft - Reason : Fast iteration, low cost High Accuracy Priority - Model : OrionMSP, OrionBix, TabDPT - Strategy : base-ft with extensive tuning - Reason : State-of-the-art performance 3. Detailed Model Profiles \u00b6 3.1 TabPFN \u00b6 Architecture : Prior-Fitted Network with approximate Bayesian inference Strengths : - \u2b50 Extremely fast inference - \u2b50 No training required for small datasets - \u2b50 Robust to hyperparameter choices - \u2b50 Good uncertainty estimates Limitations : - \u26a0\ufe0f Limited to ~10K training samples - \u26a0\ufe0f Maximum ~100 features - \u26a0\ufe0f PEFT support experimental - \u26a0\ufe0f Binary and multi-class classification only Ideal Use Cases : - Quick baseline comparisons - Small-scale classification tasks - Kaggle competitions with small data - A/B testing with limited samples Example Configuration : from tabtune import TabularPipeline pipeline = TabularPipeline ( model_name = 'TabPFN' , tuning_strategy = 'inference' , # or 'base-ft' for adaptation ) 3.2 TabICL \u00b6 Architecture : Two-stage in-context learning (column \u2192 row attention) Strengths : - \u2b50 Balanced speed and accuracy - \u2b50 Scales to 1M+ rows - \u2b50 Full PEFT support - \u2b50 Ensemble-based robustness Limitations : - \u26a0\ufe0f Requires episodic training for fine-tuning - \u26a0\ufe0f More memory than TabPFN - \u26a0\ufe0f Slower inference with high n_estimators Ideal Use Cases : - General-purpose tabular classification - Medium to large datasets - Tasks requiring model adaptation - Ensemble predictions for robustness Example Configuration : pipeline = TabularPipeline ( model_name = 'TabICL' , tuning_strategy = 'peft' , model_params = { 'n_estimators' : 32 , 'softmax_temperature' : 0.9 }, tuning_params = { 'device' : 'cuda' , 'epochs' : 5 , 'support_size' : 48 , 'query_size' : 32 , 'n_episodes' : 1000 , 'peft_config' : { 'r' : 8 , 'lora_alpha' : 16 } } ) 3.3 OrionMSP \u00b6 Architecture : Multi-scale sparse attention for scalable in-context learning Strengths : - \u2b50 Strong generalization with multi-scale priors - \u2b50 Balanced performance across dataset sizes - \u2b50 Full PEFT support - \u2b50 Efficient column-then-row attention Limitations : - \u26a0\ufe0f Requires larger datasets for best performance (\u226550K rows) - \u26a0\ufe0f Moderate memory requirements - \u26a0\ufe0f Slower than TabICL for small datasets Ideal Use Cases : - Medium to large datasets (50K-2M+ rows) - Tasks requiring strong generalization - Balanced speed/accuracy requirements - Production systems with moderate compute Example Configuration : pipeline = TabularPipeline ( model_name = 'OrionMSP' , tuning_strategy = 'base-ft' , model_params = { 'n_estimators' : 16 , 'softmax_temperature' : 0.9 }, tuning_params = { 'device' : 'cuda' , 'epochs' : 5 , 'support_size' : 1024 , 'query_size' : 256 , 'learning_rate' : 2e-5 } ) 3.4 OrionBix \u00b6 Architecture : Custom variant of TabICL with biaxial attention mechanisms Strengths : Limitations : Ideal Use Cases : - High-stakes applications (finance, healthcare) - Complex feature interactions - When accuracy > speed - Production models with tuning budget Example Configuration : pipeline = TabularPipeline ( model_name = 'OrionBix' , tuning_strategy = 'base-ft' , model_params = { 'n_estimators' : 32 }, tuning_params = { 'device' : 'cuda' , 'epochs' : 5 , 'support_size' : 48 , 'query_size' : 32 , 'learning_rate' : 2e-5 } ) 3.5 TabDPT \u00b6 Architecture : Denoising pre-trained transformer with k-NN context selection Strengths : - \u2b50 Scales to very large datasets (5M+ rows) - \u2b50 Robust to noisy features - \u2b50 Strong generalization - \u2b50 Full PEFT support Limitations : - \u26a0\ufe0f Requires large training sets for best performance - \u26a0\ufe0f Longer training time - \u26a0\ufe0f Memory-intensive for large context sizes Ideal Use Cases : - Large-scale production systems - Datasets with noisy/missing features - Long-term deployed models - High-accuracy requirements Example Configuration : pipeline = TabularPipeline ( model_name = 'TabDPT' , tuning_strategy = 'base-ft' , model_params = { 'n_ensembles' : 8 , 'temperature' : 0.3 , 'context_size' : 2048 }, tuning_params = { 'device' : 'cuda' , 'epochs' : 3 , 'support_size' : 1024 , 'query_size' : 256 , 'steps_per_epoch' : 15 } ) 3.6 Mitra \u00b6 Architecture : 2D cross-attention (Tab2D) with synthetic priors Strengths : - \u2b50 Excellent for mixed-type features - \u2b50 Captures row and column dependencies - \u2b50 Full PEFT support - \u2b50 Strong on structured data Limitations : - \u26a0\ufe0f Slowest training among ICL models - \u26a0\ufe0f High memory usage - \u26a0\ufe0f Requires careful hyperparameter tuning - \u26a0\ufe0f Small batch sizes needed Ideal Use Cases : - Structured databases (SQL-like tables) - Scientific datasets with meaningful columns - Time-series tabular data - Complex multi-variate relationships Example Configuration : pipeline = TabularPipeline ( model_name = 'Mitra' , tuning_strategy = 'peft' , tuning_params = { 'device' : 'cuda' , 'epochs' : 3 , 'support_size' : 128 , 'query_size' : 128 , 'steps_per_epoch' : 50 , 'batch_size' : 4 , 'peft_config' : { 'r' : 8 , 'lora_alpha' : 16 } } ) 3.7 ContextTab \u00b6 Architecture : Semantics-aware ICL with modality-specific embeddings Strengths : - \u2b50 Best for text-heavy features - \u2b50 Semantic understanding of column names - \u2b50 Handles heterogeneous data types - \u2b50 Pre-trained on diverse tabular corpora Limitations : - \u26a0\ufe0f Requires HuggingFace Hub access - \u26a0\ufe0f PEFT support experimental - \u26a0\ufe0f Slower inference due to embedding computation - \u26a0\ufe0f Limited to specific feature types Ideal Use Cases : - Datasets with free-text columns - Survey data with semantic features - Product catalogs, reviews - Mixed structured/unstructured data Example Configuration : # Requires HF_TOKEN environment variable pipeline = TabularPipeline ( model_name = 'ContextTab' , tuning_strategy = 'base-ft' , # Use base-ft, not peft tuning_params = { 'device' : 'cuda' , 'epochs' : 10 , 'learning_rate' : 1e-4 , 'batch_size' : 8 } ) 4. Model Selection Checklist \u00b6 Use this checklist to guide your decision: Dataset Characteristics - [ ] How many rows? (<10K, 10K-100K, 100K-1M, >1M) - [ ] How many features? (<50, 50-100, >100) - [ ] Feature types? (Numerical, Categorical, Mixed, Text) - [ ] Class balance? (Balanced, Imbalanced) - [ ] Missing values? (None, Few, Many) Requirements - [ ] Priority: Speed vs. Accuracy? - [ ] GPU available? (None, <8GB, 8-16GB, >16GB) - [ ] Training time budget? (Minutes, Hours, Days) - [ ] Deployment constraints? (Model size, inference latency) Recommendations Based on Checklist If dataset < 10K rows \u2192 TabPFN If dataset 10K-100K rows AND balanced types \u2192 TabICL If dataset 50K-2M rows AND balanced \u2192 OrionMSP If dataset 10K-100K rows AND high accuracy needed \u2192 OrionBix If dataset > 100K rows \u2192 TabDPT If text features present \u2192 ContextTab If complex patterns + mixed types \u2192 Mitra If GPU < 8GB \u2192 TabPFN or TabICL with PEFT If speed critical \u2192 TabPFN (inference) If accuracy critical \u2192 OrionBix or TabDPT (base-ft) 5. Hybrid Approaches \u00b6 Ensemble Multiple Models \u00b6 from tabtune import TabularLeaderboard leaderboard = TabularLeaderboard ( X_train , X_test , y_train , y_test ) # Add multiple models leaderboard . add_model ( 'TabPFN' , 'inference' ) leaderboard . add_model ( 'TabICL' , 'peft' , tuning_params = { 'epochs' : 5 }) leaderboard . add_model ( 'OrionMSP' , 'base-ft' , tuning_params = { 'epochs' : 5 }) leaderboard . add_model ( 'OrionBix' , 'base-ft' , tuning_params = { 'epochs' : 5 }) # Run and compare results = leaderboard . run ( rank_by = 'roc_auc_score' ) # Ensemble predictions (average probabilities) from sklearn.ensemble import VotingClassifier # Use predictions from top 3 models for ensemble 6. Advanced Selection Criteria \u00b6 6.1 Explainability Requirements \u00b6 High Explainability : TabPFN (inherent uncertainty), TabICL (attention weights) Moderate Explainability : TabDPT (feature importance) Low Explainability : Mitra, ContextTab (complex architectures) 6.2 Regulatory Compliance \u00b6 Medical/Financial : OrionMSP, OrionBix, TabDPT (reproducible, auditable) General : Any model with saved checkpoints and logged hyperparameters 6.3 Transfer Learning \u00b6 Best for Transfer : TabDPT (large pre-training corpus) Moderate Transfer : TabICL, ContextTab Limited Transfer : TabPFN (task-specific priors) 7. Next Steps \u00b6 Pipeline Overview - Learn the TabularPipeline API Model Documentation - Detailed model specifications TabularLeaderboard - Compare models systematically Select your model wisely, and iterate based on your specific requirements and constraints!","title":"Model Selection"},{"location":"user-guide/model-selection/#model-selection","text":"Choosing the right model for your tabular task is crucial for achieving optimal performance. This guide helps you navigate TabTune's model ecosystem and select the best model for your specific use case.","title":"Model Selection"},{"location":"user-guide/model-selection/#1-model-overview","text":"Model Family Best For Dataset Size PEFT Support Training Speed TabPFN PFN/ICL Small datasets, quick experiments <10K rows \u26a0\ufe0f Experimental \u2b50\u2b50\u2b50\u2b50\u2b50 TabICL Scalable ICL General tabular, balanced performance 10K-1M rows \u2705 Full \u2b50\u2b50\u2b50\u2b50 OrionMSP Scalable ICL Balanced generalization 50K-2M+ rows \u2705 Full \u2b50\u2b50\u2b50 OrionBix Scalable ICL High-accuracy scenarios 10K-1M rows \u2705 Full \u2b50\u2b50\u2b50 TabDPT Denoising Large datasets, robust features 100K-5M rows \u2705 Full \u2b50\u2b50\u2b50 Mitra 2D Attention Complex patterns, mixed types 10K-500K rows \u2705 Full \u2b50\u2b50 ContextTab Semantic ICL Text-heavy features, semantics 10K-500K rows \u26a0\ufe0f Experimental \u2b50\u2b50","title":"1. Model Overview"},{"location":"user-guide/model-selection/#2-decision-framework","text":"","title":"2. Decision Framework"},{"location":"user-guide/model-selection/#21-by-dataset-size","text":"flowchart TD A[Dataset Size?] --> B{ < 10K rows} A --> C{10K - 100K rows} A --> D{100K - 1M rows} A --> E{> 1M rows} B --> F[TabPFN] C --> G[TabICL or Mitra] D --> H[OrionBix or TabDPT] E --> I[TabDPT] Small (<10K rows) - Recommended : TabPFN, Mitra - Alternative : TabICL with small n_estimators Medium (10K-100K rows) - Recommended : TabICL - Alternatives : Mitra, OrionMSP, OrionBix Large (100K-1M rows) - Recommended : OrionMSP, OrionBix, TabDPT - Alternative : TabICL with larger n_estimators Very Large (>1M rows) - Recommended : TabDPT - Alternative : OrionBix with chunked training","title":"2.1 By Dataset Size"},{"location":"user-guide/model-selection/#22-by-feature-types","text":"Primarily Numerical - Best : TabDPT, TabICL - Reason : Efficient scaling and normalization pipelines Primarily Categorical - Best : TabPFN (if small), ContextTab - Reason : Specialized categorical encoding Mixed (Numerical + Categorical) - Best : TabICL, OrionMSP, OrionBix, Mitra - Reason : Balanced handling of both types Text/Semantic Features - Best : ContextTab - Reason : Built-in text embedding support","title":"2.2 By Feature Types"},{"location":"user-guide/model-selection/#23-by-computational-budget","text":"Limited Resources (<8GB GPU) - Recommended : TabPFN (inference), TabICL (PEFT) - Strategy : Use peft tuning strategy Moderate Resources (8-16GB GPU) - Recommended : TabICL, OrionMSP, OrionBix - Strategy : base-ft or peft Ample Resources (>16GB GPU) - Recommended : TabDPT, Mitra, OrionBix - Strategy : base-ft with mixed precision","title":"2.3 By Computational Budget"},{"location":"user-guide/model-selection/#24-by-use-case","text":"Quick Prototyping - Model : TabPFN, TabICL - Strategy : inference - Reason : Zero-shot predictions, instant results Production Deployment - Model : OrionMSP, OrionBix, TabDPT - Strategy : base-ft - Reason : Highest accuracy, stable performance Research/Experimentation - Model : Any with peft - Strategy : peft - Reason : Fast iteration, low cost High Accuracy Priority - Model : OrionMSP, OrionBix, TabDPT - Strategy : base-ft with extensive tuning - Reason : State-of-the-art performance","title":"2.4 By Use Case"},{"location":"user-guide/model-selection/#3-detailed-model-profiles","text":"","title":"3. Detailed Model Profiles"},{"location":"user-guide/model-selection/#31-tabpfn","text":"Architecture : Prior-Fitted Network with approximate Bayesian inference Strengths : - \u2b50 Extremely fast inference - \u2b50 No training required for small datasets - \u2b50 Robust to hyperparameter choices - \u2b50 Good uncertainty estimates Limitations : - \u26a0\ufe0f Limited to ~10K training samples - \u26a0\ufe0f Maximum ~100 features - \u26a0\ufe0f PEFT support experimental - \u26a0\ufe0f Binary and multi-class classification only Ideal Use Cases : - Quick baseline comparisons - Small-scale classification tasks - Kaggle competitions with small data - A/B testing with limited samples Example Configuration : from tabtune import TabularPipeline pipeline = TabularPipeline ( model_name = 'TabPFN' , tuning_strategy = 'inference' , # or 'base-ft' for adaptation )","title":"3.1 TabPFN"},{"location":"user-guide/model-selection/#32-tabicl","text":"Architecture : Two-stage in-context learning (column \u2192 row attention) Strengths : - \u2b50 Balanced speed and accuracy - \u2b50 Scales to 1M+ rows - \u2b50 Full PEFT support - \u2b50 Ensemble-based robustness Limitations : - \u26a0\ufe0f Requires episodic training for fine-tuning - \u26a0\ufe0f More memory than TabPFN - \u26a0\ufe0f Slower inference with high n_estimators Ideal Use Cases : - General-purpose tabular classification - Medium to large datasets - Tasks requiring model adaptation - Ensemble predictions for robustness Example Configuration : pipeline = TabularPipeline ( model_name = 'TabICL' , tuning_strategy = 'peft' , model_params = { 'n_estimators' : 32 , 'softmax_temperature' : 0.9 }, tuning_params = { 'device' : 'cuda' , 'epochs' : 5 , 'support_size' : 48 , 'query_size' : 32 , 'n_episodes' : 1000 , 'peft_config' : { 'r' : 8 , 'lora_alpha' : 16 } } )","title":"3.2 TabICL"},{"location":"user-guide/model-selection/#33-orionmsp","text":"Architecture : Multi-scale sparse attention for scalable in-context learning Strengths : - \u2b50 Strong generalization with multi-scale priors - \u2b50 Balanced performance across dataset sizes - \u2b50 Full PEFT support - \u2b50 Efficient column-then-row attention Limitations : - \u26a0\ufe0f Requires larger datasets for best performance (\u226550K rows) - \u26a0\ufe0f Moderate memory requirements - \u26a0\ufe0f Slower than TabICL for small datasets Ideal Use Cases : - Medium to large datasets (50K-2M+ rows) - Tasks requiring strong generalization - Balanced speed/accuracy requirements - Production systems with moderate compute Example Configuration : pipeline = TabularPipeline ( model_name = 'OrionMSP' , tuning_strategy = 'base-ft' , model_params = { 'n_estimators' : 16 , 'softmax_temperature' : 0.9 }, tuning_params = { 'device' : 'cuda' , 'epochs' : 5 , 'support_size' : 1024 , 'query_size' : 256 , 'learning_rate' : 2e-5 } )","title":"3.3 OrionMSP"},{"location":"user-guide/model-selection/#34-orionbix","text":"Architecture : Custom variant of TabICL with biaxial attention mechanisms Strengths : Limitations : Ideal Use Cases : - High-stakes applications (finance, healthcare) - Complex feature interactions - When accuracy > speed - Production models with tuning budget Example Configuration : pipeline = TabularPipeline ( model_name = 'OrionBix' , tuning_strategy = 'base-ft' , model_params = { 'n_estimators' : 32 }, tuning_params = { 'device' : 'cuda' , 'epochs' : 5 , 'support_size' : 48 , 'query_size' : 32 , 'learning_rate' : 2e-5 } )","title":"3.4 OrionBix"},{"location":"user-guide/model-selection/#35-tabdpt","text":"Architecture : Denoising pre-trained transformer with k-NN context selection Strengths : - \u2b50 Scales to very large datasets (5M+ rows) - \u2b50 Robust to noisy features - \u2b50 Strong generalization - \u2b50 Full PEFT support Limitations : - \u26a0\ufe0f Requires large training sets for best performance - \u26a0\ufe0f Longer training time - \u26a0\ufe0f Memory-intensive for large context sizes Ideal Use Cases : - Large-scale production systems - Datasets with noisy/missing features - Long-term deployed models - High-accuracy requirements Example Configuration : pipeline = TabularPipeline ( model_name = 'TabDPT' , tuning_strategy = 'base-ft' , model_params = { 'n_ensembles' : 8 , 'temperature' : 0.3 , 'context_size' : 2048 }, tuning_params = { 'device' : 'cuda' , 'epochs' : 3 , 'support_size' : 1024 , 'query_size' : 256 , 'steps_per_epoch' : 15 } )","title":"3.5 TabDPT"},{"location":"user-guide/model-selection/#36-mitra","text":"Architecture : 2D cross-attention (Tab2D) with synthetic priors Strengths : - \u2b50 Excellent for mixed-type features - \u2b50 Captures row and column dependencies - \u2b50 Full PEFT support - \u2b50 Strong on structured data Limitations : - \u26a0\ufe0f Slowest training among ICL models - \u26a0\ufe0f High memory usage - \u26a0\ufe0f Requires careful hyperparameter tuning - \u26a0\ufe0f Small batch sizes needed Ideal Use Cases : - Structured databases (SQL-like tables) - Scientific datasets with meaningful columns - Time-series tabular data - Complex multi-variate relationships Example Configuration : pipeline = TabularPipeline ( model_name = 'Mitra' , tuning_strategy = 'peft' , tuning_params = { 'device' : 'cuda' , 'epochs' : 3 , 'support_size' : 128 , 'query_size' : 128 , 'steps_per_epoch' : 50 , 'batch_size' : 4 , 'peft_config' : { 'r' : 8 , 'lora_alpha' : 16 } } )","title":"3.6 Mitra"},{"location":"user-guide/model-selection/#37-contexttab","text":"Architecture : Semantics-aware ICL with modality-specific embeddings Strengths : - \u2b50 Best for text-heavy features - \u2b50 Semantic understanding of column names - \u2b50 Handles heterogeneous data types - \u2b50 Pre-trained on diverse tabular corpora Limitations : - \u26a0\ufe0f Requires HuggingFace Hub access - \u26a0\ufe0f PEFT support experimental - \u26a0\ufe0f Slower inference due to embedding computation - \u26a0\ufe0f Limited to specific feature types Ideal Use Cases : - Datasets with free-text columns - Survey data with semantic features - Product catalogs, reviews - Mixed structured/unstructured data Example Configuration : # Requires HF_TOKEN environment variable pipeline = TabularPipeline ( model_name = 'ContextTab' , tuning_strategy = 'base-ft' , # Use base-ft, not peft tuning_params = { 'device' : 'cuda' , 'epochs' : 10 , 'learning_rate' : 1e-4 , 'batch_size' : 8 } )","title":"3.7 ContextTab"},{"location":"user-guide/model-selection/#4-model-selection-checklist","text":"Use this checklist to guide your decision: Dataset Characteristics - [ ] How many rows? (<10K, 10K-100K, 100K-1M, >1M) - [ ] How many features? (<50, 50-100, >100) - [ ] Feature types? (Numerical, Categorical, Mixed, Text) - [ ] Class balance? (Balanced, Imbalanced) - [ ] Missing values? (None, Few, Many) Requirements - [ ] Priority: Speed vs. Accuracy? - [ ] GPU available? (None, <8GB, 8-16GB, >16GB) - [ ] Training time budget? (Minutes, Hours, Days) - [ ] Deployment constraints? (Model size, inference latency) Recommendations Based on Checklist If dataset < 10K rows \u2192 TabPFN If dataset 10K-100K rows AND balanced types \u2192 TabICL If dataset 50K-2M rows AND balanced \u2192 OrionMSP If dataset 10K-100K rows AND high accuracy needed \u2192 OrionBix If dataset > 100K rows \u2192 TabDPT If text features present \u2192 ContextTab If complex patterns + mixed types \u2192 Mitra If GPU < 8GB \u2192 TabPFN or TabICL with PEFT If speed critical \u2192 TabPFN (inference) If accuracy critical \u2192 OrionBix or TabDPT (base-ft)","title":"4. Model Selection Checklist"},{"location":"user-guide/model-selection/#5-hybrid-approaches","text":"","title":"5. Hybrid Approaches"},{"location":"user-guide/model-selection/#ensemble-multiple-models","text":"from tabtune import TabularLeaderboard leaderboard = TabularLeaderboard ( X_train , X_test , y_train , y_test ) # Add multiple models leaderboard . add_model ( 'TabPFN' , 'inference' ) leaderboard . add_model ( 'TabICL' , 'peft' , tuning_params = { 'epochs' : 5 }) leaderboard . add_model ( 'OrionMSP' , 'base-ft' , tuning_params = { 'epochs' : 5 }) leaderboard . add_model ( 'OrionBix' , 'base-ft' , tuning_params = { 'epochs' : 5 }) # Run and compare results = leaderboard . run ( rank_by = 'roc_auc_score' ) # Ensemble predictions (average probabilities) from sklearn.ensemble import VotingClassifier # Use predictions from top 3 models for ensemble","title":"Ensemble Multiple Models"},{"location":"user-guide/model-selection/#6-advanced-selection-criteria","text":"","title":"6. Advanced Selection Criteria"},{"location":"user-guide/model-selection/#61-explainability-requirements","text":"High Explainability : TabPFN (inherent uncertainty), TabICL (attention weights) Moderate Explainability : TabDPT (feature importance) Low Explainability : Mitra, ContextTab (complex architectures)","title":"6.1 Explainability Requirements"},{"location":"user-guide/model-selection/#62-regulatory-compliance","text":"Medical/Financial : OrionMSP, OrionBix, TabDPT (reproducible, auditable) General : Any model with saved checkpoints and logged hyperparameters","title":"6.2 Regulatory Compliance"},{"location":"user-guide/model-selection/#63-transfer-learning","text":"Best for Transfer : TabDPT (large pre-training corpus) Moderate Transfer : TabICL, ContextTab Limited Transfer : TabPFN (task-specific priors)","title":"6.3 Transfer Learning"},{"location":"user-guide/model-selection/#7-next-steps","text":"Pipeline Overview - Learn the TabularPipeline API Model Documentation - Detailed model specifications TabularLeaderboard - Compare models systematically Select your model wisely, and iterate based on your specific requirements and constraints!","title":"7. Next Steps"},{"location":"user-guide/pipeline-overview/","text":"TabularPipeline Overview \u00b6 This document provides an in-depth look at the TabularPipeline class\u2014the central entry point for using TabTune\u2019s end-to-end workflows. 1. Architecture Diagram \u00b6 flowchart LR A[Raw Data: Pandas DataFrame] --> B[DataProcessor] B --> C[Transformed Data: Dataset / DataLoader] C --> D[TuningManager] D --> E[Model Training / Inference] E --> F[Predictions / Metrics] DataProcessor : Prepares input features and labels for models. TuningManager : Executes the specified strategy ( inference , base-ft , peft ). Model : Underlying tabular foundation model (e.g., TabPFN, TabICL). Output : Predictions and evaluation metrics. 2. Class Signature \u00b6 class TabularPipeline : def __init__ ( self , model_name : str , task_type : str = 'classification' , tuning_strategy : str = 'inference' , tuning_params : dict | None = None , processor_params : dict | None = None , model_params : dict | None = None , model_checkpoint_path : str | None = None ) model_name : Name of the model to instantiate (e.g., TabPFN , TabICL ). task_type : Either classification or regression . tuning_strategy : One of inference , base-ft , or peft . tuning_params : Hyperparameters for training/inference (device, epochs, learning_rate, etc.). processor_params : Options for data preprocessing (imputation, scaling, encoding). model_params : Direct parameters for the model constructor. model_checkpoint_path : Path to load pre-trained weights. 3. Core Methods \u00b6 3.1 .fit(X, y) \u00b6 pipeline . fit ( X_train : DataFrame , y_train : Series ) -> None - Description : Runs preprocessing and training. - Workflow : 1. Fit DataProcessor on X_train, y_train . 2. Create PyTorch Dataset and DataLoader . 3. Execute TuningManager training loop if strategy != inference . 3.2 .predict(X) \u00b6 predictions = pipeline . predict ( X_test : DataFrame ) -> np . ndarray - Description : Preprocesses X_test and returns model predictions. - Notes : In inference mode, loads pre-trained model and runs forward passes. 3.3 .evaluate(X, y) \u00b6 metrics = pipeline . evaluate ( X_test : DataFrame , y_test : Series ) -> dict - Description : Calls .predict() , then computes specified metrics. - Default Metrics : Accuracy, Weighted F1, ROC AUC. 3.4 .save(path) \u00b6 pipeline . save ( 'pipeline.joblib' ) - Description : Serializes the pipeline including processor, model state, and config. 3.5 .load(path) \u00b6 loaded = TabularPipeline . load ( 'pipeline.joblib' ) - Description : Loads a saved pipeline for inference or continued training. 4. Configuration Best Practices \u00b6 Use config files : YAML/JSON to record experiments reproducibly. Version control : Commit tuning_params and processor_params along with code. Checkpointing : Save intermediate checkpoints via model_checkpoint_path for long runs. 5. Example Usage \u00b6 from tabtune import TabularPipeline # Setup pipeline pipeline = TabularPipeline ( model_name = 'TabICL' , tuning_strategy = 'peft' , tuning_params = { 'device' : 'cuda' , 'epochs' : 5 , 'learning_rate' : 2e-4 }, processor_params = { 'imputation_strategy' : 'median' }, model_params = { 'n_estimators' : 16 } ) # Train pipeline . fit ( X_train , y_train ) # Predict & evaluate metrics = pipeline . evaluate ( X_test , y_test ) print ( metrics ) 6. Error Handling \u00b6 Invalid model_name : Raises ValueError with supported model list. Unsupported strategy : Raises ValueError advising available strategies. Missing data : Raises DataValidationError if nulls present after imputation. 7. Extending TabularPipeline \u00b6 Custom pipeline : Subclass TabularPipeline and override _setup_processors or _setup_manager . Custom metrics : Pass metrics=[your_metric_fn] to .evaluate() . Callbacks : Integrate PyTorch Lightning callbacks via tuning_params . This overview should help you understand and utilize TabularPipeline for all your tabular modeling needs.","title":"TabularPipeline Overview"},{"location":"user-guide/pipeline-overview/#tabularpipeline-overview","text":"This document provides an in-depth look at the TabularPipeline class\u2014the central entry point for using TabTune\u2019s end-to-end workflows.","title":"TabularPipeline Overview"},{"location":"user-guide/pipeline-overview/#1-architecture-diagram","text":"flowchart LR A[Raw Data: Pandas DataFrame] --> B[DataProcessor] B --> C[Transformed Data: Dataset / DataLoader] C --> D[TuningManager] D --> E[Model Training / Inference] E --> F[Predictions / Metrics] DataProcessor : Prepares input features and labels for models. TuningManager : Executes the specified strategy ( inference , base-ft , peft ). Model : Underlying tabular foundation model (e.g., TabPFN, TabICL). Output : Predictions and evaluation metrics.","title":"1. Architecture Diagram"},{"location":"user-guide/pipeline-overview/#2-class-signature","text":"class TabularPipeline : def __init__ ( self , model_name : str , task_type : str = 'classification' , tuning_strategy : str = 'inference' , tuning_params : dict | None = None , processor_params : dict | None = None , model_params : dict | None = None , model_checkpoint_path : str | None = None ) model_name : Name of the model to instantiate (e.g., TabPFN , TabICL ). task_type : Either classification or regression . tuning_strategy : One of inference , base-ft , or peft . tuning_params : Hyperparameters for training/inference (device, epochs, learning_rate, etc.). processor_params : Options for data preprocessing (imputation, scaling, encoding). model_params : Direct parameters for the model constructor. model_checkpoint_path : Path to load pre-trained weights.","title":"2. Class Signature"},{"location":"user-guide/pipeline-overview/#3-core-methods","text":"","title":"3. Core Methods"},{"location":"user-guide/pipeline-overview/#31-fitx-y","text":"pipeline . fit ( X_train : DataFrame , y_train : Series ) -> None - Description : Runs preprocessing and training. - Workflow : 1. Fit DataProcessor on X_train, y_train . 2. Create PyTorch Dataset and DataLoader . 3. Execute TuningManager training loop if strategy != inference .","title":"3.1 .fit(X, y)"},{"location":"user-guide/pipeline-overview/#32-predictx","text":"predictions = pipeline . predict ( X_test : DataFrame ) -> np . ndarray - Description : Preprocesses X_test and returns model predictions. - Notes : In inference mode, loads pre-trained model and runs forward passes.","title":"3.2 .predict(X)"},{"location":"user-guide/pipeline-overview/#33-evaluatex-y","text":"metrics = pipeline . evaluate ( X_test : DataFrame , y_test : Series ) -> dict - Description : Calls .predict() , then computes specified metrics. - Default Metrics : Accuracy, Weighted F1, ROC AUC.","title":"3.3 .evaluate(X, y)"},{"location":"user-guide/pipeline-overview/#34-savepath","text":"pipeline . save ( 'pipeline.joblib' ) - Description : Serializes the pipeline including processor, model state, and config.","title":"3.4 .save(path)"},{"location":"user-guide/pipeline-overview/#35-loadpath","text":"loaded = TabularPipeline . load ( 'pipeline.joblib' ) - Description : Loads a saved pipeline for inference or continued training.","title":"3.5 .load(path)"},{"location":"user-guide/pipeline-overview/#4-configuration-best-practices","text":"Use config files : YAML/JSON to record experiments reproducibly. Version control : Commit tuning_params and processor_params along with code. Checkpointing : Save intermediate checkpoints via model_checkpoint_path for long runs.","title":"4. Configuration Best Practices"},{"location":"user-guide/pipeline-overview/#5-example-usage","text":"from tabtune import TabularPipeline # Setup pipeline pipeline = TabularPipeline ( model_name = 'TabICL' , tuning_strategy = 'peft' , tuning_params = { 'device' : 'cuda' , 'epochs' : 5 , 'learning_rate' : 2e-4 }, processor_params = { 'imputation_strategy' : 'median' }, model_params = { 'n_estimators' : 16 } ) # Train pipeline . fit ( X_train , y_train ) # Predict & evaluate metrics = pipeline . evaluate ( X_test , y_test ) print ( metrics )","title":"5. Example Usage"},{"location":"user-guide/pipeline-overview/#6-error-handling","text":"Invalid model_name : Raises ValueError with supported model list. Unsupported strategy : Raises ValueError advising available strategies. Missing data : Raises DataValidationError if nulls present after imputation.","title":"6. Error Handling"},{"location":"user-guide/pipeline-overview/#7-extending-tabularpipeline","text":"Custom pipeline : Subclass TabularPipeline and override _setup_processors or _setup_manager . Custom metrics : Pass metrics=[your_metric_fn] to .evaluate() . Callbacks : Integrate PyTorch Lightning callbacks via tuning_params . This overview should help you understand and utilize TabularPipeline for all your tabular modeling needs.","title":"7. Extending TabularPipeline"},{"location":"user-guide/saving-loading/","text":"Saving and Loading Pipelines \u00b6 This guide explains how to persist TabTune pipelines for production deployment, reproducibility, and continued training. 1. Overview \u00b6 TabTune supports multiple serialization formats for different use cases: Format Extension Use Case Size Compatibility joblib .joblib Production, complete pipeline ~100-500MB Python only PyTorch .pt Model weights only ~50-200MB PyTorch ecosystems Checkpoint .pt Training resume, intermediate states ~50-200MB Development 2. Pipeline Serialization (joblib) \u00b6 2.1 Saving a Pipeline \u00b6 Save the complete fitted pipeline including model, preprocessor, and configuration: from tabtune import TabularPipeline # Train pipeline pipeline = TabularPipeline ( model_name = 'TabICL' , tuning_strategy = 'peft' , tuning_params = { 'device' : 'cuda' , 'epochs' : 5 } ) pipeline . fit ( X_train , y_train ) # Save entire pipeline pipeline . save ( 'my_pipeline.joblib' ) What Gets Saved : - \u2705 Trained model weights - \u2705 DataProcessor state (imputer, encoder, scaler) - \u2705 Model configuration - \u2705 Tuning parameters - \u2705 Label encoder for target variable - \u2705 PEFT adapter weights (if applicable) 2.2 Loading a Pipeline \u00b6 Load a saved pipeline for inference or continued training: from tabtune import TabularPipeline # Load pipeline loaded_pipeline = TabularPipeline . load ( 'my_pipeline.joblib' ) # Make predictions immediately predictions = loaded_pipeline . predict ( X_test ) # Or continue training with new data loaded_pipeline . fit ( X_new_train , y_new_train ) 2.3 Size Optimization \u00b6 If pipeline file is too large, compress it: import joblib # Save with compression joblib . dump ( pipeline , 'my_pipeline.joblib' , compress = 3 ) # compression=0-9 # Load compressed pipeline pipeline = joblib . load ( 'my_pipeline.joblib' ) 3. Model-Only Serialization (PyTorch) \u00b6 For minimal storage or deployment, save only model weights: 3.1 Saving Model Weights \u00b6 import torch # Save model weights only torch . save ( pipeline . model . state_dict (), 'model_weights.pt' ) 3.2 Loading Model Weights \u00b6 import torch from tabtune import TabularPipeline # Create new pipeline with same config new_pipeline = TabularPipeline ( model_name = 'TabICL' , tuning_strategy = 'inference' ) # Load weights into model state_dict = torch . load ( 'model_weights.pt' ) new_pipeline . model . load_state_dict ( state_dict ) # Ready for inference predictions = new_pipeline . predict ( X_test ) Advantages : - \u2705 Smaller file size (50-200MB) - \u2705 Faster loading - \u2705 Cross-platform compatible Disadvantages : - \u274c Requires manual DataProcessor setup - \u274c Must know original configuration - \u274c No automatic version compatibility 4. Checkpoint Management \u00b6 For long training runs, save intermediate checkpoints: 4.1 Automatic Checkpointing During Training \u00b6 pipeline = TabularPipeline ( model_name = 'TabDPT' , tuning_strategy = 'base-ft' , tuning_params = { 'device' : 'cuda' , 'epochs' : 20 , 'save_checkpoint_path' : 'checkpoints/best_model.pt' , 'save_every_n_epochs' : 5 # Save every 5 epochs } ) pipeline . fit ( X_train , y_train ) # Checkpoints saved as: # checkpoints/best_model.pt (best validation score) # checkpoints/checkpoint_epoch_5.pt # checkpoints/checkpoint_epoch_10.pt # checkpoints/checkpoint_epoch_15.pt # checkpoints/checkpoint_epoch_20.pt 4.2 Best Checkpoint Tracking \u00b6 import os from pathlib import Path checkpoint_dir = Path ( 'checkpoints' ) checkpoint_dir . mkdir ( exist_ok = True ) # Find best checkpoint best_checkpoint = None best_score = 0 for checkpoint_file in checkpoint_dir . glob ( '*.pt' ): pipeline = TabularPipeline . load ( str ( checkpoint_file )) score = pipeline . evaluate ( X_val , y_val )[ 'accuracy' ] if score > best_score : best_score = score best_checkpoint = checkpoint_file print ( f \"Best checkpoint: { best_checkpoint } with score { best_score : .4f } \" ) 5. PEFT-Specific Serialization \u00b6 When using PEFT (LoRA), save and manage adapter weights: 5.1 Save with PEFT Adapters \u00b6 # Train with PEFT pipeline = TabularPipeline ( model_name = 'TabICL' , tuning_strategy = 'peft' , tuning_params = { 'peft_config' : { 'r' : 8 , 'lora_alpha' : 16 }, 'save_checkpoint_path' : 'peft_model.joblib' } ) pipeline . fit ( X_train , y_train ) # Save includes both base model and LoRA adapters pipeline . save ( 'peft_pipeline.joblib' ) 5.2 Load PEFT Pipeline \u00b6 # Load includes automatic adapter reconstruction pipeline = TabularPipeline . load ( 'peft_pipeline.joblib' ) # Adapters are already injected predictions = pipeline . predict ( X_test ) 5.3 Extract LoRA Adapters Only \u00b6 import torch # Save only adapter weights (minimal size) if hasattr ( pipeline . model , 'lora_A' ) and hasattr ( pipeline . model , 'lora_B' ): adapter_state = { 'lora_A' : pipeline . model . lora_A . state_dict (), 'lora_B' : pipeline . model . lora_B . state_dict () } torch . save ( adapter_state , 'adapters.pt' ) 6. Configuration Serialization \u00b6 Save pipeline configuration for reproducibility: 6.1 Save Configuration to YAML \u00b6 import yaml config = { 'model_name' : pipeline . model_name , 'task_type' : pipeline . task_type , 'tuning_strategy' : pipeline . tuning_strategy , 'tuning_params' : pipeline . tuning_params , 'processor_params' : pipeline . processor_params , 'model_params' : pipeline . model_params } with open ( 'pipeline_config.yaml' , 'w' ) as f : yaml . dump ( config , f ) Example Config File : model_name : TabICL task_type : classification tuning_strategy : peft tuning_params : device : cuda epochs : 5 learning_rate : 2e-4 peft_config : r : 8 lora_alpha : 16 lora_dropout : 0.05 processor_params : imputation_strategy : median categorical_encoding : onehot scaling_strategy : standard model_params : n_estimators : 16 6.2 Load Pipeline from Configuration \u00b6 import yaml from tabtune import TabularPipeline # Load config with open ( 'pipeline_config.yaml' , 'r' ) as f : config = yaml . safe_load ( f ) # Recreate pipeline pipeline = TabularPipeline ( ** config ) 7. Best Practices \u00b6 \u2705 Do's \u00b6 \u2705 Save full pipelines for production deployment \u2705 Use checkpoints for long training runs \u2705 Store configuration separately for reproducibility \u2705 Version control YAML/JSON configs \u2705 Keep multiple backups at different timestamps \u2705 Document model metadata and creation date \u2705 Test loading on different systems \u274c Don'ts \u00b6 \u274c Don't save only model weights without config \u274c Don't overwrite checkpoints without backup \u274c Don't forget to compress large pipelines \u274c Don't hardcode paths in production code \u274c Don't commit large .joblib files to git (use git-lfs) 8. Troubleshooting \u00b6 Issue: \"ModuleNotFoundError when loading pipeline\" \u00b6 Solution : Ensure TabTune is installed in the target environment pip install tabtune Issue: \"Pickle protocol version mismatch\" \u00b6 Solution : Use compatible Python and PyTorch versions # Save with older protocol for compatibility import joblib joblib . dump ( pipeline , 'compatible.joblib' , protocol = 2 ) Issue: \"CUDA error when loading on CPU\" \u00b6 Solution : Specify device when loading pipeline = TabularPipeline . load ( 'pipeline.joblib' ) pipeline . model = pipeline . model . to ( 'cpu' ) Issue: \"Pipeline file corrupted or incomplete\" \u00b6 Solution : Restore from backup restored = TabularPipeline . load ( 'backups/pipeline_backup_20250101_120000.joblib' ) 9. Summary Table \u00b6 Task Method File Size Compatibility Full pipeline backup .save() ~300MB joblib only Model weights only torch.save(state_dict) ~100MB PyTorch Configuration only YAML/JSON <1MB Any language Training checkpoint .pt ~150MB Development Production package .joblib + config ~300MB+ Python 10. Next Steps \u00b6 Advanced Topics - Optimize for deployment API Reference - Complete API documentation Examples - Real-world examples Properly saving and loading pipelines ensures reproducibility, enables production deployment, and protects your trained models!","title":"Saving and Loading"},{"location":"user-guide/saving-loading/#saving-and-loading-pipelines","text":"This guide explains how to persist TabTune pipelines for production deployment, reproducibility, and continued training.","title":"Saving and Loading Pipelines"},{"location":"user-guide/saving-loading/#1-overview","text":"TabTune supports multiple serialization formats for different use cases: Format Extension Use Case Size Compatibility joblib .joblib Production, complete pipeline ~100-500MB Python only PyTorch .pt Model weights only ~50-200MB PyTorch ecosystems Checkpoint .pt Training resume, intermediate states ~50-200MB Development","title":"1. Overview"},{"location":"user-guide/saving-loading/#2-pipeline-serialization-joblib","text":"","title":"2. Pipeline Serialization (joblib)"},{"location":"user-guide/saving-loading/#21-saving-a-pipeline","text":"Save the complete fitted pipeline including model, preprocessor, and configuration: from tabtune import TabularPipeline # Train pipeline pipeline = TabularPipeline ( model_name = 'TabICL' , tuning_strategy = 'peft' , tuning_params = { 'device' : 'cuda' , 'epochs' : 5 } ) pipeline . fit ( X_train , y_train ) # Save entire pipeline pipeline . save ( 'my_pipeline.joblib' ) What Gets Saved : - \u2705 Trained model weights - \u2705 DataProcessor state (imputer, encoder, scaler) - \u2705 Model configuration - \u2705 Tuning parameters - \u2705 Label encoder for target variable - \u2705 PEFT adapter weights (if applicable)","title":"2.1 Saving a Pipeline"},{"location":"user-guide/saving-loading/#22-loading-a-pipeline","text":"Load a saved pipeline for inference or continued training: from tabtune import TabularPipeline # Load pipeline loaded_pipeline = TabularPipeline . load ( 'my_pipeline.joblib' ) # Make predictions immediately predictions = loaded_pipeline . predict ( X_test ) # Or continue training with new data loaded_pipeline . fit ( X_new_train , y_new_train )","title":"2.2 Loading a Pipeline"},{"location":"user-guide/saving-loading/#23-size-optimization","text":"If pipeline file is too large, compress it: import joblib # Save with compression joblib . dump ( pipeline , 'my_pipeline.joblib' , compress = 3 ) # compression=0-9 # Load compressed pipeline pipeline = joblib . load ( 'my_pipeline.joblib' )","title":"2.3 Size Optimization"},{"location":"user-guide/saving-loading/#3-model-only-serialization-pytorch","text":"For minimal storage or deployment, save only model weights:","title":"3. Model-Only Serialization (PyTorch)"},{"location":"user-guide/saving-loading/#31-saving-model-weights","text":"import torch # Save model weights only torch . save ( pipeline . model . state_dict (), 'model_weights.pt' )","title":"3.1 Saving Model Weights"},{"location":"user-guide/saving-loading/#32-loading-model-weights","text":"import torch from tabtune import TabularPipeline # Create new pipeline with same config new_pipeline = TabularPipeline ( model_name = 'TabICL' , tuning_strategy = 'inference' ) # Load weights into model state_dict = torch . load ( 'model_weights.pt' ) new_pipeline . model . load_state_dict ( state_dict ) # Ready for inference predictions = new_pipeline . predict ( X_test ) Advantages : - \u2705 Smaller file size (50-200MB) - \u2705 Faster loading - \u2705 Cross-platform compatible Disadvantages : - \u274c Requires manual DataProcessor setup - \u274c Must know original configuration - \u274c No automatic version compatibility","title":"3.2 Loading Model Weights"},{"location":"user-guide/saving-loading/#4-checkpoint-management","text":"For long training runs, save intermediate checkpoints:","title":"4. Checkpoint Management"},{"location":"user-guide/saving-loading/#41-automatic-checkpointing-during-training","text":"pipeline = TabularPipeline ( model_name = 'TabDPT' , tuning_strategy = 'base-ft' , tuning_params = { 'device' : 'cuda' , 'epochs' : 20 , 'save_checkpoint_path' : 'checkpoints/best_model.pt' , 'save_every_n_epochs' : 5 # Save every 5 epochs } ) pipeline . fit ( X_train , y_train ) # Checkpoints saved as: # checkpoints/best_model.pt (best validation score) # checkpoints/checkpoint_epoch_5.pt # checkpoints/checkpoint_epoch_10.pt # checkpoints/checkpoint_epoch_15.pt # checkpoints/checkpoint_epoch_20.pt","title":"4.1 Automatic Checkpointing During Training"},{"location":"user-guide/saving-loading/#42-best-checkpoint-tracking","text":"import os from pathlib import Path checkpoint_dir = Path ( 'checkpoints' ) checkpoint_dir . mkdir ( exist_ok = True ) # Find best checkpoint best_checkpoint = None best_score = 0 for checkpoint_file in checkpoint_dir . glob ( '*.pt' ): pipeline = TabularPipeline . load ( str ( checkpoint_file )) score = pipeline . evaluate ( X_val , y_val )[ 'accuracy' ] if score > best_score : best_score = score best_checkpoint = checkpoint_file print ( f \"Best checkpoint: { best_checkpoint } with score { best_score : .4f } \" )","title":"4.2 Best Checkpoint Tracking"},{"location":"user-guide/saving-loading/#5-peft-specific-serialization","text":"When using PEFT (LoRA), save and manage adapter weights:","title":"5. PEFT-Specific Serialization"},{"location":"user-guide/saving-loading/#51-save-with-peft-adapters","text":"# Train with PEFT pipeline = TabularPipeline ( model_name = 'TabICL' , tuning_strategy = 'peft' , tuning_params = { 'peft_config' : { 'r' : 8 , 'lora_alpha' : 16 }, 'save_checkpoint_path' : 'peft_model.joblib' } ) pipeline . fit ( X_train , y_train ) # Save includes both base model and LoRA adapters pipeline . save ( 'peft_pipeline.joblib' )","title":"5.1 Save with PEFT Adapters"},{"location":"user-guide/saving-loading/#52-load-peft-pipeline","text":"# Load includes automatic adapter reconstruction pipeline = TabularPipeline . load ( 'peft_pipeline.joblib' ) # Adapters are already injected predictions = pipeline . predict ( X_test )","title":"5.2 Load PEFT Pipeline"},{"location":"user-guide/saving-loading/#53-extract-lora-adapters-only","text":"import torch # Save only adapter weights (minimal size) if hasattr ( pipeline . model , 'lora_A' ) and hasattr ( pipeline . model , 'lora_B' ): adapter_state = { 'lora_A' : pipeline . model . lora_A . state_dict (), 'lora_B' : pipeline . model . lora_B . state_dict () } torch . save ( adapter_state , 'adapters.pt' )","title":"5.3 Extract LoRA Adapters Only"},{"location":"user-guide/saving-loading/#6-configuration-serialization","text":"Save pipeline configuration for reproducibility:","title":"6. Configuration Serialization"},{"location":"user-guide/saving-loading/#61-save-configuration-to-yaml","text":"import yaml config = { 'model_name' : pipeline . model_name , 'task_type' : pipeline . task_type , 'tuning_strategy' : pipeline . tuning_strategy , 'tuning_params' : pipeline . tuning_params , 'processor_params' : pipeline . processor_params , 'model_params' : pipeline . model_params } with open ( 'pipeline_config.yaml' , 'w' ) as f : yaml . dump ( config , f ) Example Config File : model_name : TabICL task_type : classification tuning_strategy : peft tuning_params : device : cuda epochs : 5 learning_rate : 2e-4 peft_config : r : 8 lora_alpha : 16 lora_dropout : 0.05 processor_params : imputation_strategy : median categorical_encoding : onehot scaling_strategy : standard model_params : n_estimators : 16","title":"6.1 Save Configuration to YAML"},{"location":"user-guide/saving-loading/#62-load-pipeline-from-configuration","text":"import yaml from tabtune import TabularPipeline # Load config with open ( 'pipeline_config.yaml' , 'r' ) as f : config = yaml . safe_load ( f ) # Recreate pipeline pipeline = TabularPipeline ( ** config )","title":"6.2 Load Pipeline from Configuration"},{"location":"user-guide/saving-loading/#7-best-practices","text":"","title":"7. Best Practices"},{"location":"user-guide/saving-loading/#dos","text":"\u2705 Save full pipelines for production deployment \u2705 Use checkpoints for long training runs \u2705 Store configuration separately for reproducibility \u2705 Version control YAML/JSON configs \u2705 Keep multiple backups at different timestamps \u2705 Document model metadata and creation date \u2705 Test loading on different systems","title":"\u2705 Do's"},{"location":"user-guide/saving-loading/#donts","text":"\u274c Don't save only model weights without config \u274c Don't overwrite checkpoints without backup \u274c Don't forget to compress large pipelines \u274c Don't hardcode paths in production code \u274c Don't commit large .joblib files to git (use git-lfs)","title":"\u274c Don'ts"},{"location":"user-guide/saving-loading/#8-troubleshooting","text":"","title":"8. Troubleshooting"},{"location":"user-guide/saving-loading/#issue-modulenotfounderror-when-loading-pipeline","text":"Solution : Ensure TabTune is installed in the target environment pip install tabtune","title":"Issue: \"ModuleNotFoundError when loading pipeline\""},{"location":"user-guide/saving-loading/#issue-pickle-protocol-version-mismatch","text":"Solution : Use compatible Python and PyTorch versions # Save with older protocol for compatibility import joblib joblib . dump ( pipeline , 'compatible.joblib' , protocol = 2 )","title":"Issue: \"Pickle protocol version mismatch\""},{"location":"user-guide/saving-loading/#issue-cuda-error-when-loading-on-cpu","text":"Solution : Specify device when loading pipeline = TabularPipeline . load ( 'pipeline.joblib' ) pipeline . model = pipeline . model . to ( 'cpu' )","title":"Issue: \"CUDA error when loading on CPU\""},{"location":"user-guide/saving-loading/#issue-pipeline-file-corrupted-or-incomplete","text":"Solution : Restore from backup restored = TabularPipeline . load ( 'backups/pipeline_backup_20250101_120000.joblib' )","title":"Issue: \"Pipeline file corrupted or incomplete\""},{"location":"user-guide/saving-loading/#9-summary-table","text":"Task Method File Size Compatibility Full pipeline backup .save() ~300MB joblib only Model weights only torch.save(state_dict) ~100MB PyTorch Configuration only YAML/JSON <1MB Any language Training checkpoint .pt ~150MB Development Production package .joblib + config ~300MB+ Python","title":"9. Summary Table"},{"location":"user-guide/saving-loading/#10-next-steps","text":"Advanced Topics - Optimize for deployment API Reference - Complete API documentation Examples - Real-world examples Properly saving and loading pipelines ensures reproducibility, enables production deployment, and protects your trained models!","title":"10. Next Steps"},{"location":"user-guide/troubleshooting/","text":"Troubleshooting Guide \u00b6 This guide addresses common errors, issues, and solutions when using TabTune. Common Import Errors \u00b6 ModuleNotFoundError: No module named 'tabtune' \u00b6 Cause : TabTune is not installed or not in your Python path. Solution : cd TabTune pip install -e . Verify installation : import tabtune print ( tabtune . __version__ ) ImportError: Cannot import TabularPipeline \u00b6 Cause : Incorrect import path or installation issue. Solution : Use the correct import: from tabtune import TabularPipeline # \u2705 Correct # NOT: from TabularPipeline.pipeline import TabularPipeline # \u274c Old path ImportError: No module named 'torch' \u00b6 Cause : PyTorch is not installed. Solution : # For CPU only pip install torch # For GPU support (CUDA 11.8) pip install torch --index-url https://download.pytorch.org/whl/cu118 CUDA/GPU Issues \u00b6 CUDA out of memory \u00b6 Symptoms : RuntimeError: CUDA out of memory Solutions : Use PEFT strategy (recommended): pipeline = TabularPipeline ( model_name = \"TabICL\" , tuning_strategy = \"peft\" , # Uses 40-60% less memory tuning_params = { \"peft_config\" : { \"r\" : 4 }} # Lower rank = less memory ) Reduce batch size : tuning_params = { \"batch_size\" : 4 } # Default is often 8 or 16 Use a smaller model : Switch from TabDPT to TabICL Switch from OrionBix to TabICL Process in chunks : # Split dataset into smaller batches chunk_size = 10000 for i in range ( 0 , len ( X_train ), chunk_size ): X_chunk = X_train [ i : i + chunk_size ] y_chunk = y_train [ i : i + chunk_size ] # Process chunk Clear GPU cache : import torch torch . cuda . empty_cache () Use CPU instead (slower but no memory limits): tuning_params = { \"device\" : \"cpu\" } CUDA device not found \u00b6 Symptoms : RuntimeError: CUDA error: no kernel image is available Cause : PyTorch version doesn't match your GPU's CUDA version. Solution : 1. Check your CUDA version: nvidia-smi 2. Install matching PyTorch: # For CUDA 11.8 pip install torch --index-url https://download.pytorch.org/whl/cu118 # For CUDA 12.1 pip install torch --index-url https://download.pytorch.org/whl/cu121 GPU not being used \u00b6 Symptoms : Training runs on CPU despite having GPU. Check : import torch print ( f \"CUDA available: { torch . cuda . is_available () } \" ) print ( f \"Current device: { torch . cuda . current_device () if torch . cuda . is_available () else 'CPU' } \" ) Solution : Explicitly set device in tuning_params : tuning_params = { \"device\" : \"cuda\" } Memory Errors \u00b6 Out of memory during training \u00b6 Cause : Dataset too large or batch size too high. Solutions : 1. Reduce batch size: tuning_params={\"batch_size\": 2} 2. Use gradient accumulation: tuning_params = { \"batch_size\" : 4 , \"gradient_accumulation_steps\" : 4 # Effective batch size = 16 } 3. Use PEFT instead of base-ft 4. Reduce dataset size for testing Memory leak during training \u00b6 Symptoms : Memory usage increases over epochs. Solution : # Add periodic cleanup import torch import gc for epoch in range ( epochs ): # Training code if epoch % 5 == 0 : torch . cuda . empty_cache () gc . collect () Model Loading Failures \u00b6 Model checkpoint not found \u00b6 Symptoms : FileNotFoundError when loading checkpoint. Solution : # Check if checkpoint exists import os if not os . path . exists ( checkpoint_path ): print ( f \"Checkpoint not found: { checkpoint_path } \" ) # Use inference mode instead pipeline = TabularPipeline ( model_name = \"TabPFN\" , tuning_strategy = \"inference\" ) Model state dict mismatch \u00b6 Symptoms : RuntimeError: Error(s) in loading state_dict Cause : Checkpoint from different model version or architecture. Solution : - Use checkpoints saved from the same TabTune version - Or start fresh training without loading checkpoint Preprocessing Errors \u00b6 ValueError: Found array with 0 sample(s) \u00b6 Cause : Empty dataset after preprocessing/filtering. Solution : # Check data before preprocessing print ( f \"Data shape: { X . shape } \" ) print ( f \"Null values: { X . isnull () . sum () . sum () } \" ) # Ensure sufficient samples after train/test split X_train , X_test , y_train , y_test = train_test_split ( X , y , test_size = 0.2 , random_state = 42 ) print ( f \"Train samples: { len ( X_train ) } \" ) Categorical encoding mismatch \u00b6 Symptoms : Errors during prediction about unseen categories. Cause : Test set contains categories not seen during training. Solution : - Ensure train/test split preserves all categories - Use stratify=y in train_test_split for classification - Check for new categories in test set: train_cats = set ( X_train [ 'column' ] . unique ()) test_cats = set ( X_test [ 'column' ] . unique ()) unseen = test_cats - train_cats if unseen : print ( f \"Unseen categories: { unseen } \" ) Data type mismatches \u00b6 Symptoms : Type errors during preprocessing. Solution : # Ensure correct types X = X . astype ({ 'col1' : 'float64' , 'col2' : 'category' }) # Or let DataProcessor handle it pipeline = TabularPipeline ( model_name = \"TabICL\" , processor_params = { \"override_types\" : None } # Auto-detect ) Training Convergence Issues \u00b6 Model not converging / Loss not decreasing \u00b6 Symptoms : Loss plateaus or increases. Solutions : Lower learning rate : tuning_params = { \"learning_rate\" : 1e-5 } # Default might be too high Reduce epochs : tuning_params = { \"epochs\" : 3 } # Start small Check data quality : Ensure labels are correct Check for data leakage Verify train/test split Try different model : Some models work better for certain datasets Use TabularLeaderboard to compare Warmup learning rate : tuning_params = { \"learning_rate\" : 2e-5 , \"warmup_steps\" : 100 } Overfitting \u00b6 Symptoms : High training accuracy, low validation accuracy. Solutions : 1. Reduce epochs : tuning_params={\"epochs\": 3} 2. Lower learning rate : tuning_params={\"learning_rate\": 1e-5} 3. Use PEFT : Often generalizes better than base-ft 4. More training data : Collect or use data augmentation 5. Early stopping : Implement callback to stop when validation loss increases Underfitting \u00b6 Symptoms : Both training and validation accuracy are low. Solutions : 1. Train longer : Increase epochs 2. Higher learning rate : tuning_params={\"learning_rate\": 5e-4} 3. Use base-ft : Full fine-tuning may be needed 4. Larger model : Try TabDPT or OrionBix instead of TabICL 5. Feature engineering : Add relevant features PEFT-Specific Problems \u00b6 PEFT not working / Falling back to base-ft \u00b6 Symptoms : Warning messages about PEFT compatibility. Cause : Some models have experimental PEFT support. Solution : - TabPFN and ContextTab have experimental PEFT - Use base-ft strategy for these models - Or check PEFT configuration: peft_config = { \"r\" : 8 , # Rank (lower = less memory, but may affect performance) \"lora_alpha\" : 16 , \"lora_dropout\" : 0.05 , \"target_modules\" : [ \"query\" , \"value\" ] # Model-specific } PEFT model size larger than expected \u00b6 Cause : Incorrect target modules or high rank. Solution : # Use lower rank peft_config = { \"r\" : 4 } # Instead of default 8 # Check actual model size import torch total_params = sum ( p . numel () for p in model . parameters ()) trainable = sum ( p . numel () for p in model . parameters () if p . requires_grad ) print ( f \"Total: { total_params } , Trainable: { trainable } \" ) Data Format Errors \u00b6 ValueError: Input must be pandas DataFrame \u00b6 Symptoms : Error when passing numpy arrays. Solution : Convert to DataFrame: import pandas as pd X_df = pd . DataFrame ( X , columns = feature_names ) y_series = pd . Series ( y ) Shape mismatch errors \u00b6 Symptoms : Dimension errors during prediction. Cause : Feature count differs between train and test. Solution : # Ensure same features assert X_train . columns . tolist () == X_test . columns . tolist () # Or use pipeline's built-in handling # TabTune automatically handles this Missing target column \u00b6 Symptoms : KeyError when accessing target. Solution : # Ensure target is separate Series y = df [ 'target' ] # \u2705 Correct # NOT: y = df[['target']] # \u274c DataFrame instead of Series Version Compatibility Issues \u00b6 PyTorch version conflicts \u00b6 Symptoms : Errors about tensor operations or CUDA compatibility. Solution : Check and match versions: import torch print ( f \"PyTorch: { torch . __version__ } \" ) print ( f \"CUDA: { torch . version . cuda } \" ) Update if needed: pip install --upgrade torch scikit-learn version issues \u00b6 Symptoms : Deprecation warnings or API errors. Solution : Use compatible version: pip install scikit-learn == 1 .7 Python version too old \u00b6 Symptoms : Syntax errors or unsupported features. Solution : TabTune requires Python 3.10+. Upgrade Python: # Using conda conda create -n tabtune python = 3 .10 conda activate tabtune Evaluation & Prediction Errors \u00b6 predict_proba returns incorrect shape \u00b6 Symptoms : Shape mismatch or wrong number of classes. Solution : # Check class count n_classes = len ( pipeline . processor . custom_preprocessor_ . label_encoder_ . classes_ ) print ( f \"Expected { n_classes } classes\" ) probabilities = pipeline . predict_proba ( X_test ) print ( f \"Got shape: { probabilities . shape } \" ) # Should be (n_samples, n_classes) All predictions are the same class \u00b6 Symptoms : Model predicts only one class for all samples. Possible causes : 1. Model not trained (using inference with poor weights) 2. Severe class imbalance 3. Data preprocessing issue Solutions : 1. Fine-tune the model : Use base-ft or peft 2. Check class distribution : print ( y_train . value_counts ()) 3. Use resampling : processor_params = { \"resampling_strategy\" : \"smote\" } 4. Inspect predictions : predictions = pipeline . predict ( X_test ) print ( f \"Unique predictions: { np . unique ( predictions ) } \" ) print ( f \"Prediction distribution: { pd . Series ( predictions ) . value_counts () } \" ) Performance Issues \u00b6 Training is very slow \u00b6 Solutions : 1. Use GPU : tuning_params={\"device\": \"cuda\"} 2. Reduce dataset size : Test with subset first 3. Use inference mode : For quick baselines 4. Optimize batch size : Larger batches (if memory allows) 5. Use PEFT : Faster than base-ft Inference is slow \u00b6 Solutions : 1. Batch predictions : Process multiple samples at once 2. Use GPU : tuning_params={\"device\": \"cuda\"} 3. Reduce n_estimators (for ensemble models): model_params = { \"n_estimators\" : 8 } # Instead of default 16 or 32 4. Cache preprocessing : Save and load preprocessed data Getting Help \u00b6 If you encounter an issue not covered here: Check the logs : TabTune provides detailed logging import logging logging . basicConfig ( level = logging . DEBUG ) Reproduce with minimal example : Create smallest code that reproduces the issue Check GitHub Issues : Search TabTune_Internal Issues Open new issue : Include: TabTune version Python version Full error traceback Minimal reproducible code System information Review documentation : Check relevant guides: Installation User Guide FAQ","title":"Troubleshooting"},{"location":"user-guide/troubleshooting/#troubleshooting-guide","text":"This guide addresses common errors, issues, and solutions when using TabTune.","title":"Troubleshooting Guide"},{"location":"user-guide/troubleshooting/#common-import-errors","text":"","title":"Common Import Errors"},{"location":"user-guide/troubleshooting/#modulenotfounderror-no-module-named-tabtune","text":"Cause : TabTune is not installed or not in your Python path. Solution : cd TabTune pip install -e . Verify installation : import tabtune print ( tabtune . __version__ )","title":"ModuleNotFoundError: No module named 'tabtune'"},{"location":"user-guide/troubleshooting/#importerror-cannot-import-tabularpipeline","text":"Cause : Incorrect import path or installation issue. Solution : Use the correct import: from tabtune import TabularPipeline # \u2705 Correct # NOT: from TabularPipeline.pipeline import TabularPipeline # \u274c Old path","title":"ImportError: Cannot import TabularPipeline"},{"location":"user-guide/troubleshooting/#importerror-no-module-named-torch","text":"Cause : PyTorch is not installed. Solution : # For CPU only pip install torch # For GPU support (CUDA 11.8) pip install torch --index-url https://download.pytorch.org/whl/cu118","title":"ImportError: No module named 'torch'"},{"location":"user-guide/troubleshooting/#cudagpu-issues","text":"","title":"CUDA/GPU Issues"},{"location":"user-guide/troubleshooting/#cuda-out-of-memory","text":"Symptoms : RuntimeError: CUDA out of memory Solutions : Use PEFT strategy (recommended): pipeline = TabularPipeline ( model_name = \"TabICL\" , tuning_strategy = \"peft\" , # Uses 40-60% less memory tuning_params = { \"peft_config\" : { \"r\" : 4 }} # Lower rank = less memory ) Reduce batch size : tuning_params = { \"batch_size\" : 4 } # Default is often 8 or 16 Use a smaller model : Switch from TabDPT to TabICL Switch from OrionBix to TabICL Process in chunks : # Split dataset into smaller batches chunk_size = 10000 for i in range ( 0 , len ( X_train ), chunk_size ): X_chunk = X_train [ i : i + chunk_size ] y_chunk = y_train [ i : i + chunk_size ] # Process chunk Clear GPU cache : import torch torch . cuda . empty_cache () Use CPU instead (slower but no memory limits): tuning_params = { \"device\" : \"cpu\" }","title":"CUDA out of memory"},{"location":"user-guide/troubleshooting/#cuda-device-not-found","text":"Symptoms : RuntimeError: CUDA error: no kernel image is available Cause : PyTorch version doesn't match your GPU's CUDA version. Solution : 1. Check your CUDA version: nvidia-smi 2. Install matching PyTorch: # For CUDA 11.8 pip install torch --index-url https://download.pytorch.org/whl/cu118 # For CUDA 12.1 pip install torch --index-url https://download.pytorch.org/whl/cu121","title":"CUDA device not found"},{"location":"user-guide/troubleshooting/#gpu-not-being-used","text":"Symptoms : Training runs on CPU despite having GPU. Check : import torch print ( f \"CUDA available: { torch . cuda . is_available () } \" ) print ( f \"Current device: { torch . cuda . current_device () if torch . cuda . is_available () else 'CPU' } \" ) Solution : Explicitly set device in tuning_params : tuning_params = { \"device\" : \"cuda\" }","title":"GPU not being used"},{"location":"user-guide/troubleshooting/#memory-errors","text":"","title":"Memory Errors"},{"location":"user-guide/troubleshooting/#out-of-memory-during-training","text":"Cause : Dataset too large or batch size too high. Solutions : 1. Reduce batch size: tuning_params={\"batch_size\": 2} 2. Use gradient accumulation: tuning_params = { \"batch_size\" : 4 , \"gradient_accumulation_steps\" : 4 # Effective batch size = 16 } 3. Use PEFT instead of base-ft 4. Reduce dataset size for testing","title":"Out of memory during training"},{"location":"user-guide/troubleshooting/#memory-leak-during-training","text":"Symptoms : Memory usage increases over epochs. Solution : # Add periodic cleanup import torch import gc for epoch in range ( epochs ): # Training code if epoch % 5 == 0 : torch . cuda . empty_cache () gc . collect ()","title":"Memory leak during training"},{"location":"user-guide/troubleshooting/#model-loading-failures","text":"","title":"Model Loading Failures"},{"location":"user-guide/troubleshooting/#model-checkpoint-not-found","text":"Symptoms : FileNotFoundError when loading checkpoint. Solution : # Check if checkpoint exists import os if not os . path . exists ( checkpoint_path ): print ( f \"Checkpoint not found: { checkpoint_path } \" ) # Use inference mode instead pipeline = TabularPipeline ( model_name = \"TabPFN\" , tuning_strategy = \"inference\" )","title":"Model checkpoint not found"},{"location":"user-guide/troubleshooting/#model-state-dict-mismatch","text":"Symptoms : RuntimeError: Error(s) in loading state_dict Cause : Checkpoint from different model version or architecture. Solution : - Use checkpoints saved from the same TabTune version - Or start fresh training without loading checkpoint","title":"Model state dict mismatch"},{"location":"user-guide/troubleshooting/#preprocessing-errors","text":"","title":"Preprocessing Errors"},{"location":"user-guide/troubleshooting/#valueerror-found-array-with-0-samples","text":"Cause : Empty dataset after preprocessing/filtering. Solution : # Check data before preprocessing print ( f \"Data shape: { X . shape } \" ) print ( f \"Null values: { X . isnull () . sum () . sum () } \" ) # Ensure sufficient samples after train/test split X_train , X_test , y_train , y_test = train_test_split ( X , y , test_size = 0.2 , random_state = 42 ) print ( f \"Train samples: { len ( X_train ) } \" )","title":"ValueError: Found array with 0 sample(s)"},{"location":"user-guide/troubleshooting/#categorical-encoding-mismatch","text":"Symptoms : Errors during prediction about unseen categories. Cause : Test set contains categories not seen during training. Solution : - Ensure train/test split preserves all categories - Use stratify=y in train_test_split for classification - Check for new categories in test set: train_cats = set ( X_train [ 'column' ] . unique ()) test_cats = set ( X_test [ 'column' ] . unique ()) unseen = test_cats - train_cats if unseen : print ( f \"Unseen categories: { unseen } \" )","title":"Categorical encoding mismatch"},{"location":"user-guide/troubleshooting/#data-type-mismatches","text":"Symptoms : Type errors during preprocessing. Solution : # Ensure correct types X = X . astype ({ 'col1' : 'float64' , 'col2' : 'category' }) # Or let DataProcessor handle it pipeline = TabularPipeline ( model_name = \"TabICL\" , processor_params = { \"override_types\" : None } # Auto-detect )","title":"Data type mismatches"},{"location":"user-guide/troubleshooting/#training-convergence-issues","text":"","title":"Training Convergence Issues"},{"location":"user-guide/troubleshooting/#model-not-converging-loss-not-decreasing","text":"Symptoms : Loss plateaus or increases. Solutions : Lower learning rate : tuning_params = { \"learning_rate\" : 1e-5 } # Default might be too high Reduce epochs : tuning_params = { \"epochs\" : 3 } # Start small Check data quality : Ensure labels are correct Check for data leakage Verify train/test split Try different model : Some models work better for certain datasets Use TabularLeaderboard to compare Warmup learning rate : tuning_params = { \"learning_rate\" : 2e-5 , \"warmup_steps\" : 100 }","title":"Model not converging / Loss not decreasing"},{"location":"user-guide/troubleshooting/#overfitting","text":"Symptoms : High training accuracy, low validation accuracy. Solutions : 1. Reduce epochs : tuning_params={\"epochs\": 3} 2. Lower learning rate : tuning_params={\"learning_rate\": 1e-5} 3. Use PEFT : Often generalizes better than base-ft 4. More training data : Collect or use data augmentation 5. Early stopping : Implement callback to stop when validation loss increases","title":"Overfitting"},{"location":"user-guide/troubleshooting/#underfitting","text":"Symptoms : Both training and validation accuracy are low. Solutions : 1. Train longer : Increase epochs 2. Higher learning rate : tuning_params={\"learning_rate\": 5e-4} 3. Use base-ft : Full fine-tuning may be needed 4. Larger model : Try TabDPT or OrionBix instead of TabICL 5. Feature engineering : Add relevant features","title":"Underfitting"},{"location":"user-guide/troubleshooting/#peft-specific-problems","text":"","title":"PEFT-Specific Problems"},{"location":"user-guide/troubleshooting/#peft-not-working-falling-back-to-base-ft","text":"Symptoms : Warning messages about PEFT compatibility. Cause : Some models have experimental PEFT support. Solution : - TabPFN and ContextTab have experimental PEFT - Use base-ft strategy for these models - Or check PEFT configuration: peft_config = { \"r\" : 8 , # Rank (lower = less memory, but may affect performance) \"lora_alpha\" : 16 , \"lora_dropout\" : 0.05 , \"target_modules\" : [ \"query\" , \"value\" ] # Model-specific }","title":"PEFT not working / Falling back to base-ft"},{"location":"user-guide/troubleshooting/#peft-model-size-larger-than-expected","text":"Cause : Incorrect target modules or high rank. Solution : # Use lower rank peft_config = { \"r\" : 4 } # Instead of default 8 # Check actual model size import torch total_params = sum ( p . numel () for p in model . parameters ()) trainable = sum ( p . numel () for p in model . parameters () if p . requires_grad ) print ( f \"Total: { total_params } , Trainable: { trainable } \" )","title":"PEFT model size larger than expected"},{"location":"user-guide/troubleshooting/#data-format-errors","text":"","title":"Data Format Errors"},{"location":"user-guide/troubleshooting/#valueerror-input-must-be-pandas-dataframe","text":"Symptoms : Error when passing numpy arrays. Solution : Convert to DataFrame: import pandas as pd X_df = pd . DataFrame ( X , columns = feature_names ) y_series = pd . Series ( y )","title":"ValueError: Input must be pandas DataFrame"},{"location":"user-guide/troubleshooting/#shape-mismatch-errors","text":"Symptoms : Dimension errors during prediction. Cause : Feature count differs between train and test. Solution : # Ensure same features assert X_train . columns . tolist () == X_test . columns . tolist () # Or use pipeline's built-in handling # TabTune automatically handles this","title":"Shape mismatch errors"},{"location":"user-guide/troubleshooting/#missing-target-column","text":"Symptoms : KeyError when accessing target. Solution : # Ensure target is separate Series y = df [ 'target' ] # \u2705 Correct # NOT: y = df[['target']] # \u274c DataFrame instead of Series","title":"Missing target column"},{"location":"user-guide/troubleshooting/#version-compatibility-issues","text":"","title":"Version Compatibility Issues"},{"location":"user-guide/troubleshooting/#pytorch-version-conflicts","text":"Symptoms : Errors about tensor operations or CUDA compatibility. Solution : Check and match versions: import torch print ( f \"PyTorch: { torch . __version__ } \" ) print ( f \"CUDA: { torch . version . cuda } \" ) Update if needed: pip install --upgrade torch","title":"PyTorch version conflicts"},{"location":"user-guide/troubleshooting/#scikit-learn-version-issues","text":"Symptoms : Deprecation warnings or API errors. Solution : Use compatible version: pip install scikit-learn == 1 .7","title":"scikit-learn version issues"},{"location":"user-guide/troubleshooting/#python-version-too-old","text":"Symptoms : Syntax errors or unsupported features. Solution : TabTune requires Python 3.10+. Upgrade Python: # Using conda conda create -n tabtune python = 3 .10 conda activate tabtune","title":"Python version too old"},{"location":"user-guide/troubleshooting/#evaluation-prediction-errors","text":"","title":"Evaluation &amp; Prediction Errors"},{"location":"user-guide/troubleshooting/#predict_proba-returns-incorrect-shape","text":"Symptoms : Shape mismatch or wrong number of classes. Solution : # Check class count n_classes = len ( pipeline . processor . custom_preprocessor_ . label_encoder_ . classes_ ) print ( f \"Expected { n_classes } classes\" ) probabilities = pipeline . predict_proba ( X_test ) print ( f \"Got shape: { probabilities . shape } \" ) # Should be (n_samples, n_classes)","title":"predict_proba returns incorrect shape"},{"location":"user-guide/troubleshooting/#all-predictions-are-the-same-class","text":"Symptoms : Model predicts only one class for all samples. Possible causes : 1. Model not trained (using inference with poor weights) 2. Severe class imbalance 3. Data preprocessing issue Solutions : 1. Fine-tune the model : Use base-ft or peft 2. Check class distribution : print ( y_train . value_counts ()) 3. Use resampling : processor_params = { \"resampling_strategy\" : \"smote\" } 4. Inspect predictions : predictions = pipeline . predict ( X_test ) print ( f \"Unique predictions: { np . unique ( predictions ) } \" ) print ( f \"Prediction distribution: { pd . Series ( predictions ) . value_counts () } \" )","title":"All predictions are the same class"},{"location":"user-guide/troubleshooting/#performance-issues","text":"","title":"Performance Issues"},{"location":"user-guide/troubleshooting/#training-is-very-slow","text":"Solutions : 1. Use GPU : tuning_params={\"device\": \"cuda\"} 2. Reduce dataset size : Test with subset first 3. Use inference mode : For quick baselines 4. Optimize batch size : Larger batches (if memory allows) 5. Use PEFT : Faster than base-ft","title":"Training is very slow"},{"location":"user-guide/troubleshooting/#inference-is-slow","text":"Solutions : 1. Batch predictions : Process multiple samples at once 2. Use GPU : tuning_params={\"device\": \"cuda\"} 3. Reduce n_estimators (for ensemble models): model_params = { \"n_estimators\" : 8 } # Instead of default 16 or 32 4. Cache preprocessing : Save and load preprocessed data","title":"Inference is slow"},{"location":"user-guide/troubleshooting/#getting-help","text":"If you encounter an issue not covered here: Check the logs : TabTune provides detailed logging import logging logging . basicConfig ( level = logging . DEBUG ) Reproduce with minimal example : Create smallest code that reproduces the issue Check GitHub Issues : Search TabTune_Internal Issues Open new issue : Include: TabTune version Python version Full error traceback Minimal reproducible code System information Review documentation : Check relevant guides: Installation User Guide FAQ","title":"Getting Help"},{"location":"user-guide/tuning-strategies/","text":"Tuning Strategies \u00b6 TabTune provides three distinct tuning strategies to accommodate different use cases, computational budgets, and performance requirements. This guide explains each strategy in detail, including when to use them and their tradeoffs. 1. Overview \u00b6 Strategy Training Use Case Memory Speed Accuracy inference None Baseline, zero-shot Minimal Fast Baseline base-ft Full params High accuracy, ample resources High Slow Highest peft LoRA adapters Memory-constrained, iteration Low Medium High 2. Inference Strategy \u00b6 Definition \u00b6 Zero-shot inference using pre-trained model weights without any training on your data. Use Cases \u00b6 Quick baseline comparisons Evaluating out-of-the-box model performance Time-constrained scenarios Testing data preprocessing pipeline Workflow \u00b6 flowchart LR A[Raw Data] --> B[DataProcessor] B --> C[Load Pre-trained Model] C --> D[Forward Pass Only] D --> E[Predictions] Implementation \u00b6 from tabtune import TabularPipeline # No training occurs pipeline = TabularPipeline ( model_name = 'TabPFN' , task_type = 'classification' , tuning_strategy = 'inference' , tuning_params = { 'device' : 'cuda' } ) # fit() only applies preprocessing; no model training pipeline . fit ( X_train , y_train ) # Direct prediction on test data predictions = pipeline . predict ( X_test ) metrics = pipeline . evaluate ( X_test , y_test ) Advantages \u00b6 \u2705 No training time needed \u2705 Minimal memory footprint \u2705 Immediate results \u2705 Good for baseline comparisons Disadvantages \u00b6 \u274c Generic pre-trained weights may not fit your data \u274c Typically lower accuracy than fine-tuned models \u274c Cannot adapt to task-specific patterns Performance Profile \u00b6 Training time : 0 seconds Memory usage : 2-4 GB (model + data) Inference latency : 10-50 ms per batch Example with All Models \u00b6 from tabtune import TabularPipeline models = [ 'TabPFN' , 'TabICL' , 'TabDPT' , 'Mitra' , 'ContextTab' , 'OrionMSP' , 'OrionBix' ] for model in models : pipeline = TabularPipeline ( model_name = model , tuning_strategy = 'inference' ) pipeline . fit ( X_train , y_train ) metrics = pipeline . evaluate ( X_test , y_test ) print ( f \" { model } - Accuracy: { metrics [ 'accuracy' ] : .4f } \" ) 3. Base Fine-Tuning Strategy ( base-ft ) \u00b6 Definition \u00b6 Full-parameter fine-tuning where all model weights are updated during training. Use Cases \u00b6 Maximum accuracy is priority Abundant computational resources (GPU, RAM) Large training datasets (>100K samples) Production models requiring best performance Transfer learning from related domains Workflow \u00b6 flowchart LR A[Raw Data] --> B[DataProcessor] B --> C[Load Pre-trained Model] C --> D[Update ALL Parameters] D --> E[Training Loop] E --> F[Fine-tuned Model] F --> G[Predictions] Implementation \u00b6 from tabtune import TabularPipeline pipeline = TabularPipeline ( model_name = 'OrionMSP' , task_type = 'classification' , tuning_strategy = 'base-ft' , tuning_params = { 'device' : 'cuda' , 'epochs' : 5 , 'learning_rate' : 2e-5 , 'batch_size' : 32 , 'show_progress' : True , 'gradient_accumulation_steps' : 2 , # optional 'mixed_precision' : 'fp16' # optional } ) # Full training occurs during fit() pipeline . fit ( X_train , y_train ) # Use fine-tuned model for predictions predictions = pipeline . predict ( X_test ) metrics = pipeline . evaluate ( X_test , y_test ) Supported Parameters \u00b6 Parameter Type Default Description device str 'cpu' 'cuda' or 'cpu' epochs int 3 Number of training epochs learning_rate float 2e-5 Optimizer learning rate batch_size int 32 Samples per batch optimizer str 'adamw' 'adamw' or 'sgd' show_progress bool True Display training progress bar Advantages \u00b6 \u2705 Highest accuracy potential \u2705 Fully adapts to task-specific patterns \u2705 Works with any dataset size \u2705 Best for production models \u2705 Supports all hyperparameter tuning Disadvantages \u00b6 \u274c High memory consumption (8-16GB+) \u274c Long training time (hours for large models) \u274c Risk of overfitting on small datasets \u274c Requires careful hyperparameter tuning \u274c GPU memory can become bottleneck Performance Profile \u00b6 Training time : 30 minutes - 2 hours (depending on dataset) Memory usage : 12-24 GB (full model + gradients + optimizer states) Inference latency : 10-50 ms per batch Training Loop Details \u00b6 The training process follows this pattern: Initialize optimizer (AdamW with weight decay) For each epoch : Shuffle training data For each batch : Forward pass through model Compute loss Backward pass (compute gradients) Clip gradients if specified Update weights Update learning rate scheduler Validate on development set (if available) Save best checkpoint based on validation metric Return fine-tuned model Example: Full Training Pipeline \u00b6 from tabtune import TabularPipeline from sklearn.model_selection import train_test_split # Load and split data X , y = load_your_dataset () X_train , X_val , y_train , y_val = train_test_split ( X , y , test_size = 0.2 ) # Configure base fine-tuning pipeline = TabularPipeline ( model_name = 'TabDPT' , task_type = 'classification' , tuning_strategy = 'base-ft' , tuning_params = { 'device' : 'cuda' , 'epochs' : 10 , 'learning_rate' : 1e-5 , 'batch_size' : 64 , 'scheduler' : 'cosine' , 'warmup_steps' : 500 , 'mixed_precision' : 'fp16' , 'show_progress' : True , 'save_checkpoint_path' : 'best_model.pt' } ) # Train on training data pipeline . fit ( X_train , y_train ) # Evaluate on validation set val_metrics = pipeline . evaluate ( X_val , y_val ) print ( f \"Validation Accuracy: { val_metrics [ 'accuracy' ] : .4f } \" ) # Save for later use pipeline . save ( 'fintuned_pipeline.joblib' ) 4. PEFT Fine-Tuning Strategy ( peft ) \u00b6 Definition \u00b6 Parameter-Efficient Fine-Tuning using LoRA (Low-Rank Adaptation) where only small adapter weights are trained while base model is frozen. How LoRA Works \u00b6 Use Cases \u00b6 Limited GPU memory (< 8 GB) Quick iteration cycles Fine-tuning multiple models simultaneously Rapid experimentation Deployment with minimal storage Workflow \u00b6 flowchart LR A[Raw Data] --> B[DataProcessor] B --> C[Load Pre-trained Model] C --> D[Inject LoRA Adapters] D --> E[Update ONLY Adapters] E --> F[Training Loop] F --> G[Model + LoRA Weights] G --> H[Predictions] Implementation \u00b6 from tabtune import TabularPipeline pipeline = TabularPipeline ( model_name = 'Mitra' , task_type = 'classification' , tuning_strategy = 'peft' , tuning_params = { 'device' : 'cuda' , 'epochs' : 5 , 'learning_rate' : 2e-4 , 'peft_config' : { 'r' : 8 , 'lora_alpha' : 16 , 'lora_dropout' : 0.05 , 'target_modules' : None # Uses model defaults if None }, 'show_progress' : True } ) # Training with LoRA adapters pipeline . fit ( X_train , y_train ) # Predictions with adapted model predictions = pipeline . predict ( X_test ) metrics = pipeline . evaluate ( X_test , y_test ) PEFT Parameters \u00b6 Parameter Type Default Description r int 8 LoRA rank (lower = more compression) lora_alpha int 16 Scaling factor for LoRA output lora_dropout float 0.05 Dropout applied to LoRA input target_modules list None Which linear layers to adapt (None = use defaults) Model-Specific LoRA Targets \u00b6 TabTune pre-configures optimal target modules per model: TabICL/OrionBix : col_embedder.tf_col, row_interactor, icl_predictor.tf_icl, icl_predictor.decoder TabDPT : transformer_encoder, encoder, y_encoder, head Mitra : x_embedding, layers, final_layer ContextTab : in_context_encoder, dense, output_head, embeddings TabPFN (\u26a0\ufe0f Experimental): encoder.5.layer, y_encoder.2.layer, transformer_encoder.layers, decoder_dict.standard Advantages \u00b6 \u2705 90% memory reduction vs base-ft \u2705 2-3x faster training \u2705 Only stores small adapter weights \u2705 Can run on 4GB GPUs \u2705 Fast iteration for experimentation Disadvantages \u00b6 \u274c Slightly lower accuracy than base-ft (~2-5% in practice) \u274c Not all model layers adapted (frozen backbone limits flexibility) \u274c May struggle with very different tasks \u274c Experimental support on TabPFN and ContextTab Performance Profile \u00b6 Training time : 10-30 minutes Memory usage : 3-6 GB (adapters + activations only) Inference latency : 10-50 ms per batch Model size : Original model size + 1-2% (adapters) Parameter Tuning Guidelines \u00b6 Rank Selection : r = 4 \u2192 Highest compression, faster, lower accuracy r = 8 \u2192 Good balance (default) r = 16 \u2192 More expressive, slower, higher accuracy r = 32 \u2192 Close to base-ft, but still compressed Alpha Selection : lora_alpha should typically be 2x the rank r=8 \u2192 lora_alpha=16 r=16 \u2192 lora_alpha=32 Dropout Selection : lora_dropout=0.0 \u2192 No regularization lora_dropout=0.05 \u2192 Light regularization (default) lora_dropout=0.1 \u2192 Strong regularization Example: PEFT Training with Hyperparameter Tuning \u00b6 from tabtune import TabularPipeline # Experiment with different LoRA ranks for r in [ 4 , 8 , 16 ]: pipeline = TabularPipeline ( model_name = 'TabICL' , tuning_strategy = 'peft' , tuning_params = { 'device' : 'cuda' , 'epochs' : 5 , 'learning_rate' : 2e-4 , 'peft_config' : { 'r' : r , 'lora_alpha' : 2 * r , 'lora_dropout' : 0.05 } } ) pipeline . fit ( X_train , y_train ) metrics = pipeline . evaluate ( X_test , y_test ) print ( f \"Rank { r } : Accuracy = { metrics [ 'accuracy' ] : .4f } \" ) 5. Strategy Comparison & Decision Tree \u00b6 Quick Comparison Table \u00b6 Aspect inference base-ft peft Training No Yes, all params Yes, adapters only Memory \u2b50\u2b50 \u2b50\u2b50\u2b50\u2b50\u2b50 \u2b50\u2b50\u2b50 Speed \u2b50\u2b50\u2b50\u2b50\u2b50 \u2b50 \u2b50\u2b50\u2b50 Accuracy \u2b50\u2b50\u2b50 \u2b50\u2b50\u2b50\u2b50\u2b50 \u2b50\u2b50\u2b50\u2b50 Cost Free High GPU cost Low GPU cost Production \u274c \u2705 \u2705 Decision Tree \u00b6 Start: Which strategy? \u2502 \u251c\u2500 \"I want instant results, no training\" \u2192 inference \u2502 \u2514\u2500 Best for: Baseline, quick exploration \u2502 \u251c\u2500 \"I have limited resources (<8GB GPU)\" \u2192 peft \u2502 \u2514\u2500 Best for: Rapid iteration, memory-constrained \u2502 \u2514\u2500 \"I need best accuracy, have resources\" \u2192 base-ft \u2514\u2500 Best for: Production, large datasets, high accuracy 6. Best Practices \u00b6 Start with inference to establish baseline Use PEFT for exploration when resources are limited Switch to base-ft for production models Monitor for overfitting on small datasets Save checkpoints for long training runs Use validation set to track progress Start with default hyperparameters then tune 7. Troubleshooting \u00b6 Issue: \"CUDA out of memory\" \u00b6 Solution : Reduce batch size or use PEFT strategy Issue: \"Accuracy decreasing during training\" \u00b6 Solution : Lower learning rate, reduce epochs, use regularization Issue: \"Model not improving after training\" \u00b6 Solution : Increase learning rate, use different scheduler, increase epochs Issue: \"PEFT not significantly faster\" \u00b6 Solution : Use lower rank (r=4), verify LoRA is actually applied 8. Next Steps \u00b6 PEFT & LoRA Details - Deep dive into LoRA theory Hyperparameter Tuning - Optimize model performance Model Selection - Choose right model for your task Choose the right strategy for your use case and resource constraints!","title":"Tuning Strategies"},{"location":"user-guide/tuning-strategies/#tuning-strategies","text":"TabTune provides three distinct tuning strategies to accommodate different use cases, computational budgets, and performance requirements. This guide explains each strategy in detail, including when to use them and their tradeoffs.","title":"Tuning Strategies"},{"location":"user-guide/tuning-strategies/#1-overview","text":"Strategy Training Use Case Memory Speed Accuracy inference None Baseline, zero-shot Minimal Fast Baseline base-ft Full params High accuracy, ample resources High Slow Highest peft LoRA adapters Memory-constrained, iteration Low Medium High","title":"1. Overview"},{"location":"user-guide/tuning-strategies/#2-inference-strategy","text":"","title":"2. Inference Strategy"},{"location":"user-guide/tuning-strategies/#definition","text":"Zero-shot inference using pre-trained model weights without any training on your data.","title":"Definition"},{"location":"user-guide/tuning-strategies/#use-cases","text":"Quick baseline comparisons Evaluating out-of-the-box model performance Time-constrained scenarios Testing data preprocessing pipeline","title":"Use Cases"},{"location":"user-guide/tuning-strategies/#workflow","text":"flowchart LR A[Raw Data] --> B[DataProcessor] B --> C[Load Pre-trained Model] C --> D[Forward Pass Only] D --> E[Predictions]","title":"Workflow"},{"location":"user-guide/tuning-strategies/#implementation","text":"from tabtune import TabularPipeline # No training occurs pipeline = TabularPipeline ( model_name = 'TabPFN' , task_type = 'classification' , tuning_strategy = 'inference' , tuning_params = { 'device' : 'cuda' } ) # fit() only applies preprocessing; no model training pipeline . fit ( X_train , y_train ) # Direct prediction on test data predictions = pipeline . predict ( X_test ) metrics = pipeline . evaluate ( X_test , y_test )","title":"Implementation"},{"location":"user-guide/tuning-strategies/#advantages","text":"\u2705 No training time needed \u2705 Minimal memory footprint \u2705 Immediate results \u2705 Good for baseline comparisons","title":"Advantages"},{"location":"user-guide/tuning-strategies/#disadvantages","text":"\u274c Generic pre-trained weights may not fit your data \u274c Typically lower accuracy than fine-tuned models \u274c Cannot adapt to task-specific patterns","title":"Disadvantages"},{"location":"user-guide/tuning-strategies/#performance-profile","text":"Training time : 0 seconds Memory usage : 2-4 GB (model + data) Inference latency : 10-50 ms per batch","title":"Performance Profile"},{"location":"user-guide/tuning-strategies/#example-with-all-models","text":"from tabtune import TabularPipeline models = [ 'TabPFN' , 'TabICL' , 'TabDPT' , 'Mitra' , 'ContextTab' , 'OrionMSP' , 'OrionBix' ] for model in models : pipeline = TabularPipeline ( model_name = model , tuning_strategy = 'inference' ) pipeline . fit ( X_train , y_train ) metrics = pipeline . evaluate ( X_test , y_test ) print ( f \" { model } - Accuracy: { metrics [ 'accuracy' ] : .4f } \" )","title":"Example with All Models"},{"location":"user-guide/tuning-strategies/#3-base-fine-tuning-strategy-base-ft","text":"","title":"3. Base Fine-Tuning Strategy (base-ft)"},{"location":"user-guide/tuning-strategies/#definition_1","text":"Full-parameter fine-tuning where all model weights are updated during training.","title":"Definition"},{"location":"user-guide/tuning-strategies/#use-cases_1","text":"Maximum accuracy is priority Abundant computational resources (GPU, RAM) Large training datasets (>100K samples) Production models requiring best performance Transfer learning from related domains","title":"Use Cases"},{"location":"user-guide/tuning-strategies/#workflow_1","text":"flowchart LR A[Raw Data] --> B[DataProcessor] B --> C[Load Pre-trained Model] C --> D[Update ALL Parameters] D --> E[Training Loop] E --> F[Fine-tuned Model] F --> G[Predictions]","title":"Workflow"},{"location":"user-guide/tuning-strategies/#implementation_1","text":"from tabtune import TabularPipeline pipeline = TabularPipeline ( model_name = 'OrionMSP' , task_type = 'classification' , tuning_strategy = 'base-ft' , tuning_params = { 'device' : 'cuda' , 'epochs' : 5 , 'learning_rate' : 2e-5 , 'batch_size' : 32 , 'show_progress' : True , 'gradient_accumulation_steps' : 2 , # optional 'mixed_precision' : 'fp16' # optional } ) # Full training occurs during fit() pipeline . fit ( X_train , y_train ) # Use fine-tuned model for predictions predictions = pipeline . predict ( X_test ) metrics = pipeline . evaluate ( X_test , y_test )","title":"Implementation"},{"location":"user-guide/tuning-strategies/#supported-parameters","text":"Parameter Type Default Description device str 'cpu' 'cuda' or 'cpu' epochs int 3 Number of training epochs learning_rate float 2e-5 Optimizer learning rate batch_size int 32 Samples per batch optimizer str 'adamw' 'adamw' or 'sgd' show_progress bool True Display training progress bar","title":"Supported Parameters"},{"location":"user-guide/tuning-strategies/#advantages_1","text":"\u2705 Highest accuracy potential \u2705 Fully adapts to task-specific patterns \u2705 Works with any dataset size \u2705 Best for production models \u2705 Supports all hyperparameter tuning","title":"Advantages"},{"location":"user-guide/tuning-strategies/#disadvantages_1","text":"\u274c High memory consumption (8-16GB+) \u274c Long training time (hours for large models) \u274c Risk of overfitting on small datasets \u274c Requires careful hyperparameter tuning \u274c GPU memory can become bottleneck","title":"Disadvantages"},{"location":"user-guide/tuning-strategies/#performance-profile_1","text":"Training time : 30 minutes - 2 hours (depending on dataset) Memory usage : 12-24 GB (full model + gradients + optimizer states) Inference latency : 10-50 ms per batch","title":"Performance Profile"},{"location":"user-guide/tuning-strategies/#training-loop-details","text":"The training process follows this pattern: Initialize optimizer (AdamW with weight decay) For each epoch : Shuffle training data For each batch : Forward pass through model Compute loss Backward pass (compute gradients) Clip gradients if specified Update weights Update learning rate scheduler Validate on development set (if available) Save best checkpoint based on validation metric Return fine-tuned model","title":"Training Loop Details"},{"location":"user-guide/tuning-strategies/#example-full-training-pipeline","text":"from tabtune import TabularPipeline from sklearn.model_selection import train_test_split # Load and split data X , y = load_your_dataset () X_train , X_val , y_train , y_val = train_test_split ( X , y , test_size = 0.2 ) # Configure base fine-tuning pipeline = TabularPipeline ( model_name = 'TabDPT' , task_type = 'classification' , tuning_strategy = 'base-ft' , tuning_params = { 'device' : 'cuda' , 'epochs' : 10 , 'learning_rate' : 1e-5 , 'batch_size' : 64 , 'scheduler' : 'cosine' , 'warmup_steps' : 500 , 'mixed_precision' : 'fp16' , 'show_progress' : True , 'save_checkpoint_path' : 'best_model.pt' } ) # Train on training data pipeline . fit ( X_train , y_train ) # Evaluate on validation set val_metrics = pipeline . evaluate ( X_val , y_val ) print ( f \"Validation Accuracy: { val_metrics [ 'accuracy' ] : .4f } \" ) # Save for later use pipeline . save ( 'fintuned_pipeline.joblib' )","title":"Example: Full Training Pipeline"},{"location":"user-guide/tuning-strategies/#4-peft-fine-tuning-strategy-peft","text":"","title":"4. PEFT Fine-Tuning Strategy (peft)"},{"location":"user-guide/tuning-strategies/#definition_2","text":"Parameter-Efficient Fine-Tuning using LoRA (Low-Rank Adaptation) where only small adapter weights are trained while base model is frozen.","title":"Definition"},{"location":"user-guide/tuning-strategies/#how-lora-works","text":"","title":"How LoRA Works"},{"location":"user-guide/tuning-strategies/#use-cases_2","text":"Limited GPU memory (< 8 GB) Quick iteration cycles Fine-tuning multiple models simultaneously Rapid experimentation Deployment with minimal storage","title":"Use Cases"},{"location":"user-guide/tuning-strategies/#workflow_2","text":"flowchart LR A[Raw Data] --> B[DataProcessor] B --> C[Load Pre-trained Model] C --> D[Inject LoRA Adapters] D --> E[Update ONLY Adapters] E --> F[Training Loop] F --> G[Model + LoRA Weights] G --> H[Predictions]","title":"Workflow"},{"location":"user-guide/tuning-strategies/#implementation_2","text":"from tabtune import TabularPipeline pipeline = TabularPipeline ( model_name = 'Mitra' , task_type = 'classification' , tuning_strategy = 'peft' , tuning_params = { 'device' : 'cuda' , 'epochs' : 5 , 'learning_rate' : 2e-4 , 'peft_config' : { 'r' : 8 , 'lora_alpha' : 16 , 'lora_dropout' : 0.05 , 'target_modules' : None # Uses model defaults if None }, 'show_progress' : True } ) # Training with LoRA adapters pipeline . fit ( X_train , y_train ) # Predictions with adapted model predictions = pipeline . predict ( X_test ) metrics = pipeline . evaluate ( X_test , y_test )","title":"Implementation"},{"location":"user-guide/tuning-strategies/#peft-parameters","text":"Parameter Type Default Description r int 8 LoRA rank (lower = more compression) lora_alpha int 16 Scaling factor for LoRA output lora_dropout float 0.05 Dropout applied to LoRA input target_modules list None Which linear layers to adapt (None = use defaults)","title":"PEFT Parameters"},{"location":"user-guide/tuning-strategies/#model-specific-lora-targets","text":"TabTune pre-configures optimal target modules per model: TabICL/OrionBix : col_embedder.tf_col, row_interactor, icl_predictor.tf_icl, icl_predictor.decoder TabDPT : transformer_encoder, encoder, y_encoder, head Mitra : x_embedding, layers, final_layer ContextTab : in_context_encoder, dense, output_head, embeddings TabPFN (\u26a0\ufe0f Experimental): encoder.5.layer, y_encoder.2.layer, transformer_encoder.layers, decoder_dict.standard","title":"Model-Specific LoRA Targets"},{"location":"user-guide/tuning-strategies/#advantages_2","text":"\u2705 90% memory reduction vs base-ft \u2705 2-3x faster training \u2705 Only stores small adapter weights \u2705 Can run on 4GB GPUs \u2705 Fast iteration for experimentation","title":"Advantages"},{"location":"user-guide/tuning-strategies/#disadvantages_2","text":"\u274c Slightly lower accuracy than base-ft (~2-5% in practice) \u274c Not all model layers adapted (frozen backbone limits flexibility) \u274c May struggle with very different tasks \u274c Experimental support on TabPFN and ContextTab","title":"Disadvantages"},{"location":"user-guide/tuning-strategies/#performance-profile_2","text":"Training time : 10-30 minutes Memory usage : 3-6 GB (adapters + activations only) Inference latency : 10-50 ms per batch Model size : Original model size + 1-2% (adapters)","title":"Performance Profile"},{"location":"user-guide/tuning-strategies/#parameter-tuning-guidelines","text":"Rank Selection : r = 4 \u2192 Highest compression, faster, lower accuracy r = 8 \u2192 Good balance (default) r = 16 \u2192 More expressive, slower, higher accuracy r = 32 \u2192 Close to base-ft, but still compressed Alpha Selection : lora_alpha should typically be 2x the rank r=8 \u2192 lora_alpha=16 r=16 \u2192 lora_alpha=32 Dropout Selection : lora_dropout=0.0 \u2192 No regularization lora_dropout=0.05 \u2192 Light regularization (default) lora_dropout=0.1 \u2192 Strong regularization","title":"Parameter Tuning Guidelines"},{"location":"user-guide/tuning-strategies/#example-peft-training-with-hyperparameter-tuning","text":"from tabtune import TabularPipeline # Experiment with different LoRA ranks for r in [ 4 , 8 , 16 ]: pipeline = TabularPipeline ( model_name = 'TabICL' , tuning_strategy = 'peft' , tuning_params = { 'device' : 'cuda' , 'epochs' : 5 , 'learning_rate' : 2e-4 , 'peft_config' : { 'r' : r , 'lora_alpha' : 2 * r , 'lora_dropout' : 0.05 } } ) pipeline . fit ( X_train , y_train ) metrics = pipeline . evaluate ( X_test , y_test ) print ( f \"Rank { r } : Accuracy = { metrics [ 'accuracy' ] : .4f } \" )","title":"Example: PEFT Training with Hyperparameter Tuning"},{"location":"user-guide/tuning-strategies/#5-strategy-comparison-decision-tree","text":"","title":"5. Strategy Comparison &amp; Decision Tree"},{"location":"user-guide/tuning-strategies/#quick-comparison-table","text":"Aspect inference base-ft peft Training No Yes, all params Yes, adapters only Memory \u2b50\u2b50 \u2b50\u2b50\u2b50\u2b50\u2b50 \u2b50\u2b50\u2b50 Speed \u2b50\u2b50\u2b50\u2b50\u2b50 \u2b50 \u2b50\u2b50\u2b50 Accuracy \u2b50\u2b50\u2b50 \u2b50\u2b50\u2b50\u2b50\u2b50 \u2b50\u2b50\u2b50\u2b50 Cost Free High GPU cost Low GPU cost Production \u274c \u2705 \u2705","title":"Quick Comparison Table"},{"location":"user-guide/tuning-strategies/#decision-tree","text":"Start: Which strategy? \u2502 \u251c\u2500 \"I want instant results, no training\" \u2192 inference \u2502 \u2514\u2500 Best for: Baseline, quick exploration \u2502 \u251c\u2500 \"I have limited resources (<8GB GPU)\" \u2192 peft \u2502 \u2514\u2500 Best for: Rapid iteration, memory-constrained \u2502 \u2514\u2500 \"I need best accuracy, have resources\" \u2192 base-ft \u2514\u2500 Best for: Production, large datasets, high accuracy","title":"Decision Tree"},{"location":"user-guide/tuning-strategies/#6-best-practices","text":"Start with inference to establish baseline Use PEFT for exploration when resources are limited Switch to base-ft for production models Monitor for overfitting on small datasets Save checkpoints for long training runs Use validation set to track progress Start with default hyperparameters then tune","title":"6. Best Practices"},{"location":"user-guide/tuning-strategies/#7-troubleshooting","text":"","title":"7. Troubleshooting"},{"location":"user-guide/tuning-strategies/#issue-cuda-out-of-memory","text":"Solution : Reduce batch size or use PEFT strategy","title":"Issue: \"CUDA out of memory\""},{"location":"user-guide/tuning-strategies/#issue-accuracy-decreasing-during-training","text":"Solution : Lower learning rate, reduce epochs, use regularization","title":"Issue: \"Accuracy decreasing during training\""},{"location":"user-guide/tuning-strategies/#issue-model-not-improving-after-training","text":"Solution : Increase learning rate, use different scheduler, increase epochs","title":"Issue: \"Model not improving after training\""},{"location":"user-guide/tuning-strategies/#issue-peft-not-significantly-faster","text":"Solution : Use lower rank (r=4), verify LoRA is actually applied","title":"Issue: \"PEFT not significantly faster\""},{"location":"user-guide/tuning-strategies/#8-next-steps","text":"PEFT & LoRA Details - Deep dive into LoRA theory Hyperparameter Tuning - Optimize model performance Model Selection - Choose right model for your task Choose the right strategy for your use case and resource constraints!","title":"8. Next Steps"}]}