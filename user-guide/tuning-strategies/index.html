<!DOCTYPE html>

<html lang="en">
<head>
<meta charset="utf-8"/>
<meta content="IE=edge" http-equiv="X-UA-Compatible"/>
<meta content="width=device-width, initial-scale=1.0" name="viewport"/>
<meta content="A Unified Library for Inference and Fine-Tuning Tabular Foundation Models" name="description"/>
<meta content="Lexsi Labs" name="author"/>
<link href="../../img/favicon.ico" rel="shortcut icon"/>
<title>Tuning Strategies - TabTune Documentation</title>
<link href="https://use.fontawesome.com/releases/v5.12.0/css/all.css" rel="stylesheet"/>
<link href="https://use.fontawesome.com/releases/v5.12.0/css/v4-shims.css" rel="stylesheet"/>
<link href="//cdn.jsdelivr.net/npm/hack-font@3.3.0/build/web/hack.min.css" rel="stylesheet"/>
<link href="//rsms.me/inter/inter.css" rel="stylesheet" type="text/css"/>
<link href="//fonts.googleapis.com/css?family=Open+Sans:300italic,400italic,700italic,400,300,600,700&amp;subset=latin-ext,latin" rel="stylesheet" type="text/css"/>
<link href="../../css/bootstrap-custom.min.css" rel="stylesheet"/>
<link href="../../css/base.min.css" rel="stylesheet"/>
<link href="../../css/cinder.min.css" rel="stylesheet"/>
<link href="//cdn.jsdelivr.net/gh/highlightjs/cdn-release@9.18.0/build/styles/github.min.css" rel="stylesheet"/>
<link href="../../assets/_mkdocstrings.css" rel="stylesheet"/>
<link href="../../assets/overrides.css" rel="stylesheet"/>
<!-- HTML5 shim and Respond.js IE8 support of HTML5 elements and media queries -->
<!--[if lt IE 9]>
            <script src="https://cdn.jsdelivr.net/npm/html5shiv@3.7.3/dist/html5shiv.min.js"></script>
            <script src="https://cdn.jsdelivr.net/npm/respond.js@1.4.2/dest/respond.min.js"></script>
        <![endif]-->
<link href="../../assets/lexsilabs.ico" rel="icon"/>
<link href="../../assets/lexsilabs.ico" rel="shortcut icon"/>
</head>
<body>
<div class="navbar navbar-default navbar-fixed-top" role="navigation">
<div class="container">
<!-- Collapsed navigation -->
<div class="navbar-header">
<!-- Expander button -->
<button class="navbar-toggle" data-target=".navbar-collapse" data-toggle="collapse" type="button">
<span class="sr-only">Toggle navigation</span>
<span class="icon-bar"></span>
<span class="icon-bar"></span>
<span class="icon-bar"></span>
</button>
<!-- Main title -->
<a class="navbar-brand" href="../..">TabTune Documentation</a>
</div>
<!-- Expanded navigation -->
<div class="navbar-collapse collapse">
<!-- Main navigation -->
<ul class="nav navbar-nav">
<li class="dropdown">
<a class="dropdown-toggle" data-toggle="dropdown" href="#">Getting Started <b class="caret"></b></a>
<ul class="dropdown-menu">
<li>
<a href="../../getting-started/installation/">Installation</a>
</li>
<li>
<a href="../../getting-started/quick-start/">Quick Start</a>
</li>
<li>
<a href="../../getting-started/basic-concepts/">Basic Concepts</a>
</li>
</ul>
</li>
<li class="dropdown active">
<a class="dropdown-toggle" data-toggle="dropdown" href="#">User Guide <b class="caret"></b></a>
<ul class="dropdown-menu">
<li>
<a href="../pipeline-overview/">TabularPipeline Overview</a>
</li>
<li>
<a href="../data-processing/">Data Processing</a>
</li>
<li class="active">
<a href="./">Tuning Strategies</a>
</li>
<li>
<a href="../model-selection/">Model Selection</a>
</li>
<li>
<a href="../saving-loading/">Saving and Loading</a>
</li>
<li>
<a href="../leaderboard/">Model Comparison</a>
</li>
<li>
<a href="../troubleshooting/">Troubleshooting</a>
</li>
</ul>
</li>
<li class="dropdown">
<a class="dropdown-toggle" data-toggle="dropdown" href="#">Models <b class="caret"></b></a>
<ul class="dropdown-menu">
<li>
<a href="../../models/overview/">Overview</a>
</li>
<li>
<a href="../../models/tabpfn/">TabPFN</a>
</li>
<li>
<a href="../../models/tabicl/">TabICL</a>
</li>
<li>
<a href="../../models/orion-msp/">Orion MSP</a>
</li>
<li>
<a href="../../models/orion-bix/">Orion BIX</a>
</li>
<li>
<a href="../../models/tabdpt/">TabDPT</a>
</li>
<li>
<a href="../../models/mitra/">Mitra</a>
</li>
<li>
<a href="../../models/contexttab/">ConTextTab</a>
</li>
</ul>
</li>
<li class="dropdown">
<a class="dropdown-toggle" data-toggle="dropdown" href="#">Advanced Topics <b class="caret"></b></a>
<ul class="dropdown-menu">
<li>
<a href="../../advanced/peft-lora/">PEFT &amp; LoRA</a>
</li>
<li>
<a href="../../advanced/custom-preprocessing/">Custom Preprocessing</a>
</li>
<li>
<a href="../../advanced/hyperparameter-tuning/">Hyperparameter Tuning</a>
</li>
<li>
<a href="../../advanced/memory-optimization/">Memory Optimization</a>
</li>
<li>
<a href="../../advanced/multi-gpu/">Multi-GPU Training</a>
</li>
</ul>
</li>
<li class="dropdown">
<a class="dropdown-toggle" data-toggle="dropdown" href="#">API Reference <b class="caret"></b></a>
<ul class="dropdown-menu">
<li>
<a href="../../api/pipeline/">TabularPipeline</a>
</li>
<li>
<a href="../../api/data-processor/">DataProcessor</a>
</li>
<li>
<a href="../../api/tuning-manager/">TuningManager</a>
</li>
<li>
<a href="../../api/leaderboard/">TabularLeaderboard</a>
</li>
<li>
<a href="../../api/peft-utils/">PEFT Utils</a>
</li>
</ul>
</li>
<li class="dropdown">
<a class="dropdown-toggle" data-toggle="dropdown" href="#">Examples <b class="caret"></b></a>
<ul class="dropdown-menu">
<li>
<a href="../../examples/classification/">Classification Tasks</a>
</li>
<li>
<a href="../../examples/peft-examples/">PEFT Fine-Tuning</a>
</li>
<li>
<a href="../../examples/benchmarking/">Benchmarking</a>
</li>
</ul>
</li>
<li class="dropdown">
<a class="dropdown-toggle" data-toggle="dropdown" href="#">Project <b class="caret"></b></a>
<ul class="dropdown-menu">
<li class="dropdown-submenu">
<a href="" tabindex="-1">Contributing</a>
<ul class="dropdown-menu">
<li>
<a href="../../contributing/setup/">Development Setup</a>
</li>
<li>
<a href="../../contributing/standards/">Code Standards</a>
</li>
<li>
<a href="../../contributing/new-models/">Adding New Models</a>
</li>
<li>
<a href="../../contributing/documentation/">Documentation Guide</a>
</li>
</ul>
</li>
<li class="dropdown-submenu">
<a href="" tabindex="-1">About</a>
<ul class="dropdown-menu">
<li>
<a href="../../about/release-notes/">Release Notes</a>
</li>
<li>
<a href="../../about/roadmap/">Roadmap</a>
</li>
<li>
<a href="../../about/faq/">FAQ</a>
</li>
<li>
<a href="../../about/license/">License</a>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<ul class="nav navbar-nav navbar-right">
<li>
<a data-target="#mkdocs_search_modal" data-toggle="modal" href="#">
<i class="fas fa-search"></i> Search
                        </a>
</li>
<li>
<a href="../data-processing/" rel="prev">
<i class="fas fa-arrow-left"></i> Previous
                        </a>
</li>
<li>
<a href="../model-selection/" rel="next">
                            Next <i class="fas fa-arrow-right"></i>
</a>
</li>
<li>
<a href="https://github.com/Lexsi-Labs/TabTune/edit/master/docs/user-guide/tuning-strategies.md">Edit on Lexsi-Labs/TabTune</a>
</li>
</ul>
</div>
</div>
</div>
<div class="container">
<div class="col-md-3"><div class="bs-sidebar hidden-print affix well" role="complementary">
<ul class="nav bs-sidenav">
<li class="first-level active"><a href="#tuning-strategies">Tuning Strategies</a></li>
<li class="second-level"><a href="#1-overview">1. Overview</a></li>
<li class="second-level"><a href="#2-inference-strategy">2. Inference Strategy</a></li>
<li class="third-level"><a href="#definition">Definition</a></li>
<li class="third-level"><a href="#use-cases">Use Cases</a></li>
<li class="third-level"><a href="#workflow">Workflow</a></li>
<li class="third-level"><a href="#implementation">Implementation</a></li>
<li class="third-level"><a href="#advantages">Advantages</a></li>
<li class="third-level"><a href="#disadvantages">Disadvantages</a></li>
<li class="third-level"><a href="#performance-profile">Performance Profile</a></li>
<li class="third-level"><a href="#example-with-all-models">Example with All Models</a></li>
<li class="second-level"><a href="#3-base-fine-tuning-strategy-base-ft">3. Base Fine-Tuning Strategy (base-ft)</a></li>
<li class="third-level"><a href="#definition_1">Definition</a></li>
<li class="third-level"><a href="#use-cases_1">Use Cases</a></li>
<li class="third-level"><a href="#workflow_1">Workflow</a></li>
<li class="third-level"><a href="#implementation_1">Implementation</a></li>
<li class="third-level"><a href="#supported-parameters">Supported Parameters</a></li>
<li class="third-level"><a href="#advantages_1">Advantages</a></li>
<li class="third-level"><a href="#disadvantages_1">Disadvantages</a></li>
<li class="third-level"><a href="#performance-profile_1">Performance Profile</a></li>
<li class="third-level"><a href="#training-loop-details">Training Loop Details</a></li>
<li class="third-level"><a href="#example-full-training-pipeline">Example: Full Training Pipeline</a></li>
<li class="second-level"><a href="#4-peft-fine-tuning-strategy-peft">4. PEFT Fine-Tuning Strategy (peft)</a></li>
<li class="third-level"><a href="#definition_2">Definition</a></li>
<li class="third-level"><a href="#how-lora-works">How LoRA Works</a></li>
<li class="third-level"><a href="#use-cases_2">Use Cases</a></li>
<li class="third-level"><a href="#workflow_2">Workflow</a></li>
<li class="third-level"><a href="#implementation_2">Implementation</a></li>
<li class="third-level"><a href="#peft-parameters">PEFT Parameters</a></li>
<li class="third-level"><a href="#model-specific-lora-targets">Model-Specific LoRA Targets</a></li>
<li class="third-level"><a href="#advantages_2">Advantages</a></li>
<li class="third-level"><a href="#disadvantages_2">Disadvantages</a></li>
<li class="third-level"><a href="#performance-profile_2">Performance Profile</a></li>
<li class="third-level"><a href="#parameter-tuning-guidelines">Parameter Tuning Guidelines</a></li>
<li class="third-level"><a href="#example-peft-training-with-hyperparameter-tuning">Example: PEFT Training with Hyperparameter Tuning</a></li>
<li class="second-level"><a href="#5-strategy-comparison-decision-tree">5. Strategy Comparison &amp; Decision Tree</a></li>
<li class="third-level"><a href="#quick-comparison-table">Quick Comparison Table</a></li>
<li class="third-level"><a href="#decision-tree">Decision Tree</a></li>
<li class="second-level"><a href="#6-best-practices">6. Best Practices</a></li>
<li class="second-level"><a href="#7-troubleshooting">7. Troubleshooting</a></li>
<li class="third-level"><a href="#issue-cuda-out-of-memory">Issue: "CUDA out of memory"</a></li>
<li class="third-level"><a href="#issue-accuracy-decreasing-during-training">Issue: "Accuracy decreasing during training"</a></li>
<li class="third-level"><a href="#issue-model-not-improving-after-training">Issue: "Model not improving after training"</a></li>
<li class="third-level"><a href="#issue-peft-not-significantly-faster">Issue: "PEFT not significantly faster"</a></li>
<li class="second-level"><a href="#8-next-steps">8. Next Steps</a></li>
</ul>
</div></div>
<div class="col-md-9" role="main">
<h1 id="tuning-strategies">Tuning Strategies<a class="headerlink" href="#tuning-strategies" title="Permanent link">¶</a></h1>
<p>TabTune provides three distinct tuning strategies to accommodate different use cases, computational budgets, and performance requirements. This guide explains each strategy in detail, including when to use them and their tradeoffs.</p>
<hr/>
<h2 id="1-overview">1. Overview<a class="headerlink" href="#1-overview" title="Permanent link">¶</a></h2>
<table>
<thead>
<tr>
<th>Strategy</th>
<th>Training</th>
<th>Use Case</th>
<th>Memory</th>
<th>Speed</th>
<th>Accuracy</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>inference</strong></td>
<td>None</td>
<td>Baseline, zero-shot</td>
<td>Minimal</td>
<td>Fast</td>
<td>Baseline</td>
</tr>
<tr>
<td><strong>base-ft</strong></td>
<td>Full params</td>
<td>High accuracy, ample resources</td>
<td>High</td>
<td>Slow</td>
<td>Highest</td>
</tr>
<tr>
<td><strong>peft</strong></td>
<td>LoRA adapters</td>
<td>Memory-constrained, iteration</td>
<td>Low</td>
<td>Medium</td>
<td>High</td>
</tr>
</tbody>
</table>
<hr/>
<h2 id="2-inference-strategy">2. Inference Strategy<a class="headerlink" href="#2-inference-strategy" title="Permanent link">¶</a></h2>
<h3 id="definition">Definition<a class="headerlink" href="#definition" title="Permanent link">¶</a></h3>
<p><strong>Zero-shot inference</strong> using pre-trained model weights without any training on your data.</p>
<h3 id="use-cases">Use Cases<a class="headerlink" href="#use-cases" title="Permanent link">¶</a></h3>
<ul>
<li>Quick baseline comparisons</li>
<li>Evaluating out-of-the-box model performance</li>
<li>Time-constrained scenarios</li>
<li>Testing data preprocessing pipeline</li>
</ul>
<h3 id="workflow">Workflow<a class="headerlink" href="#workflow" title="Permanent link">¶</a></h3>
<div class="mermaid">flowchart LR
    A[Raw Data] --&gt; B[DataProcessor]
    B --&gt; C[Load Pre-trained Model]
    C --&gt; D[Forward Pass Only]
    D --&gt; E[Predictions]
</div>
<h3 id="implementation">Implementation<a class="headerlink" href="#implementation" title="Permanent link">¶</a></h3>
<div class="highlight"><pre><span></span><code><span class="kn">from</span><span class="w"> </span><span class="nn">tabtune</span><span class="w"> </span><span class="kn">import</span> <span class="n">TabularPipeline</span>

<span class="c1"># No training occurs</span>
<span class="n">pipeline</span> <span class="o">=</span> <span class="n">TabularPipeline</span><span class="p">(</span>
    <span class="n">model_name</span><span class="o">=</span><span class="s1">'TabPFN'</span><span class="p">,</span>
    <span class="n">task_type</span><span class="o">=</span><span class="s1">'classification'</span><span class="p">,</span>
    <span class="n">tuning_strategy</span><span class="o">=</span><span class="s1">'inference'</span><span class="p">,</span>
    <span class="n">tuning_params</span><span class="o">=</span><span class="p">{</span><span class="s1">'device'</span><span class="p">:</span> <span class="s1">'cuda'</span><span class="p">}</span>
<span class="p">)</span>

<span class="c1"># fit() only applies preprocessing; no model training</span>
<span class="n">pipeline</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="c1"># Direct prediction on test data</span>
<span class="n">predictions</span> <span class="o">=</span> <span class="n">pipeline</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
<span class="n">metrics</span> <span class="o">=</span> <span class="n">pipeline</span><span class="o">.</span><span class="n">evaluate</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span>
</code></pre></div>
<h3 id="advantages">Advantages<a class="headerlink" href="#advantages" title="Permanent link">¶</a></h3>
<ul>
<li>✅ No training time needed</li>
<li>✅ Minimal memory footprint</li>
<li>✅ Immediate results</li>
<li>✅ Good for baseline comparisons</li>
</ul>
<h3 id="disadvantages">Disadvantages<a class="headerlink" href="#disadvantages" title="Permanent link">¶</a></h3>
<ul>
<li>❌ Generic pre-trained weights may not fit your data</li>
<li>❌ Typically lower accuracy than fine-tuned models</li>
<li>❌ Cannot adapt to task-specific patterns</li>
</ul>
<h3 id="performance-profile">Performance Profile<a class="headerlink" href="#performance-profile" title="Permanent link">¶</a></h3>
<ul>
<li><strong>Training time</strong>: 0 seconds</li>
<li><strong>Memory usage</strong>: 2-4 GB (model + data)</li>
<li><strong>Inference latency</strong>: 10-50 ms per batch</li>
</ul>
<h3 id="example-with-all-models">Example with All Models<a class="headerlink" href="#example-with-all-models" title="Permanent link">¶</a></h3>
<div class="highlight"><pre><span></span><code><span class="kn">from</span><span class="w"> </span><span class="nn">tabtune</span><span class="w"> </span><span class="kn">import</span> <span class="n">TabularPipeline</span>

<span class="n">models</span> <span class="o">=</span> <span class="p">[</span><span class="s1">'TabPFN'</span><span class="p">,</span> <span class="s1">'TabICL'</span><span class="p">,</span> <span class="s1">'TabDPT'</span><span class="p">,</span> <span class="s1">'Mitra'</span><span class="p">,</span> <span class="s1">'ContextTab'</span><span class="p">,</span> <span class="s1">'OrionMSP'</span><span class="p">,</span><span class="s1">'OrionBix'</span><span class="p">]</span>

<span class="k">for</span> <span class="n">model</span> <span class="ow">in</span> <span class="n">models</span><span class="p">:</span>
    <span class="n">pipeline</span> <span class="o">=</span> <span class="n">TabularPipeline</span><span class="p">(</span>
        <span class="n">model_name</span><span class="o">=</span><span class="n">model</span><span class="p">,</span>
        <span class="n">tuning_strategy</span><span class="o">=</span><span class="s1">'inference'</span>
    <span class="p">)</span>
    <span class="n">pipeline</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
    <span class="n">metrics</span> <span class="o">=</span> <span class="n">pipeline</span><span class="o">.</span><span class="n">evaluate</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"</span><span class="si">{</span><span class="n">model</span><span class="si">}</span><span class="s2"> - Accuracy: </span><span class="si">{</span><span class="n">metrics</span><span class="p">[</span><span class="s1">'accuracy'</span><span class="p">]</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
</code></pre></div>
<hr/>
<h2 id="3-base-fine-tuning-strategy-base-ft">3. Base Fine-Tuning Strategy (<code>base-ft</code>)<a class="headerlink" href="#3-base-fine-tuning-strategy-base-ft" title="Permanent link">¶</a></h2>
<h3 id="definition_1">Definition<a class="headerlink" href="#definition_1" title="Permanent link">¶</a></h3>
<p><strong>Full-parameter fine-tuning</strong> where all model weights are updated during training.</p>
<h3 id="use-cases_1">Use Cases<a class="headerlink" href="#use-cases_1" title="Permanent link">¶</a></h3>
<ul>
<li>Maximum accuracy is priority</li>
<li>Abundant computational resources (GPU, RAM)</li>
<li>Large training datasets (&gt;100K samples)</li>
<li>Production models requiring best performance</li>
<li>Transfer learning from related domains</li>
</ul>
<h3 id="workflow_1">Workflow<a class="headerlink" href="#workflow_1" title="Permanent link">¶</a></h3>
<div class="mermaid">flowchart LR
    A[Raw Data] --&gt; B[DataProcessor]
    B --&gt; C[Load Pre-trained Model]
    C --&gt; D[Update ALL Parameters]
    D --&gt; E[Training Loop]
    E --&gt; F[Fine-tuned Model]
    F --&gt; G[Predictions]
</div>
<h3 id="implementation_1">Implementation<a class="headerlink" href="#implementation_1" title="Permanent link">¶</a></h3>
<div class="highlight"><pre><span></span><code><span class="kn">from</span><span class="w"> </span><span class="nn">tabtune</span><span class="w"> </span><span class="kn">import</span> <span class="n">TabularPipeline</span>

<span class="n">pipeline</span> <span class="o">=</span> <span class="n">TabularPipeline</span><span class="p">(</span>
    <span class="n">model_name</span><span class="o">=</span><span class="s1">'OrionMSP'</span><span class="p">,</span>
    <span class="n">task_type</span><span class="o">=</span><span class="s1">'classification'</span><span class="p">,</span>
    <span class="n">tuning_strategy</span><span class="o">=</span><span class="s1">'base-ft'</span><span class="p">,</span>
    <span class="n">tuning_params</span><span class="o">=</span><span class="p">{</span>
        <span class="s1">'device'</span><span class="p">:</span> <span class="s1">'cuda'</span><span class="p">,</span>
        <span class="s1">'epochs'</span><span class="p">:</span> <span class="mi">5</span><span class="p">,</span>
        <span class="s1">'learning_rate'</span><span class="p">:</span> <span class="mf">2e-5</span><span class="p">,</span>
        <span class="s1">'batch_size'</span><span class="p">:</span> <span class="mi">32</span><span class="p">,</span>
        <span class="s1">'show_progress'</span><span class="p">:</span> <span class="kc">True</span><span class="p">,</span>
        <span class="s1">'gradient_accumulation_steps'</span><span class="p">:</span> <span class="mi">2</span><span class="p">,</span>  <span class="c1"># optional</span>
        <span class="s1">'mixed_precision'</span><span class="p">:</span> <span class="s1">'fp16'</span>  <span class="c1"># optional</span>
    <span class="p">}</span>
<span class="p">)</span>

<span class="c1"># Full training occurs during fit()</span>
<span class="n">pipeline</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="c1"># Use fine-tuned model for predictions</span>
<span class="n">predictions</span> <span class="o">=</span> <span class="n">pipeline</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
<span class="n">metrics</span> <span class="o">=</span> <span class="n">pipeline</span><span class="o">.</span><span class="n">evaluate</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span>
</code></pre></div>
<h3 id="supported-parameters">Supported Parameters<a class="headerlink" href="#supported-parameters" title="Permanent link">¶</a></h3>
<table>
<thead>
<tr>
<th>Parameter</th>
<th>Type</th>
<th>Default</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>device</code></td>
<td>str</td>
<td>'cpu'</td>
<td>'cuda' or 'cpu'</td>
</tr>
<tr>
<td><code>epochs</code></td>
<td>int</td>
<td>3</td>
<td>Number of training epochs</td>
</tr>
<tr>
<td><code>learning_rate</code></td>
<td>float</td>
<td>2e-5</td>
<td>Optimizer learning rate</td>
</tr>
<tr>
<td><code>batch_size</code></td>
<td>int</td>
<td>32</td>
<td>Samples per batch</td>
</tr>
<tr>
<td><code>optimizer</code></td>
<td>str</td>
<td>'adamw'</td>
<td>'adamw' or 'sgd'</td>
</tr>
<tr>
<td><code>show_progress</code></td>
<td>bool</td>
<td>True</td>
<td>Display training progress bar</td>
</tr>
</tbody>
</table>
<h3 id="advantages_1">Advantages<a class="headerlink" href="#advantages_1" title="Permanent link">¶</a></h3>
<ul>
<li>✅ Highest accuracy potential</li>
<li>✅ Fully adapts to task-specific patterns</li>
<li>✅ Works with any dataset size</li>
<li>✅ Best for production models</li>
<li>✅ Supports all hyperparameter tuning</li>
</ul>
<h3 id="disadvantages_1">Disadvantages<a class="headerlink" href="#disadvantages_1" title="Permanent link">¶</a></h3>
<ul>
<li>❌ High memory consumption (8-16GB+)</li>
<li>❌ Long training time (hours for large models)</li>
<li>❌ Risk of overfitting on small datasets</li>
<li>❌ Requires careful hyperparameter tuning</li>
<li>❌ GPU memory can become bottleneck</li>
</ul>
<h3 id="performance-profile_1">Performance Profile<a class="headerlink" href="#performance-profile_1" title="Permanent link">¶</a></h3>
<ul>
<li><strong>Training time</strong>: 30 minutes - 2 hours (depending on dataset)</li>
<li><strong>Memory usage</strong>: 12-24 GB (full model + gradients + optimizer states)</li>
<li><strong>Inference latency</strong>: 10-50 ms per batch</li>
</ul>
<h3 id="training-loop-details">Training Loop Details<a class="headerlink" href="#training-loop-details" title="Permanent link">¶</a></h3>
<p>The training process follows this pattern:</p>
<ol>
<li><strong>Initialize optimizer</strong> (AdamW with weight decay)</li>
<li><strong>For each epoch</strong>:</li>
<li>Shuffle training data</li>
<li><strong>For each batch</strong>:<ul>
<li>Forward pass through model</li>
<li>Compute loss</li>
<li>Backward pass (compute gradients)</li>
<li>Clip gradients if specified</li>
<li>Update weights</li>
<li>Update learning rate scheduler</li>
</ul>
</li>
<li>Validate on development set (if available)</li>
<li><strong>Save best checkpoint</strong> based on validation metric</li>
<li><strong>Return fine-tuned model</strong></li>
</ol>
<h3 id="example-full-training-pipeline">Example: Full Training Pipeline<a class="headerlink" href="#example-full-training-pipeline" title="Permanent link">¶</a></h3>
<div class="highlight"><pre><span></span><code><span class="kn">from</span><span class="w"> </span><span class="nn">tabtune</span><span class="w"> </span><span class="kn">import</span> <span class="n">TabularPipeline</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.model_selection</span><span class="w"> </span><span class="kn">import</span> <span class="n">train_test_split</span>

<span class="c1"># Load and split data</span>
<span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">load_your_dataset</span><span class="p">()</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_val</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_val</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)</span>

<span class="c1"># Configure base fine-tuning</span>
<span class="n">pipeline</span> <span class="o">=</span> <span class="n">TabularPipeline</span><span class="p">(</span>
    <span class="n">model_name</span><span class="o">=</span><span class="s1">'TabDPT'</span><span class="p">,</span>
    <span class="n">task_type</span><span class="o">=</span><span class="s1">'classification'</span><span class="p">,</span>
    <span class="n">tuning_strategy</span><span class="o">=</span><span class="s1">'base-ft'</span><span class="p">,</span>
    <span class="n">tuning_params</span><span class="o">=</span><span class="p">{</span>
        <span class="s1">'device'</span><span class="p">:</span> <span class="s1">'cuda'</span><span class="p">,</span>
        <span class="s1">'epochs'</span><span class="p">:</span> <span class="mi">10</span><span class="p">,</span>
        <span class="s1">'learning_rate'</span><span class="p">:</span> <span class="mf">1e-5</span><span class="p">,</span>
        <span class="s1">'batch_size'</span><span class="p">:</span> <span class="mi">64</span><span class="p">,</span>
        <span class="s1">'scheduler'</span><span class="p">:</span> <span class="s1">'cosine'</span><span class="p">,</span>
        <span class="s1">'warmup_steps'</span><span class="p">:</span> <span class="mi">500</span><span class="p">,</span>
        <span class="s1">'mixed_precision'</span><span class="p">:</span> <span class="s1">'fp16'</span><span class="p">,</span>
        <span class="s1">'show_progress'</span><span class="p">:</span> <span class="kc">True</span><span class="p">,</span>
        <span class="s1">'save_checkpoint_path'</span><span class="p">:</span> <span class="s1">'best_model.pt'</span>
    <span class="p">}</span>
<span class="p">)</span>

<span class="c1"># Train on training data</span>
<span class="n">pipeline</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="c1"># Evaluate on validation set</span>
<span class="n">val_metrics</span> <span class="o">=</span> <span class="n">pipeline</span><span class="o">.</span><span class="n">evaluate</span><span class="p">(</span><span class="n">X_val</span><span class="p">,</span> <span class="n">y_val</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Validation Accuracy: </span><span class="si">{</span><span class="n">val_metrics</span><span class="p">[</span><span class="s1">'accuracy'</span><span class="p">]</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>

<span class="c1"># Save for later use</span>
<span class="n">pipeline</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="s1">'fintuned_pipeline.joblib'</span><span class="p">)</span>
</code></pre></div>
<hr/>
<h2 id="4-peft-fine-tuning-strategy-peft">4. PEFT Fine-Tuning Strategy (<code>peft</code>)<a class="headerlink" href="#4-peft-fine-tuning-strategy-peft" title="Permanent link">¶</a></h2>
<h3 id="definition_2">Definition<a class="headerlink" href="#definition_2" title="Permanent link">¶</a></h3>
<p><strong>Parameter-Efficient Fine-Tuning using LoRA (Low-Rank Adaptation)</strong> where only small adapter weights are trained while base model is frozen.</p>
<h3 id="how-lora-works">How LoRA Works<a class="headerlink" href="#how-lora-works" title="Permanent link">¶</a></h3>
<!-- Instead of updating all weights, LoRA adds trainable low-rank matrices:

\[
\text{output} = W_0 \cdot x + \frac{\alpha}{r} \cdot B(A \cdot x)
\]

Where:
- \(W_0\) is the frozen pre-trained weight
- \(A \in \mathbb{R}^{d_{in} \times r}\) and \(B \in \mathbb{R}^{r \times d_{out}}\) are trainable
- \(r \ll \min(d_{in}, d_{out})\) is the rank

This reduces trainable parameters from millions to thousands. -->
<h3 id="use-cases_2">Use Cases<a class="headerlink" href="#use-cases_2" title="Permanent link">¶</a></h3>
<ul>
<li>Limited GPU memory (&lt; 8 GB)</li>
<li>Quick iteration cycles</li>
<li>Fine-tuning multiple models simultaneously</li>
<li>Rapid experimentation</li>
<li>Deployment with minimal storage</li>
</ul>
<h3 id="workflow_2">Workflow<a class="headerlink" href="#workflow_2" title="Permanent link">¶</a></h3>
<div class="mermaid">flowchart LR
    A[Raw Data] --&gt; B[DataProcessor]
    B --&gt; C[Load Pre-trained Model]
    C --&gt; D[Inject LoRA Adapters]
    D --&gt; E[Update ONLY Adapters]
    E --&gt; F[Training Loop]
    F --&gt; G[Model + LoRA Weights]
    G --&gt; H[Predictions]
</div>
<h3 id="implementation_2">Implementation<a class="headerlink" href="#implementation_2" title="Permanent link">¶</a></h3>
<div class="highlight"><pre><span></span><code><span class="kn">from</span><span class="w"> </span><span class="nn">tabtune</span><span class="w"> </span><span class="kn">import</span> <span class="n">TabularPipeline</span>

<span class="n">pipeline</span> <span class="o">=</span> <span class="n">TabularPipeline</span><span class="p">(</span>
    <span class="n">model_name</span><span class="o">=</span><span class="s1">'Mitra'</span><span class="p">,</span>
    <span class="n">task_type</span><span class="o">=</span><span class="s1">'classification'</span><span class="p">,</span>
    <span class="n">tuning_strategy</span><span class="o">=</span><span class="s1">'peft'</span><span class="p">,</span>
    <span class="n">tuning_params</span><span class="o">=</span><span class="p">{</span>
        <span class="s1">'device'</span><span class="p">:</span> <span class="s1">'cuda'</span><span class="p">,</span>
        <span class="s1">'epochs'</span><span class="p">:</span> <span class="mi">5</span><span class="p">,</span>
        <span class="s1">'learning_rate'</span><span class="p">:</span> <span class="mf">2e-4</span><span class="p">,</span>
        <span class="s1">'peft_config'</span><span class="p">:</span> <span class="p">{</span>
            <span class="s1">'r'</span><span class="p">:</span> <span class="mi">8</span><span class="p">,</span>
            <span class="s1">'lora_alpha'</span><span class="p">:</span> <span class="mi">16</span><span class="p">,</span>
            <span class="s1">'lora_dropout'</span><span class="p">:</span> <span class="mf">0.05</span><span class="p">,</span>
            <span class="s1">'target_modules'</span><span class="p">:</span> <span class="kc">None</span>  <span class="c1"># Uses model defaults if None</span>
        <span class="p">},</span>
        <span class="s1">'show_progress'</span><span class="p">:</span> <span class="kc">True</span>
    <span class="p">}</span>
<span class="p">)</span>

<span class="c1"># Training with LoRA adapters</span>
<span class="n">pipeline</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="c1"># Predictions with adapted model</span>
<span class="n">predictions</span> <span class="o">=</span> <span class="n">pipeline</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
<span class="n">metrics</span> <span class="o">=</span> <span class="n">pipeline</span><span class="o">.</span><span class="n">evaluate</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span>
</code></pre></div>
<h3 id="peft-parameters">PEFT Parameters<a class="headerlink" href="#peft-parameters" title="Permanent link">¶</a></h3>
<table>
<thead>
<tr>
<th>Parameter</th>
<th>Type</th>
<th>Default</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>r</code></td>
<td>int</td>
<td>8</td>
<td>LoRA rank (lower = more compression)</td>
</tr>
<tr>
<td><code>lora_alpha</code></td>
<td>int</td>
<td>16</td>
<td>Scaling factor for LoRA output</td>
</tr>
<tr>
<td><code>lora_dropout</code></td>
<td>float</td>
<td>0.05</td>
<td>Dropout applied to LoRA input</td>
</tr>
<tr>
<td><code>target_modules</code></td>
<td>list</td>
<td>None</td>
<td>Which linear layers to adapt (None = use defaults)</td>
</tr>
</tbody>
</table>
<h3 id="model-specific-lora-targets">Model-Specific LoRA Targets<a class="headerlink" href="#model-specific-lora-targets" title="Permanent link">¶</a></h3>
<p>TabTune pre-configures optimal target modules per model:</p>
<p><strong>TabICL/OrionBix</strong>:
<div class="highlight"><pre><span></span><code>col_embedder.tf_col, row_interactor, icl_predictor.tf_icl, icl_predictor.decoder
</code></pre></div></p>
<p><strong>TabDPT</strong>:
<div class="highlight"><pre><span></span><code>transformer_encoder, encoder, y_encoder, head
</code></pre></div></p>
<p><strong>Mitra</strong>:
<div class="highlight"><pre><span></span><code>x_embedding, layers, final_layer
</code></pre></div></p>
<p><strong>ContextTab</strong>:
<div class="highlight"><pre><span></span><code>in_context_encoder, dense, output_head, embeddings
</code></pre></div></p>
<p><strong>TabPFN</strong> (⚠️ Experimental):
<div class="highlight"><pre><span></span><code>encoder.5.layer, y_encoder.2.layer, transformer_encoder.layers, decoder_dict.standard
</code></pre></div></p>
<h3 id="advantages_2">Advantages<a class="headerlink" href="#advantages_2" title="Permanent link">¶</a></h3>
<ul>
<li>✅ 90% memory reduction vs base-ft</li>
<li>✅ 2-3x faster training</li>
<li>✅ Only stores small adapter weights</li>
<li>✅ Can run on 4GB GPUs</li>
<li>✅ Fast iteration for experimentation</li>
</ul>
<h3 id="disadvantages_2">Disadvantages<a class="headerlink" href="#disadvantages_2" title="Permanent link">¶</a></h3>
<ul>
<li>❌ Slightly lower accuracy than base-ft (~2-5% in practice)</li>
<li>❌ Not all model layers adapted (frozen backbone limits flexibility)</li>
<li>❌ May struggle with very different tasks</li>
<li>❌ Experimental support on TabPFN and ContextTab</li>
</ul>
<h3 id="performance-profile_2">Performance Profile<a class="headerlink" href="#performance-profile_2" title="Permanent link">¶</a></h3>
<ul>
<li><strong>Training time</strong>: 10-30 minutes</li>
<li><strong>Memory usage</strong>: 3-6 GB (adapters + activations only)</li>
<li><strong>Inference latency</strong>: 10-50 ms per batch</li>
<li><strong>Model size</strong>: Original model size + 1-2% (adapters)</li>
</ul>
<h3 id="parameter-tuning-guidelines">Parameter Tuning Guidelines<a class="headerlink" href="#parameter-tuning-guidelines" title="Permanent link">¶</a></h3>
<p><strong>Rank Selection</strong>:
<div class="highlight"><pre><span></span><code>r = 4   → Highest compression, faster, lower accuracy
r = 8   → Good balance (default)
r = 16  → More expressive, slower, higher accuracy
r = 32  → Close to base-ft, but still compressed
</code></pre></div></p>
<p><strong>Alpha Selection</strong>:
<div class="highlight"><pre><span></span><code>lora_alpha should typically be 2x the rank
r=8 → lora_alpha=16
r=16 → lora_alpha=32
</code></pre></div></p>
<p><strong>Dropout Selection</strong>:
<div class="highlight"><pre><span></span><code>lora_dropout=0.0  → No regularization
lora_dropout=0.05 → Light regularization (default)
lora_dropout=0.1  → Strong regularization
</code></pre></div></p>
<h3 id="example-peft-training-with-hyperparameter-tuning">Example: PEFT Training with Hyperparameter Tuning<a class="headerlink" href="#example-peft-training-with-hyperparameter-tuning" title="Permanent link">¶</a></h3>
<div class="highlight"><pre><span></span><code><span class="kn">from</span><span class="w"> </span><span class="nn">tabtune</span><span class="w"> </span><span class="kn">import</span> <span class="n">TabularPipeline</span>

<span class="c1"># Experiment with different LoRA ranks</span>
<span class="k">for</span> <span class="n">r</span> <span class="ow">in</span> <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">16</span><span class="p">]:</span>
    <span class="n">pipeline</span> <span class="o">=</span> <span class="n">TabularPipeline</span><span class="p">(</span>
        <span class="n">model_name</span><span class="o">=</span><span class="s1">'TabICL'</span><span class="p">,</span>
        <span class="n">tuning_strategy</span><span class="o">=</span><span class="s1">'peft'</span><span class="p">,</span>
        <span class="n">tuning_params</span><span class="o">=</span><span class="p">{</span>
            <span class="s1">'device'</span><span class="p">:</span> <span class="s1">'cuda'</span><span class="p">,</span>
            <span class="s1">'epochs'</span><span class="p">:</span> <span class="mi">5</span><span class="p">,</span>
            <span class="s1">'learning_rate'</span><span class="p">:</span> <span class="mf">2e-4</span><span class="p">,</span>
            <span class="s1">'peft_config'</span><span class="p">:</span> <span class="p">{</span>
                <span class="s1">'r'</span><span class="p">:</span> <span class="n">r</span><span class="p">,</span>
                <span class="s1">'lora_alpha'</span><span class="p">:</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">r</span><span class="p">,</span>
                <span class="s1">'lora_dropout'</span><span class="p">:</span> <span class="mf">0.05</span>
            <span class="p">}</span>
        <span class="p">}</span>
    <span class="p">)</span>

    <span class="n">pipeline</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
    <span class="n">metrics</span> <span class="o">=</span> <span class="n">pipeline</span><span class="o">.</span><span class="n">evaluate</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Rank </span><span class="si">{</span><span class="n">r</span><span class="si">}</span><span class="s2">: Accuracy = </span><span class="si">{</span><span class="n">metrics</span><span class="p">[</span><span class="s1">'accuracy'</span><span class="p">]</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
</code></pre></div>
<hr/>
<h2 id="5-strategy-comparison-decision-tree">5. Strategy Comparison &amp; Decision Tree<a class="headerlink" href="#5-strategy-comparison-decision-tree" title="Permanent link">¶</a></h2>
<h3 id="quick-comparison-table">Quick Comparison Table<a class="headerlink" href="#quick-comparison-table" title="Permanent link">¶</a></h3>
<table>
<thead>
<tr>
<th>Aspect</th>
<th>inference</th>
<th>base-ft</th>
<th>peft</th>
</tr>
</thead>
<tbody>
<tr>
<td>Training</td>
<td>No</td>
<td>Yes, all params</td>
<td>Yes, adapters only</td>
</tr>
<tr>
<td>Memory</td>
<td>⭐⭐</td>
<td>⭐⭐⭐⭐⭐</td>
<td>⭐⭐⭐</td>
</tr>
<tr>
<td>Speed</td>
<td>⭐⭐⭐⭐⭐</td>
<td>⭐</td>
<td>⭐⭐⭐</td>
</tr>
<tr>
<td>Accuracy</td>
<td>⭐⭐⭐</td>
<td>⭐⭐⭐⭐⭐</td>
<td>⭐⭐⭐⭐</td>
</tr>
<tr>
<td>Cost</td>
<td>Free</td>
<td>High GPU cost</td>
<td>Low GPU cost</td>
</tr>
<tr>
<td>Production</td>
<td>❌</td>
<td>✅</td>
<td>✅</td>
</tr>
</tbody>
</table>
<h3 id="decision-tree">Decision Tree<a class="headerlink" href="#decision-tree" title="Permanent link">¶</a></h3>
<div class="highlight"><pre><span></span><code>Start: Which strategy?
│
├─ "I want instant results, no training" → inference
│  └─ Best for: Baseline, quick exploration
│
├─ "I have limited resources (&lt;8GB GPU)" → peft
│  └─ Best for: Rapid iteration, memory-constrained
│
└─ "I need best accuracy, have resources" → base-ft
   └─ Best for: Production, large datasets, high accuracy
</code></pre></div>
<hr/>
<h2 id="6-best-practices">6. Best Practices<a class="headerlink" href="#6-best-practices" title="Permanent link">¶</a></h2>
<ol>
<li><strong>Start with inference</strong> to establish baseline</li>
<li><strong>Use PEFT for exploration</strong> when resources are limited</li>
<li><strong>Switch to base-ft for production</strong> models</li>
<li><strong>Monitor for overfitting</strong> on small datasets</li>
<li><strong>Save checkpoints</strong> for long training runs</li>
<li><strong>Use validation set</strong> to track progress</li>
<li><strong>Start with default hyperparameters</strong> then tune</li>
</ol>
<hr/>
<h2 id="7-troubleshooting">7. Troubleshooting<a class="headerlink" href="#7-troubleshooting" title="Permanent link">¶</a></h2>
<h3 id="issue-cuda-out-of-memory">Issue: "CUDA out of memory"<a class="headerlink" href="#issue-cuda-out-of-memory" title="Permanent link">¶</a></h3>
<p><strong>Solution</strong>: Reduce batch size or use PEFT strategy</p>
<h3 id="issue-accuracy-decreasing-during-training">Issue: "Accuracy decreasing during training"<a class="headerlink" href="#issue-accuracy-decreasing-during-training" title="Permanent link">¶</a></h3>
<p><strong>Solution</strong>: Lower learning rate, reduce epochs, use regularization</p>
<h3 id="issue-model-not-improving-after-training">Issue: "Model not improving after training"<a class="headerlink" href="#issue-model-not-improving-after-training" title="Permanent link">¶</a></h3>
<p><strong>Solution</strong>: Increase learning rate, use different scheduler, increase epochs</p>
<h3 id="issue-peft-not-significantly-faster">Issue: "PEFT not significantly faster"<a class="headerlink" href="#issue-peft-not-significantly-faster" title="Permanent link">¶</a></h3>
<p><strong>Solution</strong>: Use lower rank (r=4), verify LoRA is actually applied</p>
<hr/>
<h2 id="8-next-steps">8. Next Steps<a class="headerlink" href="#8-next-steps" title="Permanent link">¶</a></h2>
<ul>
<li><a href="../../advanced/peft-lora/">PEFT &amp; LoRA Details</a> - Deep dive into LoRA theory</li>
<li><a href="../../advanced/hyperparameter-tuning/">Hyperparameter Tuning</a> - Optimize model performance</li>
<li><a href="../model-selection/">Model Selection</a> - Choose right model for your task</li>
</ul>
<hr/>
<p>Choose the right strategy for your use case and resource constraints!</p></div>
</div>
<footer class="col-md-12 text-center">
<hr/>
<p>
<small>Documentation built with <a href="http://www.mkdocs.org/">MkDocs</a>.</small>
</p>
</footer>
<script src="//ajax.googleapis.com/ajax/libs/jquery/1.12.4/jquery.min.js"></script>
<script src="../../js/bootstrap-3.0.3.min.js"></script>
<script src="//cdn.jsdelivr.net/gh/highlightjs/cdn-release@9.18.0/build/highlight.min.js"></script>
<script src="//cdn.jsdelivr.net/gh/highlightjs/cdn-release@9.18.0/build/languages/python.min.js"></script>
<script src="//cdn.jsdelivr.net/gh/highlightjs/cdn-release@9.18.0/build/languages/yaml.min.js"></script>
<script>hljs.initHighlightingOnLoad();</script>
<script>var base_url = "../.."</script>
<script src="../../js/base.js"></script>
<script src="../../search/main.js"></script>
<script>
        // Initialize Mermaid v9.x after DOM loads
        // The mermaid2 plugin loads the library and sets window.mermaidConfig
        (function() {
            function initMermaid() {
                if (typeof mermaid !== 'undefined') {
                    // Get configuration from plugin or use defaults
                    const config = window.mermaidConfig || {
                        securityLevel: 'loose',
                        startOnLoad: false
                    };
                    
                    // Initialize mermaid with config
                    mermaid.initialize(config);
                    
                    // Render all mermaid diagrams - mermaid.run() automatically finds .mermaid elements
                    if (typeof mermaid.run === 'function') {
                        mermaid.run();
                    } else {
                        // Fallback for older API - manually initialize elements
                        const mermaidElements = document.querySelectorAll('.mermaid');
                        if (mermaidElements.length > 0) {
                            mermaid.init(undefined, mermaidElements);
                        }
                    }
                } else {
                    // Retry if mermaid library hasn't loaded yet
                    setTimeout(initMermaid, 100);
                }
            }
            
            // Wait for DOM and scripts to be ready
            if (document.readyState === 'loading') {
                document.addEventListener('DOMContentLoaded', initMermaid);
            } else {
                // DOM already loaded, but scripts might not be
                setTimeout(initMermaid, 100);
            }
        })();
    </script>
<div aria-hidden="true" aria-labelledby="searchModalLabel" class="modal" id="mkdocs_search_modal" role="dialog" tabindex="-1">
<div class="modal-dialog modal-lg">
<div class="modal-content">
<div class="modal-header">
<button class="close" data-dismiss="modal" type="button">
<span aria-hidden="true">×</span>
<span class="sr-only">Close</span>
</button>
<h4 class="modal-title" id="searchModalLabel">Search</h4>
</div>
<div class="modal-body">
<p>
                    From here you can search these documents. Enter
                    your search terms below.
                </p>
<form>
<div class="form-group">
<input class="form-control" id="mkdocs-search-query" placeholder="Search..." title="Type search term here" type="text"/>
</div>
</form>
<div id="mkdocs-search-results"></div>
</div>
<div class="modal-footer">
</div>
</div>
</div>
</div><div aria-hidden="true" aria-labelledby="keyboardModalLabel" class="modal" id="mkdocs_keyboard_modal" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<h4 class="modal-title" id="keyboardModalLabel">Keyboard Shortcuts</h4>
<button class="close" data-dismiss="modal" type="button"><span aria-hidden="true">×</span><span class="sr-only">Close</span></button>
</div>
<div class="modal-body">
<table class="table">
<thead>
<tr>
<th style="width: 20%;">Keys</th>
<th>Action</th>
</tr>
</thead>
<tbody>
<tr>
<td class="help shortcut"><kbd>?</kbd></td>
<td>Open this help</td>
</tr>
<tr>
<td class="next shortcut"><kbd>n</kbd></td>
<td>Next page</td>
</tr>
<tr>
<td class="prev shortcut"><kbd>p</kbd></td>
<td>Previous page</td>
</tr>
<tr>
<td class="search shortcut"><kbd>s</kbd></td>
<td>Search</td>
</tr>
</tbody>
</table>
</div>
<div class="modal-footer">
</div>
</div>
</div>
</div>
<script src="https://unpkg.com/mermaid@9.4.3/dist/mermaid.min.js"></script><script>mermaid.initialize({
    securityLevel: "loose",
    startOnLoad: false
});</script></body>
</html>
