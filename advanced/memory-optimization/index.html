<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="author" content="TabTune Development Team" />
      <link rel="shortcut icon" href="../../img/favicon.ico" />
    <title>Memory Optimization: Techniques for Training Large Models - TabTune Documentation</title>
    <link rel="stylesheet" href="../../css/theme.css" />
    <link rel="stylesheet" href="../../css/theme_extra.css" />
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/styles/github.min.css" />
        <link href="../../assets/_mkdocstrings.css" rel="stylesheet" />
    
      <script>
        // Current page data
        var mkdocs_page_name = "Memory Optimization: Techniques for Training Large Models";
        var mkdocs_page_input_path = "advanced/memory-optimization.md";
        var mkdocs_page_url = null;
      </script>
    
    <!--[if lt IE 9]>
      <script src="../../js/html5shiv.min.js"></script>
    <![endif]-->
      <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/highlight.min.js"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/languages/python.min.js"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/languages/yaml.min.js"></script>
      <script>hljs.highlightAll();</script> 
</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side stickynav">
    <div class="wy-side-scroll">
      <div class="wy-side-nav-search">
          <a href="../..">
          <img src="../../assets/tabtune.svg" class="logo" alt="Logo"/>
        </a><div role="search">
  <form id ="rtd-search-form" class="wy-form" action="../../search.html" method="get">
      <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" title="Type search term here" />
  </form>
</div>
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../..">Home</a>
                </li>
              </ul>
              <p class="caption"><span class="caption-text">Getting Started</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="../../getting-started/installation/">Installation</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../getting-started/quick-start/">Quick Start</a>
                  </li>
                  <li class="toctree-l1"><a class="" href="../../getting-started/basic-concepts.md">Basic Concepts</a>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">User Guide</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="../../user-guide/pipeline-overview/">TabularPipeline Overview</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../user-guide/data-processing/">Data Processing</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../user-guide/tuning-strategies/">Tuning Strategies</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../user-guide/model-selection/">Model Selection</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../user-guide/saving-loading/">Saving and Loading</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../user-guide/leaderboard/">Model Comparison</a>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">Models</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="../../models/overview/">Overview</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../models/tabpfn/">TabPFN</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../models/tabicl/">TabICL</a>
                  </li>
                  <li class="toctree-l1"><a class="" href="../../models/tabbiaxial.md">TabBiaxial</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../models/tabdpt/">TabDPT</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../models/mitra/">Mitra</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../models/contexttab/">ConTextTab</a>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">Advanced Topics</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="../peft-lora/">PEFT & LoRA</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../hyperparameter-tuning/">Hyperparameter Tuning</a>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">API Reference</span></p>
              <ul>
                  <li class="toctree-l1"><a class="" href="../../api/pipeline.md">TabularPipeline</a>
                  </li>
                  <li class="toctree-l1"><a class="" href="../../api/data-processor.md">DataProcessor</a>
                  </li>
                  <li class="toctree-l1"><a class="" href="../../api/tuning-manager.md">TuningManager</a>
                  </li>
                  <li class="toctree-l1"><a class="" href="../../api/leaderboard.md">TabularLeaderboard</a>
                  </li>
                  <li class="toctree-l1"><a class="" href="../../api/peft-utils.md">PEFT Utils</a>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">Examples</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="../../examples/classification/">Classification Tasks</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../examples/large-datasets/">Large Datasets</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../examples/peft-examples/">PEFT Fine-Tuning</a>
                  </li>
                  <li class="toctree-l1"><a class="" href="../../examples/benchmarking.md">Benchmarking</a>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">Contributing</span></p>
              <ul>
                  <li class="toctree-l1"><a class="" href="../../contributing/setup.md">Development Setup</a>
                  </li>
                  <li class="toctree-l1"><a class="" href="../../contributing/standards.md">Code Standards</a>
                  </li>
                  <li class="toctree-l1"><a class="" href="../../contributing/new-models.md">Adding New Models</a>
                  </li>
                  <li class="toctree-l1"><a class="" href="../../contributing/documentation.md">Documentation Guide</a>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">About</span></p>
              <ul>
                  <li class="toctree-l1"><a class="" href="../../about/release-notes.md">Release Notes</a>
                  </li>
                  <li class="toctree-l1"><a class="" href="../../about/roadmap.md">Roadmap</a>
                  </li>
                  <li class="toctree-l1"><a class="" href="../../about/faq.md">FAQ</a>
                  </li>
                  <li class="toctree-l1"><a class="" href="../../about/license.md">License</a>
                  </li>
              </ul>
      </div>
    </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">
      <nav class="wy-nav-top" role="navigation" aria-label="Mobile navigation menu">
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../..">TabTune Documentation</a>
        
      </nav>
      <div class="wy-nav-content">
        <div class="rst-content"><div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href="../.." class="icon icon-home" aria-label="Docs"></a></li>
      <li class="breadcrumb-item active">Memory Optimization: Techniques for Training Large Models</li>
    <li class="wy-breadcrumbs-aside">
          <a href="https://github.com/Lexsi-Labs/TabTune_Internal/edit/master/docs/advanced/memory-optimization.md">Edit on Lexsi-Labs/TabTune_Internal</a>
    </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
            <div class="section" itemprop="articleBody">
              
                <h1 id="memory-optimization-techniques-for-training-large-models">Memory Optimization: Techniques for Training Large Models<a class="headerlink" href="#memory-optimization-techniques-for-training-large-models" title="Permanent link">&para;</a></h1>
<p>This document provides comprehensive strategies for optimizing memory usage when training TabTune models, enabling efficient use of limited GPU/CPU resources.</p>
<hr />
<h2 id="1-introduction">1. Introduction<a class="headerlink" href="#1-introduction" title="Permanent link">&para;</a></h2>
<p>Memory optimization is critical for:</p>
<ul>
<li>Training on GPUs with limited VRAM (&lt; 8GB)</li>
<li>Processing large datasets (100K+ rows)</li>
<li>Using large model architectures</li>
<li>Running multiple experiments simultaneously</li>
<li>Deploying in resource-constrained environments</li>
</ul>
<p>This guide covers techniques, tools, and trade-offs for memory efficiency.</p>
<hr />
<h2 id="2-memory-profiling">2. Memory Profiling<a class="headerlink" href="#2-memory-profiling" title="Permanent link">&para;</a></h2>
<h3 id="21-understanding-memory-usage">2.1 Understanding Memory Usage<a class="headerlink" href="#21-understanding-memory-usage" title="Permanent link">&para;</a></h3>
<p><strong>Memory Breakdown</strong> (typical forward/backward pass):</p>
<div class="highlight"><pre><span></span><code>Model weights:        40-50% (frozen in PEFT)
Optimizer states:     20-30% (momentum, variance)
Gradients:           10-15%
Activations:         10-20% (intermediate values)
DataLoader buffers:   5-10%
─────────────────────────────
Total:               100%
</code></pre></div>
<h3 id="22-profiling-tools">2.2 Profiling Tools<a class="headerlink" href="#22-profiling-tools" title="Permanent link">&para;</a></h3>
<h4 id="pytorch-memory-profiler">PyTorch Memory Profiler<a class="headerlink" href="#pytorch-memory-profiler" title="Permanent link">&para;</a></h4>
<div class="highlight"><pre><span></span><code><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torch.profiler</span><span class="w"> </span><span class="kn">import</span> <span class="n">profile</span><span class="p">,</span> <span class="n">record_function</span><span class="p">,</span> <span class="n">ProfilerActivity</span>

<span class="c1"># Basic memory measurement</span>
<span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">empty_cache</span><span class="p">()</span>
<span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">reset_peak_memory_stats</span><span class="p">()</span>

<span class="c1"># Your training code</span>
<span class="n">pipeline</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="c1"># Print memory stats</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Peak memory: </span><span class="si">{</span><span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">max_memory_allocated</span><span class="p">()</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="mf">1e9</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2"> GB&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Current memory: </span><span class="si">{</span><span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">memory_allocated</span><span class="p">()</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="mf">1e9</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2"> GB&quot;</span><span class="p">)</span>

<span class="c1"># Detailed profiling</span>
<span class="k">with</span> <span class="n">profile</span><span class="p">(</span>
    <span class="n">activities</span><span class="o">=</span><span class="p">[</span><span class="n">ProfilerActivity</span><span class="o">.</span><span class="n">CPU</span><span class="p">,</span> <span class="n">ProfilerActivity</span><span class="o">.</span><span class="n">CUDA</span><span class="p">],</span>
    <span class="n">record_shapes</span><span class="o">=</span><span class="kc">True</span>
<span class="p">)</span> <span class="k">as</span> <span class="n">prof</span><span class="p">:</span>
    <span class="n">pipeline</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="n">prof</span><span class="o">.</span><span class="n">key_averages</span><span class="p">()</span><span class="o">.</span><span class="n">table</span><span class="p">(</span><span class="n">sort_by</span><span class="o">=</span><span class="s2">&quot;cuda_memory_usage&quot;</span><span class="p">))</span>
</code></pre></div>
<h4 id="memory-monitoring-script">Memory Monitoring Script<a class="headerlink" href="#memory-monitoring-script" title="Permanent link">&para;</a></h4>
<div class="highlight"><pre><span></span><code><span class="kn">import</span><span class="w"> </span><span class="nn">psutil</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">nvidia_smi</span>

<span class="k">class</span><span class="w"> </span><span class="nc">MemoryMonitor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Monitor memory usage during training.&quot;&quot;&quot;</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">peak_gpu</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">peak_cpu</span> <span class="o">=</span> <span class="mi">0</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">record</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Record current memory usage.&quot;&quot;&quot;</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="n">nvidia_smi</span><span class="o">.</span><span class="n">nvmlInit</span><span class="p">()</span>
            <span class="n">handle</span> <span class="o">=</span> <span class="n">nvidia_smi</span><span class="o">.</span><span class="n">nvmlDeviceGetHandleByIndex</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
            <span class="n">info</span> <span class="o">=</span> <span class="n">nvidia_smi</span><span class="o">.</span><span class="n">nvmlDeviceGetMemoryInfo</span><span class="p">(</span><span class="n">handle</span><span class="p">)</span>
            <span class="n">gpu_mem</span> <span class="o">=</span> <span class="n">info</span><span class="o">.</span><span class="n">used</span> <span class="o">/</span> <span class="mf">1e9</span>

            <span class="bp">self</span><span class="o">.</span><span class="n">peak_gpu</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">peak_gpu</span><span class="p">,</span> <span class="n">gpu_mem</span><span class="p">)</span>
        <span class="k">except</span><span class="p">:</span>
            <span class="n">gpu_mem</span> <span class="o">=</span> <span class="mi">0</span>

        <span class="c1"># CPU memory</span>
        <span class="n">process</span> <span class="o">=</span> <span class="n">psutil</span><span class="o">.</span><span class="n">Process</span><span class="p">()</span>
        <span class="n">cpu_mem</span> <span class="o">=</span> <span class="n">process</span><span class="o">.</span><span class="n">memory_info</span><span class="p">()</span><span class="o">.</span><span class="n">rss</span> <span class="o">/</span> <span class="mf">1e9</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">peak_cpu</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">peak_cpu</span><span class="p">,</span> <span class="n">cpu_mem</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">gpu_mem</span><span class="p">,</span> <span class="n">cpu_mem</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">report</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Print memory report.&quot;&quot;&quot;</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Peak GPU: </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">peak_gpu</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2"> GB&quot;</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Peak CPU: </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">peak_cpu</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2"> GB&quot;</span><span class="p">)</span>

<span class="c1"># Usage</span>
<span class="n">monitor</span> <span class="o">=</span> <span class="n">MemoryMonitor</span><span class="p">()</span>

<span class="c1"># Periodically call during training</span>
<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_epochs</span><span class="p">):</span>
    <span class="n">gpu_mem</span><span class="p">,</span> <span class="n">cpu_mem</span> <span class="o">=</span> <span class="n">monitor</span><span class="o">.</span><span class="n">record</span><span class="p">()</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Epoch </span><span class="si">{</span><span class="n">epoch</span><span class="si">}</span><span class="s2">: GPU </span><span class="si">{</span><span class="n">gpu_mem</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">GB, CPU </span><span class="si">{</span><span class="n">cpu_mem</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">GB&quot;</span><span class="p">)</span>

<span class="n">monitor</span><span class="o">.</span><span class="n">report</span><span class="p">()</span>
</code></pre></div>
<hr />
<h2 id="3-optimization-techniques">3. Optimization Techniques<a class="headerlink" href="#3-optimization-techniques" title="Permanent link">&para;</a></h2>
<h3 id="31-peft-lora-primary-technique">3.1 PEFT (LoRA) - Primary Technique<a class="headerlink" href="#31-peft-lora-primary-technique" title="Permanent link">&para;</a></h3>
<p><strong>Impact</strong>: 60-90% memory reduction</p>
<div class="highlight"><pre><span></span><code><span class="c1"># Base fine-tuning: high memory</span>
<span class="n">pipeline_base</span> <span class="o">=</span> <span class="n">TabularPipeline</span><span class="p">(</span>
    <span class="n">model_name</span><span class="o">=</span><span class="s1">&#39;TabICL&#39;</span><span class="p">,</span>
    <span class="n">tuning_strategy</span><span class="o">=</span><span class="s1">&#39;base-ft&#39;</span><span class="p">,</span>
    <span class="n">tuning_params</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;device&#39;</span><span class="p">:</span> <span class="s1">&#39;cuda&#39;</span><span class="p">,</span> <span class="s1">&#39;epochs&#39;</span><span class="p">:</span> <span class="mi">5</span><span class="p">}</span>
<span class="p">)</span>
<span class="c1"># Memory: ~12 GB for 100K samples</span>

<span class="c1"># PEFT fine-tuning: low memory</span>
<span class="n">pipeline_peft</span> <span class="o">=</span> <span class="n">TabularPipeline</span><span class="p">(</span>
    <span class="n">model_name</span><span class="o">=</span><span class="s1">&#39;TabICL&#39;</span><span class="p">,</span>
    <span class="n">tuning_strategy</span><span class="o">=</span><span class="s1">&#39;peft&#39;</span><span class="p">,</span>
    <span class="n">tuning_params</span><span class="o">=</span><span class="p">{</span>
        <span class="s1">&#39;device&#39;</span><span class="p">:</span> <span class="s1">&#39;cuda&#39;</span><span class="p">,</span>
        <span class="s1">&#39;epochs&#39;</span><span class="p">:</span> <span class="mi">5</span><span class="p">,</span>
        <span class="s1">&#39;peft_config&#39;</span><span class="p">:</span> <span class="p">{</span><span class="s1">&#39;r&#39;</span><span class="p">:</span> <span class="mi">8</span><span class="p">}</span>
    <span class="p">}</span>
<span class="p">)</span>
<span class="c1"># Memory: ~4 GB for same task</span>
</code></pre></div>
<p><strong>Choosing PEFT config for memory</strong>:</p>
<div class="highlight"><pre><span></span><code><span class="c1"># Ultra-constrained (2GB GPU)</span>
<span class="n">peft_config</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s1">&#39;r&#39;</span><span class="p">:</span> <span class="mi">2</span><span class="p">,</span>
    <span class="s1">&#39;lora_alpha&#39;</span><span class="p">:</span> <span class="mi">4</span><span class="p">,</span>
    <span class="s1">&#39;lora_dropout&#39;</span><span class="p">:</span> <span class="mf">0.2</span>
<span class="p">}</span>

<span class="c1"># Memory-constrained (4GB GPU)</span>
<span class="n">peft_config</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s1">&#39;r&#39;</span><span class="p">:</span> <span class="mi">4</span><span class="p">,</span>
    <span class="s1">&#39;lora_alpha&#39;</span><span class="p">:</span> <span class="mi">8</span><span class="p">,</span>
    <span class="s1">&#39;lora_dropout&#39;</span><span class="p">:</span> <span class="mf">0.1</span>
<span class="p">}</span>

<span class="c1"># Moderate constraint (6GB GPU)</span>
<span class="n">peft_config</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s1">&#39;r&#39;</span><span class="p">:</span> <span class="mi">8</span><span class="p">,</span>
    <span class="s1">&#39;lora_alpha&#39;</span><span class="p">:</span> <span class="mi">16</span><span class="p">,</span>
    <span class="s1">&#39;lora_dropout&#39;</span><span class="p">:</span> <span class="mf">0.05</span>
<span class="p">}</span>
</code></pre></div>
<h3 id="32-batch-size-reduction">3.2 Batch Size Reduction<a class="headerlink" href="#32-batch-size-reduction" title="Permanent link">&para;</a></h3>
<p><strong>Impact</strong>: 20-40% memory reduction per halving</p>
<div class="highlight"><pre><span></span><code><span class="c1"># Large batch (high memory)</span>
<span class="n">tuning_params</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s1">&#39;batch_size&#39;</span><span class="p">:</span> <span class="mi">64</span><span class="p">,</span>
    <span class="s1">&#39;support_size&#39;</span><span class="p">:</span> <span class="mi">512</span>
<span class="p">}</span>
<span class="c1"># Memory: ~12 GB</span>

<span class="c1"># Reduced batch (medium memory)</span>
<span class="n">tuning_params</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s1">&#39;batch_size&#39;</span><span class="p">:</span> <span class="mi">32</span><span class="p">,</span>
    <span class="s1">&#39;support_size&#39;</span><span class="p">:</span> <span class="mi">256</span>
<span class="p">}</span>
<span class="c1"># Memory: ~6 GB</span>

<span class="c1"># Small batch (low memory)</span>
<span class="n">tuning_params</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s1">&#39;batch_size&#39;</span><span class="p">:</span> <span class="mi">8</span><span class="p">,</span>
    <span class="s1">&#39;support_size&#39;</span><span class="p">:</span> <span class="mi">64</span>
<span class="p">}</span>
<span class="c1"># Memory: ~2 GB</span>
</code></pre></div>
<p><strong>Trade-offs</strong>:
- Smaller batch: More gradient noise, longer convergence
- Larger batch: Faster convergence, higher memory</p>
<h3 id="33-gradient-accumulation">3.3 Gradient Accumulation<a class="headerlink" href="#33-gradient-accumulation" title="Permanent link">&para;</a></h3>
<p>Simulate larger batch without increased memory:</p>
<div class="highlight"><pre><span></span><code><span class="n">tuning_params</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s1">&#39;batch_size&#39;</span><span class="p">:</span> <span class="mi">8</span><span class="p">,</span>                      <span class="c1"># Actual batch</span>
    <span class="s1">&#39;gradient_accumulation_steps&#39;</span><span class="p">:</span> <span class="mi">4</span><span class="p">,</span>     <span class="c1"># Accumulate 4x</span>
    <span class="c1"># Effective batch = 32</span>
    <span class="s1">&#39;device&#39;</span><span class="p">:</span> <span class="s1">&#39;cuda&#39;</span>
<span class="p">}</span>

<span class="c1"># Memory cost: Similar to batch_size=8</span>
<span class="c1"># Effective batch size benefit: batch_size=32</span>
</code></pre></div>
<h3 id="34-mixed-precision-training">3.4 Mixed Precision Training<a class="headerlink" href="#34-mixed-precision-training" title="Permanent link">&para;</a></h3>
<p><strong>Impact</strong>: 20-30% memory reduction</p>
<div class="highlight"><pre><span></span><code><span class="c1"># Standard (float32): high memory</span>
<span class="n">tuning_params</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s1">&#39;device&#39;</span><span class="p">:</span> <span class="s1">&#39;cuda&#39;</span><span class="p">,</span>
    <span class="s1">&#39;mixed_precision&#39;</span><span class="p">:</span> <span class="kc">None</span>  <span class="c1"># Full precision</span>
<span class="p">}</span>

<span class="c1"># Half precision (float16): lower memory</span>
<span class="n">tuning_params</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s1">&#39;device&#39;</span><span class="p">:</span> <span class="s1">&#39;cuda&#39;</span><span class="p">,</span>
    <span class="s1">&#39;mixed_precision&#39;</span><span class="p">:</span> <span class="s1">&#39;fp16&#39;</span>  <span class="c1"># Use 16-bit floats</span>
<span class="p">}</span>

<span class="c1"># BFloat16 (better stability)</span>
<span class="n">tuning_params</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s1">&#39;device&#39;</span><span class="p">:</span> <span class="s1">&#39;cuda&#39;</span><span class="p">,</span>
    <span class="s1">&#39;mixed_precision&#39;</span><span class="p">:</span> <span class="s1">&#39;bf16&#39;</span>
<span class="p">}</span>
</code></pre></div>
<p><strong>Implementation</strong>:</p>
<div class="highlight"><pre><span></span><code><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torch.cuda.amp</span><span class="w"> </span><span class="kn">import</span> <span class="n">autocast</span><span class="p">,</span> <span class="n">GradScaler</span>

<span class="c1"># Setup for mixed precision</span>
<span class="n">scaler</span> <span class="o">=</span> <span class="n">GradScaler</span><span class="p">()</span>

<span class="c1"># In training loop</span>
<span class="k">with</span> <span class="n">autocast</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">):</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">compute_loss</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>

<span class="n">scaler</span><span class="o">.</span><span class="n">scale</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
<span class="n">scaler</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">optimizer</span><span class="p">)</span>
<span class="n">scaler</span><span class="o">.</span><span class="n">update</span><span class="p">()</span>
</code></pre></div>
<h3 id="35-gradient-checkpointing">3.5 Gradient Checkpointing<a class="headerlink" href="#35-gradient-checkpointing" title="Permanent link">&para;</a></h3>
<p>Trade computation for memory:</p>
<div class="highlight"><pre><span></span><code><span class="n">tuning_params</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s1">&#39;device&#39;</span><span class="p">:</span> <span class="s1">&#39;cuda&#39;</span><span class="p">,</span>
    <span class="s1">&#39;gradient_checkpoint&#39;</span><span class="p">:</span> <span class="kc">True</span>  <span class="c1"># Save memory at cost of computation</span>
<span class="p">}</span>

<span class="c1"># Memory reduction: ~30-50%</span>
<span class="c1"># Computation increase: ~20-30% (recompute activations)</span>
</code></pre></div>
<h3 id="36-data-loading-optimization">3.6 Data Loading Optimization<a class="headerlink" href="#36-data-loading-optimization" title="Permanent link">&para;</a></h3>
<div class="highlight"><pre><span></span><code><span class="c1"># Efficient data loading</span>
<span class="n">loader_config</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s1">&#39;batch_size&#39;</span><span class="p">:</span> <span class="mi">32</span><span class="p">,</span>
    <span class="s1">&#39;num_workers&#39;</span><span class="p">:</span> <span class="mi">4</span><span class="p">,</span>           <span class="c1"># Parallel loading</span>
    <span class="s1">&#39;pin_memory&#39;</span><span class="p">:</span> <span class="kc">True</span><span class="p">,</span>         <span class="c1"># Faster CPU→GPU transfer</span>
    <span class="s1">&#39;persistent_workers&#39;</span><span class="p">:</span> <span class="kc">True</span><span class="p">,</span>  <span class="c1"># Keep workers alive</span>
    <span class="s1">&#39;prefetch_factor&#39;</span><span class="p">:</span> <span class="mi">2</span>        <span class="c1"># Prefetch next batches</span>
<span class="p">}</span>

<span class="c1"># Or with TabTune</span>
<span class="n">tuning_params</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s1">&#39;num_workers&#39;</span><span class="p">:</span> <span class="mi">4</span><span class="p">,</span>
    <span class="s1">&#39;pin_memory&#39;</span><span class="p">:</span> <span class="kc">True</span><span class="p">,</span>
    <span class="s1">&#39;device&#39;</span><span class="p">:</span> <span class="s1">&#39;cuda&#39;</span>
<span class="p">}</span>
</code></pre></div>
<h3 id="37-model-architecture-reduction">3.7 Model Architecture Reduction<a class="headerlink" href="#37-model-architecture-reduction" title="Permanent link">&para;</a></h3>
<p>Reduce model complexity:</p>
<div class="highlight"><pre><span></span><code><span class="c1"># Large model: high memory</span>
<span class="n">model_params</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s1">&#39;d_model&#39;</span><span class="p">:</span> <span class="mi">256</span><span class="p">,</span>      <span class="c1"># Embedding dimension</span>
    <span class="s1">&#39;num_layers&#39;</span><span class="p">:</span> <span class="mi">8</span><span class="p">,</span>     <span class="c1"># Transformer layers</span>
    <span class="s1">&#39;num_heads&#39;</span><span class="p">:</span> <span class="mi">16</span>      <span class="c1"># Attention heads</span>
<span class="p">}</span>

<span class="c1"># Medium model: medium memory</span>
<span class="n">model_params</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s1">&#39;d_model&#39;</span><span class="p">:</span> <span class="mi">128</span><span class="p">,</span>
    <span class="s1">&#39;num_layers&#39;</span><span class="p">:</span> <span class="mi">4</span><span class="p">,</span>
    <span class="s1">&#39;num_heads&#39;</span><span class="p">:</span> <span class="mi">8</span>
<span class="p">}</span>

<span class="c1"># Small model: low memory</span>
<span class="n">model_params</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s1">&#39;d_model&#39;</span><span class="p">:</span> <span class="mi">64</span><span class="p">,</span>
    <span class="s1">&#39;num_layers&#39;</span><span class="p">:</span> <span class="mi">2</span><span class="p">,</span>
    <span class="s1">&#39;num_heads&#39;</span><span class="p">:</span> <span class="mi">4</span>
<span class="p">}</span>
</code></pre></div>
<hr />
<h2 id="4-optimization-strategies-by-constraint">4. Optimization Strategies by Constraint<a class="headerlink" href="#4-optimization-strategies-by-constraint" title="Permanent link">&para;</a></h2>
<h3 id="41-severe-constraint-2gb-gpu">4.1 Severe Constraint (2GB GPU)<a class="headerlink" href="#41-severe-constraint-2gb-gpu" title="Permanent link">&para;</a></h3>
<div class="highlight"><pre><span></span><code><span class="n">pipeline</span> <span class="o">=</span> <span class="n">TabularPipeline</span><span class="p">(</span>
    <span class="n">model_name</span><span class="o">=</span><span class="s1">&#39;TabICL&#39;</span><span class="p">,</span>
    <span class="n">tuning_strategy</span><span class="o">=</span><span class="s1">&#39;peft&#39;</span><span class="p">,</span>
    <span class="n">model_params</span><span class="o">=</span><span class="p">{</span>
        <span class="s1">&#39;n_estimators&#39;</span><span class="p">:</span> <span class="mi">8</span>  <span class="c1"># Reduce ensemble</span>
    <span class="p">},</span>
    <span class="n">tuning_params</span><span class="o">=</span><span class="p">{</span>
        <span class="s1">&#39;device&#39;</span><span class="p">:</span> <span class="s1">&#39;cuda&#39;</span><span class="p">,</span>
        <span class="s1">&#39;epochs&#39;</span><span class="p">:</span> <span class="mi">3</span><span class="p">,</span>
        <span class="s1">&#39;learning_rate&#39;</span><span class="p">:</span> <span class="mf">2e-4</span><span class="p">,</span>
        <span class="s1">&#39;batch_size&#39;</span><span class="p">:</span> <span class="mi">4</span><span class="p">,</span>           <span class="c1"># Very small</span>
        <span class="s1">&#39;support_size&#39;</span><span class="p">:</span> <span class="mi">32</span><span class="p">,</span>        <span class="c1"># Small context</span>
        <span class="s1">&#39;query_size&#39;</span><span class="p">:</span> <span class="mi">16</span><span class="p">,</span>
        <span class="s1">&#39;mixed_precision&#39;</span><span class="p">:</span> <span class="s1">&#39;fp16&#39;</span><span class="p">,</span> <span class="c1"># Use half precision</span>
        <span class="s1">&#39;gradient_checkpoint&#39;</span><span class="p">:</span> <span class="kc">True</span><span class="p">,</span>
        <span class="s1">&#39;num_workers&#39;</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span>          <span class="c1"># No parallel loading</span>
        <span class="s1">&#39;peft_config&#39;</span><span class="p">:</span> <span class="p">{</span>
            <span class="s1">&#39;r&#39;</span><span class="p">:</span> <span class="mi">2</span><span class="p">,</span>                <span class="c1"># Very low rank</span>
            <span class="s1">&#39;lora_alpha&#39;</span><span class="p">:</span> <span class="mi">4</span><span class="p">,</span>
            <span class="s1">&#39;lora_dropout&#39;</span><span class="p">:</span> <span class="mf">0.2</span>
        <span class="p">}</span>
    <span class="p">}</span>
<span class="p">)</span>
</code></pre></div>
<h3 id="42-moderate-constraint-4gb-gpu">4.2 Moderate Constraint (4GB GPU)<a class="headerlink" href="#42-moderate-constraint-4gb-gpu" title="Permanent link">&para;</a></h3>
<div class="highlight"><pre><span></span><code><span class="n">pipeline</span> <span class="o">=</span> <span class="n">TabularPipeline</span><span class="p">(</span>
    <span class="n">model_name</span><span class="o">=</span><span class="s1">&#39;TabICL&#39;</span><span class="p">,</span>
    <span class="n">tuning_strategy</span><span class="o">=</span><span class="s1">&#39;peft&#39;</span><span class="p">,</span>
    <span class="n">tuning_params</span><span class="o">=</span><span class="p">{</span>
        <span class="s1">&#39;device&#39;</span><span class="p">:</span> <span class="s1">&#39;cuda&#39;</span><span class="p">,</span>
        <span class="s1">&#39;epochs&#39;</span><span class="p">:</span> <span class="mi">5</span><span class="p">,</span>
        <span class="s1">&#39;learning_rate&#39;</span><span class="p">:</span> <span class="mf">2e-4</span><span class="p">,</span>
        <span class="s1">&#39;batch_size&#39;</span><span class="p">:</span> <span class="mi">8</span><span class="p">,</span>
        <span class="s1">&#39;support_size&#39;</span><span class="p">:</span> <span class="mi">64</span><span class="p">,</span>
        <span class="s1">&#39;query_size&#39;</span><span class="p">:</span> <span class="mi">32</span><span class="p">,</span>
        <span class="s1">&#39;mixed_precision&#39;</span><span class="p">:</span> <span class="s1">&#39;fp16&#39;</span><span class="p">,</span>
        <span class="s1">&#39;num_workers&#39;</span><span class="p">:</span> <span class="mi">2</span><span class="p">,</span>
        <span class="s1">&#39;peft_config&#39;</span><span class="p">:</span> <span class="p">{</span>
            <span class="s1">&#39;r&#39;</span><span class="p">:</span> <span class="mi">4</span><span class="p">,</span>
            <span class="s1">&#39;lora_alpha&#39;</span><span class="p">:</span> <span class="mi">8</span><span class="p">,</span>
            <span class="s1">&#39;lora_dropout&#39;</span><span class="p">:</span> <span class="mf">0.1</span>
        <span class="p">}</span>
    <span class="p">}</span>
<span class="p">)</span>
</code></pre></div>
<h3 id="43-comfortable-8gb-gpu">4.3 Comfortable (8GB+ GPU)<a class="headerlink" href="#43-comfortable-8gb-gpu" title="Permanent link">&para;</a></h3>
<div class="highlight"><pre><span></span><code><span class="n">pipeline</span> <span class="o">=</span> <span class="n">TabularPipeline</span><span class="p">(</span>
    <span class="n">model_name</span><span class="o">=</span><span class="s1">&#39;TabICL&#39;</span><span class="p">,</span>
    <span class="n">tuning_strategy</span><span class="o">=</span><span class="s1">&#39;base-ft&#39;</span><span class="p">,</span>  <span class="c1"># Full fine-tuning possible</span>
    <span class="n">tuning_params</span><span class="o">=</span><span class="p">{</span>
        <span class="s1">&#39;device&#39;</span><span class="p">:</span> <span class="s1">&#39;cuda&#39;</span><span class="p">,</span>
        <span class="s1">&#39;epochs&#39;</span><span class="p">:</span> <span class="mi">5</span><span class="p">,</span>
        <span class="s1">&#39;learning_rate&#39;</span><span class="p">:</span> <span class="mf">2e-5</span><span class="p">,</span>
        <span class="s1">&#39;batch_size&#39;</span><span class="p">:</span> <span class="mi">32</span><span class="p">,</span>
        <span class="s1">&#39;support_size&#39;</span><span class="p">:</span> <span class="mi">128</span><span class="p">,</span>
        <span class="s1">&#39;query_size&#39;</span><span class="p">:</span> <span class="mi">64</span><span class="p">,</span>
        <span class="s1">&#39;num_workers&#39;</span><span class="p">:</span> <span class="mi">4</span><span class="p">,</span>
        <span class="s1">&#39;gradient_accumulation_steps&#39;</span><span class="p">:</span> <span class="mi">2</span>
    <span class="p">}</span>
<span class="p">)</span>
</code></pre></div>
<hr />
<h2 id="5-advanced-techniques">5. Advanced Techniques<a class="headerlink" href="#5-advanced-techniques" title="Permanent link">&para;</a></h2>
<h3 id="51-activation-checkpointing">5.1 Activation Checkpointing<a class="headerlink" href="#51-activation-checkpointing" title="Permanent link">&para;</a></h3>
<div class="highlight"><pre><span></span><code><span class="kn">import</span><span class="w"> </span><span class="nn">torch.utils.checkpoint</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">checkpoint</span>

<span class="k">class</span><span class="w"> </span><span class="nc">CheckpointedModel</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Wrap model with checkpointing.&quot;&quot;&quot;</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">model</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">model</span> <span class="o">=</span> <span class="n">model</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="c1"># Checkpoint during forward pass</span>
        <span class="k">return</span> <span class="n">checkpoint</span><span class="o">.</span><span class="n">checkpoint</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="p">,</span>
            <span class="n">x</span><span class="p">,</span>
            <span class="n">use_reentrant</span><span class="o">=</span><span class="kc">False</span>
        <span class="p">)</span>
</code></pre></div>
<h3 id="52-quantization">5.2 Quantization<a class="headerlink" href="#52-quantization" title="Permanent link">&para;</a></h3>
<p>Reduce model precision:</p>
<div class="highlight"><pre><span></span><code><span class="kn">import</span><span class="w"> </span><span class="nn">torch.quantization</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">quantization</span>

<span class="k">def</span><span class="w"> </span><span class="nf">quantize_model</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">backend</span><span class="o">=</span><span class="s1">&#39;fbgemm&#39;</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Quantize model to int8.&quot;&quot;&quot;</span>
    <span class="n">model</span><span class="o">.</span><span class="n">qconfig</span> <span class="o">=</span> <span class="n">quantization</span><span class="o">.</span><span class="n">get_default_qconfig</span><span class="p">(</span><span class="n">backend</span><span class="p">)</span>
    <span class="n">quantization</span><span class="o">.</span><span class="n">prepare</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="c1"># Calibrate on data...</span>
    <span class="n">quantization</span><span class="o">.</span><span class="n">convert</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">model</span>

<span class="c1"># Usage</span>
<span class="n">quantized_pipeline</span> <span class="o">=</span> <span class="n">quantize_model</span><span class="p">(</span><span class="n">pipeline</span><span class="o">.</span><span class="n">model</span><span class="p">)</span>
</code></pre></div>
<h3 id="53-parameter-sharing">5.3 Parameter Sharing<a class="headerlink" href="#53-parameter-sharing" title="Permanent link">&para;</a></h3>
<p>Share weights across layers:</p>
<div class="highlight"><pre><span></span><code><span class="k">class</span><span class="w"> </span><span class="nc">ParameterSharingModel</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Model with shared parameters.&quot;&quot;&quot;</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">shared_layer</span><span class="p">,</span> <span class="n">num_repeats</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">shared_layer</span> <span class="o">=</span> <span class="n">shared_layer</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_repeats</span> <span class="o">=</span> <span class="n">num_repeats</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">num_repeats</span><span class="p">):</span>
            <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">shared_layer</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>
</code></pre></div>
<h3 id="54-knowledge-distillation">5.4 Knowledge Distillation<a class="headerlink" href="#54-knowledge-distillation" title="Permanent link">&para;</a></h3>
<p>Train smaller student model from larger teacher:</p>
<div class="highlight"><pre><span></span><code><span class="c1"># Large teacher model (high accuracy)</span>
<span class="n">teacher</span> <span class="o">=</span> <span class="n">TabularPipeline</span><span class="p">(</span><span class="n">model_name</span><span class="o">=</span><span class="s1">&#39;TabDPT&#39;</span><span class="p">,</span> <span class="n">tuning_strategy</span><span class="o">=</span><span class="s1">&#39;base-ft&#39;</span><span class="p">)</span>
<span class="n">teacher</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="c1"># Small student model (low memory)</span>
<span class="n">student</span> <span class="o">=</span> <span class="n">TabularPipeline</span><span class="p">(</span><span class="n">model_name</span><span class="o">=</span><span class="s1">&#39;TabPFN&#39;</span><span class="p">,</span> <span class="n">tuning_strategy</span><span class="o">=</span><span class="s1">&#39;peft&#39;</span><span class="p">)</span>

<span class="c1"># Distillation: train student to match teacher</span>
<span class="k">for</span> <span class="n">batch</span> <span class="ow">in</span> <span class="n">dataloader</span><span class="p">:</span>
    <span class="n">teacher_logits</span> <span class="o">=</span> <span class="n">teacher</span><span class="o">.</span><span class="n">model</span><span class="p">(</span><span class="n">batch</span><span class="p">)</span>
    <span class="n">student_logits</span> <span class="o">=</span> <span class="n">student</span><span class="o">.</span><span class="n">model</span><span class="p">(</span><span class="n">batch</span><span class="p">)</span>

    <span class="c1"># KL divergence loss</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">KLDivLoss</span><span class="p">()(</span>
        <span class="n">nn</span><span class="o">.</span><span class="n">log_softmax</span><span class="p">(</span><span class="n">student_logits</span><span class="p">),</span>
        <span class="n">nn</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">teacher_logits</span><span class="p">)</span>
    <span class="p">)</span>
    <span class="c1"># ... optimize ...</span>
</code></pre></div>
<hr />
<h2 id="6-memory-time-trade-offs">6. Memory-Time Trade-offs<a class="headerlink" href="#6-memory-time-trade-offs" title="Permanent link">&para;</a></h2>
<h3 id="61-time-memory-pareto-frontier">6.1 Time-Memory Pareto Frontier<a class="headerlink" href="#61-time-memory-pareto-frontier" title="Permanent link">&para;</a></h3>
<div class="highlight"><pre><span></span><code><span class="n">configurations</span> <span class="o">=</span> <span class="p">[</span>
    <span class="p">{</span>
        <span class="s1">&#39;name&#39;</span><span class="p">:</span> <span class="s1">&#39;Max Speed&#39;</span><span class="p">,</span>
        <span class="s1">&#39;batch_size&#39;</span><span class="p">:</span> <span class="mi">64</span><span class="p">,</span>
        <span class="s1">&#39;precision&#39;</span><span class="p">:</span> <span class="s1">&#39;fp32&#39;</span><span class="p">,</span>
        <span class="s1">&#39;time&#39;</span><span class="p">:</span> <span class="mi">30</span><span class="p">,</span>      <span class="c1"># minutes</span>
        <span class="s1">&#39;memory&#39;</span><span class="p">:</span> <span class="mi">16</span>     <span class="c1"># GB</span>
    <span class="p">},</span>
    <span class="p">{</span>
        <span class="s1">&#39;name&#39;</span><span class="p">:</span> <span class="s1">&#39;Balanced&#39;</span><span class="p">,</span>
        <span class="s1">&#39;batch_size&#39;</span><span class="p">:</span> <span class="mi">32</span><span class="p">,</span>
        <span class="s1">&#39;precision&#39;</span><span class="p">:</span> <span class="s1">&#39;fp32&#39;</span><span class="p">,</span>
        <span class="s1">&#39;time&#39;</span><span class="p">:</span> <span class="mi">45</span><span class="p">,</span>
        <span class="s1">&#39;memory&#39;</span><span class="p">:</span> <span class="mi">12</span>
    <span class="p">},</span>
    <span class="p">{</span>
        <span class="s1">&#39;name&#39;</span><span class="p">:</span> <span class="s1">&#39;Efficient&#39;</span><span class="p">,</span>
        <span class="s1">&#39;batch_size&#39;</span><span class="p">:</span> <span class="mi">16</span><span class="p">,</span>
        <span class="s1">&#39;precision&#39;</span><span class="p">:</span> <span class="s1">&#39;fp16&#39;</span><span class="p">,</span>
        <span class="s1">&#39;time&#39;</span><span class="p">:</span> <span class="mi">60</span><span class="p">,</span>
        <span class="s1">&#39;memory&#39;</span><span class="p">:</span> <span class="mi">6</span>
    <span class="p">},</span>
    <span class="p">{</span>
        <span class="s1">&#39;name&#39;</span><span class="p">:</span> <span class="s1">&#39;Ultra-Efficient&#39;</span><span class="p">,</span>
        <span class="s1">&#39;batch_size&#39;</span><span class="p">:</span> <span class="mi">8</span><span class="p">,</span>
        <span class="s1">&#39;precision&#39;</span><span class="p">:</span> <span class="s1">&#39;fp16&#39;</span><span class="p">,</span>
        <span class="s1">&#39;peft&#39;</span><span class="p">:</span> <span class="kc">True</span><span class="p">,</span>
        <span class="s1">&#39;time&#39;</span><span class="p">:</span> <span class="mi">90</span><span class="p">,</span>
        <span class="s1">&#39;memory&#39;</span><span class="p">:</span> <span class="mi">3</span>
    <span class="p">}</span>
<span class="p">]</span>
</code></pre></div>
<h3 id="62-choosing-configuration">6.2 Choosing Configuration<a class="headerlink" href="#62-choosing-configuration" title="Permanent link">&para;</a></h3>
<div class="highlight"><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">choose_config</span><span class="p">(</span><span class="n">gpu_memory_gb</span><span class="p">,</span> <span class="n">time_budget_hours</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Choose config based on constraints.&quot;&quot;&quot;</span>

    <span class="k">if</span> <span class="n">gpu_memory_gb</span> <span class="o">&lt;</span> <span class="mi">4</span><span class="p">:</span>
        <span class="k">return</span> <span class="s1">&#39;peft&#39;</span>  <span class="c1"># Must use PEFT</span>
    <span class="k">elif</span> <span class="n">gpu_memory_gb</span> <span class="o">&lt;</span> <span class="mi">8</span><span class="p">:</span>
        <span class="k">return</span> <span class="s1">&#39;fp16_batch16&#39;</span>
    <span class="k">elif</span> <span class="n">gpu_memory_gb</span> <span class="o">&lt;</span> <span class="mi">16</span><span class="p">:</span>
        <span class="k">return</span> <span class="s1">&#39;fp16_batch32&#39;</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="s1">&#39;base_ft&#39;</span>

<span class="c1"># Usage</span>
<span class="n">config</span> <span class="o">=</span> <span class="n">choose_config</span><span class="p">(</span><span class="n">gpu_memory_gb</span><span class="o">=</span><span class="mi">6</span><span class="p">,</span> <span class="n">time_budget</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
</code></pre></div>
<hr />
<h2 id="7-monitoring-during-training">7. Monitoring During Training<a class="headerlink" href="#7-monitoring-during-training" title="Permanent link">&para;</a></h2>
<h3 id="71-real-time-memory-tracking">7.1 Real-time Memory Tracking<a class="headerlink" href="#71-real-time-memory-tracking" title="Permanent link">&para;</a></h3>
<div class="highlight"><pre><span></span><code><span class="kn">import</span><span class="w"> </span><span class="nn">logging</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">datetime</span><span class="w"> </span><span class="kn">import</span> <span class="n">datetime</span>

<span class="k">class</span><span class="w"> </span><span class="nc">MemoryTracker</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Track memory throughout training.&quot;&quot;&quot;</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">log_interval</span><span class="o">=</span><span class="mi">100</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">log_interval</span> <span class="o">=</span> <span class="n">log_interval</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">iteration</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">history</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">step</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Call after each training step.&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">iteration</span> <span class="o">+=</span> <span class="mi">1</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">iteration</span> <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">log_interval</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">peak</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">max_memory_allocated</span><span class="p">()</span> <span class="o">/</span> <span class="mf">1e9</span>
            <span class="n">current</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">memory_allocated</span><span class="p">()</span> <span class="o">/</span> <span class="mf">1e9</span>

            <span class="bp">self</span><span class="o">.</span><span class="n">history</span><span class="o">.</span><span class="n">append</span><span class="p">({</span>
                <span class="s1">&#39;iteration&#39;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">iteration</span><span class="p">,</span>
                <span class="s1">&#39;peak&#39;</span><span class="p">:</span> <span class="n">peak</span><span class="p">,</span>
                <span class="s1">&#39;current&#39;</span><span class="p">:</span> <span class="n">current</span><span class="p">,</span>
                <span class="s1">&#39;time&#39;</span><span class="p">:</span> <span class="n">datetime</span><span class="o">.</span><span class="n">now</span><span class="p">()</span>
            <span class="p">})</span>

            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;[</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">iteration</span><span class="si">}</span><span class="s2">] Peak: </span><span class="si">{</span><span class="n">peak</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">GB, Current: </span><span class="si">{</span><span class="n">current</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">GB&quot;</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">plot</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Plot memory over time.&quot;&quot;&quot;</span>
        <span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>

        <span class="n">iterations</span> <span class="o">=</span> <span class="p">[</span><span class="n">h</span><span class="p">[</span><span class="s1">&#39;iteration&#39;</span><span class="p">]</span> <span class="k">for</span> <span class="n">h</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">history</span><span class="p">]</span>
        <span class="n">peaks</span> <span class="o">=</span> <span class="p">[</span><span class="n">h</span><span class="p">[</span><span class="s1">&#39;peak&#39;</span><span class="p">]</span> <span class="k">for</span> <span class="n">h</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">history</span><span class="p">]</span>

        <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">iterations</span><span class="p">,</span> <span class="n">peaks</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Iteration&#39;</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Peak Memory (GB)&#39;</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;GPU Memory Usage Over Time&#39;</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="c1"># Usage</span>
<span class="n">tracker</span> <span class="o">=</span> <span class="n">MemoryTracker</span><span class="p">()</span>

<span class="k">for</span> <span class="n">batch</span> <span class="ow">in</span> <span class="n">dataloader</span><span class="p">:</span>
    <span class="c1"># ... training step ...</span>
    <span class="n">tracker</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>

<span class="n">tracker</span><span class="o">.</span><span class="n">plot</span><span class="p">()</span>
</code></pre></div>
<hr />
<h2 id="8-debugging-memory-issues">8. Debugging Memory Issues<a class="headerlink" href="#8-debugging-memory-issues" title="Permanent link">&para;</a></h2>
<h3 id="81-oom-out-of-memory-error-handling">8.1 OOM (Out of Memory) Error Handling<a class="headerlink" href="#81-oom-out-of-memory-error-handling" title="Permanent link">&para;</a></h3>
<div class="highlight"><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">safe_train</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">dataloader</span><span class="p">,</span> <span class="n">max_retries</span><span class="o">=</span><span class="mi">3</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Train with automatic memory adaptation.&quot;&quot;&quot;</span>

    <span class="n">batch_size</span> <span class="o">=</span> <span class="mi">32</span>

    <span class="k">for</span> <span class="n">attempt</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">max_retries</span><span class="p">):</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="c1"># Try training with current batch size</span>
            <span class="k">for</span> <span class="n">batch</span> <span class="ow">in</span> <span class="n">dataloader</span><span class="p">:</span>
                <span class="c1"># ... training code ...</span>
                <span class="k">pass</span>

            <span class="k">return</span>  <span class="c1"># Success</span>

        <span class="k">except</span> <span class="ne">RuntimeError</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
            <span class="k">if</span> <span class="s1">&#39;out of memory&#39;</span> <span class="ow">in</span> <span class="nb">str</span><span class="p">(</span><span class="n">e</span><span class="p">)</span><span class="o">.</span><span class="n">lower</span><span class="p">():</span>
                <span class="c1"># Reduce batch size and retry</span>
                <span class="n">batch_size</span> <span class="o">//=</span> <span class="mi">2</span>
                <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;OOM detected. Retrying with batch_size=</span><span class="si">{</span><span class="n">batch_size</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

                <span class="c1"># Clear cache</span>
                <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">empty_cache</span><span class="p">()</span>

                <span class="c1"># Recreate dataloader with smaller batch</span>
                <span class="n">dataloader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span>
                    <span class="n">dataset</span><span class="p">,</span>
                    <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span>
                <span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">raise</span>

    <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Failed after </span><span class="si">{</span><span class="n">max_retries</span><span class="si">}</span><span class="s2"> attempts&quot;</span><span class="p">)</span>
</code></pre></div>
<h3 id="82-memory-leak-detection">8.2 Memory Leak Detection<a class="headerlink" href="#82-memory-leak-detection" title="Permanent link">&para;</a></h3>
<div class="highlight"><pre><span></span><code><span class="kn">import</span><span class="w"> </span><span class="nn">tracemalloc</span>

<span class="n">tracemalloc</span><span class="o">.</span><span class="n">start</span><span class="p">()</span>

<span class="c1"># Your training code</span>
<span class="n">pipeline</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="n">current</span><span class="p">,</span> <span class="n">peak</span> <span class="o">=</span> <span class="n">tracemalloc</span><span class="o">.</span><span class="n">get_traced_memory</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Current: </span><span class="si">{</span><span class="n">current</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="mf">1e9</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">GB; Peak: </span><span class="si">{</span><span class="n">peak</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="mf">1e9</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">GB&quot;</span><span class="p">)</span>

<span class="c1"># Find top memory allocations</span>
<span class="n">snapshot</span> <span class="o">=</span> <span class="n">tracemalloc</span><span class="o">.</span><span class="n">take_snapshot</span><span class="p">()</span>
<span class="n">top_stats</span> <span class="o">=</span> <span class="n">snapshot</span><span class="o">.</span><span class="n">statistics</span><span class="p">(</span><span class="s1">&#39;lineno&#39;</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;[ Top 10 ]&quot;</span><span class="p">)</span>
<span class="k">for</span> <span class="n">stat</span> <span class="ow">in</span> <span class="n">top_stats</span><span class="p">[:</span><span class="mi">10</span><span class="p">]:</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">stat</span><span class="p">)</span>
</code></pre></div>
<hr />
<h2 id="9-complete-example-memory-optimized-training">9. Complete Example: Memory-Optimized Training<a class="headerlink" href="#9-complete-example-memory-optimized-training" title="Permanent link">&para;</a></h2>
<div class="highlight"><pre><span></span><code><span class="kn">from</span><span class="w"> </span><span class="nn">tabtune</span><span class="w"> </span><span class="kn">import</span> <span class="n">TabularPipeline</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>

<span class="k">def</span><span class="w"> </span><span class="nf">memory_optimized_training</span><span class="p">(</span>
    <span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">X_val</span><span class="p">,</span> <span class="n">y_val</span><span class="p">,</span>
    <span class="n">gpu_memory_gb</span><span class="o">=</span><span class="mi">4</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Train with automatic memory optimization.&quot;&quot;&quot;</span>

    <span class="c1"># Determine configuration</span>
    <span class="k">if</span> <span class="n">gpu_memory_gb</span> <span class="o">&lt;</span> <span class="mi">4</span><span class="p">:</span>
        <span class="n">use_peft</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="n">batch_size</span> <span class="o">=</span> <span class="mi">4</span>
        <span class="n">support_size</span> <span class="o">=</span> <span class="mi">32</span>
        <span class="n">mixed_precision</span> <span class="o">=</span> <span class="s1">&#39;fp16&#39;</span>
        <span class="n">rank</span> <span class="o">=</span> <span class="mi">2</span>
    <span class="k">elif</span> <span class="n">gpu_memory_gb</span> <span class="o">&lt;</span> <span class="mi">8</span><span class="p">:</span>
        <span class="n">use_peft</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="n">batch_size</span> <span class="o">=</span> <span class="mi">8</span>
        <span class="n">support_size</span> <span class="o">=</span> <span class="mi">64</span>
        <span class="n">mixed_precision</span> <span class="o">=</span> <span class="s1">&#39;fp16&#39;</span>
        <span class="n">rank</span> <span class="o">=</span> <span class="mi">4</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">use_peft</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="n">batch_size</span> <span class="o">=</span> <span class="mi">32</span>
        <span class="n">support_size</span> <span class="o">=</span> <span class="mi">128</span>
        <span class="n">mixed_precision</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="n">rank</span> <span class="o">=</span> <span class="mi">8</span>

    <span class="c1"># Create pipeline</span>
    <span class="n">strategy</span> <span class="o">=</span> <span class="s1">&#39;peft&#39;</span> <span class="k">if</span> <span class="n">use_peft</span> <span class="k">else</span> <span class="s1">&#39;base-ft&#39;</span>

    <span class="n">tuning_params</span> <span class="o">=</span> <span class="p">{</span>
        <span class="s1">&#39;device&#39;</span><span class="p">:</span> <span class="s1">&#39;cuda&#39;</span><span class="p">,</span>
        <span class="s1">&#39;epochs&#39;</span><span class="p">:</span> <span class="mi">5</span><span class="p">,</span>
        <span class="s1">&#39;learning_rate&#39;</span><span class="p">:</span> <span class="mf">2e-4</span> <span class="k">if</span> <span class="n">use_peft</span> <span class="k">else</span> <span class="mf">2e-5</span><span class="p">,</span>
        <span class="s1">&#39;batch_size&#39;</span><span class="p">:</span> <span class="n">batch_size</span><span class="p">,</span>
        <span class="s1">&#39;support_size&#39;</span><span class="p">:</span> <span class="n">support_size</span><span class="p">,</span>
        <span class="s1">&#39;num_workers&#39;</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span>
        <span class="s1">&#39;pin_memory&#39;</span><span class="p">:</span> <span class="kc">False</span> <span class="k">if</span> <span class="n">gpu_memory_gb</span> <span class="o">&lt;</span> <span class="mi">4</span> <span class="k">else</span> <span class="kc">True</span>
    <span class="p">}</span>

    <span class="k">if</span> <span class="n">mixed_precision</span><span class="p">:</span>
        <span class="n">tuning_params</span><span class="p">[</span><span class="s1">&#39;mixed_precision&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">mixed_precision</span>

    <span class="k">if</span> <span class="n">use_peft</span><span class="p">:</span>
        <span class="n">tuning_params</span><span class="p">[</span><span class="s1">&#39;peft_config&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="p">{</span>
            <span class="s1">&#39;r&#39;</span><span class="p">:</span> <span class="n">rank</span><span class="p">,</span>
            <span class="s1">&#39;lora_alpha&#39;</span><span class="p">:</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">rank</span><span class="p">,</span>
            <span class="s1">&#39;lora_dropout&#39;</span><span class="p">:</span> <span class="mf">0.05</span>
        <span class="p">}</span>

    <span class="n">pipeline</span> <span class="o">=</span> <span class="n">TabularPipeline</span><span class="p">(</span>
        <span class="n">model_name</span><span class="o">=</span><span class="s1">&#39;TabICL&#39;</span><span class="p">,</span>
        <span class="n">tuning_strategy</span><span class="o">=</span><span class="n">strategy</span><span class="p">,</span>
        <span class="n">tuning_params</span><span class="o">=</span><span class="n">tuning_params</span>
    <span class="p">)</span>

    <span class="c1"># Train with monitoring</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Training with strategy=</span><span class="si">{</span><span class="n">strategy</span><span class="si">}</span><span class="s2">, batch_size=</span><span class="si">{</span><span class="n">batch_size</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="n">pipeline</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

    <span class="c1"># Evaluate</span>
    <span class="n">metrics</span> <span class="o">=</span> <span class="n">pipeline</span><span class="o">.</span><span class="n">evaluate</span><span class="p">(</span><span class="n">X_val</span><span class="p">,</span> <span class="n">y_val</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Validation accuracy: </span><span class="si">{</span><span class="n">metrics</span><span class="p">[</span><span class="s1">&#39;accuracy&#39;</span><span class="p">]</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">pipeline</span>

<span class="c1"># Usage</span>
<span class="n">pipeline</span> <span class="o">=</span> <span class="n">memory_optimized_training</span><span class="p">(</span>
    <span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">X_val</span><span class="p">,</span> <span class="n">y_val</span><span class="p">,</span>
    <span class="n">gpu_memory_gb</span><span class="o">=</span><span class="mi">4</span>
<span class="p">)</span>
</code></pre></div>
<hr />
<h2 id="10-quick-reference">10. Quick Reference<a class="headerlink" href="#10-quick-reference" title="Permanent link">&para;</a></h2>
<h3 id="memory-reduction-techniques-by-impact">Memory Reduction Techniques (by impact)<a class="headerlink" href="#memory-reduction-techniques-by-impact" title="Permanent link">&para;</a></h3>
<table>
<thead>
<tr>
<th>Technique</th>
<th>Memory Saving</th>
<th>Time Overhead</th>
<th>Effort</th>
</tr>
</thead>
<tbody>
<tr>
<td>PEFT</td>
<td>60-90%</td>
<td>None</td>
<td>Low</td>
</tr>
<tr>
<td>Batch size ÷2</td>
<td>50%</td>
<td>None</td>
<td>Low</td>
</tr>
<tr>
<td>Mixed precision</td>
<td>20-30%</td>
<td>20-30%</td>
<td>Medium</td>
</tr>
<tr>
<td>Gradient accumulation</td>
<td>0%</td>
<td>0%</td>
<td>Low</td>
</tr>
<tr>
<td>Gradient checkpoint</td>
<td>30-50%</td>
<td>20-30%</td>
<td>Medium</td>
</tr>
<tr>
<td>Quantization</td>
<td>75%</td>
<td>5-10%</td>
<td>High</td>
</tr>
</tbody>
</table>
<hr />
<h2 id="11-best-practices">11. Best Practices<a class="headerlink" href="#11-best-practices" title="Permanent link">&para;</a></h2>
<h3 id="dos">✅ Do's<a class="headerlink" href="#dos" title="Permanent link">&para;</a></h3>
<ul>
<li>✅ Profile memory before optimization</li>
<li>✅ Use PEFT for memory-constrained environments</li>
<li>✅ Start with small batch sizes</li>
<li>✅ Use mixed precision when possible</li>
<li>✅ Monitor memory during training</li>
<li>✅ Empty cache between experiments</li>
<li>✅ Use gradient accumulation for large effective batches</li>
</ul>
<h3 id="donts">❌ Don'ts<a class="headerlink" href="#donts" title="Permanent link">&para;</a></h3>
<ul>
<li>❌ Don't forget to clear cache between runs</li>
<li>❌ Don't use full precision when half works</li>
<li>❌ Don't load entire dataset to memory</li>
<li>❌ Don't tune without monitoring memory</li>
<li>❌ Don't ignore OOM errors</li>
</ul>
<hr />
<h2 id="12-next-steps">12. Next Steps<a class="headerlink" href="#12-next-steps" title="Permanent link">&para;</a></h2>
<ul>
<li><a href="../../user-guide/tuning-strategies/">Tuning Strategies</a> - PEFT details</li>
<li><a href="../peft-lora/">PEFT &amp; LoRA</a> - Memory-efficient fine-tuning</li>
<li><a href="../peft-lora/">Advanced Topics</a> - Advanced optimizations</li>
<li><a href="../multi-gpu/">Multi-GPU</a> - Distributed training</li>
</ul>
<hr />
<p>Optimize memory strategically to train powerful models on limited resources!</p>
              
            </div>
          </div><footer>

  <hr/>

  <div role="contentinfo">
    <!-- Copyright etc -->
  </div>

  Built with <a href="https://www.mkdocs.org/">MkDocs</a> using a <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
</footer>
          
        </div>
      </div>

    </section>

  </div>

  <div class="rst-versions" role="note" aria-label="Versions">
  <span class="rst-current-version" data-toggle="rst-current-version">
    
        <span>
          <a href="https://github.com/Lexsi-Labs/TabTune_Internal" class="fa fa-code-fork" style="color: #fcfcfc"> Lexsi-Labs/TabTune_Internal</a>
        </span>
    
    
    
  </span>
</div>
    <script src="../../js/jquery-3.6.0.min.js"></script>
    <script>var base_url = "../..";</script>
    <script src="../../js/theme_extra.js"></script>
    <script src="../../js/theme.js"></script>
      <script src="../../search/main.js"></script>
    <script>
        jQuery(function () {
            SphinxRtdTheme.Navigation.enable(true);
        });
    </script>

</body>
</html>
