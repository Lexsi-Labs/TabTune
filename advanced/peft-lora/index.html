<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="author" content="TabTune Development Team" />
      <link rel="shortcut icon" href="../../img/favicon.ico" />
    <title>PEFT & LoRA - TabTune Documentation</title>
    <link rel="stylesheet" href="../../css/theme.css" />
    <link rel="stylesheet" href="../../css/theme_extra.css" />
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/styles/github.min.css" />
        <link href="../../assets/_mkdocstrings.css" rel="stylesheet" />
    
      <script>
        // Current page data
        var mkdocs_page_name = "PEFT \u0026 LoRA";
        var mkdocs_page_input_path = "advanced/peft-lora.md";
        var mkdocs_page_url = null;
      </script>
    
    <!--[if lt IE 9]>
      <script src="../../js/html5shiv.min.js"></script>
    <![endif]-->
      <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/highlight.min.js"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/languages/python.min.js"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/languages/yaml.min.js"></script>
      <script>hljs.highlightAll();</script> 
</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side stickynav">
    <div class="wy-side-scroll">
      <div class="wy-side-nav-search">
          <a href="../..">
          <img src="../../assets/tabtune.svg" class="logo" alt="Logo"/>
        </a><div role="search">
  <form id ="rtd-search-form" class="wy-form" action="../../search.html" method="get">
      <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" title="Type search term here" />
  </form>
</div>
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../..">Home</a>
                </li>
              </ul>
              <p class="caption"><span class="caption-text">Getting Started</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="../../getting-started/installation/">Installation</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../getting-started/quick-start/">Quick Start</a>
                  </li>
                  <li class="toctree-l1"><a class="" href="../../getting-started/basic-concepts.md">Basic Concepts</a>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">User Guide</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="../../user-guide/pipeline-overview/">TabularPipeline Overview</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../user-guide/data-processing/">Data Processing</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../user-guide/tuning-strategies/">Tuning Strategies</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../user-guide/model-selection/">Model Selection</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../user-guide/saving-loading/">Saving and Loading</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../user-guide/leaderboard/">Model Comparison</a>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">Models</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="../../models/overview/">Overview</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../models/tabpfn/">TabPFN</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../models/tabicl/">TabICL</a>
                  </li>
                  <li class="toctree-l1"><a class="" href="../../models/tabbiaxial.md">TabBiaxial</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../models/tabdpt/">TabDPT</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../models/mitra/">Mitra</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../models/contexttab/">ConTextTab</a>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">Advanced Topics</span></p>
              <ul class="current">
                  <li class="toctree-l1 current"><a class="reference internal current" href="#">PEFT & LoRA</a>
    <ul class="current">
    <li class="toctree-l2"><a class="reference internal" href="#1-introduction-to-peft-and-lora">1. Introduction to PEFT and LoRA</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#11-what-is-peft">1.1 What is PEFT?</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#12-what-is-lora">1.2 What is LoRA?</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#13-key-innovation">1.3 Key Innovation</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#2-mathematical-foundation">2. Mathematical Foundation</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#21-low-rank-decomposition">2.1 Low-Rank Decomposition</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#22-scaling">2.2 Scaling</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#23-dropout">2.3 Dropout</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#3-lora-in-tabtune">3. LoRA in TabTune</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#31-lora-configuration">3.1 LoRA Configuration</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#32-lora-linear-layer">3.2 LoRA Linear Layer</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#33-weight-freezing">3.3 Weight Freezing</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#4-lora-hyperparameter-tuning">4. LoRA Hyperparameter Tuning</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#41-rank-selection">4.1 Rank Selection</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#42-alpha-selection">4.2 Alpha Selection</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#43-dropout-probability">4.3 Dropout Probability</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#5-target-module-selection">5. Target Module Selection</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#51-module-hierarchy">5.1 Module Hierarchy</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#52-model-specific-defaults">5.2 Model-Specific Defaults</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#53-custom-target-selection">5.3 Custom Target Selection</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#6-lora-training">6. LoRA Training</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#61-typical-training-loop">6.1 Typical Training Loop</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#62-learning-rate-strategy">6.2 Learning Rate Strategy</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#63-optimizer-configuration">6.3 Optimizer Configuration</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#7-memory-analysis">7. Memory Analysis</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#71-memory-breakdown">7.1 Memory Breakdown</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#72-practical-memory-savings">7.2 Practical Memory Savings</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#9-complete-example">9. Complete Example</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#91-memory-constrained-scenario">9.1 Memory-Constrained Scenario</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#92-rank-exploration">9.2 Rank Exploration</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#93-memory-speed-trade-off">9.3 Memory-Speed Trade-off</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#10-saving-and-loading-lora-models">10. Saving and Loading LoRA Models</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#101-save-lora-adapters-only">10.1 Save LoRA Adapters Only</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#102-load-and-merge">10.2 Load and Merge</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#103-full-pipeline-serialization">10.3 Full Pipeline Serialization</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#11-troubleshooting">11. Troubleshooting</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#issue-lora-accuracy-much-lower-than-base-ft">Issue: "LoRA accuracy much lower than base-FT"</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#issue-training-diverging-with-lora">Issue: "Training diverging with LoRA"</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#issue-still-out-of-memory-with-lora">Issue: "Still out of memory with LoRA"</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#issue-lora-inference-slow">Issue: "LoRA inference slow"</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#13-best-practices">13. Best Practices</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#dos">✅ Do's</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#donts">❌ Don'ts</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#14-comparison-base-ft-vs-lora">14. Comparison: Base-FT vs LoRA</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#15-quick-reference">15. Quick Reference</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#16-next-steps">16. Next Steps</a>
    </li>
    </ul>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../hyperparameter-tuning/">Hyperparameter Tuning</a>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">API Reference</span></p>
              <ul>
                  <li class="toctree-l1"><a class="" href="../../api/pipeline.md">TabularPipeline</a>
                  </li>
                  <li class="toctree-l1"><a class="" href="../../api/data-processor.md">DataProcessor</a>
                  </li>
                  <li class="toctree-l1"><a class="" href="../../api/tuning-manager.md">TuningManager</a>
                  </li>
                  <li class="toctree-l1"><a class="" href="../../api/leaderboard.md">TabularLeaderboard</a>
                  </li>
                  <li class="toctree-l1"><a class="" href="../../api/peft-utils.md">PEFT Utils</a>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">Examples</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="../../examples/classification/">Classification Tasks</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../examples/large-datasets/">Large Datasets</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../examples/peft-examples/">PEFT Fine-Tuning</a>
                  </li>
                  <li class="toctree-l1"><a class="" href="../../examples/benchmarking.md">Benchmarking</a>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">Contributing</span></p>
              <ul>
                  <li class="toctree-l1"><a class="" href="../../contributing/setup.md">Development Setup</a>
                  </li>
                  <li class="toctree-l1"><a class="" href="../../contributing/standards.md">Code Standards</a>
                  </li>
                  <li class="toctree-l1"><a class="" href="../../contributing/new-models.md">Adding New Models</a>
                  </li>
                  <li class="toctree-l1"><a class="" href="../../contributing/documentation.md">Documentation Guide</a>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">About</span></p>
              <ul>
                  <li class="toctree-l1"><a class="" href="../../about/release-notes.md">Release Notes</a>
                  </li>
                  <li class="toctree-l1"><a class="" href="../../about/roadmap.md">Roadmap</a>
                  </li>
                  <li class="toctree-l1"><a class="" href="../../about/faq.md">FAQ</a>
                  </li>
                  <li class="toctree-l1"><a class="" href="../../about/license.md">License</a>
                  </li>
              </ul>
      </div>
    </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">
      <nav class="wy-nav-top" role="navigation" aria-label="Mobile navigation menu">
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../..">TabTune Documentation</a>
        
      </nav>
      <div class="wy-nav-content">
        <div class="rst-content"><div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href="../.." class="icon icon-home" aria-label="Docs"></a></li>
          <li class="breadcrumb-item">Advanced Topics</li>
      <li class="breadcrumb-item active">PEFT & LoRA</li>
    <li class="wy-breadcrumbs-aside">
          <a href="https://github.com/Lexsi-Labs/TabTune_Internal/edit/master/docs/advanced/peft-lora.md">Edit on Lexsi-Labs/TabTune_Internal</a>
    </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
            <div class="section" itemprop="articleBody">
              
                <h1 id="peft-lora-parameter-efficient-fine-tuning-for-tabular-models">PEFT &amp; LoRA: Parameter-Efficient Fine-Tuning for Tabular Models<a class="headerlink" href="#peft-lora-parameter-efficient-fine-tuning-for-tabular-models" title="Permanent link">&para;</a></h1>
<p>This document provides an in-depth guide to Parameter-Efficient Fine-Tuning (PEFT) using Low-Rank Adaptation (LoRA) for TabTune models. Learn the theory, implementation, and best practices for memory-efficient model adaptation.</p>
<hr />
<h2 id="1-introduction-to-peft-and-lora">1. Introduction to PEFT and LoRA<a class="headerlink" href="#1-introduction-to-peft-and-lora" title="Permanent link">&para;</a></h2>
<h3 id="11-what-is-peft">1.1 What is PEFT?<a class="headerlink" href="#11-what-is-peft" title="Permanent link">&para;</a></h3>
<p><strong>Parameter-Efficient Fine-Tuning (PEFT)</strong> is a set of techniques to adapt large pre-trained models using only a small fraction of the total parameters, dramatically reducing:</p>
<ul>
<li>Memory consumption (90% reduction)</li>
<li>Training time (2-3x speedup)</li>
<li>Storage requirements (only store small adapters)</li>
</ul>
<h3 id="12-what-is-lora">1.2 What is LoRA?<a class="headerlink" href="#12-what-is-lora" title="Permanent link">&para;</a></h3>
<p><strong>Low-Rank Adaptation (LoRA)</strong> is a specific PEFT technique that:</p>
<ul>
<li>Freezes pre-trained model weights</li>
<li>Adds small trainable "adapter" layers</li>
<li>Uses low-rank decomposition for efficiency</li>
<li>Trains only 1-10% of parameters</li>
</ul>
<h3 id="13-key-innovation">1.3 Key Innovation<a class="headerlink" href="#13-key-innovation" title="Permanent link">&para;</a></h3>
<p>Instead of updating all weights, LoRA learns a low-rank approximation of weight updates:</p>
<p>[
W' = W_0 + \Delta W = W_0 + BA
]</p>
<p>Where:
- (W_0): Original frozen weights (large)
- (\Delta W = BA): Low-rank decomposition
- (A): Input projection (small)
- (B): Output projection (small)
- (r): Rank (typically 4-16, much smaller than weight dimensions)</p>
<hr />
<h2 id="2-mathematical-foundation">2. Mathematical Foundation<a class="headerlink" href="#2-mathematical-foundation" title="Permanent link">&para;</a></h2>
<h3 id="21-low-rank-decomposition">2.1 Low-Rank Decomposition<a class="headerlink" href="#21-low-rank-decomposition" title="Permanent link">&para;</a></h3>
<p>For a weight matrix (W \in \mathbb{R}^{d_{out} \times d_{in}}), LoRA represents updates as:</p>
<p>[
\Delta W = BA, \quad B \in \mathbb{R}^{d_{out} \times r}, A \in \mathbb{R}^{r \times d_{in}}
]</p>
<p><strong>Complexity Reduction</strong>:
- Full weights: (d_{out} \times d_{in}) parameters
- LoRA: (r(d_{out} + d_{in})) parameters
- Compression ratio: (\frac{r(d_{out} + d_{in})}{d_{out} \times d_{in}})</p>
<p><strong>Example</strong>: For a 768×768 weight matrix:
- Full: 589,824 parameters
- LoRA (r=8): 12,288 parameters
- Compression: 98% reduction</p>
<h3 id="22-scaling">2.2 Scaling<a class="headerlink" href="#22-scaling" title="Permanent link">&para;</a></h3>
<p>To balance adaptation magnitude, LoRA scales the output:</p>
<p>[
h = W_0 x + \alpha \frac{1}{r} B(Ax)
]</p>
<p>Where (\alpha) (lora_alpha) controls the scaling factor (\frac{\alpha}{r}).</p>
<p><strong>Effect of Alpha</strong>:
- Higher alpha: Larger adaptation magnitude
- Default: (\alpha = 2r) (empirically optimal)</p>
<h3 id="23-dropout">2.3 Dropout<a class="headerlink" href="#23-dropout" title="Permanent link">&para;</a></h3>
<p>Dropout is applied to the input before LoRA projection for regularization:</p>
<p>[
h = W_0 x + \alpha \frac{1}{r} B(\text{dropout}(Ax))
]</p>
<hr />
<h2 id="3-lora-in-tabtune">3. LoRA in TabTune<a class="headerlink" href="#3-lora-in-tabtune" title="Permanent link">&para;</a></h2>
<h3 id="31-lora-configuration">3.1 LoRA Configuration<a class="headerlink" href="#31-lora-configuration" title="Permanent link">&para;</a></h3>
<div class="highlight"><pre><span></span><code><span class="n">peft_config</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s1">&#39;r&#39;</span><span class="p">:</span> <span class="mi">8</span><span class="p">,</span>                    <span class="c1"># Rank (main hyperparameter)</span>
    <span class="s1">&#39;lora_alpha&#39;</span><span class="p">:</span> <span class="mi">16</span><span class="p">,</span>          <span class="c1"># Scaling factor</span>
    <span class="s1">&#39;lora_dropout&#39;</span><span class="p">:</span> <span class="mf">0.05</span><span class="p">,</span>      <span class="c1"># Dropout probability</span>
    <span class="s1">&#39;target_modules&#39;</span><span class="p">:</span> <span class="kc">None</span><span class="p">,</span>    <span class="c1"># Modules to adapt (model default)</span>
    <span class="s1">&#39;bias&#39;</span><span class="p">:</span> <span class="s1">&#39;none&#39;</span>             <span class="c1"># Bias handling</span>
<span class="p">}</span>
</code></pre></div>
<h3 id="32-lora-linear-layer">3.2 LoRA Linear Layer<a class="headerlink" href="#32-lora-linear-layer" title="Permanent link">&para;</a></h3>
<p>TabTune implements <code>LoRALinear</code> that wraps standard PyTorch linear layers:</p>
<div class="highlight"><pre><span></span><code><span class="k">class</span><span class="w"> </span><span class="nc">LoRALinear</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">base_linear</span><span class="p">,</span> <span class="n">r</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="mf">0.05</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">base</span> <span class="o">=</span> <span class="n">base_linear</span>          <span class="c1"># Frozen base layer</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">lora_A</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="o">...</span><span class="p">,</span> <span class="n">r</span><span class="p">)</span>  <span class="c1"># Adapter A</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">lora_B</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">r</span><span class="p">,</span> <span class="o">...</span><span class="p">)</span>  <span class="c1"># Adapter B</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">dropout</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">scaling</span> <span class="o">=</span> <span class="n">alpha</span> <span class="o">/</span> <span class="n">r</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="c1"># Base forward (no gradients)</span>
        <span class="n">base_out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">base</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="c1"># LoRA forward</span>
        <span class="n">lora_out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">lora_B</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">lora_A</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">x</span><span class="p">)))</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">scaling</span>
        <span class="k">return</span> <span class="n">base_out</span> <span class="o">+</span> <span class="n">lora_out</span>
</code></pre></div>
<h3 id="33-weight-freezing">3.3 Weight Freezing<a class="headerlink" href="#33-weight-freezing" title="Permanent link">&para;</a></h3>
<ul>
<li>Base model weights: <code>requires_grad=False</code></li>
<li>LoRA adapters: <code>requires_grad=True</code></li>
<li>Enables gradient computation only on adapters</li>
</ul>
<hr />
<h2 id="4-lora-hyperparameter-tuning">4. LoRA Hyperparameter Tuning<a class="headerlink" href="#4-lora-hyperparameter-tuning" title="Permanent link">&para;</a></h2>
<h3 id="41-rank-selection">4.1 Rank Selection<a class="headerlink" href="#41-rank-selection" title="Permanent link">&para;</a></h3>
<p>The rank <code>r</code> is the most critical hyperparameter:</p>
<table>
<thead>
<tr>
<th>Rank</th>
<th>Parameters</th>
<th>Memory</th>
<th>Accuracy</th>
<th>Speed</th>
</tr>
</thead>
<tbody>
<tr>
<td>r=2</td>
<td>Minimal</td>
<td>Very Low</td>
<td>Lower</td>
<td>⭐⭐⭐⭐⭐</td>
</tr>
<tr>
<td>r=4</td>
<td>Low</td>
<td>Low</td>
<td>Good</td>
<td>⭐⭐⭐⭐⭐</td>
</tr>
<tr>
<td>r=8</td>
<td>Moderate</td>
<td>Moderate</td>
<td>Better</td>
<td>⭐⭐⭐⭐</td>
</tr>
<tr>
<td>r=16</td>
<td>High</td>
<td>High</td>
<td>Best</td>
<td>⭐⭐⭐</td>
</tr>
<tr>
<td>r=32</td>
<td>Very High</td>
<td>Very High</td>
<td>Optimal</td>
<td>⭐⭐</td>
</tr>
</tbody>
</table>
<p><strong>Guidelines</strong>:
- <strong>Small data (10K)</strong>: r=4 (enough for adaptation)
- <strong>Medium data (100K)</strong>: r=8 (balanced)
- <strong>Large data (1M)</strong>: r=16 (more expressive)
- <strong>Very constrained</strong>: r=2 (minimum viable)</p>
<p><strong>Rule of Thumb</strong>:
[
r = \max(4, \frac{\text{dataset_size}}{50000})
]</p>
<h3 id="42-alpha-selection">4.2 Alpha Selection<a class="headerlink" href="#42-alpha-selection" title="Permanent link">&para;</a></h3>
<p>Alpha controls the magnitude of LoRA contribution:</p>
<div class="highlight"><pre><span></span><code><span class="c1"># Recommended: alpha = 2 * rank</span>
<span class="n">peft_config</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s1">&#39;r&#39;</span><span class="p">:</span> <span class="mi">8</span><span class="p">,</span>
    <span class="s1">&#39;lora_alpha&#39;</span><span class="p">:</span> <span class="mi">16</span><span class="p">,</span>  <span class="c1"># = 2 * 8</span>
    <span class="s1">&#39;lora_dropout&#39;</span><span class="p">:</span> <span class="mf">0.05</span>
<span class="p">}</span>
</code></pre></div>
<p><strong>Effects</strong>:
- <strong>Alpha too low</strong>: Adaptation weak, training slow
- <strong>Alpha = 2r</strong>: Empirically optimal
- <strong>Alpha too high</strong>: Training unstable, may diverge</p>
<h3 id="43-dropout-probability">4.3 Dropout Probability<a class="headerlink" href="#43-dropout-probability" title="Permanent link">&para;</a></h3>
<p>LoRA dropout acts as regularization:</p>
<div class="highlight"><pre><span></span><code><span class="n">lora_dropout_values</span> <span class="o">=</span> <span class="p">{</span>
    <span class="mf">0.0</span><span class="p">:</span> <span class="s1">&#39;No regularization (may overfit)&#39;</span><span class="p">,</span>
    <span class="mf">0.05</span><span class="p">:</span> <span class="s1">&#39;Light regularization (default)&#39;</span><span class="p">,</span>
    <span class="mf">0.1</span><span class="p">:</span> <span class="s1">&#39;Moderate regularization&#39;</span><span class="p">,</span>
    <span class="mf">0.2</span><span class="p">:</span> <span class="s1">&#39;Strong regularization (for small data)&#39;</span>
<span class="p">}</span>
</code></pre></div>
<p><strong>Selection</strong>:
- <strong>Large data</strong>: 0.05 (default)
- <strong>Small data</strong>: 0.1-0.2 (prevent overfitting)
- <strong>Already regularized model</strong>: 0.0-0.05</p>
<hr />
<h2 id="5-target-module-selection">5. Target Module Selection<a class="headerlink" href="#5-target-module-selection" title="Permanent link">&para;</a></h2>
<h3 id="51-module-hierarchy">5.1 Module Hierarchy<a class="headerlink" href="#51-module-hierarchy" title="Permanent link">&para;</a></h3>
<p>TabTune identifies target modules via pattern matching:</p>
<div class="highlight"><pre><span></span><code><span class="c1"># LoRA targets linear transformation layers</span>
<span class="n">target_modules</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s1">&#39;column_embeddings&#39;</span><span class="p">:</span> <span class="s1">&#39;Column feature processing&#39;</span><span class="p">,</span>
    <span class="s1">&#39;row_attention&#39;</span><span class="p">:</span> <span class="s1">&#39;Row-wise interactions&#39;</span><span class="p">,</span>
    <span class="s1">&#39;prediction_head&#39;</span><span class="p">:</span> <span class="s1">&#39;Final prediction&#39;</span><span class="p">,</span>
    <span class="s1">&#39;decoder&#39;</span><span class="p">:</span> <span class="s1">&#39;Feature reconstruction&#39;</span>
<span class="p">}</span>
</code></pre></div>
<h3 id="52-model-specific-defaults">5.2 Model-Specific Defaults<a class="headerlink" href="#52-model-specific-defaults" title="Permanent link">&para;</a></h3>
<p>Each model has pre-configured target modules optimized for LoRA:</p>
<p><strong>TabICL</strong>:
<div class="highlight"><pre><span></span><code><span class="n">target_modules</span> <span class="o">=</span> <span class="p">[</span>
    <span class="s1">&#39;col_embedder.tf_col&#39;</span><span class="p">,</span>
    <span class="s1">&#39;row_interactor&#39;</span><span class="p">,</span>
    <span class="s1">&#39;icl_predictor.tf_icl&#39;</span><span class="p">,</span>
    <span class="s1">&#39;icl_predictor.decoder&#39;</span>
<span class="p">]</span>
</code></pre></div></p>
<p><strong>TabDPT</strong>:
<div class="highlight"><pre><span></span><code><span class="n">target_modules</span> <span class="o">=</span> <span class="p">[</span>
    <span class="s1">&#39;transformer_encoder&#39;</span><span class="p">,</span>
    <span class="s1">&#39;encoder&#39;</span><span class="p">,</span>
    <span class="s1">&#39;y_encoder&#39;</span><span class="p">,</span>
    <span class="s1">&#39;head&#39;</span>
<span class="p">]</span>
</code></pre></div></p>
<p><strong>Mitra</strong>:
<div class="highlight"><pre><span></span><code><span class="n">target_modules</span> <span class="o">=</span> <span class="p">[</span>
    <span class="s1">&#39;x_embedding&#39;</span><span class="p">,</span>
    <span class="s1">&#39;layers&#39;</span><span class="p">,</span>
    <span class="s1">&#39;final_layer&#39;</span>
<span class="p">]</span>
</code></pre></div></p>
<h3 id="53-custom-target-selection">5.3 Custom Target Selection<a class="headerlink" href="#53-custom-target-selection" title="Permanent link">&para;</a></h3>
<p>Override defaults for specific needs:</p>
<div class="highlight"><pre><span></span><code><span class="n">pipeline</span> <span class="o">=</span> <span class="n">TabularPipeline</span><span class="p">(</span>
    <span class="n">model_name</span><span class="o">=</span><span class="s1">&#39;TabICL&#39;</span><span class="p">,</span>
    <span class="n">tuning_strategy</span><span class="o">=</span><span class="s1">&#39;peft&#39;</span><span class="p">,</span>
    <span class="n">tuning_params</span><span class="o">=</span><span class="p">{</span>
        <span class="s1">&#39;peft_config&#39;</span><span class="p">:</span> <span class="p">{</span>
            <span class="s1">&#39;r&#39;</span><span class="p">:</span> <span class="mi">8</span><span class="p">,</span>
            <span class="s1">&#39;lora_alpha&#39;</span><span class="p">:</span> <span class="mi">16</span><span class="p">,</span>
            <span class="s1">&#39;target_modules&#39;</span><span class="p">:</span> <span class="p">[</span>
                <span class="s1">&#39;col_embedder.tf_col&#39;</span><span class="p">,</span>  <span class="c1"># Only column embedder</span>
                <span class="s1">&#39;icl_predictor.decoder&#39;</span>   <span class="c1"># Plus decoder</span>
            <span class="p">]</span>
        <span class="p">}</span>
    <span class="p">}</span>
<span class="p">)</span>
</code></pre></div>
<hr />
<h2 id="6-lora-training">6. LoRA Training<a class="headerlink" href="#6-lora-training" title="Permanent link">&para;</a></h2>
<h3 id="61-typical-training-loop">6.1 Typical Training Loop<a class="headerlink" href="#61-typical-training-loop" title="Permanent link">&para;</a></h3>
<div class="highlight"><pre><span></span><code><span class="kn">from</span><span class="w"> </span><span class="nn">tabtune</span><span class="w"> </span><span class="kn">import</span> <span class="n">TabularPipeline</span>

<span class="c1"># Create pipeline with PEFT</span>
<span class="n">pipeline</span> <span class="o">=</span> <span class="n">TabularPipeline</span><span class="p">(</span>
    <span class="n">model_name</span><span class="o">=</span><span class="s1">&#39;TabICL&#39;</span><span class="p">,</span>
    <span class="n">tuning_strategy</span><span class="o">=</span><span class="s1">&#39;peft&#39;</span><span class="p">,</span>
    <span class="n">tuning_params</span><span class="o">=</span><span class="p">{</span>
        <span class="s1">&#39;device&#39;</span><span class="p">:</span> <span class="s1">&#39;cuda&#39;</span><span class="p">,</span>
        <span class="s1">&#39;epochs&#39;</span><span class="p">:</span> <span class="mi">5</span><span class="p">,</span>
        <span class="s1">&#39;learning_rate&#39;</span><span class="p">:</span> <span class="mf">2e-4</span><span class="p">,</span>  <span class="c1"># Typically higher than base-ft</span>
        <span class="s1">&#39;peft_config&#39;</span><span class="p">:</span> <span class="p">{</span><span class="s1">&#39;r&#39;</span><span class="p">:</span> <span class="mi">8</span><span class="p">,</span> <span class="s1">&#39;lora_alpha&#39;</span><span class="p">:</span> <span class="mi">16</span><span class="p">}</span>
    <span class="p">}</span>
<span class="p">)</span>

<span class="c1"># Training (only adapters are updated)</span>
<span class="n">pipeline</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="c1"># Inference</span>
<span class="n">predictions</span> <span class="o">=</span> <span class="n">pipeline</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
</code></pre></div>
<h3 id="62-learning-rate-strategy">6.2 Learning Rate Strategy<a class="headerlink" href="#62-learning-rate-strategy" title="Permanent link">&para;</a></h3>
<p>LoRA uses different learning rates than base fine-tuning:</p>
<div class="highlight"><pre><span></span><code><span class="c1"># Base fine-tuning (all parameters)</span>
<span class="n">learning_rate_base_ft</span> <span class="o">=</span> <span class="mf">2e-5</span>

<span class="c1"># LoRA fine-tuning (small parameters)</span>
<span class="n">learning_rate_peft</span> <span class="o">=</span> <span class="mf">2e-4</span>  <span class="c1"># 10x higher typical</span>

<span class="c1"># Rationale: Smaller parameter updates need larger learning rates</span>
</code></pre></div>
<h3 id="63-optimizer-configuration">6.3 Optimizer Configuration<a class="headerlink" href="#63-optimizer-configuration" title="Permanent link">&para;</a></h3>
<div class="highlight"><pre><span></span><code><span class="c1"># LoRA-specific optimizer settings</span>
<span class="n">optimizer_config</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s1">&#39;optimizer&#39;</span><span class="p">:</span> <span class="s1">&#39;adamw&#39;</span><span class="p">,</span>
    <span class="s1">&#39;learning_rate&#39;</span><span class="p">:</span> <span class="mf">2e-4</span><span class="p">,</span>
    <span class="s1">&#39;weight_decay&#39;</span><span class="p">:</span> <span class="mf">0.01</span><span class="p">,</span>
    <span class="s1">&#39;eps&#39;</span><span class="p">:</span> <span class="mf">1e-8</span><span class="p">,</span>
    <span class="s1">&#39;betas&#39;</span><span class="p">:</span> <span class="p">(</span><span class="mf">0.9</span><span class="p">,</span> <span class="mf">0.999</span><span class="p">)</span>
<span class="p">}</span>
</code></pre></div>
<hr />
<h2 id="7-memory-analysis">7. Memory Analysis<a class="headerlink" href="#7-memory-analysis" title="Permanent link">&para;</a></h2>
<h3 id="71-memory-breakdown">7.1 Memory Breakdown<a class="headerlink" href="#71-memory-breakdown" title="Permanent link">&para;</a></h3>
<p><strong>Base Fine-Tuning</strong> (full model):
<div class="highlight"><pre><span></span><code>Model weights:     500 MB
Optimizer states:  1 GB  (Adam: 2x weights)
Gradients:         500 MB
Activations:       200 MB
─────────────────────────
Total:             ~2.2 GB per forward/backward
</code></pre></div></p>
<p><strong>LoRA Fine-Tuning</strong> (adapters only):
<div class="highlight"><pre><span></span><code>Model weights:     500 MB (frozen, no gradients)
LoRA adapters:     5 MB
Optimizer states:  10 MB (only adapters)
Gradients:         5 MB
Activations:       200 MB
─────────────────────────
Total:             ~700 MB per forward/backward
~70% reduction
</code></pre></div></p>
<h3 id="72-practical-memory-savings">7.2 Practical Memory Savings<a class="headerlink" href="#72-practical-memory-savings" title="Permanent link">&para;</a></h3>
<table>
<thead>
<tr>
<th>Model</th>
<th>Base-FT</th>
<th>LoRA</th>
<th>Savings</th>
</tr>
</thead>
<tbody>
<tr>
<td>TabICL</td>
<td>12 GB</td>
<td>4 GB</td>
<td>66%</td>
</tr>
<tr>
<td>TabDPT</td>
<td>24 GB</td>
<td>8 GB</td>
<td>66%</td>
</tr>
<tr>
<td>Mitra</td>
<td>20 GB</td>
<td>6 GB</td>
<td>70%</td>
</tr>
</tbody>
</table>
<!-- 
## 8. Performance Impact

### 8.1 Accuracy Trade-off

LoRA typically shows minor accuracy loss compared to base fine-tuning:

<div class="highlight"><pre><span></span><code><span class="c1"># Typical results on TabICL</span>
<span class="n">base_ft_accuracy</span> <span class="o">=</span> <span class="mf">90.5</span><span class="o">%</span>
<span class="n">lora_accuracy</span> <span class="o">=</span> <span class="mf">89.8</span><span class="o">%</span>
<span class="n">loss</span> <span class="o">=</span> <span class="o">-</span><span class="mf">0.7</span><span class="o">%</span>  <span class="c1"># Acceptable trade-off for 66% memory savings</span>
</code></pre></div>

**Accuracy vs Memory**: Empirical findings
- r=4: 91% accuracy, 50% memory
- r=8: 89.8% accuracy, 35% memory
- r=16: 90.2% accuracy, 45% memory

### 8.2 Training Speed

| Strategy | Time | Notes |
|----------|------|-------|
| Base-FT | 100% | Baseline |
| LoRA | 40-60% | Faster due to fewer parameters |
| PEFT+Mixed Precision | 25-35% | Optimal speed |

### 8.3 Inference Speed

- **Base-FT**: Baseline
- **LoRA**: +5-10% overhead (LoRA forward pass)
- **With caching**: Negligible difference
 -->
<hr />
<h2 id="9-complete-example">9. Complete Example<a class="headerlink" href="#9-complete-example" title="Permanent link">&para;</a></h2>
<h3 id="91-memory-constrained-scenario">9.1 Memory-Constrained Scenario<a class="headerlink" href="#91-memory-constrained-scenario" title="Permanent link">&para;</a></h3>
<div class="highlight"><pre><span></span><code><span class="kn">from</span><span class="w"> </span><span class="nn">tabtune</span><span class="w"> </span><span class="kn">import</span> <span class="n">TabularPipeline</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>

<span class="c1"># Check available GPU memory</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Available GPU memory: </span><span class="si">{</span><span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">get_device_properties</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">total_memory</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="mf">1e9</span><span class="si">:</span><span class="s2">.1f</span><span class="si">}</span><span class="s2"> GB&quot;</span><span class="p">)</span>

<span class="c1"># LoRA for 4GB GPU</span>
<span class="n">pipeline</span> <span class="o">=</span> <span class="n">TabularPipeline</span><span class="p">(</span>
    <span class="n">model_name</span><span class="o">=</span><span class="s1">&#39;TabICL&#39;</span><span class="p">,</span>
    <span class="n">tuning_strategy</span><span class="o">=</span><span class="s1">&#39;peft&#39;</span><span class="p">,</span>
    <span class="n">tuning_params</span><span class="o">=</span><span class="p">{</span>
        <span class="s1">&#39;device&#39;</span><span class="p">:</span> <span class="s1">&#39;cuda&#39;</span><span class="p">,</span>
        <span class="s1">&#39;epochs&#39;</span><span class="p">:</span> <span class="mi">5</span><span class="p">,</span>
        <span class="s1">&#39;learning_rate&#39;</span><span class="p">:</span> <span class="mf">2e-4</span><span class="p">,</span>
        <span class="s1">&#39;batch_size&#39;</span><span class="p">:</span> <span class="mi">16</span><span class="p">,</span>
        <span class="s1">&#39;peft_config&#39;</span><span class="p">:</span> <span class="p">{</span>
            <span class="s1">&#39;r&#39;</span><span class="p">:</span> <span class="mi">4</span><span class="p">,</span>              <span class="c1"># Lower rank for less memory</span>
            <span class="s1">&#39;lora_alpha&#39;</span><span class="p">:</span> <span class="mi">8</span><span class="p">,</span>
            <span class="s1">&#39;lora_dropout&#39;</span><span class="p">:</span> <span class="mf">0.1</span>
        <span class="p">}</span>
    <span class="p">}</span>
<span class="p">)</span>

<span class="n">pipeline</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="n">metrics</span> <span class="o">=</span> <span class="n">pipeline</span><span class="o">.</span><span class="n">evaluate</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;LoRA Accuracy: </span><span class="si">{</span><span class="n">metrics</span><span class="p">[</span><span class="s1">&#39;accuracy&#39;</span><span class="p">]</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</code></pre></div>
<h3 id="92-rank-exploration">9.2 Rank Exploration<a class="headerlink" href="#92-rank-exploration" title="Permanent link">&para;</a></h3>
<div class="highlight"><pre><span></span><code><span class="kn">from</span><span class="w"> </span><span class="nn">tabtune</span><span class="w"> </span><span class="kn">import</span> <span class="n">TabularLeaderboard</span>

<span class="c1"># Compare different LoRA ranks</span>
<span class="n">lb</span> <span class="o">=</span> <span class="n">TabularLeaderboard</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span>

<span class="k">for</span> <span class="n">r</span> <span class="ow">in</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">16</span><span class="p">]:</span>
    <span class="n">lb</span><span class="o">.</span><span class="n">add_model</span><span class="p">(</span>
        <span class="s1">&#39;TabICL&#39;</span><span class="p">,</span>
        <span class="s1">&#39;peft&#39;</span><span class="p">,</span>
        <span class="n">name</span><span class="o">=</span><span class="sa">f</span><span class="s1">&#39;LoRA-r</span><span class="si">{</span><span class="n">r</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">,</span>
        <span class="n">tuning_params</span><span class="o">=</span><span class="p">{</span>
            <span class="s1">&#39;epochs&#39;</span><span class="p">:</span> <span class="mi">5</span><span class="p">,</span>
            <span class="s1">&#39;peft_config&#39;</span><span class="p">:</span> <span class="p">{</span><span class="s1">&#39;r&#39;</span><span class="p">:</span> <span class="n">r</span><span class="p">,</span> <span class="s1">&#39;lora_alpha&#39;</span><span class="p">:</span> <span class="mi">2</span><span class="o">*</span><span class="n">r</span><span class="p">}</span>
        <span class="p">}</span>
    <span class="p">)</span>

<span class="n">results</span> <span class="o">=</span> <span class="n">lb</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">rank_by</span><span class="o">=</span><span class="s1">&#39;accuracy&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">lb</span><span class="o">.</span><span class="n">get_ranking</span><span class="p">())</span>
</code></pre></div>
<h3 id="93-memory-speed-trade-off">9.3 Memory-Speed Trade-off<a class="headerlink" href="#93-memory-speed-trade-off" title="Permanent link">&para;</a></h3>
<div class="highlight"><pre><span></span><code><span class="c1"># Explore memory-speed-accuracy trade-off</span>
<span class="n">configs</span> <span class="o">=</span> <span class="p">[</span>
    <span class="p">{</span><span class="s1">&#39;r&#39;</span><span class="p">:</span> <span class="mi">2</span><span class="p">,</span> <span class="s1">&#39;name&#39;</span><span class="p">:</span> <span class="s1">&#39;Ultra-Light&#39;</span><span class="p">},</span>
    <span class="p">{</span><span class="s1">&#39;r&#39;</span><span class="p">:</span> <span class="mi">4</span><span class="p">,</span> <span class="s1">&#39;name&#39;</span><span class="p">:</span> <span class="s1">&#39;Light&#39;</span><span class="p">},</span>
    <span class="p">{</span><span class="s1">&#39;r&#39;</span><span class="p">:</span> <span class="mi">8</span><span class="p">,</span> <span class="s1">&#39;name&#39;</span><span class="p">:</span> <span class="s1">&#39;Medium&#39;</span><span class="p">},</span>
    <span class="p">{</span><span class="s1">&#39;r&#39;</span><span class="p">:</span> <span class="mi">16</span><span class="p">,</span> <span class="s1">&#39;name&#39;</span><span class="p">:</span> <span class="s1">&#39;Heavy&#39;</span><span class="p">}</span>
<span class="p">]</span>

<span class="k">for</span> <span class="n">config</span> <span class="ow">in</span> <span class="n">configs</span><span class="p">:</span>
    <span class="n">pipeline</span> <span class="o">=</span> <span class="n">TabularPipeline</span><span class="p">(</span>
        <span class="n">model_name</span><span class="o">=</span><span class="s1">&#39;TabICL&#39;</span><span class="p">,</span>
        <span class="n">tuning_strategy</span><span class="o">=</span><span class="s1">&#39;peft&#39;</span><span class="p">,</span>
        <span class="n">tuning_params</span><span class="o">=</span><span class="p">{</span>
            <span class="s1">&#39;device&#39;</span><span class="p">:</span> <span class="s1">&#39;cuda&#39;</span><span class="p">,</span>
            <span class="s1">&#39;epochs&#39;</span><span class="p">:</span> <span class="mi">3</span><span class="p">,</span>
            <span class="s1">&#39;peft_config&#39;</span><span class="p">:</span> <span class="p">{</span><span class="s1">&#39;r&#39;</span><span class="p">:</span> <span class="n">config</span><span class="p">[</span><span class="s1">&#39;r&#39;</span><span class="p">]}</span>
        <span class="p">}</span>
    <span class="p">)</span>

    <span class="c1"># Time training</span>
    <span class="kn">import</span><span class="w"> </span><span class="nn">time</span>
    <span class="n">start</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
    <span class="n">pipeline</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
    <span class="n">elapsed</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">start</span>

    <span class="c1"># Get memory usage</span>
    <span class="n">mem</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">max_memory_allocated</span><span class="p">()</span> <span class="o">/</span> <span class="mf">1e9</span>

    <span class="c1"># Evaluate</span>
    <span class="n">metrics</span> <span class="o">=</span> <span class="n">pipeline</span><span class="o">.</span><span class="n">evaluate</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span>

    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">config</span><span class="p">[</span><span class="s1">&#39;name&#39;</span><span class="p">]</span><span class="si">:</span><span class="s2">12</span><span class="si">}</span><span class="s2"> | Rank: </span><span class="si">{</span><span class="n">config</span><span class="p">[</span><span class="s1">&#39;r&#39;</span><span class="p">]</span><span class="si">:</span><span class="s2">2</span><span class="si">}</span><span class="s2"> | &quot;</span>
          <span class="sa">f</span><span class="s2">&quot;Time: </span><span class="si">{</span><span class="n">elapsed</span><span class="si">:</span><span class="s2">6.1f</span><span class="si">}</span><span class="s2">s | Memory: </span><span class="si">{</span><span class="n">mem</span><span class="si">:</span><span class="s2">5.1f</span><span class="si">}</span><span class="s2">GB | &quot;</span>
          <span class="sa">f</span><span class="s2">&quot;Accuracy: </span><span class="si">{</span><span class="n">metrics</span><span class="p">[</span><span class="s1">&#39;accuracy&#39;</span><span class="p">]</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</code></pre></div>
<hr />
<h2 id="10-saving-and-loading-lora-models">10. Saving and Loading LoRA Models<a class="headerlink" href="#10-saving-and-loading-lora-models" title="Permanent link">&para;</a></h2>
<h3 id="101-save-lora-adapters-only">10.1 Save LoRA Adapters Only<a class="headerlink" href="#101-save-lora-adapters-only" title="Permanent link">&para;</a></h3>
<div class="highlight"><pre><span></span><code><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>

<span class="c1"># Save only LoRA adapter weights (minimal storage)</span>
<span class="n">lora_state</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s1">&#39;rank&#39;</span><span class="p">:</span> <span class="mi">8</span><span class="p">,</span>
    <span class="s1">&#39;alpha&#39;</span><span class="p">:</span> <span class="mi">16</span><span class="p">,</span>
    <span class="s1">&#39;lora_a&#39;</span><span class="p">:</span> <span class="n">pipeline</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">lora_A</span><span class="o">.</span><span class="n">state_dict</span><span class="p">(),</span>
    <span class="s1">&#39;lora_b&#39;</span><span class="p">:</span> <span class="n">pipeline</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">lora_B</span><span class="o">.</span><span class="n">state_dict</span><span class="p">()</span>
<span class="p">}</span>

<span class="n">torch</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">lora_state</span><span class="p">,</span> <span class="s1">&#39;lora_adapters.pt&#39;</span><span class="p">)</span>  <span class="c1"># ~1-5 MB</span>
</code></pre></div>
<h3 id="102-load-and-merge">10.2 Load and Merge<a class="headerlink" href="#102-load-and-merge" title="Permanent link">&para;</a></h3>
<div class="highlight"><pre><span></span><code><span class="c1"># Load adapters and merge with base model</span>
<span class="n">lora_state</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s1">&#39;lora_adapters.pt&#39;</span><span class="p">)</span>

<span class="c1"># Merge LoRA into base weights (optional, for inference optimization)</span>
<span class="n">merged_weights</span> <span class="o">=</span> <span class="n">base_weights</span> <span class="o">+</span> <span class="p">(</span><span class="n">lora_B</span> <span class="o">@</span> <span class="n">lora_A</span><span class="p">)</span> <span class="o">*</span> <span class="n">alpha</span> <span class="o">/</span> <span class="n">r</span>
</code></pre></div>
<h3 id="103-full-pipeline-serialization">10.3 Full Pipeline Serialization<a class="headerlink" href="#103-full-pipeline-serialization" title="Permanent link">&para;</a></h3>
<div class="highlight"><pre><span></span><code><span class="c1"># Save complete pipeline with LoRA adapters</span>
<span class="n">pipeline</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="s1">&#39;pipeline_with_lora.joblib&#39;</span><span class="p">)</span>

<span class="c1"># Load and use</span>
<span class="n">loaded</span> <span class="o">=</span> <span class="n">TabularPipeline</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s1">&#39;pipeline_with_lora.joblib&#39;</span><span class="p">)</span>
<span class="n">predictions</span> <span class="o">=</span> <span class="n">loaded</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
</code></pre></div>
<hr />
<h2 id="11-troubleshooting">11. Troubleshooting<a class="headerlink" href="#11-troubleshooting" title="Permanent link">&para;</a></h2>
<h3 id="issue-lora-accuracy-much-lower-than-base-ft">Issue: "LoRA accuracy much lower than base-FT"<a class="headerlink" href="#issue-lora-accuracy-much-lower-than-base-ft" title="Permanent link">&para;</a></h3>
<p><strong>Solution</strong>: Increase rank
<div class="highlight"><pre><span></span><code><span class="n">peft_config</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s1">&#39;r&#39;</span><span class="p">:</span> <span class="mi">16</span><span class="p">,</span>  <span class="c1"># Instead of 8</span>
    <span class="s1">&#39;lora_alpha&#39;</span><span class="p">:</span> <span class="mi">32</span>
<span class="p">}</span>
</code></pre></div></p>
<h3 id="issue-training-diverging-with-lora">Issue: "Training diverging with LoRA"<a class="headerlink" href="#issue-training-diverging-with-lora" title="Permanent link">&para;</a></h3>
<p><strong>Solution</strong>: Reduce learning rate
<div class="highlight"><pre><span></span><code><span class="n">tuning_params</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s1">&#39;learning_rate&#39;</span><span class="p">:</span> <span class="mf">1e-4</span><span class="p">,</span>  <span class="c1"># Instead of 2e-4</span>
    <span class="s1">&#39;warmup_steps&#39;</span><span class="p">:</span> <span class="mi">500</span>
<span class="p">}</span>
</code></pre></div></p>
<h3 id="issue-still-out-of-memory-with-lora">Issue: "Still out of memory with LoRA"<a class="headerlink" href="#issue-still-out-of-memory-with-lora" title="Permanent link">&para;</a></h3>
<p><strong>Solution</strong>: Further reduce parameters
<div class="highlight"><pre><span></span><code><span class="n">peft_config</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s1">&#39;r&#39;</span><span class="p">:</span> <span class="mi">2</span><span class="p">,</span>  <span class="c1"># Minimum viable</span>
    <span class="s1">&#39;lora_dropout&#39;</span><span class="p">:</span> <span class="mf">0.2</span>  <span class="c1"># Stronger regularization</span>
<span class="p">}</span>

<span class="n">tuning_params</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s1">&#39;batch_size&#39;</span><span class="p">:</span> <span class="mi">4</span>  <span class="c1"># Smaller batches</span>
<span class="p">}</span>
</code></pre></div></p>
<h3 id="issue-lora-inference-slow">Issue: "LoRA inference slow"<a class="headerlink" href="#issue-lora-inference-slow" title="Permanent link">&para;</a></h3>
<p><strong>Solution</strong>: Use merged weights
<div class="highlight"><pre><span></span><code><span class="c1"># Merge LoRA weights into base after training</span>
<span class="n">merged_model</span> <span class="o">=</span> <span class="n">merge_lora_weights</span><span class="p">(</span><span class="n">pipeline</span><span class="o">.</span><span class="n">model</span><span class="p">)</span>
</code></pre></div></p>
<hr />
<!-- ## 12. Advanced Topics

### 12.1 LoRA+ Variant

Enhanced LoRA with different learning rates per layer:

<div class="highlight"><pre><span></span><code><span class="c1"># LoRA+ uses higher LR for B matrix</span>
<span class="n">lora_plus_config</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s1">&#39;lr_ratio&#39;</span><span class="p">:</span> <span class="mi">10</span><span class="p">,</span>  <span class="c1"># B matrix gets 10x higher LR</span>
    <span class="s1">&#39;lora_alpha&#39;</span><span class="p">:</span> <span class="mi">16</span><span class="p">,</span>
    <span class="s1">&#39;r&#39;</span><span class="p">:</span> <span class="mi">8</span>
<span class="p">}</span>
</code></pre></div>

### 12.2 QLoRA (Quantized LoRA)

Combine LoRA with quantization for extreme compression:

<div class="highlight"><pre><span></span><code><span class="c1"># 4-bit quantized model with LoRA</span>
<span class="n">quantization_config</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s1">&#39;load_in_4bit&#39;</span><span class="p">:</span> <span class="kc">True</span><span class="p">,</span>
    <span class="s1">&#39;bnb_4bit_compute_dtype&#39;</span><span class="p">:</span> <span class="s1">&#39;float16&#39;</span>
<span class="p">}</span>
<span class="c1"># Reduces model size by 4x more</span>
</code></pre></div>

### 12.3 Multi-LoRA (Mixture of Adapters)

Multiple task-specific LoRA adapters:

<div class="highlight"><pre><span></span><code><span class="c1"># Switch between different LoRA adapters</span>
<span class="n">task_loras</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s1">&#39;classification&#39;</span><span class="p">:</span> <span class="s1">&#39;lora_classification.pt&#39;</span><span class="p">,</span>
    <span class="s1">&#39;regression&#39;</span><span class="p">:</span> <span class="s1">&#39;lora_regression.pt&#39;</span><span class="p">,</span>
    <span class="s1">&#39;clustering&#39;</span><span class="p">:</span> <span class="s1">&#39;lora_clustering.pt&#39;</span>
<span class="p">}</span>

<span class="c1"># Load task-specific adapter</span>
<span class="n">selected_adapter</span> <span class="o">=</span> <span class="n">load_lora_adapter</span><span class="p">(</span><span class="n">task_loras</span><span class="p">[</span><span class="s1">&#39;classification&#39;</span><span class="p">])</span>
</code></pre></div>
 -->
<hr />
<h2 id="13-best-practices">13. Best Practices<a class="headerlink" href="#13-best-practices" title="Permanent link">&para;</a></h2>
<h3 id="dos">✅ Do's<a class="headerlink" href="#dos" title="Permanent link">&para;</a></h3>
<ul>
<li>✅ Start with r=8 (good default)</li>
<li>✅ Use 2x learning rate for LoRA vs base-FT</li>
<li>✅ Include warmup phase (prevent instability)</li>
<li>✅ Monitor gradient norms</li>
<li>✅ Use gradient clipping</li>
<li>✅ Save adapter weights separately</li>
<li>✅ Test rank selection with leaderboard</li>
</ul>
<h3 id="donts">❌ Don'ts<a class="headerlink" href="#donts" title="Permanent link">&para;</a></h3>
<ul>
<li>❌ Don't use same learning rate as base-FT</li>
<li>❌ Don't train very low ranks (r&lt;2) without good reason</li>
<li>❌ Don't skip regularization on small data</li>
<li>❌ Don't forget to freeze base model</li>
<li>❌ Don't use LoRA on tiny models (overhead not worth it)</li>
</ul>
<hr />
<h2 id="14-comparison-base-ft-vs-lora">14. Comparison: Base-FT vs LoRA<a class="headerlink" href="#14-comparison-base-ft-vs-lora" title="Permanent link">&para;</a></h2>
<table>
<thead>
<tr>
<th>Aspect</th>
<th>Base-FT</th>
<th>LoRA</th>
<th>Winner</th>
</tr>
</thead>
<tbody>
<tr>
<td>Accuracy</td>
<td>High</td>
<td>Medium-High</td>
<td>Base-FT (~1% better)</td>
</tr>
<tr>
<td>Memory</td>
<td>Very High</td>
<td>Low</td>
<td>LoRA (70% savings)</td>
</tr>
<tr>
<td>Speed</td>
<td>Slow</td>
<td>Fast</td>
<td>LoRA (2-3x faster)</td>
</tr>
<tr>
<td>Storage</td>
<td>Huge</td>
<td>Tiny</td>
<td>LoRA (100x smaller)</td>
</tr>
<tr>
<td>Scalability</td>
<td>Limited</td>
<td>Excellent</td>
<td>LoRA</td>
</tr>
<tr>
<td>Production</td>
<td>Complex</td>
<td>Simple</td>
<td>LoRA</td>
</tr>
<tr>
<td>Learning Curve</td>
<td>Medium</td>
<td>Low</td>
<td>LoRA</td>
</tr>
</tbody>
</table>
<hr />
<h2 id="15-quick-reference">15. Quick Reference<a class="headerlink" href="#15-quick-reference" title="Permanent link">&para;</a></h2>
<table>
<thead>
<tr>
<th>Task</th>
<th>r</th>
<th>Alpha</th>
<th>Dropout</th>
<th>LR</th>
</tr>
</thead>
<tbody>
<tr>
<td>Small data (10K)</td>
<td>4</td>
<td>8</td>
<td>0.1</td>
<td>2e-4</td>
</tr>
<tr>
<td>Medium data (100K)</td>
<td>8</td>
<td>16</td>
<td>0.05</td>
<td>2e-4</td>
</tr>
<tr>
<td>Large data (1M)</td>
<td>16</td>
<td>32</td>
<td>0.02</td>
<td>1e-4</td>
</tr>
<tr>
<td>Memory limited</td>
<td>2</td>
<td>4</td>
<td>0.2</td>
<td>1e-4</td>
</tr>
<tr>
<td>Max accuracy</td>
<td>16</td>
<td>32</td>
<td>0.05</td>
<td>5e-5</td>
</tr>
</tbody>
</table>
<hr />
<h2 id="16-next-steps">16. Next Steps<a class="headerlink" href="#16-next-steps" title="Permanent link">&para;</a></h2>
<ul>
<li><a href="../../user-guide/tuning-strategies/">Tuning Strategies</a> - Compare strategies</li>
<li><a href="../hyperparameter-tuning/">Hyperparameter Tuning</a> - Full optimization guide</li>
<li><a href="../../models/overview/">Models Overview</a> - PEFT support per model</li>
<li><a href="../../user-guide/leaderboard/">TabularLeaderboard</a> - Compare configurations</li>
</ul>
<hr />
<p>LoRA enables efficient fine-tuning of large tabular models. Use it for memory-constrained environments while maintaining strong performance!</p>
              
            </div>
          </div><footer>
    <div class="rst-footer-buttons" role="navigation" aria-label="Footer Navigation">
        <a href="../../models/contexttab/" class="btn btn-neutral float-left" title="ConTextTab"><span class="icon icon-circle-arrow-left"></span> Previous</a>
        <a href="../hyperparameter-tuning/" class="btn btn-neutral float-right" title="Hyperparameter Tuning">Next <span class="icon icon-circle-arrow-right"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <!-- Copyright etc -->
  </div>

  Built with <a href="https://www.mkdocs.org/">MkDocs</a> using a <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
</footer>
          
        </div>
      </div>

    </section>

  </div>

  <div class="rst-versions" role="note" aria-label="Versions">
  <span class="rst-current-version" data-toggle="rst-current-version">
    
        <span>
          <a href="https://github.com/Lexsi-Labs/TabTune_Internal" class="fa fa-code-fork" style="color: #fcfcfc"> Lexsi-Labs/TabTune_Internal</a>
        </span>
    
    
      <span><a href="../../models/contexttab/" style="color: #fcfcfc">&laquo; Previous</a></span>
    
    
      <span><a href="../hyperparameter-tuning/" style="color: #fcfcfc">Next &raquo;</a></span>
    
  </span>
</div>
    <script src="../../js/jquery-3.6.0.min.js"></script>
    <script>var base_url = "../..";</script>
    <script src="../../js/theme_extra.js"></script>
    <script src="../../js/theme.js"></script>
      <script src="../../search/main.js"></script>
    <script>
        jQuery(function () {
            SphinxRtdTheme.Navigation.enable(true);
        });
    </script>

</body>
</html>
