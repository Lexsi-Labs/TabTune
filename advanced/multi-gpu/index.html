<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="author" content="TabTune Development Team" />
      <link rel="shortcut icon" href="../../img/favicon.ico" />
    <title>Multi-GPU Training: Scaling TabTune Across Multiple GPUs - TabTune Documentation</title>
    <link rel="stylesheet" href="../../css/theme.css" />
    <link rel="stylesheet" href="../../css/theme_extra.css" />
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/styles/github.min.css" />
        <link href="../../assets/_mkdocstrings.css" rel="stylesheet" />
    
      <script>
        // Current page data
        var mkdocs_page_name = "Multi-GPU Training: Scaling TabTune Across Multiple GPUs";
        var mkdocs_page_input_path = "advanced/multi-gpu.md";
        var mkdocs_page_url = null;
      </script>
    
    <!--[if lt IE 9]>
      <script src="../../js/html5shiv.min.js"></script>
    <![endif]-->
      <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/highlight.min.js"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/languages/python.min.js"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/languages/yaml.min.js"></script>
      <script>hljs.highlightAll();</script> 
</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side stickynav">
    <div class="wy-side-scroll">
      <div class="wy-side-nav-search">
          <a href="../..">
          <img src="../../assets/tabtune.svg" class="logo" alt="Logo"/>
        </a><div role="search">
  <form id ="rtd-search-form" class="wy-form" action="../../search.html" method="get">
      <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" title="Type search term here" />
  </form>
</div>
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../..">Home</a>
                </li>
              </ul>
              <p class="caption"><span class="caption-text">Getting Started</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="../../getting-started/installation/">Installation</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../getting-started/quick-start/">Quick Start</a>
                  </li>
                  <li class="toctree-l1"><a class="" href="../../getting-started/basic-concepts.md">Basic Concepts</a>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">User Guide</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="../../user-guide/pipeline-overview/">TabularPipeline Overview</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../user-guide/data-processing/">Data Processing</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../user-guide/tuning-strategies/">Tuning Strategies</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../user-guide/model-selection/">Model Selection</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../user-guide/saving-loading/">Saving and Loading</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../user-guide/leaderboard/">Model Comparison</a>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">Models</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="../../models/overview/">Overview</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../models/tabpfn/">TabPFN</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../models/tabicl/">TabICL</a>
                  </li>
                  <li class="toctree-l1"><a class="" href="../../models/tabbiaxial.md">TabBiaxial</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../models/tabdpt/">TabDPT</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../models/mitra/">Mitra</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../models/contexttab/">ConTextTab</a>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">Advanced Topics</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="../peft-lora/">PEFT & LoRA</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../hyperparameter-tuning/">Hyperparameter Tuning</a>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">API Reference</span></p>
              <ul>
                  <li class="toctree-l1"><a class="" href="../../api/pipeline.md">TabularPipeline</a>
                  </li>
                  <li class="toctree-l1"><a class="" href="../../api/data-processor.md">DataProcessor</a>
                  </li>
                  <li class="toctree-l1"><a class="" href="../../api/tuning-manager.md">TuningManager</a>
                  </li>
                  <li class="toctree-l1"><a class="" href="../../api/leaderboard.md">TabularLeaderboard</a>
                  </li>
                  <li class="toctree-l1"><a class="" href="../../api/peft-utils.md">PEFT Utils</a>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">Examples</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="../../examples/classification/">Classification Tasks</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../examples/large-datasets/">Large Datasets</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../examples/peft-examples/">PEFT Fine-Tuning</a>
                  </li>
                  <li class="toctree-l1"><a class="" href="../../examples/benchmarking.md">Benchmarking</a>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">Contributing</span></p>
              <ul>
                  <li class="toctree-l1"><a class="" href="../../contributing/setup.md">Development Setup</a>
                  </li>
                  <li class="toctree-l1"><a class="" href="../../contributing/standards.md">Code Standards</a>
                  </li>
                  <li class="toctree-l1"><a class="" href="../../contributing/new-models.md">Adding New Models</a>
                  </li>
                  <li class="toctree-l1"><a class="" href="../../contributing/documentation.md">Documentation Guide</a>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">About</span></p>
              <ul>
                  <li class="toctree-l1"><a class="" href="../../about/release-notes.md">Release Notes</a>
                  </li>
                  <li class="toctree-l1"><a class="" href="../../about/roadmap.md">Roadmap</a>
                  </li>
                  <li class="toctree-l1"><a class="" href="../../about/faq.md">FAQ</a>
                  </li>
                  <li class="toctree-l1"><a class="" href="../../about/license.md">License</a>
                  </li>
              </ul>
      </div>
    </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">
      <nav class="wy-nav-top" role="navigation" aria-label="Mobile navigation menu">
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../..">TabTune Documentation</a>
        
      </nav>
      <div class="wy-nav-content">
        <div class="rst-content"><div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href="../.." class="icon icon-home" aria-label="Docs"></a></li>
      <li class="breadcrumb-item active">Multi-GPU Training: Scaling TabTune Across Multiple GPUs</li>
    <li class="wy-breadcrumbs-aside">
          <a href="https://github.com/Lexsi-Labs/TabTune_Internal/edit/master/docs/advanced/multi-gpu.md">Edit on Lexsi-Labs/TabTune_Internal</a>
    </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
            <div class="section" itemprop="articleBody">
              
                <h1 id="multi-gpu-training-scaling-tabtune-across-multiple-gpus">Multi-GPU Training: Scaling TabTune Across Multiple GPUs<a class="headerlink" href="#multi-gpu-training-scaling-tabtune-across-multiple-gpus" title="Permanent link">&para;</a></h1>
<p>This document provides comprehensive guidance on leveraging multiple GPUs for distributed training with TabTune, enabling faster training and handling of larger models and datasets.</p>
<hr />
<h2 id="1-introduction">1. Introduction<a class="headerlink" href="#1-introduction" title="Permanent link">&para;</a></h2>
<p>Multi-GPU training accelerates TabTune workflows through:</p>
<ul>
<li><strong>Data Parallelism</strong>: Distribute data across GPUs</li>
<li><strong>Model Parallelism</strong>: Distribute model across GPUs</li>
<li><strong>Distributed Optimization</strong>: Synchronized gradient updates</li>
<li><strong>Scaling</strong>: Near-linear speedup with multiple GPUs</li>
</ul>
<p>This guide covers setup, strategies, and best practices.</p>
<hr />
<h2 id="2-multi-gpu-fundamentals">2. Multi-GPU Fundamentals<a class="headerlink" href="#2-multi-gpu-fundamentals" title="Permanent link">&para;</a></h2>
<h3 id="21-parallelism-strategies">2.1 Parallelism Strategies<a class="headerlink" href="#21-parallelism-strategies" title="Permanent link">&para;</a></h3>
<div class="highlight"><pre><span></span><code>flowchart TD
    A[Multi-GPU Training] --&gt; B[Data Parallelism]
    A --&gt; C[Model Parallelism]
    A --&gt; D[Pipeline Parallelism]

    B --&gt; B1[&quot;Each GPU gets data batch&quot;]
    B --&gt; B2[&quot;Model replicated on each GPU&quot;]
    B --&gt; B3[&quot;Gradients averaged across GPUs&quot;]

    C --&gt; C1[&quot;Model layers split across GPUs&quot;]
    C --&gt; C2[&quot;Each GPU processes different layers&quot;]
    C --&gt; C3[&quot;Communication between GPUs&quot;]

    D --&gt; D1[&quot;Stages of model pipeline&quot;]
    D --&gt; D2[&quot;Different stages on different GPUs&quot;]
    D --&gt; D3[&quot;Micro-batching for efficiency&quot;]
</code></pre></div>
<h3 id="22-data-parallelism-most-common">2.2 Data Parallelism (Most Common)<a class="headerlink" href="#22-data-parallelism-most-common" title="Permanent link">&para;</a></h3>
<p><strong>Recommended for TabTune</strong> - simplest and most effective:</p>
<div class="highlight"><pre><span></span><code><span class="c1"># Single GPU training</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">TabularPipeline</span><span class="p">(</span><span class="n">model_name</span><span class="o">=</span><span class="s1">&#39;TabICL&#39;</span><span class="p">)</span>
<span class="c1"># Loss: 100% on GPU 0</span>

<span class="c1"># Data parallel (2 GPUs)</span>
<span class="c1"># Batch split into 2 sub-batches</span>
<span class="c1"># GPU 0: sub-batch 1</span>
<span class="c1"># GPU 1: sub-batch 2</span>
<span class="c1"># Gradients averaged</span>
</code></pre></div>
<hr />
<h2 id="3-setup-requirements">3. Setup Requirements<a class="headerlink" href="#3-setup-requirements" title="Permanent link">&para;</a></h2>
<h3 id="31-hardware-prerequisites">3.1 Hardware Prerequisites<a class="headerlink" href="#31-hardware-prerequisites" title="Permanent link">&para;</a></h3>
<div class="highlight"><pre><span></span><code><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>

<span class="c1"># Check GPU availability</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;GPUs available: </span><span class="si">{</span><span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">device_count</span><span class="p">()</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Current GPU: </span><span class="si">{</span><span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">current_device</span><span class="p">()</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">device_count</span><span class="p">()):</span>
    <span class="n">gpu</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">get_device_properties</span><span class="p">(</span><span class="n">i</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;GPU </span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">gpu</span><span class="o">.</span><span class="n">name</span><span class="si">}</span><span class="s2"> (</span><span class="si">{</span><span class="n">gpu</span><span class="o">.</span><span class="n">total_memory</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="mf">1e9</span><span class="si">:</span><span class="s2">.1f</span><span class="si">}</span><span class="s2">GB)&quot;</span><span class="p">)</span>

<span class="c1"># Recommended</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Availability check:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;CUDA available: </span><span class="si">{</span><span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;NCCL available: </span><span class="si">{</span><span class="n">torch</span><span class="o">.</span><span class="n">distributed</span><span class="o">.</span><span class="n">is_nccl_available</span><span class="p">()</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</code></pre></div>
<h3 id="32-software-stack">3.2 Software Stack<a class="headerlink" href="#32-software-stack" title="Permanent link">&para;</a></h3>
<div class="highlight"><pre><span></span><code><span class="c1"># Install required packages</span>
pip<span class="w"> </span>install<span class="w"> </span>torch<span class="w"> </span>torchvision<span class="w"> </span>torchaudio<span class="w"> </span>--index-url<span class="w"> </span>https://download.pytorch.org/whl/cu121
pip<span class="w"> </span>install<span class="w"> </span>torch-distributed-rpc
pip<span class="w"> </span>install<span class="w"> </span>horovod<span class="w">  </span><span class="c1"># Optional: for advanced distributed training</span>
pip<span class="w"> </span>install<span class="w"> </span>pytorch-lightning<span class="w">  </span><span class="c1"># Optional: simplified multi-GPU setup</span>
</code></pre></div>
<hr />
<h2 id="4-data-parallel-training">4. Data Parallel Training<a class="headerlink" href="#4-data-parallel-training" title="Permanent link">&para;</a></h2>
<h3 id="41-dataparallel-simpler-single-machine">4.1 DataParallel (Simpler, Single-Machine)<a class="headerlink" href="#41-dataparallel-simpler-single-machine" title="Permanent link">&para;</a></h3>
<div class="highlight"><pre><span></span><code><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.nn</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">nn</span>

<span class="k">class</span><span class="w"> </span><span class="nc">DataParallelWrapper</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Wrap TabTune pipeline with DataParallel.&quot;&quot;&quot;</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">device_ids</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">model</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">DataParallel</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">device_ids</span><span class="o">=</span><span class="n">device_ids</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">device_ids</span> <span class="o">=</span> <span class="n">device_ids</span> <span class="ow">or</span> <span class="nb">list</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">device_count</span><span class="p">()))</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Training with DataParallel.&quot;&quot;&quot;</span>
        <span class="c1"># Batch automatically split across GPUs</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">module</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X_test</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Inference on primary GPU.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">module</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>

<span class="c1"># Usage</span>
<span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">device_count</span><span class="p">()</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">DataParallelWrapper</span><span class="p">(</span>
        <span class="n">TabularPipeline</span><span class="p">(</span><span class="n">model_name</span><span class="o">=</span><span class="s1">&#39;TabICL&#39;</span><span class="p">),</span>
        <span class="n">device_ids</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">]</span>  <span class="c1"># Use GPUs 0-3</span>
    <span class="p">)</span>
<span class="k">else</span><span class="p">:</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">TabularPipeline</span><span class="p">(</span><span class="n">model_name</span><span class="o">=</span><span class="s1">&#39;TabICL&#39;</span><span class="p">)</span>

<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
</code></pre></div>
<h3 id="42-distributeddataparallel-recommended">4.2 DistributedDataParallel (Recommended)<a class="headerlink" href="#42-distributeddataparallel-recommended" title="Permanent link">&para;</a></h3>
<p>More efficient than DataParallel:</p>
<div class="highlight"><pre><span></span><code><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.distributed</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">dist</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torch.nn.parallel</span><span class="w"> </span><span class="kn">import</span> <span class="n">DistributedDataParallel</span>

<span class="k">def</span><span class="w"> </span><span class="nf">setup_distributed</span><span class="p">():</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Setup distributed training environment.&quot;&quot;&quot;</span>
    <span class="n">dist</span><span class="o">.</span><span class="n">init_process_group</span><span class="p">(</span>
        <span class="n">backend</span><span class="o">=</span><span class="s1">&#39;nccl&#39;</span><span class="p">,</span>  <span class="c1"># NVIDIA Collective Communications Library</span>
        <span class="n">init_method</span><span class="o">=</span><span class="s1">&#39;env://&#39;</span>
    <span class="p">)</span>

<span class="k">def</span><span class="w"> </span><span class="nf">cleanup</span><span class="p">():</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Cleanup distributed environment.&quot;&quot;&quot;</span>
    <span class="n">dist</span><span class="o">.</span><span class="n">destroy_process_group</span><span class="p">()</span>

<span class="k">def</span><span class="w"> </span><span class="nf">train_distributed</span><span class="p">():</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Distributed training.&quot;&quot;&quot;</span>
    <span class="n">setup_distributed</span><span class="p">()</span>

    <span class="c1"># Get process rank and world size</span>
    <span class="n">rank</span> <span class="o">=</span> <span class="n">dist</span><span class="o">.</span><span class="n">get_rank</span><span class="p">()</span>
    <span class="n">world_size</span> <span class="o">=</span> <span class="n">dist</span><span class="o">.</span><span class="n">get_world_size</span><span class="p">()</span>

    <span class="c1"># Create model on current device</span>
    <span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;cuda:</span><span class="si">{</span><span class="n">rank</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>

    <span class="n">pipeline</span> <span class="o">=</span> <span class="n">TabularPipeline</span><span class="p">(</span>
        <span class="n">model_name</span><span class="o">=</span><span class="s1">&#39;TabICL&#39;</span><span class="p">,</span>
        <span class="n">tuning_strategy</span><span class="o">=</span><span class="s1">&#39;base-ft&#39;</span><span class="p">,</span>
        <span class="n">tuning_params</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;device&#39;</span><span class="p">:</span> <span class="n">device</span><span class="p">}</span>
    <span class="p">)</span>

    <span class="c1"># Wrap with DistributedDataParallel</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">DistributedDataParallel</span><span class="p">(</span><span class="n">pipeline</span><span class="o">.</span><span class="n">model</span><span class="p">,</span> <span class="n">device_ids</span><span class="o">=</span><span class="p">[</span><span class="n">rank</span><span class="p">])</span>

    <span class="c1"># Training code</span>
    <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

    <span class="n">cleanup</span><span class="p">()</span>

<span class="c1"># Launch with: python -m torch.distributed.launch --nproc_per_node=4 script.py</span>
</code></pre></div>
<hr />
<h2 id="5-launch-methods">5. Launch Methods<a class="headerlink" href="#5-launch-methods" title="Permanent link">&para;</a></h2>
<h3 id="51-torchrun-pytorch-110">5.1 torchrun (PyTorch 1.10+)<a class="headerlink" href="#51-torchrun-pytorch-110" title="Permanent link">&para;</a></h3>
<p><strong>Recommended method</strong>:</p>
<div class="highlight"><pre><span></span><code><span class="c1"># Single-machine, 4 GPUs</span>
torchrun<span class="w"> </span>--nproc_per_node<span class="o">=</span><span class="m">4</span><span class="w"> </span>train_script.py

<span class="c1"># Multi-machine (8 GPUs total)</span>
torchrun<span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--nproc_per_node<span class="o">=</span><span class="m">4</span><span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--nnodes<span class="o">=</span><span class="m">2</span><span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--node_rank<span class="o">=</span><span class="m">0</span><span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--master_addr<span class="o">=</span><span class="m">192</span>.168.1.100<span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--master_port<span class="o">=</span><span class="m">29500</span><span class="w"> </span><span class="se">\</span>
<span class="w">  </span>train_script.py
</code></pre></div>
<h3 id="52-torchdistributedlaunch-legacy">5.2 torch.distributed.launch (Legacy)<a class="headerlink" href="#52-torchdistributedlaunch-legacy" title="Permanent link">&para;</a></h3>
<div class="highlight"><pre><span></span><code><span class="c1"># Single-machine, 4 GPUs</span>
python<span class="w"> </span>-m<span class="w"> </span>torch.distributed.launch<span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--nproc_per_node<span class="o">=</span><span class="m">4</span><span class="w"> </span><span class="se">\</span>
<span class="w">  </span>train_script.py

<span class="c1"># With additional args</span>
python<span class="w"> </span>-m<span class="w"> </span>torch.distributed.launch<span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--nproc_per_node<span class="o">=</span><span class="m">4</span><span class="w"> </span><span class="se">\</span>
<span class="w">  </span>train_script.py<span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--learning_rate<span class="w"> </span>2e-5<span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--epochs<span class="w"> </span><span class="m">5</span>
</code></pre></div>
<h3 id="53-manual-launch">5.3 Manual Launch<a class="headerlink" href="#53-manual-launch" title="Permanent link">&para;</a></h3>
<div class="highlight"><pre><span></span><code><span class="c1"># Terminal 1: GPU 0</span>
<span class="nv">CUDA_VISIBLE_DEVICES</span><span class="o">=</span><span class="m">0</span><span class="w"> </span>python<span class="w"> </span>train_script.py

<span class="c1"># Terminal 2: GPU 1</span>
<span class="nv">CUDA_VISIBLE_DEVICES</span><span class="o">=</span><span class="m">1</span><span class="w"> </span>python<span class="w"> </span>train_script.py

<span class="c1"># Terminal 3: GPU 2</span>
<span class="nv">CUDA_VISIBLE_DEVICES</span><span class="o">=</span><span class="m">2</span><span class="w"> </span>python<span class="w"> </span>train_script.py

<span class="c1"># Terminal 4: GPU 3</span>
<span class="nv">CUDA_VISIBLE_DEVICES</span><span class="o">=</span><span class="m">3</span><span class="w"> </span>python<span class="w"> </span>train_script.py
</code></pre></div>
<hr />
<h2 id="6-training-scripts">6. Training Scripts<a class="headerlink" href="#6-training-scripts" title="Permanent link">&para;</a></h2>
<h3 id="61-complete-distributed-training-script">6.1 Complete Distributed Training Script<a class="headerlink" href="#61-complete-distributed-training-script" title="Permanent link">&para;</a></h3>
<div class="highlight"><pre><span></span><code><span class="kn">import</span><span class="w"> </span><span class="nn">os</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.distributed</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">dist</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torch.nn.parallel</span><span class="w"> </span><span class="kn">import</span> <span class="n">DistributedDataParallel</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torch.utils.data</span><span class="w"> </span><span class="kn">import</span> <span class="n">DataLoader</span><span class="p">,</span> <span class="n">DistributedSampler</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">tabtune</span><span class="w"> </span><span class="kn">import</span> <span class="n">TabularPipeline</span>

<span class="k">def</span><span class="w"> </span><span class="nf">main</span><span class="p">():</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Main training function.&quot;&quot;&quot;</span>

    <span class="c1"># Initialize distributed training</span>
    <span class="n">dist</span><span class="o">.</span><span class="n">init_process_group</span><span class="p">(</span><span class="n">backend</span><span class="o">=</span><span class="s1">&#39;nccl&#39;</span><span class="p">)</span>
    <span class="n">rank</span> <span class="o">=</span> <span class="n">dist</span><span class="o">.</span><span class="n">get_rank</span><span class="p">()</span>
    <span class="n">world_size</span> <span class="o">=</span> <span class="n">dist</span><span class="o">.</span><span class="n">get_world_size</span><span class="p">()</span>

    <span class="c1"># Set device</span>
    <span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;cuda:</span><span class="si">{</span><span class="n">rank</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">set_device</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

    <span class="c1"># Print rank info</span>
    <span class="k">if</span> <span class="n">rank</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Training on </span><span class="si">{</span><span class="n">world_size</span><span class="si">}</span><span class="s2"> GPUs&quot;</span><span class="p">)</span>

    <span class="c1"># Load data</span>
    <span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span> <span class="o">=</span> <span class="n">load_data</span><span class="p">()</span>

    <span class="c1"># Create distributed sampler</span>
    <span class="n">train_sampler</span> <span class="o">=</span> <span class="n">DistributedSampler</span><span class="p">(</span>
        <span class="n">dataset</span><span class="o">=</span><span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">X_train</span><span class="p">)),</span>
        <span class="n">num_replicas</span><span class="o">=</span><span class="n">world_size</span><span class="p">,</span>
        <span class="n">rank</span><span class="o">=</span><span class="n">rank</span><span class="p">,</span>
        <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span>
    <span class="p">)</span>

    <span class="c1"># Create dataloader</span>
    <span class="n">train_loader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span>
        <span class="n">train_sampler</span><span class="p">,</span>
        <span class="n">batch_size</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span>
        <span class="n">num_workers</span><span class="o">=</span><span class="mi">2</span>
    <span class="p">)</span>

    <span class="c1"># Create model</span>
    <span class="n">pipeline</span> <span class="o">=</span> <span class="n">TabularPipeline</span><span class="p">(</span>
        <span class="n">model_name</span><span class="o">=</span><span class="s1">&#39;TabICL&#39;</span><span class="p">,</span>
        <span class="n">tuning_strategy</span><span class="o">=</span><span class="s1">&#39;base-ft&#39;</span><span class="p">,</span>
        <span class="n">tuning_params</span><span class="o">=</span><span class="p">{</span>
            <span class="s1">&#39;device&#39;</span><span class="p">:</span> <span class="n">device</span><span class="p">,</span>
            <span class="s1">&#39;epochs&#39;</span><span class="p">:</span> <span class="mi">5</span><span class="p">,</span>
            <span class="s1">&#39;learning_rate&#39;</span><span class="p">:</span> <span class="mf">2e-5</span>
        <span class="p">}</span>
    <span class="p">)</span>

    <span class="c1"># Wrap with DistributedDataParallel</span>
    <span class="n">pipeline</span><span class="o">.</span><span class="n">model</span> <span class="o">=</span> <span class="n">DistributedDataParallel</span><span class="p">(</span>
        <span class="n">pipeline</span><span class="o">.</span><span class="n">model</span><span class="p">,</span>
        <span class="n">device_ids</span><span class="o">=</span><span class="p">[</span><span class="n">rank</span><span class="p">]</span>
    <span class="p">)</span>

    <span class="c1"># Training</span>
    <span class="k">if</span> <span class="n">rank</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Starting training...&quot;</span><span class="p">)</span>

    <span class="n">pipeline</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">[</span><span class="n">train_sampler</span><span class="o">.</span><span class="n">indices</span><span class="p">],</span> <span class="n">y_train</span><span class="p">[</span><span class="n">train_sampler</span><span class="o">.</span><span class="n">indices</span><span class="p">])</span>

    <span class="c1"># Evaluation on rank 0 only</span>
    <span class="k">if</span> <span class="n">rank</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">metrics</span> <span class="o">=</span> <span class="n">pipeline</span><span class="o">.</span><span class="n">evaluate</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Validation Accuracy: </span><span class="si">{</span><span class="n">metrics</span><span class="p">[</span><span class="s1">&#39;accuracy&#39;</span><span class="p">]</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

    <span class="c1"># Cleanup</span>
    <span class="n">dist</span><span class="o">.</span><span class="n">destroy_process_group</span><span class="p">()</span>

<span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s1">&#39;__main__&#39;</span><span class="p">:</span>
    <span class="n">main</span><span class="p">()</span>
</code></pre></div>
<h3 id="62-synchronization-across-ranks">6.2 Synchronization Across Ranks<a class="headerlink" href="#62-synchronization-across-ranks" title="Permanent link">&para;</a></h3>
<div class="highlight"><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">train_with_sync</span><span class="p">():</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Training with synchronization points.&quot;&quot;&quot;</span>

    <span class="n">dist</span><span class="o">.</span><span class="n">init_process_group</span><span class="p">(</span><span class="n">backend</span><span class="o">=</span><span class="s1">&#39;nccl&#39;</span><span class="p">)</span>
    <span class="n">rank</span> <span class="o">=</span> <span class="n">dist</span><span class="o">.</span><span class="n">get_rank</span><span class="p">()</span>

    <span class="c1"># Training</span>
    <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_epochs</span><span class="p">):</span>
        <span class="c1"># ... training code ...</span>

        <span class="c1"># Synchronize all ranks</span>
        <span class="n">dist</span><span class="o">.</span><span class="n">barrier</span><span class="p">()</span>

        <span class="c1"># Evaluation (on rank 0 only)</span>
        <span class="k">if</span> <span class="n">rank</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">metrics</span> <span class="o">=</span> <span class="n">evaluate</span><span class="p">()</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Epoch </span><span class="si">{</span><span class="n">epoch</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">metrics</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

        <span class="c1"># Broadcast best checkpoint from rank 0</span>
        <span class="k">if</span> <span class="n">rank</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">best_state</span> <span class="o">=</span> <span class="n">pipeline</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">state_dict</span><span class="p">()</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">best_state</span> <span class="o">=</span> <span class="kc">None</span>

        <span class="c1"># Distribute to all ranks</span>
        <span class="n">dist</span><span class="o">.</span><span class="n">broadcast_object_list</span><span class="p">([</span><span class="n">best_state</span><span class="p">],</span> <span class="n">src</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">rank</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">pipeline</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">best_state</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>

    <span class="n">dist</span><span class="o">.</span><span class="n">destroy_process_group</span><span class="p">()</span>
</code></pre></div>
<hr />
<h2 id="7-performance-optimization">7. Performance Optimization<a class="headerlink" href="#7-performance-optimization" title="Permanent link">&para;</a></h2>
<h3 id="71-gradient-accumulation">7.1 Gradient Accumulation<a class="headerlink" href="#71-gradient-accumulation" title="Permanent link">&para;</a></h3>
<p>Increase effective batch size without memory increase:</p>
<div class="highlight"><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">train_with_accumulation</span><span class="p">(</span><span class="n">num_accumulation_steps</span><span class="o">=</span><span class="mi">4</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Training with gradient accumulation.&quot;&quot;&quot;</span>

    <span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">AdamW</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">())</span>

    <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_epochs</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">batch</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">dataloader</span><span class="p">):</span>
            <span class="c1"># Forward pass</span>
            <span class="n">loss</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">batch</span><span class="p">)</span>

            <span class="c1"># Backward (accumulate gradients)</span>
            <span class="p">(</span><span class="n">loss</span> <span class="o">/</span> <span class="n">num_accumulation_steps</span><span class="p">)</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>

            <span class="c1"># Update weights</span>
            <span class="k">if</span> <span class="p">(</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">%</span> <span class="n">num_accumulation_steps</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
                <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
</code></pre></div>
<h3 id="72-overlapping-computation-communication">7.2 Overlapping Computation &amp; Communication<a class="headerlink" href="#72-overlapping-computation-communication" title="Permanent link">&para;</a></h3>
<div class="highlight"><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">train_with_overlap</span><span class="p">():</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Training with async communication.&quot;&quot;&quot;</span>

    <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_epochs</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">batch</span> <span class="ow">in</span> <span class="n">dataloader</span><span class="p">:</span>
            <span class="c1"># Forward &amp; backward (computation)</span>
            <span class="n">loss</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">batch</span><span class="p">)</span>
            <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>

            <span class="c1"># Start async gradient reduction</span>
            <span class="n">reduction_future</span> <span class="o">=</span> <span class="n">dist</span><span class="o">.</span><span class="n">all_reduce</span><span class="p">(</span>
                <span class="n">model</span><span class="o">.</span><span class="n">grad</span><span class="p">,</span>
                <span class="n">async_op</span><span class="o">=</span><span class="kc">True</span>  <span class="c1"># Non-blocking</span>
            <span class="p">)</span>

            <span class="c1"># Do other work while reducing</span>
            <span class="c1"># ...</span>

            <span class="c1"># Wait for reduction to complete</span>
            <span class="n">reduction_future</span><span class="o">.</span><span class="n">wait</span><span class="p">()</span>

            <span class="c1"># Update weights</span>
            <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
</code></pre></div>
<h3 id="73-mixed-precision-with-distributed-training">7.3 Mixed Precision with Distributed Training<a class="headerlink" href="#73-mixed-precision-with-distributed-training" title="Permanent link">&para;</a></h3>
<div class="highlight"><pre><span></span><code><span class="kn">from</span><span class="w"> </span><span class="nn">torch.cuda.amp</span><span class="w"> </span><span class="kn">import</span> <span class="n">autocast</span><span class="p">,</span> <span class="n">GradScaler</span>

<span class="k">def</span><span class="w"> </span><span class="nf">train_mixed_precision</span><span class="p">():</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Multi-GPU training with mixed precision.&quot;&quot;&quot;</span>

    <span class="n">scaler</span> <span class="o">=</span> <span class="n">GradScaler</span><span class="p">()</span>

    <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_epochs</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">batch</span> <span class="ow">in</span> <span class="n">dataloader</span><span class="p">:</span>
            <span class="c1"># Forward with autocast</span>
            <span class="k">with</span> <span class="n">autocast</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">):</span>
                <span class="n">loss</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">batch</span><span class="p">)</span>

            <span class="c1"># Backward with scaling</span>
            <span class="n">scaler</span><span class="o">.</span><span class="n">scale</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>

            <span class="c1"># All-reduce scaled gradients</span>
            <span class="k">for</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">():</span>
                <span class="k">if</span> <span class="n">param</span><span class="o">.</span><span class="n">grad</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                    <span class="n">dist</span><span class="o">.</span><span class="n">all_reduce</span><span class="p">(</span><span class="n">param</span><span class="o">.</span><span class="n">grad</span><span class="p">)</span>

            <span class="c1"># Update weights</span>
            <span class="n">scaler</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">optimizer</span><span class="p">)</span>
            <span class="n">scaler</span><span class="o">.</span><span class="n">update</span><span class="p">()</span>
</code></pre></div>
<hr />
<h2 id="8-scaling-efficiency">8. Scaling Efficiency<a class="headerlink" href="#8-scaling-efficiency" title="Permanent link">&para;</a></h2>
<h3 id="81-linear-scaling-rule">8.1 Linear Scaling Rule<a class="headerlink" href="#81-linear-scaling-rule" title="Permanent link">&para;</a></h3>
<div class="highlight"><pre><span></span><code><span class="c1"># Optimal learning rate for N GPUs</span>
<span class="n">base_learning_rate</span> <span class="o">=</span> <span class="mf">2e-5</span>
<span class="n">num_gpus</span> <span class="o">=</span> <span class="mi">4</span>

<span class="n">optimal_learning_rate</span> <span class="o">=</span> <span class="n">base_learning_rate</span> <span class="o">*</span> <span class="n">num_gpus</span>
<span class="c1"># Why: Batch size  num_gpus, so learning rate should scale</span>

<span class="c1"># Or more conservatively:</span>
<span class="n">optimal_learning_rate</span> <span class="o">=</span> <span class="n">base_learning_rate</span> <span class="o">*</span> <span class="p">(</span><span class="n">num_gpus</span> <span class="o">**</span> <span class="mf">0.5</span><span class="p">)</span>
</code></pre></div>
<h3 id="82-speedup-analysis">8.2 Speedup Analysis<a class="headerlink" href="#82-speedup-analysis" title="Permanent link">&para;</a></h3>
<div class="highlight"><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">analyze_scaling</span><span class="p">(</span><span class="n">times_single_gpu</span><span class="p">,</span> <span class="n">times_multi_gpu</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Analyze multi-GPU speedup.&quot;&quot;&quot;</span>

    <span class="n">num_gpus</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">times_multi_gpu</span><span class="p">)</span>

    <span class="n">speedup</span> <span class="o">=</span> <span class="n">times_single_gpu</span> <span class="o">/</span> <span class="n">times_multi_gpu</span>
    <span class="n">efficiency</span> <span class="o">=</span> <span class="n">speedup</span> <span class="o">/</span> <span class="n">num_gpus</span>  <span class="c1"># Ideally 1.0 (100%)</span>

    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;GPU Count: </span><span class="si">{</span><span class="n">num_gpus</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Time (1 GPU): </span><span class="si">{</span><span class="n">times_single_gpu</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">s&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Time (</span><span class="si">{</span><span class="n">num_gpus</span><span class="si">}</span><span class="s2"> GPUs): </span><span class="si">{</span><span class="n">times_multi_gpu</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">s&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Speedup: </span><span class="si">{</span><span class="n">speedup</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">x (</span><span class="si">{</span><span class="n">efficiency</span><span class="o">*</span><span class="mi">100</span><span class="si">:</span><span class="s2">.1f</span><span class="si">}</span><span class="s2">% efficiency)&quot;</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">speedup</span><span class="p">,</span> <span class="n">efficiency</span>

<span class="c1"># Typical results</span>
<span class="c1"># 2 GPUs: 1.8x speedup (90% efficiency)</span>
<span class="c1"># 4 GPUs: 3.5x speedup (87.5% efficiency)</span>
<span class="c1"># 8 GPUs: 6.5x speedup (81% efficiency)</span>
</code></pre></div>
<hr />
<h2 id="9-distributed-challenges-solutions">9. Distributed Challenges &amp; Solutions<a class="headerlink" href="#9-distributed-challenges-solutions" title="Permanent link">&para;</a></h2>
<h3 id="91-communication-overhead">9.1 Communication Overhead<a class="headerlink" href="#91-communication-overhead" title="Permanent link">&para;</a></h3>
<div class="highlight"><pre><span></span><code><span class="c1"># Problem: Communication becomes bottleneck</span>
<span class="c1"># Solution 1: Larger batch size</span>
<span class="n">tuning_params</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s1">&#39;batch_size&#39;</span><span class="p">:</span> <span class="mi">128</span><span class="p">,</span>  <span class="c1"># Instead of 32</span>
<span class="p">}</span>

<span class="c1"># Solution 2: Gradient accumulation</span>
<span class="n">tuning_params</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s1">&#39;batch_size&#39;</span><span class="p">:</span> <span class="mi">32</span><span class="p">,</span>
    <span class="s1">&#39;gradient_accumulation_steps&#39;</span><span class="p">:</span> <span class="mi">4</span>  <span class="c1"># Effective: 128</span>
<span class="p">}</span>

<span class="c1"># Solution 3: Reduce communication frequency</span>
<span class="c1"># Communicate every N steps instead of every step</span>
</code></pre></div>
<h3 id="92-load-imbalance">9.2 Load Imbalance<a class="headerlink" href="#92-load-imbalance" title="Permanent link">&para;</a></h3>
<div class="highlight"><pre><span></span><code><span class="c1"># Problem: Some GPUs finish before others</span>
<span class="c1"># Solution: Dynamic load balancing</span>

<span class="k">def</span><span class="w"> </span><span class="nf">balanced_sampler</span><span class="p">(</span><span class="n">dataset</span><span class="p">,</span> <span class="n">num_replicas</span><span class="p">,</span> <span class="n">rank</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Create balanced sampler across ranks.&quot;&quot;&quot;</span>

    <span class="c1"># Ensure each rank gets similar amount of work</span>
    <span class="n">samples_per_rank</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">dataset</span><span class="p">)</span> <span class="o">//</span> <span class="n">num_replicas</span>
    <span class="n">remainder</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">dataset</span><span class="p">)</span> <span class="o">%</span> <span class="n">num_replicas</span>

    <span class="n">start</span> <span class="o">=</span> <span class="n">rank</span> <span class="o">*</span> <span class="n">samples_per_rank</span> <span class="o">+</span> <span class="nb">min</span><span class="p">(</span><span class="n">rank</span><span class="p">,</span> <span class="n">remainder</span><span class="p">)</span>
    <span class="n">end</span> <span class="o">=</span> <span class="n">start</span> <span class="o">+</span> <span class="n">samples_per_rank</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="k">if</span> <span class="n">rank</span> <span class="o">&lt;</span> <span class="n">remainder</span> <span class="k">else</span> <span class="mi">0</span><span class="p">)</span>

    <span class="n">indices</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">start</span><span class="p">,</span> <span class="n">end</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">DistributedSampler</span><span class="p">(</span>
        <span class="n">indices</span><span class="p">,</span>
        <span class="n">num_replicas</span><span class="o">=</span><span class="n">num_replicas</span><span class="p">,</span>
        <span class="n">rank</span><span class="o">=</span><span class="n">rank</span>
    <span class="p">)</span>
</code></pre></div>
<h3 id="93-gradient-divergence">9.3 Gradient Divergence<a class="headerlink" href="#93-gradient-divergence" title="Permanent link">&para;</a></h3>
<div class="highlight"><pre><span></span><code><span class="c1"># Problem: Different GPUs compute different gradients</span>
<span class="c1"># Solution: Proper synchronization</span>

<span class="k">def</span><span class="w"> </span><span class="nf">synchronized_training</span><span class="p">():</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Ensure all ranks have synchronized gradients.&quot;&quot;&quot;</span>

    <span class="k">for</span> <span class="n">batch</span> <span class="ow">in</span> <span class="n">dataloader</span><span class="p">:</span>
        <span class="c1"># Compute gradients</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">batch</span><span class="p">)</span>
        <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>

        <span class="c1"># Synchronize gradients across all ranks</span>
        <span class="k">for</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">():</span>
            <span class="n">dist</span><span class="o">.</span><span class="n">all_reduce</span><span class="p">(</span><span class="n">param</span><span class="o">.</span><span class="n">grad</span><span class="p">)</span>
            <span class="n">param</span><span class="o">.</span><span class="n">grad</span> <span class="o">/=</span> <span class="n">world_size</span>  <span class="c1"># Average</span>

        <span class="c1"># Update</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
</code></pre></div>
<hr />
<h2 id="10-complete-multi-gpu-example">10. Complete Multi-GPU Example<a class="headerlink" href="#10-complete-multi-gpu-example" title="Permanent link">&para;</a></h2>
<div class="highlight"><pre><span></span><code><span class="ch">#!/usr/bin/env python</span>
<span class="sd">&quot;&quot;&quot;Multi-GPU training example.&quot;&quot;&quot;</span>

<span class="kn">import</span><span class="w"> </span><span class="nn">os</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.distributed</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">dist</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torch.nn.parallel</span><span class="w"> </span><span class="kn">import</span> <span class="n">DistributedDataParallel</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">tabtune</span><span class="w"> </span><span class="kn">import</span> <span class="n">TabularPipeline</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">argparse</span>

<span class="k">def</span><span class="w"> </span><span class="nf">main</span><span class="p">():</span>
    <span class="n">parser</span> <span class="o">=</span> <span class="n">argparse</span><span class="o">.</span><span class="n">ArgumentParser</span><span class="p">()</span>
    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s1">&#39;--epochs&#39;</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s1">&#39;--batch_size&#39;</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mi">32</span><span class="p">)</span>
    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s1">&#39;--learning_rate&#39;</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">float</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mf">2e-5</span><span class="p">)</span>
    <span class="n">args</span> <span class="o">=</span> <span class="n">parser</span><span class="o">.</span><span class="n">parse_args</span><span class="p">()</span>

    <span class="c1"># Initialize distributed training</span>
    <span class="n">dist</span><span class="o">.</span><span class="n">init_process_group</span><span class="p">(</span><span class="n">backend</span><span class="o">=</span><span class="s1">&#39;nccl&#39;</span><span class="p">)</span>
    <span class="n">rank</span> <span class="o">=</span> <span class="n">dist</span><span class="o">.</span><span class="n">get_rank</span><span class="p">()</span>
    <span class="n">world_size</span> <span class="o">=</span> <span class="n">dist</span><span class="o">.</span><span class="n">get_world_size</span><span class="p">()</span>
    <span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;cuda:</span><span class="si">{</span><span class="n">rank</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">rank</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Starting training on </span><span class="si">{</span><span class="n">world_size</span><span class="si">}</span><span class="s2"> GPUs&quot;</span><span class="p">)</span>

    <span class="c1"># Load data</span>
    <span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">load_dataset</span><span class="p">()</span>

    <span class="c1"># Scale learning rate with batch size</span>
    <span class="n">scaled_lr</span> <span class="o">=</span> <span class="n">args</span><span class="o">.</span><span class="n">learning_rate</span> <span class="o">*</span> <span class="n">world_size</span>

    <span class="c1"># Create pipeline</span>
    <span class="n">pipeline</span> <span class="o">=</span> <span class="n">TabularPipeline</span><span class="p">(</span>
        <span class="n">model_name</span><span class="o">=</span><span class="s1">&#39;TabICL&#39;</span><span class="p">,</span>
        <span class="n">tuning_strategy</span><span class="o">=</span><span class="s1">&#39;base-ft&#39;</span><span class="p">,</span>
        <span class="n">tuning_params</span><span class="o">=</span><span class="p">{</span>
            <span class="s1">&#39;device&#39;</span><span class="p">:</span> <span class="n">device</span><span class="p">,</span>
            <span class="s1">&#39;epochs&#39;</span><span class="p">:</span> <span class="n">args</span><span class="o">.</span><span class="n">epochs</span><span class="p">,</span>
            <span class="s1">&#39;learning_rate&#39;</span><span class="p">:</span> <span class="n">scaled_lr</span><span class="p">,</span>
            <span class="s1">&#39;batch_size&#39;</span><span class="p">:</span> <span class="n">args</span><span class="o">.</span><span class="n">batch_size</span>
        <span class="p">}</span>
    <span class="p">)</span>

    <span class="c1"># Wrap with DDP</span>
    <span class="n">pipeline</span><span class="o">.</span><span class="n">model</span> <span class="o">=</span> <span class="n">DistributedDataParallel</span><span class="p">(</span>
        <span class="n">pipeline</span><span class="o">.</span><span class="n">model</span><span class="p">,</span>
        <span class="n">device_ids</span><span class="o">=</span><span class="p">[</span><span class="n">rank</span><span class="p">]</span>
    <span class="p">)</span>

    <span class="c1"># Training</span>
    <span class="k">if</span> <span class="n">rank</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Training with LR=</span><span class="si">{</span><span class="n">scaled_lr</span><span class="si">:</span><span class="s2">.2e</span><span class="si">}</span><span class="s2">, batch_size=</span><span class="si">{</span><span class="n">args</span><span class="o">.</span><span class="n">batch_size</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

    <span class="n">pipeline</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

    <span class="c1"># Synchronize before evaluation</span>
    <span class="n">dist</span><span class="o">.</span><span class="n">barrier</span><span class="p">()</span>

    <span class="c1"># Evaluation on primary GPU only</span>
    <span class="k">if</span> <span class="n">rank</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">metrics</span> <span class="o">=</span> <span class="n">pipeline</span><span class="o">.</span><span class="n">evaluate</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Final accuracy: </span><span class="si">{</span><span class="n">metrics</span><span class="p">[</span><span class="s1">&#39;accuracy&#39;</span><span class="p">]</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

    <span class="n">dist</span><span class="o">.</span><span class="n">destroy_process_group</span><span class="p">()</span>

<span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s1">&#39;__main__&#39;</span><span class="p">:</span>
    <span class="n">main</span><span class="p">()</span>
</code></pre></div>
<p><strong>Launch:</strong>
<div class="highlight"><pre><span></span><code>torchrun<span class="w"> </span>--nproc_per_node<span class="o">=</span><span class="m">4</span><span class="w"> </span>train_distributed.py<span class="w"> </span>--epochs<span class="w"> </span><span class="m">5</span><span class="w"> </span>--batch_size<span class="w"> </span><span class="m">32</span>
</code></pre></div></p>
<hr />
<h2 id="11-debugging-multi-gpu-issues">11. Debugging Multi-GPU Issues<a class="headerlink" href="#11-debugging-multi-gpu-issues" title="Permanent link">&para;</a></h2>
<h3 id="111-common-problems">11.1 Common Problems<a class="headerlink" href="#111-common-problems" title="Permanent link">&para;</a></h3>
<div class="highlight"><pre><span></span><code><span class="c1"># Problem: Hanging/deadlock</span>
<span class="c1"># Solution: Use timeout and debug flags</span>
<span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s1">&#39;NCCL_DEBUG&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="s1">&#39;INFO&#39;</span>
<span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s1">&#39;TORCH_DISTRIBUTED_DEBUG&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="s1">&#39;DETAIL&#39;</span>

<span class="n">dist</span><span class="o">.</span><span class="n">init_process_group</span><span class="p">(</span>
    <span class="n">backend</span><span class="o">=</span><span class="s1">&#39;nccl&#39;</span><span class="p">,</span>
    <span class="n">timeout</span><span class="o">=</span><span class="n">timedelta</span><span class="p">(</span><span class="n">minutes</span><span class="o">=</span><span class="mi">30</span><span class="p">)</span>
<span class="p">)</span>

<span class="c1"># Problem: GPU memory imbalance</span>
<span class="c1"># Solution: Check and balance</span>
<span class="k">if</span> <span class="n">rank</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">world_size</span><span class="p">):</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;GPU </span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">get_device_properties</span><span class="p">(</span><span class="n">i</span><span class="p">)</span><span class="o">.</span><span class="n">total_memory</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="mf">1e9</span><span class="si">:</span><span class="s2">.1f</span><span class="si">}</span><span class="s2">GB&quot;</span><span class="p">)</span>

<span class="c1"># Problem: Rank synchronization issues</span>
<span class="c1"># Solution: Add explicit barriers</span>
<span class="n">dist</span><span class="o">.</span><span class="n">barrier</span><span class="p">()</span>  <span class="c1"># Wait for all ranks</span>
</code></pre></div>
<hr />
<h2 id="12-best-practices">12. Best Practices<a class="headerlink" href="#12-best-practices" title="Permanent link">&para;</a></h2>
<h3 id="dos"> Do's<a class="headerlink" href="#dos" title="Permanent link">&para;</a></h3>
<ul>
<li> Use DistributedDataParallel over DataParallel</li>
<li> Scale learning rate with batch size</li>
<li> Use proper samplers (DistributedSampler)</li>
<li> Synchronize at checkpoints</li>
<li> Evaluate on rank 0 only</li>
<li> Monitor all GPU memory usage</li>
<li> Test on fewer GPUs first</li>
</ul>
<h3 id="donts"> Don'ts<a class="headerlink" href="#donts" title="Permanent link">&para;</a></h3>
<ul>
<li> Don't use DataParallel for multi-machine</li>
<li> Don't forget to set environment variables</li>
<li> Don't run inference on all ranks</li>
<li> Don't ignore communication overhead</li>
<li> Don't over-subscribe GPU memory</li>
<li> Don't change data on different ranks</li>
</ul>
<hr />
<h2 id="13-performance-benchmarks">13. Performance Benchmarks<a class="headerlink" href="#13-performance-benchmarks" title="Permanent link">&para;</a></h2>
<div class="highlight"><pre><span></span><code>Dataset: 500K samples, TabICL model

1 GPU:
  Time: 120 minutes
  Memory: 12 GB

2 GPUs (DDP):
  Time: 65 minutes (1.85x speedup)
  Memory: 6 GB per GPU
  Efficiency: 92.5%

4 GPUs (DDP):
  Time: 35 minutes (3.43x speedup)
  Memory: 3 GB per GPU
  Efficiency: 85.7%

8 GPUs (DDP):
  Time: 20 minutes (6.0x speedup)
  Memory: 1.5 GB per GPU
  Efficiency: 75%
</code></pre></div>
<hr />
<h2 id="14-quick-reference">14. Quick Reference<a class="headerlink" href="#14-quick-reference" title="Permanent link">&para;</a></h2>
<table>
<thead>
<tr>
<th>Aspect</th>
<th>Single GPU</th>
<th>Multi-GPU DDP</th>
</tr>
</thead>
<tbody>
<tr>
<td>Setup</td>
<td>Simple</td>
<td>Moderate</td>
</tr>
<tr>
<td>Communication</td>
<td>None</td>
<td>NCCL</td>
</tr>
<tr>
<td>Speedup</td>
<td>1x</td>
<td>~(GPUs-0.3)</td>
</tr>
<tr>
<td>Memory/GPU</td>
<td>Full</td>
<td>Full/GPUs</td>
</tr>
<tr>
<td>Best Use</td>
<td>Development</td>
<td>Production</td>
</tr>
</tbody>
</table>
<hr />
<h2 id="15-next-steps">15. Next Steps<a class="headerlink" href="#15-next-steps" title="Permanent link">&para;</a></h2>
<ul>
<li><a href="../memory-optimization/">Memory Optimization</a> - Memory management with DDP</li>
<li><a href="../hyperparameter-tuning/">Hyperparameter Tuning</a> - Scaling learning rates</li>
<li><a href="../../user-guide/tuning-strategies/">Tuning Strategies</a> - PEFT with DDP</li>
<li><a href="../examples/benchmarking.md">Examples</a> - Multi-GPU benchmarks</li>
</ul>
<hr />
<p>Scale TabTune efficiently across multiple GPUs for production-grade training!</p>
              
            </div>
          </div><footer>

  <hr/>

  <div role="contentinfo">
    <!-- Copyright etc -->
  </div>

  Built with <a href="https://www.mkdocs.org/">MkDocs</a> using a <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
</footer>
          
        </div>
      </div>

    </section>

  </div>

  <div class="rst-versions" role="note" aria-label="Versions">
  <span class="rst-current-version" data-toggle="rst-current-version">
    
        <span>
          <a href="https://github.com/Lexsi-Labs/TabTune_Internal" class="fa fa-code-fork" style="color: #fcfcfc"> Lexsi-Labs/TabTune_Internal</a>
        </span>
    
    
    
  </span>
</div>
    <script src="../../js/jquery-3.6.0.min.js"></script>
    <script>var base_url = "../..";</script>
    <script src="../../js/theme_extra.js"></script>
    <script src="../../js/theme.js"></script>
      <script src="../../search/main.js"></script>
    <script>
        jQuery(function () {
            SphinxRtdTheme.Navigation.enable(true);
        });
    </script>

</body>
</html>
