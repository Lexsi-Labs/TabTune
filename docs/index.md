<div style="text-align: center;">
  <img src="assets/tabtune.svg" alt="TabTune Logo" style="width: 600px; height: auto;" />
</div>
<div style="text-align: center;">
<a href="https://www.python.org/downloads/"><img src="https://img.shields.io/badge/python-3.11+-blue.svg"/></a>
<a href="https://pytorch.org/"><img src="https://img.shields.io/badge/PyTorch-2.6+-red.svg"/></a>
<a href="https://github.com/Lexsi-Labs/TabTune"><img src="https://img.shields.io/badge/docs-available-green.svg"/></a>
<a href="https://arxiv.org/abs/2511.02802"><img src="https://img.shields.io/badge/arXiv-2511.02802-b31b1b.svg"/></a>
<a href="https://discord.gg/dSB62Q7A"><img src="https://img.shields.io/badge/Discord-%235865F2.svg?&amp;logo=discord&amp;logoColor=white"/></a>
</div>

---

**TabTune** is a powerful and flexible Python library designed to simplify the training and fine-tuning of modern foundation models on tabular data.  
It provides a high-level, scikit-learn-compatible API that abstracts away the complexities of data preprocessing, model-specific training loops, and benchmarking, letting you focus on delivering results.

Whether you are a practitioner aiming for production-grade pipelines or a researcher exploring advanced architectures, TabTune streamlines your workflow for tabular deep learning.

---

## üöÄ TabTune v1.0 - First Release

**Welcome to TabTune! This initial release provides a complete, production-ready framework for tabular foundation models.**

**Core Components:**

- **Unified API (`TabularPipeline`):** Single, scikit-learn-compatible interface for all models with `.fit()`, `.predict()`, `.save()`, and `.load()` methods.
- **Smart Data Processing (`DataProcessor`):** Model-aware preprocessing that automatically handles imputation, scaling, categorical encoding, and feature transformations for each model.
- **Flexible Tuning (`TuningManager`):** Three tuning strategies‚Äîzero-shot `inference`, supervised fine-tuning (`base-ft`) with full parameter updates, and memory-efficient `peft` (LoRA) adapters. Supports episodic meta-learning for ICL models.
- **Model Comparison (`TabularLeaderboard`):** Systematic benchmarking tool for comparing multiple models and strategies on your datasets.

**Supported Models (7 Models):**

- **TabPFN-v2:** Fast Bayesian inference for small datasets (<10K rows) with experimental PEFT support.
- **TabICL:** Scalable ICL with two-stage attention, full PEFT support, ideal for 10K-1M row datasets.
- **OrionMSP:** Multi-scale prior ICL for balanced generalization on 50K-2M+ row datasets, full PEFT support.
- **OrionBix:** Biaxial interaction expert for high-accuracy scenarios (50K-2M+ rows), full PEFT support.
- **TabDPT:** Denoising transformer for very large datasets (100K-5M rows), full PEFT support.
- **Mitra:** 2D cross-attention model for complex patterns and mixed data types, full PEFT support.
- **ContextTab:** Semantics-aware ICL with text embedding integration, experimental PEFT support.

**Key Capabilities:**

- ‚úÖ **Multiple Training Paradigms:** Supports supervised fine-tuning (SFT) with full parameter updates, episodic meta-learning for in-context learning models, and parameter-efficient PEFT strategies.
- ‚úÖ **PEFT (LoRA) Support:** Parameter-efficient fine-tuning for 5 out of 7 models (TabICL, OrionMSP, OrionBix, TabDPT, Mitra) with full support.
- ‚úÖ **Meta-Learning Integration:** Episodic training with support/query sets for ICL models (TabICL, OrionMSP, OrionBix, Mitra) enabling fast task adaptation.
- ‚úÖ **Comprehensive Documentation:** Extensive guides, API references, troubleshooting, and model-specific documentation.
- ‚úÖ **Production Ready:** Model serialization, reproducible training, and deployment-ready pipelines.
- ‚úÖ **Extensible Architecture:** Modular design for easy integration of custom processors and models.

---

## ‚≠ê Core Features

- **Unified API:** Single interface for model training, inference, and evaluation across multiple tabular model families.
- **Automated Preprocessing:** Model-aware data processing for feature scaling, encoding, imputation, and transformation.
- **Flexible Fine-Tuning:** Choose between zero-shot inference, full fine-tuning, or memory-efficient PEFT strategies.
- **Model Comparison:** Built-in leaderboard for systematic benchmarking and strategy evaluation.
- **Extensible Design:** Modular codebase for easy integration of custom data processors and models.

---

## üì¶ Supported Models

| Model          | Family / Paradigm         | Key Innovation                       | PEFT Support    |
|----------------|--------------------------|--------------------------------------|-----------------|
| **TabPFN-v2**  | PFN / ICL                | Bayesian approximation on synthetic  | ‚ö†Ô∏è Experimental |
| **TabICL**     | Scalable ICL             | Two-stage column-row attention       | ‚úÖ Full Support |
| **OrionMSP**  | Scalable ICL             | Multi-Scale Prior (MSP) ICL          | ‚úÖ Full Support |
| **Orion BIX**  | Scalable ICL             | Biaxial Interaction eXpert           | ‚úÖ Full Support |
| **Mitra**      | Scalable ICL             | 2D attention, synthetic priors       | ‚úÖ Full Support |
| **ContextTab** | Semantics-Aware ICL      | Modality-specific embeddings         | ‚ö†Ô∏è Experimental |
| **TabDPT**     | Denoising Transformer    | Denoising pre-trained transformer    | ‚úÖ Full Support |

<sup>‚úÖ Full Support: Reliable LoRA integration<br>‚ö†Ô∏è Experimental: Known issues may occur; use base-ft if unstable</sup>

---

## ‚ö° Quick Start

```python
import pandas as pd
from sklearn.model_selection import train_test_split
import openml
from tabtune import TabularPipeline

# Load dataset
dataset = openml.datasets.get_dataset(42178)
X, y, _, _ = dataset.get_data(target=dataset.default_target_attribute)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)

# Init and fit pipeline
pipeline = TabularPipeline(
    model_name="TabPFN",
    task_type="classification",
    tuning_strategy="base-ft",
    tuning_params={"device": "cpu"}
)
pipeline.fit(X_train, y_train)

# Save and load pipeline for prediction
pipeline.save("churn_pipeline.joblib")
loaded_pipeline = TabularPipeline.load("churn_pipeline.joblib")
predictions = loaded_pipeline.predict(X_test)
metrics = pipeline.evaluate(X_test, y_test)
print(metrics)
```

---

## üìù Why TabTune?

- **No Boilerplate:** Avoids repetitive code for model-specific data loading, training, and inference.
- **Consistent Results:** Automates best practices for tabular DL research and model selection.
- **Fast Iteration:** Easily compare new models with your data, using the same consistent API.
- **Production Ready:** Model and config serialization for robust deployment and reproducibility.
- **Community-Driven:** Extensible design and open contribution policy.

---

## üìë Explore the Documentation

- **[Getting Started](getting-started/installation.md):** Installation, setup, and basic usage.
- **[User Guide](user-guide/pipeline-overview.md):** In-depth tutorials for each component.
- **[Supported Models](models/overview.md):** Model details and design notes.
- **[Advanced Topics](advanced/peft-lora.md):** PEFT/LoRA, custom preprocessing, and more.
- **[API Reference](api/pipeline.md):** Complete Python API and class/method details.
- **[Examples & Benchmarks](examples/classification.md):** End-to-end code notebooks.

---

## üèÜ Example Notebooks

|Below are 9 Example Notebooks showcasing all the features of the Library in-depth!

| Serial No. | Name | Task Performed | Link To Notebook |
|---|------|------|------|
| 1 | Unified API | Showcasing A Unified API Across Multiple Models |[![Open In Colab](https://img.shields.io/badge/Open%20in%20Colab-F9AB00?style=for-the-badge&logo=googlecolab&logoColor=white)](https://colab.research.google.com/drive/1KcaSdYRjZnMlb0MLmQ5IlnbPDiuEr1Ld?usp=sharing) |
| 2 |  Automated Model-Aware Preprocessing | The Automated preprocessing system explained |[![Open In Colab](https://img.shields.io/badge/Open%20in%20Colab-F9AB00?style=for-the-badge&logo=googlecolab&logoColor=white)](https://colab.research.google.com/drive/12BQ12VJrxtTDslgjnjm26yi3a0PYXqZT?usp=sharing) |
| 3 | Fine-Tuning Strategies | TabTune's four fine-tuning strategies |[![Open In Colab](https://img.shields.io/badge/Open%20in%20Colab-F9AB00?style=for-the-badge&logo=googlecolab&logoColor=white)](https://colab.research.google.com/drive/1QixfiNCjF1IQV9NooMipPpnH4ETcEQwg?usp=sharing) |
| 4 | Model Comparison | Model Comparison with TabularLeaderboard |[![Open In Colab](https://img.shields.io/badge/Open%20in%20Colab-F9AB00?style=for-the-badge&logo=googlecolab&logoColor=white)](https://colab.research.google.com/drive/1PZW3iPQOvwh0kroGytMzYTGc6ZVUzuvg?usp=sharing) |
| 5 | Checkpoint Management | Checkpoint Management - Save/Load Pipelines |[![Open In Colab](https://img.shields.io/badge/Open%20in%20Colab-F9AB00?style=for-the-badge&logo=googlecolab&logoColor=white)](https://colab.research.google.com/drive/1DBTGEPpYLJjU9Aj7lzHoX3JtwaNOC0jn?usp=sharing) |
| 6 | Advanced Usage | PEFT Configuration and Hybrid Strategies |[![Open In Colab](https://img.shields.io/badge/Open%20in%20Colab-F9AB00?style=for-the-badge&logo=googlecolab&logoColor=white)](https://colab.research.google.com/drive/1V3XGLeKrXSJwavaULMncZiM7uVE8sz0h?usp=sharing) |
| 7 | Data Sampling |  Data Sampling and Resampling Strategies for Inference |[![Open In Colab](https://img.shields.io/badge/Open%20in%20Colab-F9AB00?style=for-the-badge&logo=googlecolab&logoColor=white)](https://colab.research.google.com/drive/1TUwxsfk6E0LDepc3bONeDZLslYAMesbZ?usp=sharing) |
| 8 | Evaluation Metrics | Evaluation Metrics involved |[![Open In Colab](https://img.shields.io/badge/Open%20in%20Colab-F9AB00?style=for-the-badge&logo=googlecolab&logoColor=white)](https://colab.research.google.com/drive/18TxyTyBGAGrIVf6zLjURDChG0vM4V02M?usp=sharing) |
| 9 | Benchmarking | Standard Benchmarking Techniques |[![Open In Colab](https://img.shields.io/badge/Open%20in%20Colab-F9AB00?style=for-the-badge&logo=googlecolab&logoColor=white)](https://colab.research.google.com/drive/1lcoVMPz_3X5_5taNdB9doTGoN05krNRw?usp=sharing) |

---

## üìÇ Project Structure

```
tabtune/
‚îú‚îÄ‚îÄ Dataprocess/
‚îú‚îÄ‚îÄ models/
‚îú‚îÄ‚îÄ TabularPipeline/
‚îú‚îÄ‚îÄ TuningManager/
‚îú‚îÄ‚îÄ TabularLeaderboard/
‚îú‚îÄ‚îÄ benchmarking/
‚îú‚îÄ‚îÄ data/
‚îú‚îÄ‚îÄ logger.py
‚îî‚îÄ‚îÄ run.py
```

See [User Guide](user-guide/pipeline-overview.md) for a full file/module breakdown.

---

## üè¢ Developed by Lexsi Labs

<div style="text-align: center; margin: 30px 0;">
  <img src="assets/lexsi-labs-logo.svg" alt="Lexsi Labs Logo" style="height: 390px; width: auto; margin-bottom: 20px; position: relative;" />
  <p style="font-size: 1.1em; color: #666; margin-top: 10px;">
    Created by the team at <strong>Lexsi Labs</strong>, TabTune extends frontier AI research into the tabular domain.
  </p>
</div>

---

## üóÉÔ∏è License

This project is released under the MIT License.  
Please cite appropriately if used in academic or production projects.

**BibTeX Citation:**

```bibtex
@misc{tanna2025tabtuneunifiedlibraryinference,
      title={TabTune: A Unified Library for Inference and Fine-Tuning Tabular Foundation Models}, 
      author={Aditya Tanna and Pratinav Seth and Mohamed Bouadi and Utsav Avaiya and Vinay Kumar Sankarapu},
      year={2025},
      eprint={2511.02802},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2511.02802}, 
}
```

---

## üì´ Join Community / Contribute

- Issues and discussions are welcomed on the [GitHub issue tracker](https://github.com/Lexsi-Labs/TabTune/issues).
- Please see the **Contributing** section for contribution standards, code reviews, and documentation tips.

---

**Get started with TabTune and accelerate your tabular deep learning workflows today!**
